{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648313238496,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "zzsWH1ZADSaV"
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2283,
     "status": "ok",
     "timestamp": 1648313240777,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "zg95X1yOqG1h"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1648313243460,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "rfAFjOlYrMKf",
    "outputId": "c8134e15-ddb8-42e6-ad57-b6af375b0937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /p1ch7/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b699e437724ce893558493948da6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /p1ch7/cifar-10-python.tar.gz to /p1ch7/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True, transform =  transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jclkMe3jgav"
   },
   "source": [
    "Problem 1 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648313243460,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "tRJxnkdgshBn"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,8, kernel_size=3, padding=1),\n",
    "            #???#\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Linear(8*8*8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1648313243599,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "ERDHJc9PwqvH"
   },
   "outputs": [],
   "source": [
    "class Net (nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "    self.act1 = nn.Tanh()\n",
    "    self.pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2 = nn.Conv2d(16,8, kernel_size=3, padding=1)\n",
    "    self.act2 = nn.Tanh()\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.fc1 = nn.Linear(8*8*8,32)\n",
    "    self.act3 = nn.Tanh()\n",
    "    self.fc2 = nn.Linear(32,10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.pool1(self.act1(self.conv1(x)))\n",
    "    out = self.pool2(self.act2(self.conv2(out)))\n",
    "    out = out.view(-1, 8*8*8)\n",
    "    out = self.act3(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648313243599,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "O4gTlppBygIp"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(16,8, kernel_size=3, padding=1)\n",
    "    self.fc1 = nn.Linear(8*8*8, 32)\n",
    "    self.fc2 = nn.Linear(32, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "    out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "    out = out.view(-1, 8*8*8)\n",
    "    out = torch.tanh(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1648313243709,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "1j8sYfqAyhME",
    "outputId": "6fe8c3f7-f0a9-4259-e3bd-a54d657fd10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648313243709,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "JdZCNPtu_zIx",
    "outputId": "3b736ff9-ba17-44eb-82a2-feda87c3415d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1648313243709,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "eONr4dytyhbl"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "  for epoch in range(1, n_epochs +1):\n",
    "    loss_train = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "      outputs = model(imgs.to('cuda:0'))\n",
    "      loss = loss_fn(outputs.to('cuda:0'), labels.to('cuda:0'))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_train += loss.item()\n",
    "      if epoch == 1 or epoch % 50 == 0:\n",
    "        print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5407430,
     "status": "ok",
     "timestamp": 1648318651137,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "UA--2fIVyhVQ",
    "outputId": "89ab0a3d-08fc-40dd-c2f9-e087cbb20ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "2022-03-26 16:47:38.314182 Epoch 1, Training Loss 1.285469469664347\n",
      "2022-03-26 16:47:38.340856 Epoch 1, Training Loss 1.2878747910184933\n",
      "2022-03-26 16:47:38.365614 Epoch 1, Training Loss 1.2903980629523393\n",
      "2022-03-26 16:47:38.396854 Epoch 1, Training Loss 1.2928680350713413\n",
      "2022-03-26 16:47:38.421509 Epoch 1, Training Loss 1.2955021685956385\n",
      "2022-03-26 16:47:38.447661 Epoch 1, Training Loss 1.2981444659745296\n",
      "2022-03-26 16:47:38.471285 Epoch 1, Training Loss 1.3006800179896147\n",
      "2022-03-26 16:47:38.495678 Epoch 1, Training Loss 1.3032757685617413\n",
      "2022-03-26 16:47:38.520349 Epoch 1, Training Loss 1.3057935788198505\n",
      "2022-03-26 16:47:38.544043 Epoch 1, Training Loss 1.3082712557919496\n",
      "2022-03-26 16:47:38.572927 Epoch 1, Training Loss 1.3108228306331293\n",
      "2022-03-26 16:47:38.597812 Epoch 1, Training Loss 1.3133904978137492\n",
      "2022-03-26 16:47:38.628945 Epoch 1, Training Loss 1.3158069612729886\n",
      "2022-03-26 16:47:38.653129 Epoch 1, Training Loss 1.31838992657259\n",
      "2022-03-26 16:47:38.677037 Epoch 1, Training Loss 1.3208497003521151\n",
      "2022-03-26 16:47:38.701020 Epoch 1, Training Loss 1.3232657831648122\n",
      "2022-03-26 16:47:38.726561 Epoch 1, Training Loss 1.3258421446966089\n",
      "2022-03-26 16:47:38.750710 Epoch 1, Training Loss 1.3282588009944047\n",
      "2022-03-26 16:47:38.784058 Epoch 1, Training Loss 1.3308599882418541\n",
      "2022-03-26 16:47:38.807787 Epoch 1, Training Loss 1.3333176186932323\n",
      "2022-03-26 16:47:38.835332 Epoch 1, Training Loss 1.3359094402369331\n",
      "2022-03-26 16:47:38.866011 Epoch 1, Training Loss 1.3384132624587135\n",
      "2022-03-26 16:47:38.892781 Epoch 1, Training Loss 1.3406348879379995\n",
      "2022-03-26 16:47:38.917478 Epoch 1, Training Loss 1.343150194496145\n",
      "2022-03-26 16:47:38.942689 Epoch 1, Training Loss 1.3456625208220518\n",
      "2022-03-26 16:47:38.965992 Epoch 1, Training Loss 1.34820361103853\n",
      "2022-03-26 16:47:38.988874 Epoch 1, Training Loss 1.3506932400376594\n",
      "2022-03-26 16:47:39.012807 Epoch 1, Training Loss 1.35324767605423\n",
      "2022-03-26 16:47:39.036197 Epoch 1, Training Loss 1.355680301976021\n",
      "2022-03-26 16:47:39.073754 Epoch 1, Training Loss 1.3582850444652235\n",
      "2022-03-26 16:47:39.098397 Epoch 1, Training Loss 1.3607080592523755\n",
      "2022-03-26 16:47:39.123368 Epoch 1, Training Loss 1.3631362178746391\n",
      "2022-03-26 16:47:39.153718 Epoch 1, Training Loss 1.3656650043814385\n",
      "2022-03-26 16:47:39.176979 Epoch 1, Training Loss 1.3682086903725743\n",
      "2022-03-26 16:47:39.200123 Epoch 1, Training Loss 1.3706471221831145\n",
      "2022-03-26 16:47:39.223336 Epoch 1, Training Loss 1.3732993901538117\n",
      "2022-03-26 16:47:39.246226 Epoch 1, Training Loss 1.375796046708246\n",
      "2022-03-26 16:47:39.269863 Epoch 1, Training Loss 1.378340600091783\n",
      "2022-03-26 16:47:39.298705 Epoch 1, Training Loss 1.3808146254790714\n",
      "2022-03-26 16:47:39.323023 Epoch 1, Training Loss 1.3832120604222389\n",
      "2022-03-26 16:47:39.350345 Epoch 1, Training Loss 1.3857749195964746\n",
      "2022-03-26 16:47:39.378392 Epoch 1, Training Loss 1.388376384287539\n",
      "2022-03-26 16:47:39.401230 Epoch 1, Training Loss 1.3907944581392782\n",
      "2022-03-26 16:47:39.424862 Epoch 1, Training Loss 1.3935026209372694\n",
      "2022-03-26 16:47:39.448333 Epoch 1, Training Loss 1.396002332572742\n",
      "2022-03-26 16:47:39.473016 Epoch 1, Training Loss 1.3985122638895078\n",
      "2022-03-26 16:47:39.495722 Epoch 1, Training Loss 1.4011575951600623\n",
      "2022-03-26 16:47:39.527963 Epoch 1, Training Loss 1.4036403559053037\n",
      "2022-03-26 16:47:39.552199 Epoch 1, Training Loss 1.4061636549737446\n",
      "2022-03-26 16:47:39.575369 Epoch 1, Training Loss 1.4088509628534926\n",
      "2022-03-26 16:47:39.604056 Epoch 1, Training Loss 1.4113017633138105\n",
      "2022-03-26 16:47:39.627433 Epoch 1, Training Loss 1.4138928290523227\n",
      "2022-03-26 16:47:39.650207 Epoch 1, Training Loss 1.4164209958842344\n",
      "2022-03-26 16:47:39.673138 Epoch 1, Training Loss 1.4188590616826207\n",
      "2022-03-26 16:47:39.696276 Epoch 1, Training Loss 1.421354257542154\n",
      "2022-03-26 16:47:39.719392 Epoch 1, Training Loss 1.4239409622328971\n",
      "2022-03-26 16:47:39.748399 Epoch 1, Training Loss 1.426532827069997\n",
      "2022-03-26 16:47:39.772262 Epoch 1, Training Loss 1.4289877039697163\n",
      "2022-03-26 16:47:39.794978 Epoch 1, Training Loss 1.431520912653345\n",
      "2022-03-26 16:47:39.821631 Epoch 1, Training Loss 1.4340147534599694\n",
      "2022-03-26 16:47:39.844574 Epoch 1, Training Loss 1.4364383306039874\n",
      "2022-03-26 16:47:39.868019 Epoch 1, Training Loss 1.4389703084745675\n",
      "2022-03-26 16:47:39.900897 Epoch 1, Training Loss 1.4415851264353603\n",
      "2022-03-26 16:47:39.924616 Epoch 1, Training Loss 1.444083631648432\n",
      "2022-03-26 16:47:39.948546 Epoch 1, Training Loss 1.4464634755993133\n",
      "2022-03-26 16:47:39.982333 Epoch 1, Training Loss 1.4489181450260875\n",
      "2022-03-26 16:47:40.005777 Epoch 1, Training Loss 1.4514533229496167\n",
      "2022-03-26 16:47:40.030688 Epoch 1, Training Loss 1.45393953237997\n",
      "2022-03-26 16:47:40.054802 Epoch 1, Training Loss 1.4564113119983917\n",
      "2022-03-26 16:47:40.077941 Epoch 1, Training Loss 1.458796334388616\n",
      "2022-03-26 16:47:40.105632 Epoch 1, Training Loss 1.4612583832057846\n",
      "2022-03-26 16:47:40.128580 Epoch 1, Training Loss 1.4637626741853211\n",
      "2022-03-26 16:47:40.151403 Epoch 1, Training Loss 1.4663975244897711\n",
      "2022-03-26 16:47:40.174733 Epoch 1, Training Loss 1.468870160982127\n",
      "2022-03-26 16:47:40.203738 Epoch 1, Training Loss 1.471263478174234\n",
      "2022-03-26 16:47:40.227311 Epoch 1, Training Loss 1.4737750172919934\n",
      "2022-03-26 16:47:40.249735 Epoch 1, Training Loss 1.4762616962423105\n",
      "2022-03-26 16:47:40.276796 Epoch 1, Training Loss 1.4786788522434966\n",
      "2022-03-26 16:47:40.299736 Epoch 1, Training Loss 1.4812661447488438\n",
      "2022-03-26 16:47:40.323349 Epoch 1, Training Loss 1.4837968602509755\n",
      "2022-03-26 16:47:40.348271 Epoch 1, Training Loss 1.4862830545896155\n",
      "2022-03-26 16:47:40.371366 Epoch 1, Training Loss 1.488850756526908\n",
      "2022-03-26 16:47:40.395862 Epoch 1, Training Loss 1.491339838870651\n",
      "2022-03-26 16:47:40.425925 Epoch 1, Training Loss 1.4937660341982342\n",
      "2022-03-26 16:47:40.450486 Epoch 1, Training Loss 1.4966193027508534\n",
      "2022-03-26 16:47:40.475008 Epoch 1, Training Loss 1.4990882384197792\n",
      "2022-03-26 16:47:40.501001 Epoch 1, Training Loss 1.5016577879486182\n",
      "2022-03-26 16:47:40.525270 Epoch 1, Training Loss 1.504173973942047\n",
      "2022-03-26 16:47:40.548790 Epoch 1, Training Loss 1.5065790044377223\n",
      "2022-03-26 16:47:40.572557 Epoch 1, Training Loss 1.508969504357604\n",
      "2022-03-26 16:47:40.596416 Epoch 1, Training Loss 1.5116599354597613\n",
      "2022-03-26 16:47:40.621112 Epoch 1, Training Loss 1.514143451400425\n",
      "2022-03-26 16:47:40.654708 Epoch 1, Training Loss 1.516496735460618\n",
      "2022-03-26 16:47:40.678645 Epoch 1, Training Loss 1.5191536038123128\n",
      "2022-03-26 16:47:40.702848 Epoch 1, Training Loss 1.5216341181789212\n",
      "2022-03-26 16:47:40.730413 Epoch 1, Training Loss 1.524105230865576\n",
      "2022-03-26 16:47:40.754021 Epoch 1, Training Loss 1.5263947975605041\n",
      "2022-03-26 16:47:40.778011 Epoch 1, Training Loss 1.5287331790875291\n",
      "2022-03-26 16:47:40.801654 Epoch 1, Training Loss 1.5311923025514158\n",
      "2022-03-26 16:47:40.825776 Epoch 1, Training Loss 1.5337074473690804\n",
      "2022-03-26 16:47:40.849695 Epoch 1, Training Loss 1.5360766916018922\n",
      "2022-03-26 16:47:40.886036 Epoch 1, Training Loss 1.5386281886978832\n",
      "2022-03-26 16:47:40.918643 Epoch 1, Training Loss 1.5411242120101323\n",
      "2022-03-26 16:47:40.947455 Epoch 1, Training Loss 1.5436760495080972\n",
      "2022-03-26 16:47:40.974893 Epoch 1, Training Loss 1.5462847530384503\n",
      "2022-03-26 16:47:40.999092 Epoch 1, Training Loss 1.5489821854759664\n",
      "2022-03-26 16:47:41.023033 Epoch 1, Training Loss 1.5513629056608584\n",
      "2022-03-26 16:47:41.047324 Epoch 1, Training Loss 1.5538121614309832\n",
      "2022-03-26 16:47:41.073742 Epoch 1, Training Loss 1.556260580754341\n",
      "2022-03-26 16:47:41.104026 Epoch 1, Training Loss 1.5586790001910666\n",
      "2022-03-26 16:47:41.130202 Epoch 1, Training Loss 1.5612164080295416\n",
      "2022-03-26 16:47:41.154815 Epoch 1, Training Loss 1.5636849920158191\n",
      "2022-03-26 16:47:41.182305 Epoch 1, Training Loss 1.5662853655302922\n",
      "2022-03-26 16:47:41.205992 Epoch 1, Training Loss 1.5687577921106381\n",
      "2022-03-26 16:47:41.230023 Epoch 1, Training Loss 1.5712496408111298\n",
      "2022-03-26 16:47:41.254164 Epoch 1, Training Loss 1.5736999090980082\n",
      "2022-03-26 16:47:41.278011 Epoch 1, Training Loss 1.5762625688787006\n",
      "2022-03-26 16:47:41.301646 Epoch 1, Training Loss 1.5785535627313891\n",
      "2022-03-26 16:47:41.335047 Epoch 1, Training Loss 1.5809937884740513\n",
      "2022-03-26 16:47:41.359611 Epoch 1, Training Loss 1.5835943769311052\n",
      "2022-03-26 16:47:41.383780 Epoch 1, Training Loss 1.5859199161724666\n",
      "2022-03-26 16:47:41.410342 Epoch 1, Training Loss 1.5883232483168697\n",
      "2022-03-26 16:47:41.434734 Epoch 1, Training Loss 1.5907467125017014\n",
      "2022-03-26 16:47:41.458500 Epoch 1, Training Loss 1.5931887968117013\n",
      "2022-03-26 16:47:41.481851 Epoch 1, Training Loss 1.5957253907647584\n",
      "2022-03-26 16:47:41.504403 Epoch 1, Training Loss 1.5982229278215667\n",
      "2022-03-26 16:47:41.528016 Epoch 1, Training Loss 1.6007969893153062\n",
      "2022-03-26 16:47:41.557169 Epoch 1, Training Loss 1.603379707659602\n",
      "2022-03-26 16:47:41.580674 Epoch 1, Training Loss 1.6058930195201084\n",
      "2022-03-26 16:47:41.603981 Epoch 1, Training Loss 1.6084255652354502\n",
      "2022-03-26 16:47:41.632782 Epoch 1, Training Loss 1.610840564493633\n",
      "2022-03-26 16:47:41.655792 Epoch 1, Training Loss 1.613297657893442\n",
      "2022-03-26 16:47:41.678447 Epoch 1, Training Loss 1.6157380566572594\n",
      "2022-03-26 16:47:41.701281 Epoch 1, Training Loss 1.6182919845861548\n",
      "2022-03-26 16:47:41.723952 Epoch 1, Training Loss 1.6208974837951953\n",
      "2022-03-26 16:47:41.745925 Epoch 1, Training Loss 1.623618905958922\n",
      "2022-03-26 16:47:41.775383 Epoch 1, Training Loss 1.6260604939192458\n",
      "2022-03-26 16:47:41.799669 Epoch 1, Training Loss 1.6283744172671872\n",
      "2022-03-26 16:47:41.822244 Epoch 1, Training Loss 1.6307923063597716\n",
      "2022-03-26 16:47:41.846990 Epoch 1, Training Loss 1.6333055031269104\n",
      "2022-03-26 16:47:41.874345 Epoch 1, Training Loss 1.635771612377118\n",
      "2022-03-26 16:47:41.898318 Epoch 1, Training Loss 1.6383516556771516\n",
      "2022-03-26 16:47:41.921353 Epoch 1, Training Loss 1.6408312342051046\n",
      "2022-03-26 16:47:41.956495 Epoch 1, Training Loss 1.6432251160407005\n",
      "2022-03-26 16:47:41.983461 Epoch 1, Training Loss 1.6457233912194782\n",
      "2022-03-26 16:47:42.008523 Epoch 1, Training Loss 1.648281133540756\n",
      "2022-03-26 16:47:42.031438 Epoch 1, Training Loss 1.650656556686782\n",
      "2022-03-26 16:47:42.061121 Epoch 1, Training Loss 1.6532354691754216\n",
      "2022-03-26 16:47:42.092667 Epoch 1, Training Loss 1.6555827891125399\n",
      "2022-03-26 16:47:42.117915 Epoch 1, Training Loss 1.658183626354198\n",
      "2022-03-26 16:47:42.141239 Epoch 1, Training Loss 1.6608086457032987\n",
      "2022-03-26 16:47:42.164119 Epoch 1, Training Loss 1.6632436705977105\n",
      "2022-03-26 16:47:42.187150 Epoch 1, Training Loss 1.6658343860255482\n",
      "2022-03-26 16:47:42.215491 Epoch 1, Training Loss 1.6682400160738269\n",
      "2022-03-26 16:47:42.238550 Epoch 1, Training Loss 1.6707270361883255\n",
      "2022-03-26 16:47:42.267059 Epoch 1, Training Loss 1.673251220027504\n",
      "2022-03-26 16:47:42.290642 Epoch 1, Training Loss 1.6757617926658572\n",
      "2022-03-26 16:47:42.313627 Epoch 1, Training Loss 1.678267046466203\n",
      "2022-03-26 16:47:42.338694 Epoch 1, Training Loss 1.6808288425130917\n",
      "2022-03-26 16:47:42.362714 Epoch 1, Training Loss 1.6833282126794995\n",
      "2022-03-26 16:47:42.389354 Epoch 1, Training Loss 1.685710227550448\n",
      "2022-03-26 16:47:42.419124 Epoch 1, Training Loss 1.688144047394433\n",
      "2022-03-26 16:47:42.443812 Epoch 1, Training Loss 1.6906235166217969\n",
      "2022-03-26 16:47:42.467561 Epoch 1, Training Loss 1.6929023050896042\n",
      "2022-03-26 16:47:42.491447 Epoch 1, Training Loss 1.695298723857421\n",
      "2022-03-26 16:47:42.515569 Epoch 1, Training Loss 1.6977533163012142\n",
      "2022-03-26 16:47:42.543709 Epoch 1, Training Loss 1.700145956500412\n",
      "2022-03-26 16:47:42.567696 Epoch 1, Training Loss 1.7024763627430362\n",
      "2022-03-26 16:47:42.591570 Epoch 1, Training Loss 1.705170414941695\n",
      "2022-03-26 16:47:42.615566 Epoch 1, Training Loss 1.7079466413658904\n",
      "2022-03-26 16:47:42.645226 Epoch 1, Training Loss 1.7105736195888666\n",
      "2022-03-26 16:47:42.669256 Epoch 1, Training Loss 1.7129923237864013\n",
      "2022-03-26 16:47:42.693157 Epoch 1, Training Loss 1.7153208841143361\n",
      "2022-03-26 16:47:42.721122 Epoch 1, Training Loss 1.717663170431581\n",
      "2022-03-26 16:47:42.744877 Epoch 1, Training Loss 1.7199785184982184\n",
      "2022-03-26 16:47:42.768553 Epoch 1, Training Loss 1.7223142289444613\n",
      "2022-03-26 16:47:42.792333 Epoch 1, Training Loss 1.724695527797465\n",
      "2022-03-26 16:47:42.815769 Epoch 1, Training Loss 1.7271365780964532\n",
      "2022-03-26 16:47:42.839043 Epoch 1, Training Loss 1.7294705597031148\n",
      "2022-03-26 16:47:42.869096 Epoch 1, Training Loss 1.7317357548057575\n",
      "2022-03-26 16:47:42.892605 Epoch 1, Training Loss 1.73403764548509\n",
      "2022-03-26 16:47:42.915789 Epoch 1, Training Loss 1.736607027633111\n",
      "2022-03-26 16:47:42.943416 Epoch 1, Training Loss 1.7390733588382106\n",
      "2022-03-26 16:47:42.976995 Epoch 1, Training Loss 1.741459157003466\n",
      "2022-03-26 16:47:43.005321 Epoch 1, Training Loss 1.743847749574715\n",
      "2022-03-26 16:47:43.029062 Epoch 1, Training Loss 1.7462392601820513\n",
      "2022-03-26 16:47:43.052678 Epoch 1, Training Loss 1.7486540423634718\n",
      "2022-03-26 16:47:43.083769 Epoch 1, Training Loss 1.7511554261302704\n",
      "2022-03-26 16:47:43.108171 Epoch 1, Training Loss 1.7535120280807281\n",
      "2022-03-26 16:47:43.134002 Epoch 1, Training Loss 1.755966522047282\n",
      "2022-03-26 16:47:43.160268 Epoch 1, Training Loss 1.7582894960022948\n",
      "2022-03-26 16:47:43.183588 Epoch 1, Training Loss 1.7606961513724169\n",
      "2022-03-26 16:47:43.206532 Epoch 1, Training Loss 1.7631462504491782\n",
      "2022-03-26 16:47:43.230032 Epoch 1, Training Loss 1.7655228737675015\n",
      "2022-03-26 16:47:43.252755 Epoch 1, Training Loss 1.767974539028714\n",
      "2022-03-26 16:47:43.275672 Epoch 1, Training Loss 1.7704937618101955\n",
      "2022-03-26 16:47:43.305226 Epoch 1, Training Loss 1.7728585020050673\n",
      "2022-03-26 16:47:43.331149 Epoch 1, Training Loss 1.775317093447956\n",
      "2022-03-26 16:47:43.354521 Epoch 1, Training Loss 1.7777871565745615\n",
      "2022-03-26 16:47:43.382687 Epoch 1, Training Loss 1.780173503224502\n",
      "2022-03-26 16:47:43.407429 Epoch 1, Training Loss 1.7827130990564977\n",
      "2022-03-26 16:47:43.432093 Epoch 1, Training Loss 1.7853192994966531\n",
      "2022-03-26 16:47:43.456320 Epoch 1, Training Loss 1.7878072014854998\n",
      "2022-03-26 16:47:43.480111 Epoch 1, Training Loss 1.790299215127745\n",
      "2022-03-26 16:47:43.504512 Epoch 1, Training Loss 1.7925955964171367\n",
      "2022-03-26 16:47:43.535208 Epoch 1, Training Loss 1.7951479932231367\n",
      "2022-03-26 16:47:43.559399 Epoch 1, Training Loss 1.7975230127039468\n",
      "2022-03-26 16:47:43.586450 Epoch 1, Training Loss 1.7998921854416732\n",
      "2022-03-26 16:47:43.610549 Epoch 1, Training Loss 1.802231870648806\n",
      "2022-03-26 16:47:43.634921 Epoch 1, Training Loss 1.8046377241763922\n",
      "2022-03-26 16:47:43.658294 Epoch 1, Training Loss 1.8071976518996842\n",
      "2022-03-26 16:47:43.684526 Epoch 1, Training Loss 1.8097275067168428\n",
      "2022-03-26 16:47:43.708651 Epoch 1, Training Loss 1.8121392920498958\n",
      "2022-03-26 16:47:43.733143 Epoch 1, Training Loss 1.8145776064804449\n",
      "2022-03-26 16:47:43.762493 Epoch 1, Training Loss 1.8169701075005105\n",
      "2022-03-26 16:47:43.786555 Epoch 1, Training Loss 1.8194087249848543\n",
      "2022-03-26 16:47:43.810118 Epoch 1, Training Loss 1.8217234859990952\n",
      "2022-03-26 16:47:43.837419 Epoch 1, Training Loss 1.8242607212737394\n",
      "2022-03-26 16:47:43.860982 Epoch 1, Training Loss 1.826611633648348\n",
      "2022-03-26 16:47:43.888199 Epoch 1, Training Loss 1.8290450617175578\n",
      "2022-03-26 16:47:43.911974 Epoch 1, Training Loss 1.8313905740028147\n",
      "2022-03-26 16:47:43.936743 Epoch 1, Training Loss 1.8338750751731951\n",
      "2022-03-26 16:47:43.963425 Epoch 1, Training Loss 1.8363939676138445\n",
      "2022-03-26 16:47:44.004049 Epoch 1, Training Loss 1.8389037795688794\n",
      "2022-03-26 16:47:44.027801 Epoch 1, Training Loss 1.84148934765545\n",
      "2022-03-26 16:47:44.054147 Epoch 1, Training Loss 1.8438311720748082\n",
      "2022-03-26 16:47:44.077090 Epoch 1, Training Loss 1.8463125000219516\n",
      "2022-03-26 16:47:44.100572 Epoch 1, Training Loss 1.8487050264997555\n",
      "2022-03-26 16:47:44.125707 Epoch 1, Training Loss 1.851058019091711\n",
      "2022-03-26 16:47:44.149092 Epoch 1, Training Loss 1.8534167129975145\n",
      "2022-03-26 16:47:44.171664 Epoch 1, Training Loss 1.8558866299326768\n",
      "2022-03-26 16:47:44.195250 Epoch 1, Training Loss 1.8583023656359718\n",
      "2022-03-26 16:47:44.225662 Epoch 1, Training Loss 1.8607545539241312\n",
      "2022-03-26 16:47:44.250069 Epoch 1, Training Loss 1.8630856583490396\n",
      "2022-03-26 16:47:44.272584 Epoch 1, Training Loss 1.8654012670907218\n",
      "2022-03-26 16:47:44.295503 Epoch 1, Training Loss 1.8677008792262553\n",
      "2022-03-26 16:47:44.320271 Epoch 1, Training Loss 1.8700930558507094\n",
      "2022-03-26 16:47:44.348773 Epoch 1, Training Loss 1.8724729923336096\n",
      "2022-03-26 16:47:44.372701 Epoch 1, Training Loss 1.874670955073803\n",
      "2022-03-26 16:47:44.396347 Epoch 1, Training Loss 1.8770673877138007\n",
      "2022-03-26 16:47:44.419797 Epoch 1, Training Loss 1.8796233807683296\n",
      "2022-03-26 16:47:44.449173 Epoch 1, Training Loss 1.882034225232156\n",
      "2022-03-26 16:47:44.473769 Epoch 1, Training Loss 1.8843209014829163\n",
      "2022-03-26 16:47:44.498148 Epoch 1, Training Loss 1.8868045011139891\n",
      "2022-03-26 16:47:44.525341 Epoch 1, Training Loss 1.8891124551558434\n",
      "2022-03-26 16:47:44.552746 Epoch 1, Training Loss 1.8915576235107754\n",
      "2022-03-26 16:47:44.576099 Epoch 1, Training Loss 1.8940152333825446\n",
      "2022-03-26 16:47:44.600201 Epoch 1, Training Loss 1.8964145651558781\n",
      "2022-03-26 16:47:44.623585 Epoch 1, Training Loss 1.8988011377241911\n",
      "2022-03-26 16:47:44.646969 Epoch 1, Training Loss 1.9012016899445479\n",
      "2022-03-26 16:47:44.674939 Epoch 1, Training Loss 1.9034951248437242\n",
      "2022-03-26 16:47:44.698706 Epoch 1, Training Loss 1.9057885002906976\n",
      "2022-03-26 16:47:44.721201 Epoch 1, Training Loss 1.9083352366371837\n",
      "2022-03-26 16:47:44.746573 Epoch 1, Training Loss 1.910657443963658\n",
      "2022-03-26 16:47:44.768640 Epoch 1, Training Loss 1.9129644935698156\n",
      "2022-03-26 16:47:44.791615 Epoch 1, Training Loss 1.9154353972591098\n",
      "2022-03-26 16:47:44.814075 Epoch 1, Training Loss 1.9177657974040723\n",
      "2022-03-26 16:47:44.836975 Epoch 1, Training Loss 1.9200962716356262\n",
      "2022-03-26 16:47:44.859741 Epoch 1, Training Loss 1.9227007028391905\n",
      "2022-03-26 16:47:44.886375 Epoch 1, Training Loss 1.9251241589446202\n",
      "2022-03-26 16:47:44.910328 Epoch 1, Training Loss 1.9274547735748389\n",
      "2022-03-26 16:47:44.933499 Epoch 1, Training Loss 1.9296526148191193\n",
      "2022-03-26 16:47:44.959690 Epoch 1, Training Loss 1.9321071395788656\n",
      "2022-03-26 16:47:44.983584 Epoch 1, Training Loss 1.9347289969854038\n",
      "2022-03-26 16:47:45.010781 Epoch 1, Training Loss 1.937118709697138\n",
      "2022-03-26 16:47:45.038143 Epoch 1, Training Loss 1.939560736841558\n",
      "2022-03-26 16:47:45.061043 Epoch 1, Training Loss 1.9420125524101355\n",
      "2022-03-26 16:47:45.084164 Epoch 1, Training Loss 1.9442616387096512\n",
      "2022-03-26 16:47:45.116325 Epoch 1, Training Loss 1.9465216892149748\n",
      "2022-03-26 16:47:45.141048 Epoch 1, Training Loss 1.9490730477415996\n",
      "2022-03-26 16:47:45.164606 Epoch 1, Training Loss 1.9516044792616765\n",
      "2022-03-26 16:47:45.187438 Epoch 1, Training Loss 1.9541265902007023\n",
      "2022-03-26 16:47:45.215388 Epoch 1, Training Loss 1.9566444382643151\n",
      "2022-03-26 16:47:45.238411 Epoch 1, Training Loss 1.959112170864554\n",
      "2022-03-26 16:47:45.260895 Epoch 1, Training Loss 1.9614056583560642\n",
      "2022-03-26 16:47:45.283800 Epoch 1, Training Loss 1.9635894958625364\n",
      "2022-03-26 16:47:45.306402 Epoch 1, Training Loss 1.9659688102314845\n",
      "2022-03-26 16:47:45.337226 Epoch 1, Training Loss 1.968424265189549\n",
      "2022-03-26 16:47:45.360800 Epoch 1, Training Loss 1.9707492132625921\n",
      "2022-03-26 16:47:45.385365 Epoch 1, Training Loss 1.972960212041655\n",
      "2022-03-26 16:47:45.410349 Epoch 1, Training Loss 1.975452703283266\n",
      "2022-03-26 16:47:45.435454 Epoch 1, Training Loss 1.97784848804669\n",
      "2022-03-26 16:47:45.457706 Epoch 1, Training Loss 1.9801109855437218\n",
      "2022-03-26 16:47:45.480310 Epoch 1, Training Loss 1.9823820764756264\n",
      "2022-03-26 16:47:45.503758 Epoch 1, Training Loss 1.984798405633863\n",
      "2022-03-26 16:47:45.526857 Epoch 1, Training Loss 1.987135894615632\n",
      "2022-03-26 16:47:45.556781 Epoch 1, Training Loss 1.9895779732852945\n",
      "2022-03-26 16:47:45.580612 Epoch 1, Training Loss 1.9920361234099053\n",
      "2022-03-26 16:47:45.603655 Epoch 1, Training Loss 1.9944595552771294\n",
      "2022-03-26 16:47:45.631545 Epoch 1, Training Loss 1.996906268779579\n",
      "2022-03-26 16:47:45.654603 Epoch 1, Training Loss 1.9993357565396888\n",
      "2022-03-26 16:47:45.677205 Epoch 1, Training Loss 2.001669889978131\n",
      "2022-03-26 16:47:45.700573 Epoch 1, Training Loss 2.003924669359651\n",
      "2022-03-26 16:47:45.723549 Epoch 1, Training Loss 2.006346432906587\n",
      "2022-03-26 16:47:45.746450 Epoch 1, Training Loss 2.008776527384053\n",
      "2022-03-26 16:47:45.774652 Epoch 1, Training Loss 2.0110277835365453\n",
      "2022-03-26 16:47:45.799151 Epoch 1, Training Loss 2.0134294557449457\n",
      "2022-03-26 16:47:45.822089 Epoch 1, Training Loss 2.015848674432701\n",
      "2022-03-26 16:47:45.849280 Epoch 1, Training Loss 2.018477292621837\n",
      "2022-03-26 16:47:45.873063 Epoch 1, Training Loss 2.020991948254578\n",
      "2022-03-26 16:47:45.896271 Epoch 1, Training Loss 2.023223448135054\n",
      "2022-03-26 16:47:45.919898 Epoch 1, Training Loss 2.0255965299313634\n",
      "2022-03-26 16:47:45.945434 Epoch 1, Training Loss 2.0277252055494985\n",
      "2022-03-26 16:47:45.969159 Epoch 1, Training Loss 2.030111820649003\n",
      "2022-03-26 16:47:46.004140 Epoch 1, Training Loss 2.0325442765984696\n",
      "2022-03-26 16:47:46.032684 Epoch 1, Training Loss 2.0349235150515272\n",
      "2022-03-26 16:47:46.063108 Epoch 1, Training Loss 2.037270232692094\n",
      "2022-03-26 16:47:46.074156 Epoch 1, Training Loss 2.039806863688447\n",
      "2022-03-26 17:02:16.773642 Epoch 50, Training Loss 0.0010962002265178944\n",
      "2022-03-26 17:02:16.797398 Epoch 50, Training Loss 0.002037425251568065\n",
      "2022-03-26 17:02:16.819980 Epoch 50, Training Loss 0.002939691933829461\n",
      "2022-03-26 17:02:16.843947 Epoch 50, Training Loss 0.003832460974183534\n",
      "2022-03-26 17:02:16.866610 Epoch 50, Training Loss 0.004827840020284628\n",
      "2022-03-26 17:02:16.889333 Epoch 50, Training Loss 0.005799998179116212\n",
      "2022-03-26 17:02:16.911730 Epoch 50, Training Loss 0.006790672421760267\n",
      "2022-03-26 17:02:16.934365 Epoch 50, Training Loss 0.007801313534417115\n",
      "2022-03-26 17:02:16.956240 Epoch 50, Training Loss 0.009053950876835972\n",
      "2022-03-26 17:02:16.984696 Epoch 50, Training Loss 0.010262929341372322\n",
      "2022-03-26 17:02:17.009231 Epoch 50, Training Loss 0.01153977623071207\n",
      "2022-03-26 17:02:17.033962 Epoch 50, Training Loss 0.012529173013194443\n",
      "2022-03-26 17:02:17.066867 Epoch 50, Training Loss 0.013325690918261439\n",
      "2022-03-26 17:02:17.091041 Epoch 50, Training Loss 0.014453583361242738\n",
      "2022-03-26 17:02:17.114280 Epoch 50, Training Loss 0.015436478130652777\n",
      "2022-03-26 17:02:17.139739 Epoch 50, Training Loss 0.016362367459880116\n",
      "2022-03-26 17:02:17.162204 Epoch 50, Training Loss 0.01718939882715035\n",
      "2022-03-26 17:02:17.184616 Epoch 50, Training Loss 0.018127773347718028\n",
      "2022-03-26 17:02:17.212740 Epoch 50, Training Loss 0.018894872397108152\n",
      "2022-03-26 17:02:17.236501 Epoch 50, Training Loss 0.01976884097394431\n",
      "2022-03-26 17:02:17.259723 Epoch 50, Training Loss 0.020861481690345823\n",
      "2022-03-26 17:02:17.286686 Epoch 50, Training Loss 0.022036681013643894\n",
      "2022-03-26 17:02:17.309691 Epoch 50, Training Loss 0.023235748567239707\n",
      "2022-03-26 17:02:17.334881 Epoch 50, Training Loss 0.02476222260528818\n",
      "2022-03-26 17:02:17.358643 Epoch 50, Training Loss 0.025614180070969758\n",
      "2022-03-26 17:02:17.383030 Epoch 50, Training Loss 0.026704199250091983\n",
      "2022-03-26 17:02:17.406691 Epoch 50, Training Loss 0.02780642968309505\n",
      "2022-03-26 17:02:17.447299 Epoch 50, Training Loss 0.02879629072630802\n",
      "2022-03-26 17:02:17.471517 Epoch 50, Training Loss 0.029995490065620987\n",
      "2022-03-26 17:02:17.496118 Epoch 50, Training Loss 0.030978593603729287\n",
      "2022-03-26 17:02:17.519281 Epoch 50, Training Loss 0.0317814924832805\n",
      "2022-03-26 17:02:17.541657 Epoch 50, Training Loss 0.03295435838382262\n",
      "2022-03-26 17:02:17.563762 Epoch 50, Training Loss 0.03400722656713422\n",
      "2022-03-26 17:02:17.586506 Epoch 50, Training Loss 0.03511046166615108\n",
      "2022-03-26 17:02:17.610071 Epoch 50, Training Loss 0.03633479983605387\n",
      "2022-03-26 17:02:17.633555 Epoch 50, Training Loss 0.037367417074530326\n",
      "2022-03-26 17:02:17.660355 Epoch 50, Training Loss 0.038409043303536027\n",
      "2022-03-26 17:02:17.684389 Epoch 50, Training Loss 0.03937551257250559\n",
      "2022-03-26 17:02:17.706976 Epoch 50, Training Loss 0.04053048442696671\n",
      "2022-03-26 17:02:17.730422 Epoch 50, Training Loss 0.041550897118990376\n",
      "2022-03-26 17:02:17.755508 Epoch 50, Training Loss 0.04260206862788676\n",
      "2022-03-26 17:02:17.778169 Epoch 50, Training Loss 0.044093687820922385\n",
      "2022-03-26 17:02:17.800744 Epoch 50, Training Loss 0.04519384093296802\n",
      "2022-03-26 17:02:17.823315 Epoch 50, Training Loss 0.04596457639923486\n",
      "2022-03-26 17:02:17.845799 Epoch 50, Training Loss 0.047255818191391734\n",
      "2022-03-26 17:02:17.874910 Epoch 50, Training Loss 0.04803449723421765\n",
      "2022-03-26 17:02:17.898174 Epoch 50, Training Loss 0.04906694411926562\n",
      "2022-03-26 17:02:17.920667 Epoch 50, Training Loss 0.050385542309192746\n",
      "2022-03-26 17:02:17.946247 Epoch 50, Training Loss 0.05122919651248571\n",
      "2022-03-26 17:02:17.974485 Epoch 50, Training Loss 0.05225180542987326\n",
      "2022-03-26 17:02:17.997334 Epoch 50, Training Loss 0.052975658413089446\n",
      "2022-03-26 17:02:18.020494 Epoch 50, Training Loss 0.05386152787281729\n",
      "2022-03-26 17:02:18.043703 Epoch 50, Training Loss 0.05496162915473704\n",
      "2022-03-26 17:02:18.066810 Epoch 50, Training Loss 0.05599309424000323\n",
      "2022-03-26 17:02:18.096009 Epoch 50, Training Loss 0.05714092115916864\n",
      "2022-03-26 17:02:18.119237 Epoch 50, Training Loss 0.05819459499605476\n",
      "2022-03-26 17:02:18.141997 Epoch 50, Training Loss 0.0591853892101961\n",
      "2022-03-26 17:02:18.165399 Epoch 50, Training Loss 0.060254956983849216\n",
      "2022-03-26 17:02:18.190454 Epoch 50, Training Loss 0.061420164068641564\n",
      "2022-03-26 17:02:18.212656 Epoch 50, Training Loss 0.06233890564240458\n",
      "2022-03-26 17:02:18.235833 Epoch 50, Training Loss 0.0633059085330085\n",
      "2022-03-26 17:02:18.258334 Epoch 50, Training Loss 0.06451344878777214\n",
      "2022-03-26 17:02:18.282081 Epoch 50, Training Loss 0.06548835424815908\n",
      "2022-03-26 17:02:18.309342 Epoch 50, Training Loss 0.06637738336382619\n",
      "2022-03-26 17:02:18.335170 Epoch 50, Training Loss 0.06718358946273394\n",
      "2022-03-26 17:02:18.356856 Epoch 50, Training Loss 0.06845081279344876\n",
      "2022-03-26 17:02:18.382119 Epoch 50, Training Loss 0.06956855659289739\n",
      "2022-03-26 17:02:18.404179 Epoch 50, Training Loss 0.07098030129357068\n",
      "2022-03-26 17:02:18.426980 Epoch 50, Training Loss 0.07208820034171004\n",
      "2022-03-26 17:02:18.456058 Epoch 50, Training Loss 0.07334908126565197\n",
      "2022-03-26 17:02:18.484032 Epoch 50, Training Loss 0.0746057143296732\n",
      "2022-03-26 17:02:18.508309 Epoch 50, Training Loss 0.07573484795172807\n",
      "2022-03-26 17:02:18.539397 Epoch 50, Training Loss 0.07677961234241495\n",
      "2022-03-26 17:02:18.562688 Epoch 50, Training Loss 0.07791253436556862\n",
      "2022-03-26 17:02:18.586492 Epoch 50, Training Loss 0.07872063310249992\n",
      "2022-03-26 17:02:18.615384 Epoch 50, Training Loss 0.07980884912678653\n",
      "2022-03-26 17:02:18.639900 Epoch 50, Training Loss 0.08096078594627283\n",
      "2022-03-26 17:02:18.663605 Epoch 50, Training Loss 0.08183082458003403\n",
      "2022-03-26 17:02:18.687026 Epoch 50, Training Loss 0.08284442565020393\n",
      "2022-03-26 17:02:18.719689 Epoch 50, Training Loss 0.08371703932657266\n",
      "2022-03-26 17:02:18.748407 Epoch 50, Training Loss 0.08451141123576543\n",
      "2022-03-26 17:02:18.774404 Epoch 50, Training Loss 0.08542104183560442\n",
      "2022-03-26 17:02:18.799877 Epoch 50, Training Loss 0.08637925891010352\n",
      "2022-03-26 17:02:18.823649 Epoch 50, Training Loss 0.087374596796987\n",
      "2022-03-26 17:02:18.847676 Epoch 50, Training Loss 0.08840112391945042\n",
      "2022-03-26 17:02:18.881511 Epoch 50, Training Loss 0.08943093127911658\n",
      "2022-03-26 17:02:18.905549 Epoch 50, Training Loss 0.09035353191063532\n",
      "2022-03-26 17:02:18.929443 Epoch 50, Training Loss 0.09108669350824088\n",
      "2022-03-26 17:02:18.959181 Epoch 50, Training Loss 0.09248732430550753\n",
      "2022-03-26 17:02:18.984740 Epoch 50, Training Loss 0.09324966481579539\n",
      "2022-03-26 17:02:19.008391 Epoch 50, Training Loss 0.09435368994312823\n",
      "2022-03-26 17:02:19.036562 Epoch 50, Training Loss 0.09557428407242231\n",
      "2022-03-26 17:02:19.060460 Epoch 50, Training Loss 0.09643779698845066\n",
      "2022-03-26 17:02:19.084492 Epoch 50, Training Loss 0.09758076018384655\n",
      "2022-03-26 17:02:19.108519 Epoch 50, Training Loss 0.09874747887901637\n",
      "2022-03-26 17:02:19.133417 Epoch 50, Training Loss 0.09968892966999728\n",
      "2022-03-26 17:02:19.156290 Epoch 50, Training Loss 0.10073174471440523\n",
      "2022-03-26 17:02:19.186314 Epoch 50, Training Loss 0.10166793291830956\n",
      "2022-03-26 17:02:19.209913 Epoch 50, Training Loss 0.10276592006463833\n",
      "2022-03-26 17:02:19.233349 Epoch 50, Training Loss 0.10366168968817767\n",
      "2022-03-26 17:02:19.258008 Epoch 50, Training Loss 0.10475647380894712\n",
      "2022-03-26 17:02:19.281929 Epoch 50, Training Loss 0.10587270165343418\n",
      "2022-03-26 17:02:19.305316 Epoch 50, Training Loss 0.10683307676668972\n",
      "2022-03-26 17:02:19.328974 Epoch 50, Training Loss 0.10805672635812588\n",
      "2022-03-26 17:02:19.351645 Epoch 50, Training Loss 0.10909811676005877\n",
      "2022-03-26 17:02:19.375196 Epoch 50, Training Loss 0.11032675782128064\n",
      "2022-03-26 17:02:19.405533 Epoch 50, Training Loss 0.11147859311469681\n",
      "2022-03-26 17:02:19.430713 Epoch 50, Training Loss 0.11259258243129076\n",
      "2022-03-26 17:02:19.454359 Epoch 50, Training Loss 0.11342619012688737\n",
      "2022-03-26 17:02:19.490237 Epoch 50, Training Loss 0.11455345321494295\n",
      "2022-03-26 17:02:19.513922 Epoch 50, Training Loss 0.11568800415224431\n",
      "2022-03-26 17:02:19.537525 Epoch 50, Training Loss 0.11671532031215365\n",
      "2022-03-26 17:02:19.560494 Epoch 50, Training Loss 0.11781208548704376\n",
      "2022-03-26 17:02:19.582591 Epoch 50, Training Loss 0.11866994190703878\n",
      "2022-03-26 17:02:19.605336 Epoch 50, Training Loss 0.11980597396640827\n",
      "2022-03-26 17:02:19.635574 Epoch 50, Training Loss 0.12113003124056569\n",
      "2022-03-26 17:02:19.658569 Epoch 50, Training Loss 0.12182698309269098\n",
      "2022-03-26 17:02:19.682697 Epoch 50, Training Loss 0.12286488738511224\n",
      "2022-03-26 17:02:19.706420 Epoch 50, Training Loss 0.1236189246330115\n",
      "2022-03-26 17:02:19.728357 Epoch 50, Training Loss 0.12454212809462681\n",
      "2022-03-26 17:02:19.750583 Epoch 50, Training Loss 0.12532738727681778\n",
      "2022-03-26 17:02:19.774111 Epoch 50, Training Loss 0.12643192514129306\n",
      "2022-03-26 17:02:19.797066 Epoch 50, Training Loss 0.1276869149616612\n",
      "2022-03-26 17:02:19.819988 Epoch 50, Training Loss 0.1288054754666965\n",
      "2022-03-26 17:02:19.853472 Epoch 50, Training Loss 0.13006986910119997\n",
      "2022-03-26 17:02:19.880307 Epoch 50, Training Loss 0.1308142546650089\n",
      "2022-03-26 17:02:19.903537 Epoch 50, Training Loss 0.13192926373932978\n",
      "2022-03-26 17:02:19.926184 Epoch 50, Training Loss 0.13262335388251886\n",
      "2022-03-26 17:02:19.950402 Epoch 50, Training Loss 0.13362349710805946\n",
      "2022-03-26 17:02:19.973418 Epoch 50, Training Loss 0.13467896457218453\n",
      "2022-03-26 17:02:19.997556 Epoch 50, Training Loss 0.1360586762733167\n",
      "2022-03-26 17:02:20.020225 Epoch 50, Training Loss 0.13716421346835164\n",
      "2022-03-26 17:02:20.043356 Epoch 50, Training Loss 0.1381476407923052\n",
      "2022-03-26 17:02:20.074617 Epoch 50, Training Loss 0.1395502452502775\n",
      "2022-03-26 17:02:20.099453 Epoch 50, Training Loss 0.14050788289445745\n",
      "2022-03-26 17:02:20.122241 Epoch 50, Training Loss 0.14164013486079244\n",
      "2022-03-26 17:02:20.146491 Epoch 50, Training Loss 0.14271383113263514\n",
      "2022-03-26 17:02:20.174828 Epoch 50, Training Loss 0.14377085068036832\n",
      "2022-03-26 17:02:20.197707 Epoch 50, Training Loss 0.1450163037575724\n",
      "2022-03-26 17:02:20.219986 Epoch 50, Training Loss 0.14612986928666644\n",
      "2022-03-26 17:02:20.242662 Epoch 50, Training Loss 0.14708017853214916\n",
      "2022-03-26 17:02:20.265065 Epoch 50, Training Loss 0.14832578184049758\n",
      "2022-03-26 17:02:20.294486 Epoch 50, Training Loss 0.1493373509410702\n",
      "2022-03-26 17:02:20.317468 Epoch 50, Training Loss 0.15056824973782004\n",
      "2022-03-26 17:02:20.342740 Epoch 50, Training Loss 0.15165530911187078\n",
      "2022-03-26 17:02:20.368393 Epoch 50, Training Loss 0.1529046913699421\n",
      "2022-03-26 17:02:20.391980 Epoch 50, Training Loss 0.15374625415143456\n",
      "2022-03-26 17:02:20.414466 Epoch 50, Training Loss 0.1546931275168953\n",
      "2022-03-26 17:02:20.437425 Epoch 50, Training Loss 0.15569485087528864\n",
      "2022-03-26 17:02:20.460347 Epoch 50, Training Loss 0.15657902145019884\n",
      "2022-03-26 17:02:20.483773 Epoch 50, Training Loss 0.15761814718051334\n",
      "2022-03-26 17:02:20.522733 Epoch 50, Training Loss 0.15841127044099676\n",
      "2022-03-26 17:02:20.547555 Epoch 50, Training Loss 0.1595463352587522\n",
      "2022-03-26 17:02:20.571035 Epoch 50, Training Loss 0.1604891818807558\n",
      "2022-03-26 17:02:20.600524 Epoch 50, Training Loss 0.16176807499297743\n",
      "2022-03-26 17:02:20.623996 Epoch 50, Training Loss 0.1628632724590009\n",
      "2022-03-26 17:02:20.648559 Epoch 50, Training Loss 0.16384622988188663\n",
      "2022-03-26 17:02:20.672648 Epoch 50, Training Loss 0.16480343253411295\n",
      "2022-03-26 17:02:20.695952 Epoch 50, Training Loss 0.1658244889867885\n",
      "2022-03-26 17:02:20.718996 Epoch 50, Training Loss 0.16684216695368442\n",
      "2022-03-26 17:02:20.749365 Epoch 50, Training Loss 0.16776032246592099\n",
      "2022-03-26 17:02:20.772873 Epoch 50, Training Loss 0.16901477439629148\n",
      "2022-03-26 17:02:20.796228 Epoch 50, Training Loss 0.16994889221532875\n",
      "2022-03-26 17:02:20.823818 Epoch 50, Training Loss 0.17089244730942085\n",
      "2022-03-26 17:02:20.847501 Epoch 50, Training Loss 0.17190323674770266\n",
      "2022-03-26 17:02:20.872750 Epoch 50, Training Loss 0.17305699166129618\n",
      "2022-03-26 17:02:20.896080 Epoch 50, Training Loss 0.17414848792278553\n",
      "2022-03-26 17:02:20.919806 Epoch 50, Training Loss 0.1753535740210882\n",
      "2022-03-26 17:02:20.943448 Epoch 50, Training Loss 0.17652068929294187\n",
      "2022-03-26 17:02:20.973459 Epoch 50, Training Loss 0.17765854112327556\n",
      "2022-03-26 17:02:21.000071 Epoch 50, Training Loss 0.17882013198969615\n",
      "2022-03-26 17:02:21.024753 Epoch 50, Training Loss 0.1799312631035095\n",
      "2022-03-26 17:02:21.052612 Epoch 50, Training Loss 0.18119228678895993\n",
      "2022-03-26 17:02:21.076518 Epoch 50, Training Loss 0.18214604967390485\n",
      "2022-03-26 17:02:21.104350 Epoch 50, Training Loss 0.1833930152761357\n",
      "2022-03-26 17:02:21.128168 Epoch 50, Training Loss 0.184148134782796\n",
      "2022-03-26 17:02:21.150992 Epoch 50, Training Loss 0.1852384137223139\n",
      "2022-03-26 17:02:21.174467 Epoch 50, Training Loss 0.18605016389161425\n",
      "2022-03-26 17:02:21.204090 Epoch 50, Training Loss 0.18707495645793806\n",
      "2022-03-26 17:02:21.228320 Epoch 50, Training Loss 0.18817289207902405\n",
      "2022-03-26 17:02:21.251249 Epoch 50, Training Loss 0.18918923854522998\n",
      "2022-03-26 17:02:21.278615 Epoch 50, Training Loss 0.19039017838590286\n",
      "2022-03-26 17:02:21.302530 Epoch 50, Training Loss 0.19151108626209562\n",
      "2022-03-26 17:02:21.326038 Epoch 50, Training Loss 0.19226768300356462\n",
      "2022-03-26 17:02:21.351076 Epoch 50, Training Loss 0.19323214454114285\n",
      "2022-03-26 17:02:21.375275 Epoch 50, Training Loss 0.194238368964866\n",
      "2022-03-26 17:02:21.399973 Epoch 50, Training Loss 0.19562924655197222\n",
      "2022-03-26 17:02:21.430437 Epoch 50, Training Loss 0.1965793801847931\n",
      "2022-03-26 17:02:21.454834 Epoch 50, Training Loss 0.1975521825616012\n",
      "2022-03-26 17:02:21.477390 Epoch 50, Training Loss 0.198666053888438\n",
      "2022-03-26 17:02:21.505022 Epoch 50, Training Loss 0.199647815254948\n",
      "2022-03-26 17:02:21.530588 Epoch 50, Training Loss 0.20073239615811106\n",
      "2022-03-26 17:02:21.558969 Epoch 50, Training Loss 0.20167582846053725\n",
      "2022-03-26 17:02:21.581994 Epoch 50, Training Loss 0.20260248007371906\n",
      "2022-03-26 17:02:21.604743 Epoch 50, Training Loss 0.20333975553512573\n",
      "2022-03-26 17:02:21.627549 Epoch 50, Training Loss 0.20416353578152863\n",
      "2022-03-26 17:02:21.661065 Epoch 50, Training Loss 0.20541703129363487\n",
      "2022-03-26 17:02:21.684473 Epoch 50, Training Loss 0.2065839860445398\n",
      "2022-03-26 17:02:21.706996 Epoch 50, Training Loss 0.20777909537715375\n",
      "2022-03-26 17:02:21.733471 Epoch 50, Training Loss 0.20877124037584074\n",
      "2022-03-26 17:02:21.756087 Epoch 50, Training Loss 0.2098958571548657\n",
      "2022-03-26 17:02:21.778560 Epoch 50, Training Loss 0.21092869741532505\n",
      "2022-03-26 17:02:21.801018 Epoch 50, Training Loss 0.21205492512039517\n",
      "2022-03-26 17:02:21.823735 Epoch 50, Training Loss 0.21282736572158306\n",
      "2022-03-26 17:02:21.846576 Epoch 50, Training Loss 0.21422914882450153\n",
      "2022-03-26 17:02:21.876099 Epoch 50, Training Loss 0.2152499762337531\n",
      "2022-03-26 17:02:21.906215 Epoch 50, Training Loss 0.21648279472690105\n",
      "2022-03-26 17:02:21.929050 Epoch 50, Training Loss 0.21771214029673117\n",
      "2022-03-26 17:02:21.952676 Epoch 50, Training Loss 0.2187774077705715\n",
      "2022-03-26 17:02:21.975846 Epoch 50, Training Loss 0.2196634450684423\n",
      "2022-03-26 17:02:22.002321 Epoch 50, Training Loss 0.22061537072786588\n",
      "2022-03-26 17:02:22.024060 Epoch 50, Training Loss 0.22155511089603006\n",
      "2022-03-26 17:02:22.047571 Epoch 50, Training Loss 0.22286704982942937\n",
      "2022-03-26 17:02:22.070650 Epoch 50, Training Loss 0.22382597316561453\n",
      "2022-03-26 17:02:22.100706 Epoch 50, Training Loss 0.22494223736741048\n",
      "2022-03-26 17:02:22.124004 Epoch 50, Training Loss 0.22571195749675527\n",
      "2022-03-26 17:02:22.146805 Epoch 50, Training Loss 0.22666453385292112\n",
      "2022-03-26 17:02:22.169071 Epoch 50, Training Loss 0.2277192479509222\n",
      "2022-03-26 17:02:22.196039 Epoch 50, Training Loss 0.22880587065616226\n",
      "2022-03-26 17:02:22.219423 Epoch 50, Training Loss 0.22983266836237115\n",
      "2022-03-26 17:02:22.243095 Epoch 50, Training Loss 0.23098421881875725\n",
      "2022-03-26 17:02:22.265969 Epoch 50, Training Loss 0.23201726098804523\n",
      "2022-03-26 17:02:22.289103 Epoch 50, Training Loss 0.23293842013229799\n",
      "2022-03-26 17:02:22.317763 Epoch 50, Training Loss 0.2338535012034199\n",
      "2022-03-26 17:02:22.343511 Epoch 50, Training Loss 0.23485508164786317\n",
      "2022-03-26 17:02:22.365682 Epoch 50, Training Loss 0.23588668461650839\n",
      "2022-03-26 17:02:22.388355 Epoch 50, Training Loss 0.23686736692552982\n",
      "2022-03-26 17:02:22.412663 Epoch 50, Training Loss 0.23831063714783515\n",
      "2022-03-26 17:02:22.435356 Epoch 50, Training Loss 0.2394164162676048\n",
      "2022-03-26 17:02:22.458634 Epoch 50, Training Loss 0.2404782147816075\n",
      "2022-03-26 17:02:22.483415 Epoch 50, Training Loss 0.24143178772438517\n",
      "2022-03-26 17:02:22.507408 Epoch 50, Training Loss 0.24244726443534617\n",
      "2022-03-26 17:02:22.537701 Epoch 50, Training Loss 0.24339962455317798\n",
      "2022-03-26 17:02:22.572242 Epoch 50, Training Loss 0.24438314524757893\n",
      "2022-03-26 17:02:22.596952 Epoch 50, Training Loss 0.24571154436187062\n",
      "2022-03-26 17:02:22.620535 Epoch 50, Training Loss 0.24662192123930168\n",
      "2022-03-26 17:02:22.650361 Epoch 50, Training Loss 0.24760541921991217\n",
      "2022-03-26 17:02:22.673286 Epoch 50, Training Loss 0.24889080725667423\n",
      "2022-03-26 17:02:22.696644 Epoch 50, Training Loss 0.24982517134503027\n",
      "2022-03-26 17:02:22.720308 Epoch 50, Training Loss 0.2508045822153311\n",
      "2022-03-26 17:02:22.749483 Epoch 50, Training Loss 0.2521223287143366\n",
      "2022-03-26 17:02:22.773418 Epoch 50, Training Loss 0.253283460884143\n",
      "2022-03-26 17:02:22.798027 Epoch 50, Training Loss 0.2542277496793996\n",
      "2022-03-26 17:02:22.822812 Epoch 50, Training Loss 0.25521749852563413\n",
      "2022-03-26 17:02:22.849981 Epoch 50, Training Loss 0.25619079671857303\n",
      "2022-03-26 17:02:22.876681 Epoch 50, Training Loss 0.257456124819758\n",
      "2022-03-26 17:02:22.900373 Epoch 50, Training Loss 0.2586098340008875\n",
      "2022-03-26 17:02:22.924210 Epoch 50, Training Loss 0.2592765661456701\n",
      "2022-03-26 17:02:22.948163 Epoch 50, Training Loss 0.2604691662142039\n",
      "2022-03-26 17:02:22.979864 Epoch 50, Training Loss 0.2613066936392918\n",
      "2022-03-26 17:02:23.005290 Epoch 50, Training Loss 0.2622823068095595\n",
      "2022-03-26 17:02:23.029275 Epoch 50, Training Loss 0.26337068899513205\n",
      "2022-03-26 17:02:23.052477 Epoch 50, Training Loss 0.2643768369694195\n",
      "2022-03-26 17:02:23.078948 Epoch 50, Training Loss 0.26556214057576016\n",
      "2022-03-26 17:02:23.110073 Epoch 50, Training Loss 0.26648473411874696\n",
      "2022-03-26 17:02:23.133214 Epoch 50, Training Loss 0.2674313714284726\n",
      "2022-03-26 17:02:23.156117 Epoch 50, Training Loss 0.26862237009855794\n",
      "2022-03-26 17:02:23.184667 Epoch 50, Training Loss 0.26971291283817245\n",
      "2022-03-26 17:02:23.213384 Epoch 50, Training Loss 0.27100252335333763\n",
      "2022-03-26 17:02:23.236219 Epoch 50, Training Loss 0.2718567577621821\n",
      "2022-03-26 17:02:23.259678 Epoch 50, Training Loss 0.2728225578127615\n",
      "2022-03-26 17:02:23.286882 Epoch 50, Training Loss 0.2738022174676666\n",
      "2022-03-26 17:02:23.310671 Epoch 50, Training Loss 0.274995542776859\n",
      "2022-03-26 17:02:23.341538 Epoch 50, Training Loss 0.27622931082840163\n",
      "2022-03-26 17:02:23.365041 Epoch 50, Training Loss 0.2772527811929698\n",
      "2022-03-26 17:02:23.388119 Epoch 50, Training Loss 0.2781568375389899\n",
      "2022-03-26 17:02:23.417325 Epoch 50, Training Loss 0.2793122270070683\n",
      "2022-03-26 17:02:23.441520 Epoch 50, Training Loss 0.2803796456597955\n",
      "2022-03-26 17:02:23.464903 Epoch 50, Training Loss 0.28136716855456456\n",
      "2022-03-26 17:02:23.490586 Epoch 50, Training Loss 0.28245745290575736\n",
      "2022-03-26 17:02:23.515648 Epoch 50, Training Loss 0.2834103544197424\n",
      "2022-03-26 17:02:23.539756 Epoch 50, Training Loss 0.2843037061679089\n",
      "2022-03-26 17:02:23.564085 Epoch 50, Training Loss 0.2858030994225036\n",
      "2022-03-26 17:02:23.599012 Epoch 50, Training Loss 0.2868300290668712\n",
      "2022-03-26 17:02:23.626037 Epoch 50, Training Loss 0.2880153143802262\n",
      "2022-03-26 17:02:23.651418 Epoch 50, Training Loss 0.2890839077475126\n",
      "2022-03-26 17:02:23.674210 Epoch 50, Training Loss 0.28996897025791274\n",
      "2022-03-26 17:02:23.697573 Epoch 50, Training Loss 0.2909907322862874\n",
      "2022-03-26 17:02:23.726390 Epoch 50, Training Loss 0.29177720863800827\n",
      "2022-03-26 17:02:23.750558 Epoch 50, Training Loss 0.29294913991942734\n",
      "2022-03-26 17:02:23.773215 Epoch 50, Training Loss 0.2939003121365062\n",
      "2022-03-26 17:02:23.799019 Epoch 50, Training Loss 0.2948689737435802\n",
      "2022-03-26 17:02:23.821579 Epoch 50, Training Loss 0.29581329912480797\n",
      "2022-03-26 17:02:23.851183 Epoch 50, Training Loss 0.29666232940790904\n",
      "2022-03-26 17:02:23.876242 Epoch 50, Training Loss 0.2976744972226565\n",
      "2022-03-26 17:02:23.898973 Epoch 50, Training Loss 0.2985720728974208\n",
      "2022-03-26 17:02:23.923566 Epoch 50, Training Loss 0.2993856315570109\n",
      "2022-03-26 17:02:23.947507 Epoch 50, Training Loss 0.3004322596218275\n",
      "2022-03-26 17:02:23.970374 Epoch 50, Training Loss 0.30128764542167447\n",
      "2022-03-26 17:02:23.999083 Epoch 50, Training Loss 0.30250199577387643\n",
      "2022-03-26 17:02:24.022549 Epoch 50, Training Loss 0.3034265475047519\n",
      "2022-03-26 17:02:24.045675 Epoch 50, Training Loss 0.30457886192194944\n",
      "2022-03-26 17:02:24.077817 Epoch 50, Training Loss 0.3054529014603256\n",
      "2022-03-26 17:02:24.101561 Epoch 50, Training Loss 0.3062507984278452\n",
      "2022-03-26 17:02:24.124508 Epoch 50, Training Loss 0.30720408432319035\n",
      "2022-03-26 17:02:24.152835 Epoch 50, Training Loss 0.3084584405202695\n",
      "2022-03-26 17:02:24.175404 Epoch 50, Training Loss 0.3093429254296491\n",
      "2022-03-26 17:02:24.199397 Epoch 50, Training Loss 0.31046208815501475\n",
      "2022-03-26 17:02:24.222342 Epoch 50, Training Loss 0.3113549113121179\n",
      "2022-03-26 17:02:24.244944 Epoch 50, Training Loss 0.31253714924273285\n",
      "2022-03-26 17:02:24.267882 Epoch 50, Training Loss 0.31348827946216556\n",
      "2022-03-26 17:02:24.299522 Epoch 50, Training Loss 0.31475724962056445\n",
      "2022-03-26 17:02:24.323221 Epoch 50, Training Loss 0.3160483043669435\n",
      "2022-03-26 17:02:24.348539 Epoch 50, Training Loss 0.3168106367216086\n",
      "2022-03-26 17:02:24.375137 Epoch 50, Training Loss 0.3178636098609251\n",
      "2022-03-26 17:02:24.398170 Epoch 50, Training Loss 0.3188363934112022\n",
      "2022-03-26 17:02:24.420526 Epoch 50, Training Loss 0.3202176257167631\n",
      "2022-03-26 17:02:24.445241 Epoch 50, Training Loss 0.3210070362633756\n",
      "2022-03-26 17:02:24.468384 Epoch 50, Training Loss 0.32182871106335575\n",
      "2022-03-26 17:02:24.492219 Epoch 50, Training Loss 0.32282331524907476\n",
      "2022-03-26 17:02:24.522160 Epoch 50, Training Loss 0.3242309734492046\n",
      "2022-03-26 17:02:24.546084 Epoch 50, Training Loss 0.3252804181002595\n",
      "2022-03-26 17:02:24.570613 Epoch 50, Training Loss 0.3262331529194132\n",
      "2022-03-26 17:02:24.601509 Epoch 50, Training Loss 0.3274269030831964\n",
      "2022-03-26 17:02:24.631369 Epoch 50, Training Loss 0.3287497356419673\n",
      "2022-03-26 17:02:24.655445 Epoch 50, Training Loss 0.3295012046309078\n",
      "2022-03-26 17:02:24.679765 Epoch 50, Training Loss 0.3301900959075869\n",
      "2022-03-26 17:02:24.703441 Epoch 50, Training Loss 0.33095760738758173\n",
      "2022-03-26 17:02:24.730784 Epoch 50, Training Loss 0.33162864943599457\n",
      "2022-03-26 17:02:24.755710 Epoch 50, Training Loss 0.33269715522561233\n",
      "2022-03-26 17:02:24.779802 Epoch 50, Training Loss 0.333604387615038\n",
      "2022-03-26 17:02:24.804762 Epoch 50, Training Loss 0.3346832376306929\n",
      "2022-03-26 17:02:24.832669 Epoch 50, Training Loss 0.33577236944757155\n",
      "2022-03-26 17:02:24.857001 Epoch 50, Training Loss 0.3367403574154505\n",
      "2022-03-26 17:02:24.881388 Epoch 50, Training Loss 0.33768624082550674\n",
      "2022-03-26 17:02:24.911102 Epoch 50, Training Loss 0.33882671449800283\n",
      "2022-03-26 17:02:24.941318 Epoch 50, Training Loss 0.3396701278436519\n",
      "2022-03-26 17:02:24.968009 Epoch 50, Training Loss 0.3406822160076912\n",
      "2022-03-26 17:02:24.991639 Epoch 50, Training Loss 0.34176574643615565\n",
      "2022-03-26 17:02:25.016976 Epoch 50, Training Loss 0.3423948462509438\n",
      "2022-03-26 17:02:25.044104 Epoch 50, Training Loss 0.3435472765237169\n",
      "2022-03-26 17:02:25.068439 Epoch 50, Training Loss 0.34448915994380747\n",
      "2022-03-26 17:02:25.091346 Epoch 50, Training Loss 0.3457004695444766\n",
      "2022-03-26 17:02:25.114812 Epoch 50, Training Loss 0.3468502657797635\n",
      "2022-03-26 17:02:25.137725 Epoch 50, Training Loss 0.34780568616164614\n",
      "2022-03-26 17:02:25.167187 Epoch 50, Training Loss 0.34899200091276633\n",
      "2022-03-26 17:02:25.190850 Epoch 50, Training Loss 0.3502621774173454\n",
      "2022-03-26 17:02:25.213783 Epoch 50, Training Loss 0.3512867725718662\n",
      "2022-03-26 17:02:25.241314 Epoch 50, Training Loss 0.3523551833904003\n",
      "2022-03-26 17:02:25.264517 Epoch 50, Training Loss 0.35321918061322266\n",
      "2022-03-26 17:02:25.287826 Epoch 50, Training Loss 0.3545060383389368\n",
      "2022-03-26 17:02:25.311470 Epoch 50, Training Loss 0.35548050240482515\n",
      "2022-03-26 17:02:25.336798 Epoch 50, Training Loss 0.35632244179315886\n",
      "2022-03-26 17:02:25.359769 Epoch 50, Training Loss 0.3574140465930295\n",
      "2022-03-26 17:02:25.392223 Epoch 50, Training Loss 0.35852408835954985\n",
      "2022-03-26 17:02:25.416587 Epoch 50, Training Loss 0.35951572755718475\n",
      "2022-03-26 17:02:25.440275 Epoch 50, Training Loss 0.3602754013312747\n",
      "2022-03-26 17:02:25.464166 Epoch 50, Training Loss 0.36157587467861907\n",
      "2022-03-26 17:02:25.491722 Epoch 50, Training Loss 0.3626282656436686\n",
      "2022-03-26 17:02:25.515241 Epoch 50, Training Loss 0.36384159524727355\n",
      "2022-03-26 17:02:25.538332 Epoch 50, Training Loss 0.3648554212449457\n",
      "2022-03-26 17:02:25.561316 Epoch 50, Training Loss 0.3657706879898715\n",
      "2022-03-26 17:02:25.584073 Epoch 50, Training Loss 0.3668512424544605\n",
      "2022-03-26 17:02:25.613218 Epoch 50, Training Loss 0.367925916486384\n",
      "2022-03-26 17:02:25.648065 Epoch 50, Training Loss 0.36922594089337324\n",
      "2022-03-26 17:02:25.671130 Epoch 50, Training Loss 0.3702421348418116\n",
      "2022-03-26 17:02:25.697482 Epoch 50, Training Loss 0.37184785439840057\n",
      "2022-03-26 17:02:25.720074 Epoch 50, Training Loss 0.3731695790120098\n",
      "2022-03-26 17:02:25.743855 Epoch 50, Training Loss 0.37395155887164727\n",
      "2022-03-26 17:02:25.766940 Epoch 50, Training Loss 0.3750706709101987\n",
      "2022-03-26 17:02:25.791050 Epoch 50, Training Loss 0.37613355755196204\n",
      "2022-03-26 17:02:25.815319 Epoch 50, Training Loss 0.3771748152535285\n",
      "2022-03-26 17:02:25.845444 Epoch 50, Training Loss 0.37847649761478\n",
      "2022-03-26 17:02:25.870148 Epoch 50, Training Loss 0.37986280226036717\n",
      "2022-03-26 17:02:25.893746 Epoch 50, Training Loss 0.3809193849106274\n",
      "2022-03-26 17:02:25.920705 Epoch 50, Training Loss 0.38179776224943685\n",
      "2022-03-26 17:02:25.943863 Epoch 50, Training Loss 0.3826812578131781\n",
      "2022-03-26 17:02:25.967400 Epoch 50, Training Loss 0.3834776087185306\n",
      "2022-03-26 17:02:25.990923 Epoch 50, Training Loss 0.3847570166258556\n",
      "2022-03-26 17:02:26.014193 Epoch 50, Training Loss 0.385792368513239\n",
      "2022-03-26 17:02:26.044037 Epoch 50, Training Loss 0.3868866636015265\n",
      "2022-03-26 17:02:26.067576 Epoch 50, Training Loss 0.38797281061292\n",
      "2022-03-26 17:02:26.090773 Epoch 50, Training Loss 0.3892948710552567\n",
      "2022-03-26 17:02:26.123011 Epoch 50, Training Loss 0.39029350914918554\n",
      "2022-03-26 17:02:26.146312 Epoch 50, Training Loss 0.39119849538864077\n",
      "2022-03-26 17:02:26.169751 Epoch 50, Training Loss 0.392102770640722\n",
      "2022-03-26 17:02:26.197419 Epoch 50, Training Loss 0.3929445522520548\n",
      "2022-03-26 17:02:26.220947 Epoch 50, Training Loss 0.39428683162650185\n",
      "2022-03-26 17:02:26.243169 Epoch 50, Training Loss 0.3951688994989371\n",
      "2022-03-26 17:02:26.272322 Epoch 50, Training Loss 0.39639128336821067\n",
      "2022-03-26 17:02:26.295642 Epoch 50, Training Loss 0.39744804368909364\n",
      "2022-03-26 17:02:26.318663 Epoch 50, Training Loss 0.39846783167565875\n",
      "2022-03-26 17:02:26.347504 Epoch 50, Training Loss 0.3995548405915575\n",
      "2022-03-26 17:02:26.369715 Epoch 50, Training Loss 0.40051806689528247\n",
      "2022-03-26 17:02:26.392598 Epoch 50, Training Loss 0.40131237073932463\n",
      "2022-03-26 17:02:26.415371 Epoch 50, Training Loss 0.40195714909097424\n",
      "2022-03-26 17:02:26.438421 Epoch 50, Training Loss 0.4027662624788406\n",
      "2022-03-26 17:02:26.462633 Epoch 50, Training Loss 0.4036163183124474\n",
      "2022-03-26 17:02:26.492894 Epoch 50, Training Loss 0.40457235730212665\n",
      "2022-03-26 17:02:26.517337 Epoch 50, Training Loss 0.4057415381569387\n",
      "2022-03-26 17:02:26.541379 Epoch 50, Training Loss 0.4065848803123855\n",
      "2022-03-26 17:02:26.569014 Epoch 50, Training Loss 0.40772305585234364\n",
      "2022-03-26 17:02:26.591539 Epoch 50, Training Loss 0.40861188587935077\n",
      "2022-03-26 17:02:26.614766 Epoch 50, Training Loss 0.40989236590807393\n",
      "2022-03-26 17:02:26.638772 Epoch 50, Training Loss 0.4109586391912397\n",
      "2022-03-26 17:02:26.671735 Epoch 50, Training Loss 0.41211066343595304\n",
      "2022-03-26 17:02:26.695611 Epoch 50, Training Loss 0.41324364140515435\n",
      "2022-03-26 17:02:26.724172 Epoch 50, Training Loss 0.41432385882148354\n",
      "2022-03-26 17:02:26.747263 Epoch 50, Training Loss 0.41552629106489897\n",
      "2022-03-26 17:02:26.770616 Epoch 50, Training Loss 0.41637106617088515\n",
      "2022-03-26 17:02:26.797218 Epoch 50, Training Loss 0.4174031160981454\n",
      "2022-03-26 17:02:26.820349 Epoch 50, Training Loss 0.4183893996431395\n",
      "2022-03-26 17:02:26.844370 Epoch 50, Training Loss 0.4195547570352969\n",
      "2022-03-26 17:02:26.869207 Epoch 50, Training Loss 0.42090938387014676\n",
      "2022-03-26 17:02:26.892921 Epoch 50, Training Loss 0.4219238654426906\n",
      "2022-03-26 17:02:26.922419 Epoch 50, Training Loss 0.42283148510986585\n",
      "2022-03-26 17:02:26.946456 Epoch 50, Training Loss 0.4239397993325577\n",
      "2022-03-26 17:02:26.970082 Epoch 50, Training Loss 0.4250072515224252\n",
      "2022-03-26 17:02:26.993698 Epoch 50, Training Loss 0.4261422105457472\n",
      "2022-03-26 17:02:27.027245 Epoch 50, Training Loss 0.4275354912213962\n",
      "2022-03-26 17:02:27.051168 Epoch 50, Training Loss 0.42843367262264653\n",
      "2022-03-26 17:02:27.074602 Epoch 50, Training Loss 0.4293080418158675\n",
      "2022-03-26 17:02:27.101580 Epoch 50, Training Loss 0.4303770872485607\n",
      "2022-03-26 17:02:27.132307 Epoch 50, Training Loss 0.43169469395866783\n",
      "2022-03-26 17:02:27.156048 Epoch 50, Training Loss 0.4329638703704795\n",
      "2022-03-26 17:02:27.179073 Epoch 50, Training Loss 0.4343124495442871\n",
      "2022-03-26 17:02:27.207780 Epoch 50, Training Loss 0.43531634755756543\n",
      "2022-03-26 17:02:27.231262 Epoch 50, Training Loss 0.4363571340623109\n",
      "2022-03-26 17:02:27.254028 Epoch 50, Training Loss 0.43713952299883907\n",
      "2022-03-26 17:02:27.277316 Epoch 50, Training Loss 0.43810660675968355\n",
      "2022-03-26 17:02:27.300519 Epoch 50, Training Loss 0.4389518811117353\n",
      "2022-03-26 17:02:27.324018 Epoch 50, Training Loss 0.440176145652371\n",
      "2022-03-26 17:02:27.356985 Epoch 50, Training Loss 0.44144689709024354\n",
      "2022-03-26 17:02:27.381081 Epoch 50, Training Loss 0.4423803381450341\n",
      "2022-03-26 17:02:27.406648 Epoch 50, Training Loss 0.4434728342706285\n",
      "2022-03-26 17:02:27.430388 Epoch 50, Training Loss 0.4446269246318456\n",
      "2022-03-26 17:02:27.454377 Epoch 50, Training Loss 0.44546309715646615\n",
      "2022-03-26 17:02:27.477780 Epoch 50, Training Loss 0.44662315750975745\n",
      "2022-03-26 17:02:27.501072 Epoch 50, Training Loss 0.4479216978982891\n",
      "2022-03-26 17:02:27.524995 Epoch 50, Training Loss 0.4485772332114637\n",
      "2022-03-26 17:02:27.549714 Epoch 50, Training Loss 0.44976969379598225\n",
      "2022-03-26 17:02:27.580003 Epoch 50, Training Loss 0.4506808273170305\n",
      "2022-03-26 17:02:27.604661 Epoch 50, Training Loss 0.4520146175266227\n",
      "2022-03-26 17:02:27.629791 Epoch 50, Training Loss 0.4529905805502401\n",
      "2022-03-26 17:02:27.656253 Epoch 50, Training Loss 0.453841285098849\n",
      "2022-03-26 17:02:27.689659 Epoch 50, Training Loss 0.45473287546116375\n",
      "2022-03-26 17:02:27.713477 Epoch 50, Training Loss 0.4558910340299387\n",
      "2022-03-26 17:02:27.737486 Epoch 50, Training Loss 0.45725875696562746\n",
      "2022-03-26 17:02:27.761471 Epoch 50, Training Loss 0.458887271259142\n",
      "2022-03-26 17:02:27.789819 Epoch 50, Training Loss 0.45994405162608837\n",
      "2022-03-26 17:02:27.816574 Epoch 50, Training Loss 0.4609983872879497\n",
      "2022-03-26 17:02:27.840513 Epoch 50, Training Loss 0.46199215228295387\n",
      "2022-03-26 17:02:27.867134 Epoch 50, Training Loss 0.46302422507644614\n",
      "2022-03-26 17:02:27.894409 Epoch 50, Training Loss 0.46434057871703904\n",
      "2022-03-26 17:02:27.925740 Epoch 50, Training Loss 0.4652916618320338\n",
      "2022-03-26 17:02:27.950055 Epoch 50, Training Loss 0.46661365199881744\n",
      "2022-03-26 17:02:27.973816 Epoch 50, Training Loss 0.46790174069002155\n",
      "2022-03-26 17:02:28.006832 Epoch 50, Training Loss 0.4688463887900038\n",
      "2022-03-26 17:02:28.031764 Epoch 50, Training Loss 0.46975225027259965\n",
      "2022-03-26 17:02:28.055807 Epoch 50, Training Loss 0.4707559636791649\n",
      "2022-03-26 17:02:28.081645 Epoch 50, Training Loss 0.47170097215096357\n",
      "2022-03-26 17:02:28.105750 Epoch 50, Training Loss 0.47267227938108125\n",
      "2022-03-26 17:02:28.129767 Epoch 50, Training Loss 0.4740275104942224\n",
      "2022-03-26 17:02:28.153000 Epoch 50, Training Loss 0.4750635935674848\n",
      "2022-03-26 17:02:28.176411 Epoch 50, Training Loss 0.4761060461821154\n",
      "2022-03-26 17:02:28.200961 Epoch 50, Training Loss 0.477371253320933\n",
      "2022-03-26 17:02:28.233056 Epoch 50, Training Loss 0.4784193736360506\n",
      "2022-03-26 17:02:28.257915 Epoch 50, Training Loss 0.47939547812542344\n",
      "2022-03-26 17:02:28.282514 Epoch 50, Training Loss 0.480161980061275\n",
      "2022-03-26 17:02:28.308349 Epoch 50, Training Loss 0.48154001017970505\n",
      "2022-03-26 17:02:28.334428 Epoch 50, Training Loss 0.48228261568357267\n",
      "2022-03-26 17:02:28.358415 Epoch 50, Training Loss 0.483330126003841\n",
      "2022-03-26 17:02:28.381721 Epoch 50, Training Loss 0.48431094253764434\n",
      "2022-03-26 17:02:28.404539 Epoch 50, Training Loss 0.4854525102069006\n",
      "2022-03-26 17:02:28.428431 Epoch 50, Training Loss 0.48654908155236404\n",
      "2022-03-26 17:02:28.458390 Epoch 50, Training Loss 0.48742030618135884\n",
      "2022-03-26 17:02:28.482134 Epoch 50, Training Loss 0.4883846835712033\n",
      "2022-03-26 17:02:28.505191 Epoch 50, Training Loss 0.48940765446104356\n",
      "2022-03-26 17:02:28.531624 Epoch 50, Training Loss 0.49021690443653587\n",
      "2022-03-26 17:02:28.554477 Epoch 50, Training Loss 0.49128262168915987\n",
      "2022-03-26 17:02:28.577508 Epoch 50, Training Loss 0.492546280555408\n",
      "2022-03-26 17:02:28.600360 Epoch 50, Training Loss 0.4934359469529613\n",
      "2022-03-26 17:02:28.623610 Epoch 50, Training Loss 0.4943918463824045\n",
      "2022-03-26 17:02:28.647408 Epoch 50, Training Loss 0.495345481292671\n",
      "2022-03-26 17:02:28.676777 Epoch 50, Training Loss 0.49625457003903206\n",
      "2022-03-26 17:02:28.708292 Epoch 50, Training Loss 0.4971924282400809\n",
      "2022-03-26 17:02:28.733229 Epoch 50, Training Loss 0.4984869380741168\n",
      "2022-03-26 17:02:28.760303 Epoch 50, Training Loss 0.49945407945786596\n",
      "2022-03-26 17:02:28.783500 Epoch 50, Training Loss 0.5004758016227762\n",
      "2022-03-26 17:02:28.807275 Epoch 50, Training Loss 0.5014008320201083\n",
      "2022-03-26 17:02:28.830747 Epoch 50, Training Loss 0.5026022156943446\n",
      "2022-03-26 17:02:28.854419 Epoch 50, Training Loss 0.5037153918877282\n",
      "2022-03-26 17:02:28.879246 Epoch 50, Training Loss 0.504739205779322\n",
      "2022-03-26 17:02:28.907669 Epoch 50, Training Loss 0.5058634847478793\n",
      "2022-03-26 17:02:28.930383 Epoch 50, Training Loss 0.507030082099578\n",
      "2022-03-26 17:02:28.953480 Epoch 50, Training Loss 0.5083052756079017\n",
      "2022-03-26 17:02:28.980324 Epoch 50, Training Loss 0.5091680246393394\n",
      "2022-03-26 17:02:29.003506 Epoch 50, Training Loss 0.5100831843702994\n",
      "2022-03-26 17:02:29.026480 Epoch 50, Training Loss 0.5110281038924557\n",
      "2022-03-26 17:02:29.050047 Epoch 50, Training Loss 0.5120885319569531\n",
      "2022-03-26 17:02:29.073321 Epoch 50, Training Loss 0.5132004362542916\n",
      "2022-03-26 17:02:29.102383 Epoch 50, Training Loss 0.5142198392497305\n",
      "2022-03-26 17:02:29.132381 Epoch 50, Training Loss 0.5150567021058954\n",
      "2022-03-26 17:02:29.155817 Epoch 50, Training Loss 0.5162210560515713\n",
      "2022-03-26 17:02:29.180739 Epoch 50, Training Loss 0.5172687141639193\n",
      "2022-03-26 17:02:29.205965 Epoch 50, Training Loss 0.5185173977061611\n",
      "2022-03-26 17:02:29.231909 Epoch 50, Training Loss 0.5199000510718207\n",
      "2022-03-26 17:02:29.254842 Epoch 50, Training Loss 0.5209026699480803\n",
      "2022-03-26 17:02:29.278201 Epoch 50, Training Loss 0.5220385510140978\n",
      "2022-03-26 17:02:29.300997 Epoch 50, Training Loss 0.5234486865418037\n",
      "2022-03-26 17:02:29.332118 Epoch 50, Training Loss 0.5249257182983487\n",
      "2022-03-26 17:02:29.355623 Epoch 50, Training Loss 0.5258498741385272\n",
      "2022-03-26 17:02:29.378259 Epoch 50, Training Loss 0.5266970011126965\n",
      "2022-03-26 17:02:29.404679 Epoch 50, Training Loss 0.5283222810539139\n",
      "2022-03-26 17:02:29.427574 Epoch 50, Training Loss 0.5294982867929942\n",
      "2022-03-26 17:02:29.450944 Epoch 50, Training Loss 0.5304409144327159\n",
      "2022-03-26 17:02:29.473475 Epoch 50, Training Loss 0.5316907784060749\n",
      "2022-03-26 17:02:29.495762 Epoch 50, Training Loss 0.5329487361871373\n",
      "2022-03-26 17:02:29.518520 Epoch 50, Training Loss 0.533951917039159\n",
      "2022-03-26 17:02:29.548273 Epoch 50, Training Loss 0.534998603793971\n",
      "2022-03-26 17:02:29.572133 Epoch 50, Training Loss 0.5359165075489932\n",
      "2022-03-26 17:02:29.594436 Epoch 50, Training Loss 0.5371297073486211\n",
      "2022-03-26 17:02:29.619252 Epoch 50, Training Loss 0.5382796474125074\n",
      "2022-03-26 17:02:29.642534 Epoch 50, Training Loss 0.5391626839747514\n",
      "2022-03-26 17:02:29.665465 Epoch 50, Training Loss 0.5402150947572021\n",
      "2022-03-26 17:02:29.688838 Epoch 50, Training Loss 0.5412007303494016\n",
      "2022-03-26 17:02:29.711378 Epoch 50, Training Loss 0.5421342759028726\n",
      "2022-03-26 17:02:29.744992 Epoch 50, Training Loss 0.5434211834769724\n",
      "2022-03-26 17:02:29.774495 Epoch 50, Training Loss 0.5443947934890951\n",
      "2022-03-26 17:02:29.797596 Epoch 50, Training Loss 0.5454226104957064\n",
      "2022-03-26 17:02:29.820245 Epoch 50, Training Loss 0.5467763618587533\n",
      "2022-03-26 17:02:29.847143 Epoch 50, Training Loss 0.5478994943906584\n",
      "2022-03-26 17:02:29.869756 Epoch 50, Training Loss 0.5490192084208779\n",
      "2022-03-26 17:02:29.892963 Epoch 50, Training Loss 0.549779585712706\n",
      "2022-03-26 17:02:29.915611 Epoch 50, Training Loss 0.5509159325638695\n",
      "2022-03-26 17:02:29.938450 Epoch 50, Training Loss 0.5522127021913943\n",
      "2022-03-26 17:02:29.960808 Epoch 50, Training Loss 0.5531192682588192\n",
      "2022-03-26 17:02:29.990107 Epoch 50, Training Loss 0.554537237452729\n",
      "2022-03-26 17:02:30.015407 Epoch 50, Training Loss 0.5555052412745288\n",
      "2022-03-26 17:02:30.044419 Epoch 50, Training Loss 0.5564057846813251\n",
      "2022-03-26 17:02:30.072221 Epoch 50, Training Loss 0.5576106687945783\n",
      "2022-03-26 17:02:30.098437 Epoch 50, Training Loss 0.5586699798436421\n",
      "2022-03-26 17:02:30.122640 Epoch 50, Training Loss 0.5595298223483288\n",
      "2022-03-26 17:02:30.150071 Epoch 50, Training Loss 0.560565165241661\n",
      "2022-03-26 17:02:30.173370 Epoch 50, Training Loss 0.5615524949167695\n",
      "2022-03-26 17:02:30.203589 Epoch 50, Training Loss 0.5626014910085732\n",
      "2022-03-26 17:02:30.233975 Epoch 50, Training Loss 0.5636677194739241\n",
      "2022-03-26 17:02:30.257422 Epoch 50, Training Loss 0.5649499867273413\n",
      "2022-03-26 17:02:30.281310 Epoch 50, Training Loss 0.5658924717579961\n",
      "2022-03-26 17:02:30.309553 Epoch 50, Training Loss 0.5667285913091791\n",
      "2022-03-26 17:02:30.336432 Epoch 50, Training Loss 0.5677052481704966\n",
      "2022-03-26 17:02:30.359892 Epoch 50, Training Loss 0.5684813142127698\n",
      "2022-03-26 17:02:30.382760 Epoch 50, Training Loss 0.5693695664863148\n",
      "2022-03-26 17:02:30.405093 Epoch 50, Training Loss 0.5703116119517695\n",
      "2022-03-26 17:02:30.434098 Epoch 50, Training Loss 0.5714821311671411\n",
      "2022-03-26 17:02:30.457485 Epoch 50, Training Loss 0.572737400565306\n",
      "2022-03-26 17:02:30.482079 Epoch 50, Training Loss 0.5737835099477597\n",
      "2022-03-26 17:02:30.510043 Epoch 50, Training Loss 0.5750407964524711\n",
      "2022-03-26 17:02:30.533174 Epoch 50, Training Loss 0.5759100970404837\n",
      "2022-03-26 17:02:30.556585 Epoch 50, Training Loss 0.5769594169943534\n",
      "2022-03-26 17:02:30.580758 Epoch 50, Training Loss 0.5778952816410747\n",
      "2022-03-26 17:02:30.605022 Epoch 50, Training Loss 0.578921471486616\n",
      "2022-03-26 17:02:30.629513 Epoch 50, Training Loss 0.5797300103222928\n",
      "2022-03-26 17:02:30.660405 Epoch 50, Training Loss 0.5807894694683192\n",
      "2022-03-26 17:02:30.684875 Epoch 50, Training Loss 0.5817175477057162\n",
      "2022-03-26 17:02:30.708542 Epoch 50, Training Loss 0.5829143905273789\n",
      "2022-03-26 17:02:30.731711 Epoch 50, Training Loss 0.5839517445820371\n",
      "2022-03-26 17:02:30.769688 Epoch 50, Training Loss 0.5851027659900353\n",
      "2022-03-26 17:02:30.793256 Epoch 50, Training Loss 0.5863010614271968\n",
      "2022-03-26 17:02:30.816988 Epoch 50, Training Loss 0.5872855102619552\n",
      "2022-03-26 17:02:30.840732 Epoch 50, Training Loss 0.5882735487140353\n",
      "2022-03-26 17:02:30.867094 Epoch 50, Training Loss 0.5891300037388911\n",
      "2022-03-26 17:02:30.894720 Epoch 50, Training Loss 0.590107652613574\n",
      "2022-03-26 17:02:30.919460 Epoch 50, Training Loss 0.5914981451333331\n",
      "2022-03-26 17:02:30.953497 Epoch 50, Training Loss 0.5925566487757447\n",
      "2022-03-26 17:02:30.977695 Epoch 50, Training Loss 0.5935248337743227\n",
      "2022-03-26 17:02:31.001735 Epoch 50, Training Loss 0.5948682703325511\n",
      "2022-03-26 17:02:31.030306 Epoch 50, Training Loss 0.5961799463042823\n",
      "2022-03-26 17:02:31.054825 Epoch 50, Training Loss 0.5971003508628787\n",
      "2022-03-26 17:02:31.085007 Epoch 50, Training Loss 0.5981702855633347\n",
      "2022-03-26 17:02:31.110394 Epoch 50, Training Loss 0.5993385755497477\n",
      "2022-03-26 17:02:31.133802 Epoch 50, Training Loss 0.6003196282917277\n",
      "2022-03-26 17:02:31.162058 Epoch 50, Training Loss 0.6011990511508853\n",
      "2022-03-26 17:02:31.185663 Epoch 50, Training Loss 0.601985268961743\n",
      "2022-03-26 17:02:31.208916 Epoch 50, Training Loss 0.6030801414986096\n",
      "2022-03-26 17:02:31.231977 Epoch 50, Training Loss 0.6041059985642543\n",
      "2022-03-26 17:02:31.255204 Epoch 50, Training Loss 0.605300569854429\n",
      "2022-03-26 17:02:31.279061 Epoch 50, Training Loss 0.6064389040098166\n",
      "2022-03-26 17:02:31.310428 Epoch 50, Training Loss 0.6078552977203409\n",
      "2022-03-26 17:02:31.337387 Epoch 50, Training Loss 0.6088158328209996\n",
      "2022-03-26 17:02:31.360486 Epoch 50, Training Loss 0.6099716052984643\n",
      "2022-03-26 17:02:31.386943 Epoch 50, Training Loss 0.6111424168205017\n",
      "2022-03-26 17:02:31.409778 Epoch 50, Training Loss 0.6120547364892253\n",
      "2022-03-26 17:02:31.433383 Epoch 50, Training Loss 0.6129038252336595\n",
      "2022-03-26 17:02:31.456475 Epoch 50, Training Loss 0.6137052516803108\n",
      "2022-03-26 17:02:31.479334 Epoch 50, Training Loss 0.6146704989778417\n",
      "2022-03-26 17:02:31.502026 Epoch 50, Training Loss 0.6154804305957101\n",
      "2022-03-26 17:02:31.531675 Epoch 50, Training Loss 0.6167126983937705\n",
      "2022-03-26 17:02:31.555434 Epoch 50, Training Loss 0.617767875456749\n",
      "2022-03-26 17:02:31.578900 Epoch 50, Training Loss 0.6188336602409782\n",
      "2022-03-26 17:02:31.605452 Epoch 50, Training Loss 0.6196106243164033\n",
      "2022-03-26 17:02:31.628144 Epoch 50, Training Loss 0.6209042121839645\n",
      "2022-03-26 17:02:31.650777 Epoch 50, Training Loss 0.6220820187912572\n",
      "2022-03-26 17:02:31.673108 Epoch 50, Training Loss 0.6233900956180699\n",
      "2022-03-26 17:02:31.695834 Epoch 50, Training Loss 0.6244474389516484\n",
      "2022-03-26 17:02:31.730921 Epoch 50, Training Loss 0.625396217302898\n",
      "2022-03-26 17:02:31.761973 Epoch 50, Training Loss 0.6263631993090101\n",
      "2022-03-26 17:02:31.794400 Epoch 50, Training Loss 0.6270274964287458\n",
      "2022-03-26 17:02:31.817180 Epoch 50, Training Loss 0.6283214146371388\n",
      "2022-03-26 17:02:31.847186 Epoch 50, Training Loss 0.6295061253983042\n",
      "2022-03-26 17:02:31.871022 Epoch 50, Training Loss 0.6305258011878909\n",
      "2022-03-26 17:02:31.895047 Epoch 50, Training Loss 0.6315435351008345\n",
      "2022-03-26 17:02:31.919766 Epoch 50, Training Loss 0.6323443138233537\n",
      "2022-03-26 17:02:31.943358 Epoch 50, Training Loss 0.6335195743519327\n",
      "2022-03-26 17:02:31.972347 Epoch 50, Training Loss 0.6344409944761135\n",
      "2022-03-26 17:02:31.997041 Epoch 50, Training Loss 0.635705921503589\n",
      "2022-03-26 17:02:32.020530 Epoch 50, Training Loss 0.6366161517322521\n",
      "2022-03-26 17:02:32.045484 Epoch 50, Training Loss 0.6378342338535182\n",
      "2022-03-26 17:02:32.071259 Epoch 50, Training Loss 0.6387987882279984\n",
      "2022-03-26 17:02:32.094861 Epoch 50, Training Loss 0.6403897965655607\n",
      "2022-03-26 17:02:32.118520 Epoch 50, Training Loss 0.6416930434344065\n",
      "2022-03-26 17:02:32.141043 Epoch 50, Training Loss 0.6426640152931213\n",
      "2022-03-26 17:02:32.164112 Epoch 50, Training Loss 0.6436602721738693\n",
      "2022-03-26 17:02:32.194463 Epoch 50, Training Loss 0.6443719766329011\n",
      "2022-03-26 17:02:32.218102 Epoch 50, Training Loss 0.645313959825984\n",
      "2022-03-26 17:02:32.241490 Epoch 50, Training Loss 0.6466748788381171\n",
      "2022-03-26 17:02:32.269303 Epoch 50, Training Loss 0.6478526566339575\n",
      "2022-03-26 17:02:32.292909 Epoch 50, Training Loss 0.6491506506719857\n",
      "2022-03-26 17:02:32.315900 Epoch 50, Training Loss 0.6500170890937376\n",
      "2022-03-26 17:02:32.342046 Epoch 50, Training Loss 0.6509796119559451\n",
      "2022-03-26 17:02:32.365270 Epoch 50, Training Loss 0.6519797494649278\n",
      "2022-03-26 17:02:32.388551 Epoch 50, Training Loss 0.6530643039957031\n",
      "2022-03-26 17:02:32.416746 Epoch 50, Training Loss 0.6540317769398165\n",
      "2022-03-26 17:02:32.439833 Epoch 50, Training Loss 0.6555002321825003\n",
      "2022-03-26 17:02:32.462740 Epoch 50, Training Loss 0.6565347931269184\n",
      "2022-03-26 17:02:32.489759 Epoch 50, Training Loss 0.6574630895081688\n",
      "2022-03-26 17:02:32.512546 Epoch 50, Training Loss 0.6584323449512882\n",
      "2022-03-26 17:02:32.535592 Epoch 50, Training Loss 0.6597170233726501\n",
      "2022-03-26 17:02:32.558659 Epoch 50, Training Loss 0.6606143547598359\n",
      "2022-03-26 17:02:32.581635 Epoch 50, Training Loss 0.661889573725898\n",
      "2022-03-26 17:02:32.604731 Epoch 50, Training Loss 0.6628078392247105\n",
      "2022-03-26 17:02:32.635032 Epoch 50, Training Loss 0.6638144531365856\n",
      "2022-03-26 17:02:32.658746 Epoch 50, Training Loss 0.6648536509717516\n",
      "2022-03-26 17:02:32.681809 Epoch 50, Training Loss 0.6657463006503747\n",
      "2022-03-26 17:02:32.708105 Epoch 50, Training Loss 0.6666766572029085\n",
      "2022-03-26 17:02:32.730730 Epoch 50, Training Loss 0.6677589678703366\n",
      "2022-03-26 17:02:32.753275 Epoch 50, Training Loss 0.6685463437034042\n",
      "2022-03-26 17:02:32.775767 Epoch 50, Training Loss 0.6697811821232671\n",
      "2022-03-26 17:02:32.807228 Epoch 50, Training Loss 0.6706378160382781\n",
      "2022-03-26 17:02:32.830439 Epoch 50, Training Loss 0.6715056677456097\n",
      "2022-03-26 17:02:32.859161 Epoch 50, Training Loss 0.6726236605583249\n",
      "2022-03-26 17:02:32.884472 Epoch 50, Training Loss 0.6738085525724894\n",
      "2022-03-26 17:02:32.907363 Epoch 50, Training Loss 0.6748543094338664\n",
      "2022-03-26 17:02:32.930262 Epoch 50, Training Loss 0.6762674976035458\n",
      "2022-03-26 17:02:32.957251 Epoch 50, Training Loss 0.6772736107449397\n",
      "2022-03-26 17:02:32.980004 Epoch 50, Training Loss 0.678104383027767\n",
      "2022-03-26 17:02:33.003465 Epoch 50, Training Loss 0.6791263122845184\n",
      "2022-03-26 17:02:33.026681 Epoch 50, Training Loss 0.6802881999546305\n",
      "2022-03-26 17:02:33.057001 Epoch 50, Training Loss 0.6813781711146655\n",
      "2022-03-26 17:02:33.090431 Epoch 50, Training Loss 0.6826420991164645\n",
      "2022-03-26 17:02:33.115706 Epoch 50, Training Loss 0.6834964387861969\n",
      "2022-03-26 17:02:33.138452 Epoch 50, Training Loss 0.6846985036454847\n",
      "2022-03-26 17:02:33.165543 Epoch 50, Training Loss 0.6858722203985199\n",
      "2022-03-26 17:02:33.188834 Epoch 50, Training Loss 0.6870040334856419\n",
      "2022-03-26 17:02:33.213302 Epoch 50, Training Loss 0.6882419271389847\n",
      "2022-03-26 17:02:33.236563 Epoch 50, Training Loss 0.6891748838107604\n",
      "2022-03-26 17:02:33.260139 Epoch 50, Training Loss 0.6902240587164984\n",
      "2022-03-26 17:02:33.284314 Epoch 50, Training Loss 0.6912216883333747\n",
      "2022-03-26 17:02:33.315007 Epoch 50, Training Loss 0.6925047219104474\n",
      "2022-03-26 17:02:33.341587 Epoch 50, Training Loss 0.6935032393468921\n",
      "2022-03-26 17:02:33.366238 Epoch 50, Training Loss 0.694467329994187\n",
      "2022-03-26 17:02:33.392328 Epoch 50, Training Loss 0.6956062137775714\n",
      "2022-03-26 17:02:33.416071 Epoch 50, Training Loss 0.6965538698541539\n",
      "2022-03-26 17:02:33.440310 Epoch 50, Training Loss 0.6975702242473202\n",
      "2022-03-26 17:02:33.464568 Epoch 50, Training Loss 0.6990536942201502\n",
      "2022-03-26 17:02:33.488532 Epoch 50, Training Loss 0.7000747540265398\n",
      "2022-03-26 17:02:33.512092 Epoch 50, Training Loss 0.7011942042566627\n",
      "2022-03-26 17:02:33.543058 Epoch 50, Training Loss 0.7021239206309209\n",
      "2022-03-26 17:02:33.566601 Epoch 50, Training Loss 0.7028737221380024\n",
      "2022-03-26 17:02:33.591481 Epoch 50, Training Loss 0.7039348845896514\n",
      "2022-03-26 17:02:33.615268 Epoch 50, Training Loss 0.7049648117683732\n",
      "2022-03-26 17:02:33.638743 Epoch 50, Training Loss 0.7061301370715851\n",
      "2022-03-26 17:02:33.662297 Epoch 50, Training Loss 0.707598369292286\n",
      "2022-03-26 17:02:33.685633 Epoch 50, Training Loss 0.7089074185437254\n",
      "2022-03-26 17:02:33.708573 Epoch 50, Training Loss 0.709848610793843\n",
      "2022-03-26 17:02:33.731371 Epoch 50, Training Loss 0.7106730849541667\n",
      "2022-03-26 17:02:33.761122 Epoch 50, Training Loss 0.7116008407776923\n",
      "2022-03-26 17:02:33.785480 Epoch 50, Training Loss 0.7128156281035879\n",
      "2022-03-26 17:02:33.811659 Epoch 50, Training Loss 0.7138644540706254\n",
      "2022-03-26 17:02:33.845277 Epoch 50, Training Loss 0.7148560667434312\n",
      "2022-03-26 17:02:33.869235 Epoch 50, Training Loss 0.7163009717488837\n",
      "2022-03-26 17:02:33.894381 Epoch 50, Training Loss 0.7174349926469271\n",
      "2022-03-26 17:02:33.917651 Epoch 50, Training Loss 0.7183889084307434\n",
      "2022-03-26 17:02:33.940472 Epoch 50, Training Loss 0.7192469239234924\n",
      "2022-03-26 17:02:33.976251 Epoch 50, Training Loss 0.7203853044211103\n",
      "2022-03-26 17:02:34.001088 Epoch 50, Training Loss 0.7213552091889979\n",
      "2022-03-26 17:02:34.024742 Epoch 50, Training Loss 0.7221079768274751\n",
      "2022-03-26 17:02:34.052810 Epoch 50, Training Loss 0.7232159840328919\n",
      "2022-03-26 17:02:34.076353 Epoch 50, Training Loss 0.7243051609724683\n",
      "2022-03-26 17:02:34.099877 Epoch 50, Training Loss 0.7252691943017419\n",
      "2022-03-26 17:02:34.123846 Epoch 50, Training Loss 0.7261205205069784\n",
      "2022-03-26 17:02:34.147381 Epoch 50, Training Loss 0.7272004715317045\n",
      "2022-03-26 17:02:34.170604 Epoch 50, Training Loss 0.7282672009961989\n",
      "2022-03-26 17:02:34.203874 Epoch 50, Training Loss 0.7291067323416395\n",
      "2022-03-26 17:02:34.228053 Epoch 50, Training Loss 0.7301934436154183\n",
      "2022-03-26 17:02:34.250930 Epoch 50, Training Loss 0.7312993106939604\n",
      "2022-03-26 17:02:34.278213 Epoch 50, Training Loss 0.7325345375348845\n",
      "2022-03-26 17:02:34.301479 Epoch 50, Training Loss 0.7336744368076324\n",
      "2022-03-26 17:02:34.324832 Epoch 50, Training Loss 0.7347806761484317\n",
      "2022-03-26 17:02:34.350568 Epoch 50, Training Loss 0.735809317101603\n",
      "2022-03-26 17:02:34.375211 Epoch 50, Training Loss 0.7367415900730416\n",
      "2022-03-26 17:02:34.398656 Epoch 50, Training Loss 0.7380430282229353\n",
      "2022-03-26 17:02:34.428404 Epoch 50, Training Loss 0.7391564740091944\n",
      "2022-03-26 17:02:34.453173 Epoch 50, Training Loss 0.7400570787737132\n",
      "2022-03-26 17:02:34.477939 Epoch 50, Training Loss 0.7412576564894918\n",
      "2022-03-26 17:02:34.506544 Epoch 50, Training Loss 0.7422532752499251\n",
      "2022-03-26 17:02:34.530492 Epoch 50, Training Loss 0.743254133731203\n",
      "2022-03-26 17:02:34.554522 Epoch 50, Training Loss 0.7440784922646134\n",
      "2022-03-26 17:02:34.577080 Epoch 50, Training Loss 0.7452382052036197\n",
      "2022-03-26 17:02:34.600343 Epoch 50, Training Loss 0.7464626563327087\n",
      "2022-03-26 17:02:34.623639 Epoch 50, Training Loss 0.7476301844162709\n",
      "2022-03-26 17:02:34.653817 Epoch 50, Training Loss 0.7487317494419224\n",
      "2022-03-26 17:02:34.677723 Epoch 50, Training Loss 0.7498986368898846\n",
      "2022-03-26 17:02:34.701210 Epoch 50, Training Loss 0.7508092475364275\n",
      "2022-03-26 17:02:34.728398 Epoch 50, Training Loss 0.7518158217372797\n",
      "2022-03-26 17:02:34.751278 Epoch 50, Training Loss 0.7530575252097585\n",
      "2022-03-26 17:02:34.774128 Epoch 50, Training Loss 0.7539881863405028\n",
      "2022-03-26 17:02:34.796985 Epoch 50, Training Loss 0.7551001894961843\n",
      "2022-03-26 17:02:34.819829 Epoch 50, Training Loss 0.756081004886676\n",
      "2022-03-26 17:02:34.849886 Epoch 50, Training Loss 0.7569695515248477\n",
      "2022-03-26 17:02:34.882378 Epoch 50, Training Loss 0.7582424339431021\n",
      "2022-03-26 17:02:34.908196 Epoch 50, Training Loss 0.7594352228104916\n",
      "2022-03-26 17:02:34.930732 Epoch 50, Training Loss 0.7609785020046527\n",
      "2022-03-26 17:02:34.954567 Epoch 50, Training Loss 0.7618484749361072\n",
      "2022-03-26 17:02:34.982690 Epoch 50, Training Loss 0.7628373799421598\n",
      "2022-03-26 17:02:35.006619 Epoch 50, Training Loss 0.7643636379705365\n",
      "2022-03-26 17:02:35.029578 Epoch 50, Training Loss 0.765609351570344\n",
      "2022-03-26 17:02:35.052447 Epoch 50, Training Loss 0.7663656667522762\n",
      "2022-03-26 17:02:35.076754 Epoch 50, Training Loss 0.7673879925094907\n",
      "2022-03-26 17:02:35.107430 Epoch 50, Training Loss 0.7685135090747452\n",
      "2022-03-26 17:02:35.131187 Epoch 50, Training Loss 0.7694513925810909\n",
      "2022-03-26 17:02:35.155423 Epoch 50, Training Loss 0.7703494432637149\n",
      "2022-03-26 17:02:35.181127 Epoch 50, Training Loss 0.771342960083881\n",
      "2022-03-26 17:02:35.204442 Epoch 50, Training Loss 0.7724195666935133\n",
      "2022-03-26 17:02:35.227443 Epoch 50, Training Loss 0.7732834396764751\n",
      "2022-03-26 17:02:35.250796 Epoch 50, Training Loss 0.7742418721508797\n",
      "2022-03-26 17:02:35.274048 Epoch 50, Training Loss 0.7753437479286243\n",
      "2022-03-26 17:02:35.297307 Epoch 50, Training Loss 0.7767929859326014\n",
      "2022-03-26 17:02:35.331777 Epoch 50, Training Loss 0.7777945136322695\n",
      "2022-03-26 17:02:35.355847 Epoch 50, Training Loss 0.7788551341542198\n",
      "2022-03-26 17:02:35.378544 Epoch 50, Training Loss 0.7800680764800753\n",
      "2022-03-26 17:02:35.404868 Epoch 50, Training Loss 0.781258098990716\n",
      "2022-03-26 17:02:35.427875 Epoch 50, Training Loss 0.7823468911678285\n",
      "2022-03-26 17:02:35.451120 Epoch 50, Training Loss 0.7832753581311697\n",
      "2022-03-26 17:02:35.475304 Epoch 50, Training Loss 0.7842523983829771\n",
      "2022-03-26 17:02:35.499658 Epoch 50, Training Loss 0.7850646976467288\n",
      "2022-03-26 17:02:35.523309 Epoch 50, Training Loss 0.7864376224977586\n",
      "2022-03-26 17:02:35.553460 Epoch 50, Training Loss 0.7873465382229642\n",
      "2022-03-26 17:02:35.576471 Epoch 50, Training Loss 0.7882501240581503\n",
      "2022-03-26 17:02:35.599388 Epoch 50, Training Loss 0.789131335513976\n",
      "2022-03-26 17:02:35.627126 Epoch 50, Training Loss 0.7902417139476522\n",
      "2022-03-26 17:02:35.651022 Epoch 50, Training Loss 0.7913986035167714\n",
      "2022-03-26 17:02:35.675217 Epoch 50, Training Loss 0.7924862885871506\n",
      "2022-03-26 17:02:35.699333 Epoch 50, Training Loss 0.7933564330153453\n",
      "2022-03-26 17:02:35.722712 Epoch 50, Training Loss 0.794319286447047\n",
      "2022-03-26 17:02:35.746307 Epoch 50, Training Loss 0.7952587211223514\n",
      "2022-03-26 17:02:35.776653 Epoch 50, Training Loss 0.7962618576901038\n",
      "2022-03-26 17:02:35.800841 Epoch 50, Training Loss 0.7973274621359833\n",
      "2022-03-26 17:02:35.824161 Epoch 50, Training Loss 0.7983683452886694\n",
      "2022-03-26 17:02:35.847838 Epoch 50, Training Loss 0.7992781555408712\n",
      "2022-03-26 17:02:35.880773 Epoch 50, Training Loss 0.8003881165133718\n",
      "2022-03-26 17:02:35.907964 Epoch 50, Training Loss 0.8012556395567286\n",
      "2022-03-26 17:02:35.931087 Epoch 50, Training Loss 0.8024546352341352\n",
      "2022-03-26 17:02:35.954037 Epoch 50, Training Loss 0.8037413504270031\n",
      "2022-03-26 17:02:35.977466 Epoch 50, Training Loss 0.8050103766838913\n",
      "2022-03-26 17:02:36.009678 Epoch 50, Training Loss 0.8061789827578513\n",
      "2022-03-26 17:02:36.033926 Epoch 50, Training Loss 0.8070326168518847\n",
      "2022-03-26 17:02:36.066970 Epoch 50, Training Loss 0.8081862155128928\n",
      "2022-03-26 17:02:36.091611 Epoch 50, Training Loss 0.8092573586174899\n",
      "2022-03-26 17:02:36.115073 Epoch 50, Training Loss 0.8101897859359946\n",
      "2022-03-26 17:02:36.145082 Epoch 50, Training Loss 0.811391357570658\n",
      "2022-03-26 17:02:36.168310 Epoch 50, Training Loss 0.8124992235389816\n",
      "2022-03-26 17:02:36.192062 Epoch 50, Training Loss 0.8135751906563254\n",
      "2022-03-26 17:02:36.218835 Epoch 50, Training Loss 0.8146593940379979\n",
      "2022-03-26 17:02:36.245607 Epoch 50, Training Loss 0.8159162736000003\n",
      "2022-03-26 17:02:36.269825 Epoch 50, Training Loss 0.8171886314668924\n",
      "2022-03-26 17:02:36.293555 Epoch 50, Training Loss 0.8182796508150028\n",
      "2022-03-26 17:02:36.320772 Epoch 50, Training Loss 0.8193038531276576\n",
      "2022-03-26 17:02:36.347008 Epoch 50, Training Loss 0.8207671607241911\n",
      "2022-03-26 17:02:36.370832 Epoch 50, Training Loss 0.8219041133780614\n",
      "2022-03-26 17:02:36.394344 Epoch 50, Training Loss 0.8229466428232315\n",
      "2022-03-26 17:02:36.403238 Epoch 50, Training Loss 0.8239739708735815\n",
      "2022-03-26 17:17:16.067327 Epoch 100, Training Loss 0.0008576445262450392\n",
      "2022-03-26 17:17:16.091605 Epoch 100, Training Loss 0.001805203878666129\n",
      "2022-03-26 17:17:16.114724 Epoch 100, Training Loss 0.0026508032360954966\n",
      "2022-03-26 17:17:16.137917 Epoch 100, Training Loss 0.003278386028831267\n",
      "2022-03-26 17:17:16.161481 Epoch 100, Training Loss 0.004026202518311913\n",
      "2022-03-26 17:17:16.185385 Epoch 100, Training Loss 0.004748750373225688\n",
      "2022-03-26 17:17:16.209602 Epoch 100, Training Loss 0.005954260716352926\n",
      "2022-03-26 17:17:16.234446 Epoch 100, Training Loss 0.006640945096759845\n",
      "2022-03-26 17:17:16.258350 Epoch 100, Training Loss 0.007926510423040756\n",
      "2022-03-26 17:17:16.288744 Epoch 100, Training Loss 0.008722268102114157\n",
      "2022-03-26 17:17:16.312704 Epoch 100, Training Loss 0.009765290238363358\n",
      "2022-03-26 17:17:16.338738 Epoch 100, Training Loss 0.010813943412907593\n",
      "2022-03-26 17:17:16.362044 Epoch 100, Training Loss 0.011891561395981732\n",
      "2022-03-26 17:17:16.389426 Epoch 100, Training Loss 0.012814958077257552\n",
      "2022-03-26 17:17:16.413626 Epoch 100, Training Loss 0.013595560856182557\n",
      "2022-03-26 17:17:16.436795 Epoch 100, Training Loss 0.01469471052174678\n",
      "2022-03-26 17:17:16.459244 Epoch 100, Training Loss 0.016033729019067477\n",
      "2022-03-26 17:17:16.482111 Epoch 100, Training Loss 0.016768610843307222\n",
      "2022-03-26 17:17:16.510765 Epoch 100, Training Loss 0.01760967597937035\n",
      "2022-03-26 17:17:16.543755 Epoch 100, Training Loss 0.018684938892988903\n",
      "2022-03-26 17:17:16.566030 Epoch 100, Training Loss 0.019564272040296395\n",
      "2022-03-26 17:17:16.592900 Epoch 100, Training Loss 0.020275318561612493\n",
      "2022-03-26 17:17:16.617195 Epoch 100, Training Loss 0.02109369993819605\n",
      "2022-03-26 17:17:16.641139 Epoch 100, Training Loss 0.022069970588854817\n",
      "2022-03-26 17:17:16.664638 Epoch 100, Training Loss 0.022735166961274794\n",
      "2022-03-26 17:17:16.689135 Epoch 100, Training Loss 0.02338372426264731\n",
      "2022-03-26 17:17:16.714032 Epoch 100, Training Loss 0.0242641459188193\n",
      "2022-03-26 17:17:16.743318 Epoch 100, Training Loss 0.025025315906690514\n",
      "2022-03-26 17:17:16.766499 Epoch 100, Training Loss 0.025961574691030984\n",
      "2022-03-26 17:17:16.789793 Epoch 100, Training Loss 0.02695370665596574\n",
      "2022-03-26 17:17:16.813382 Epoch 100, Training Loss 0.028173851158917712\n",
      "2022-03-26 17:17:16.839771 Epoch 100, Training Loss 0.02877992307743453\n",
      "2022-03-26 17:17:16.862344 Epoch 100, Training Loss 0.029833567797985225\n",
      "2022-03-26 17:17:16.885241 Epoch 100, Training Loss 0.030643402691692342\n",
      "2022-03-26 17:17:16.907993 Epoch 100, Training Loss 0.031570837015995896\n",
      "2022-03-26 17:17:16.937485 Epoch 100, Training Loss 0.03250834772653897\n",
      "2022-03-26 17:17:16.963644 Epoch 100, Training Loss 0.03342242489385483\n",
      "2022-03-26 17:17:16.987377 Epoch 100, Training Loss 0.03393246157242514\n",
      "2022-03-26 17:17:17.011075 Epoch 100, Training Loss 0.034893823363591946\n",
      "2022-03-26 17:17:17.037250 Epoch 100, Training Loss 0.0356486691614551\n",
      "2022-03-26 17:17:17.070542 Epoch 100, Training Loss 0.036595641842583564\n",
      "2022-03-26 17:17:17.094852 Epoch 100, Training Loss 0.03728892960969139\n",
      "2022-03-26 17:17:17.118878 Epoch 100, Training Loss 0.03833000415273945\n",
      "2022-03-26 17:17:17.153938 Epoch 100, Training Loss 0.039423607385067075\n",
      "2022-03-26 17:17:17.178498 Epoch 100, Training Loss 0.040293004849682686\n",
      "2022-03-26 17:17:17.201930 Epoch 100, Training Loss 0.04107355122523539\n",
      "2022-03-26 17:17:17.225625 Epoch 100, Training Loss 0.0418317909054744\n",
      "2022-03-26 17:17:17.253159 Epoch 100, Training Loss 0.04258666749652999\n",
      "2022-03-26 17:17:17.276708 Epoch 100, Training Loss 0.043460037816515966\n",
      "2022-03-26 17:17:17.300864 Epoch 100, Training Loss 0.0443685472087787\n",
      "2022-03-26 17:17:17.324571 Epoch 100, Training Loss 0.045126121938990814\n",
      "2022-03-26 17:17:17.351042 Epoch 100, Training Loss 0.04578169071308487\n",
      "2022-03-26 17:17:17.382107 Epoch 100, Training Loss 0.04677984922590768\n",
      "2022-03-26 17:17:17.405926 Epoch 100, Training Loss 0.04739232529002382\n",
      "2022-03-26 17:17:17.428567 Epoch 100, Training Loss 0.04818767587394666\n",
      "2022-03-26 17:17:17.455015 Epoch 100, Training Loss 0.049324363431967125\n",
      "2022-03-26 17:17:17.478041 Epoch 100, Training Loss 0.050048800571190426\n",
      "2022-03-26 17:17:17.501627 Epoch 100, Training Loss 0.05114348202257815\n",
      "2022-03-26 17:17:17.524944 Epoch 100, Training Loss 0.05215424485981007\n",
      "2022-03-26 17:17:17.556187 Epoch 100, Training Loss 0.05300631901949568\n",
      "2022-03-26 17:17:17.579491 Epoch 100, Training Loss 0.053955318258546506\n",
      "2022-03-26 17:17:17.614066 Epoch 100, Training Loss 0.05504539140197627\n",
      "2022-03-26 17:17:17.638134 Epoch 100, Training Loss 0.055905298465658025\n",
      "2022-03-26 17:17:17.662584 Epoch 100, Training Loss 0.0566965857201525\n",
      "2022-03-26 17:17:17.686752 Epoch 100, Training Loss 0.05754073551091392\n",
      "2022-03-26 17:17:17.712624 Epoch 100, Training Loss 0.058332632226712255\n",
      "2022-03-26 17:17:17.735979 Epoch 100, Training Loss 0.059169491500500826\n",
      "2022-03-26 17:17:17.759499 Epoch 100, Training Loss 0.06004046124722952\n",
      "2022-03-26 17:17:17.782974 Epoch 100, Training Loss 0.06108566024876617\n",
      "2022-03-26 17:17:17.806258 Epoch 100, Training Loss 0.06225685264600817\n",
      "2022-03-26 17:17:17.838668 Epoch 100, Training Loss 0.06322882688411362\n",
      "2022-03-26 17:17:17.862811 Epoch 100, Training Loss 0.06420484966481738\n",
      "2022-03-26 17:17:17.886120 Epoch 100, Training Loss 0.06490914485491145\n",
      "2022-03-26 17:17:17.909024 Epoch 100, Training Loss 0.06572696227399284\n",
      "2022-03-26 17:17:17.937174 Epoch 100, Training Loss 0.06653195341377308\n",
      "2022-03-26 17:17:17.968345 Epoch 100, Training Loss 0.06741421610650504\n",
      "2022-03-26 17:17:17.991929 Epoch 100, Training Loss 0.06830880179277161\n",
      "2022-03-26 17:17:18.015536 Epoch 100, Training Loss 0.06913807519409053\n",
      "2022-03-26 17:17:18.047187 Epoch 100, Training Loss 0.0700358044537132\n",
      "2022-03-26 17:17:18.075136 Epoch 100, Training Loss 0.07064431158782881\n",
      "2022-03-26 17:17:18.099406 Epoch 100, Training Loss 0.07157884785891189\n",
      "2022-03-26 17:17:18.123743 Epoch 100, Training Loss 0.07239779784246479\n",
      "2022-03-26 17:17:18.147700 Epoch 100, Training Loss 0.07332242533678898\n",
      "2022-03-26 17:17:18.177435 Epoch 100, Training Loss 0.07419527209628268\n",
      "2022-03-26 17:17:18.201918 Epoch 100, Training Loss 0.075094358542996\n",
      "2022-03-26 17:17:18.225884 Epoch 100, Training Loss 0.07592241713762893\n",
      "2022-03-26 17:17:18.249647 Epoch 100, Training Loss 0.07692024226078902\n",
      "2022-03-26 17:17:18.282961 Epoch 100, Training Loss 0.07799522475818234\n",
      "2022-03-26 17:17:18.307522 Epoch 100, Training Loss 0.07899231854302194\n",
      "2022-03-26 17:17:18.333735 Epoch 100, Training Loss 0.07994789952207404\n",
      "2022-03-26 17:17:18.359888 Epoch 100, Training Loss 0.08097801077396363\n",
      "2022-03-26 17:17:18.383441 Epoch 100, Training Loss 0.08179525851898486\n",
      "2022-03-26 17:17:18.407225 Epoch 100, Training Loss 0.08277360511862714\n",
      "2022-03-26 17:17:18.431475 Epoch 100, Training Loss 0.0834663982129158\n",
      "2022-03-26 17:17:18.454873 Epoch 100, Training Loss 0.08444867963376253\n",
      "2022-03-26 17:17:18.478721 Epoch 100, Training Loss 0.08542661974802042\n",
      "2022-03-26 17:17:18.508719 Epoch 100, Training Loss 0.08603799240211087\n",
      "2022-03-26 17:17:18.532115 Epoch 100, Training Loss 0.08711436143159257\n",
      "2022-03-26 17:17:18.556430 Epoch 100, Training Loss 0.08793467820605354\n",
      "2022-03-26 17:17:18.592242 Epoch 100, Training Loss 0.08859283238878031\n",
      "2022-03-26 17:17:18.615620 Epoch 100, Training Loss 0.08951957470468243\n",
      "2022-03-26 17:17:18.638996 Epoch 100, Training Loss 0.09031305856564466\n",
      "2022-03-26 17:17:18.662589 Epoch 100, Training Loss 0.09139253160990107\n",
      "2022-03-26 17:17:18.686148 Epoch 100, Training Loss 0.09223433913629683\n",
      "2022-03-26 17:17:18.711070 Epoch 100, Training Loss 0.09320498957201039\n",
      "2022-03-26 17:17:18.742083 Epoch 100, Training Loss 0.0942471578831563\n",
      "2022-03-26 17:17:18.765380 Epoch 100, Training Loss 0.09551360307599577\n",
      "2022-03-26 17:17:18.789584 Epoch 100, Training Loss 0.09620415192583333\n",
      "2022-03-26 17:17:18.814418 Epoch 100, Training Loss 0.09719934857562375\n",
      "2022-03-26 17:17:18.841184 Epoch 100, Training Loss 0.09828446523460281\n",
      "2022-03-26 17:17:18.872065 Epoch 100, Training Loss 0.09892592447645523\n",
      "2022-03-26 17:17:18.895990 Epoch 100, Training Loss 0.09956427852211096\n",
      "2022-03-26 17:17:18.919916 Epoch 100, Training Loss 0.1006075003567864\n",
      "2022-03-26 17:17:18.955379 Epoch 100, Training Loss 0.10160138936298888\n",
      "2022-03-26 17:17:18.979410 Epoch 100, Training Loss 0.1027013447583484\n",
      "2022-03-26 17:17:19.004016 Epoch 100, Training Loss 0.10352626862123494\n",
      "2022-03-26 17:17:19.031159 Epoch 100, Training Loss 0.10458564697324163\n",
      "2022-03-26 17:17:19.055416 Epoch 100, Training Loss 0.10551685369228159\n",
      "2022-03-26 17:17:19.080335 Epoch 100, Training Loss 0.10635493402285955\n",
      "2022-03-26 17:17:19.104651 Epoch 100, Training Loss 0.10736129838792259\n",
      "2022-03-26 17:17:19.128701 Epoch 100, Training Loss 0.10812891108910445\n",
      "2022-03-26 17:17:19.152936 Epoch 100, Training Loss 0.10901812221997839\n",
      "2022-03-26 17:17:19.184686 Epoch 100, Training Loss 0.1098501593865397\n",
      "2022-03-26 17:17:19.208834 Epoch 100, Training Loss 0.11084426318288154\n",
      "2022-03-26 17:17:19.233035 Epoch 100, Training Loss 0.11176865515501602\n",
      "2022-03-26 17:17:19.260484 Epoch 100, Training Loss 0.11260396516536508\n",
      "2022-03-26 17:17:19.283705 Epoch 100, Training Loss 0.1133445729227627\n",
      "2022-03-26 17:17:19.307822 Epoch 100, Training Loss 0.11426146164574587\n",
      "2022-03-26 17:17:19.333829 Epoch 100, Training Loss 0.11557079291404665\n",
      "2022-03-26 17:17:19.357639 Epoch 100, Training Loss 0.11614467798139128\n",
      "2022-03-26 17:17:19.382560 Epoch 100, Training Loss 0.11714260287754372\n",
      "2022-03-26 17:17:19.414340 Epoch 100, Training Loss 0.11802725417687156\n",
      "2022-03-26 17:17:19.439187 Epoch 100, Training Loss 0.11891883074322625\n",
      "2022-03-26 17:17:19.462894 Epoch 100, Training Loss 0.11996647970908134\n",
      "2022-03-26 17:17:19.489577 Epoch 100, Training Loss 0.1207482475225273\n",
      "2022-03-26 17:17:19.512879 Epoch 100, Training Loss 0.12168285323073492\n",
      "2022-03-26 17:17:19.535969 Epoch 100, Training Loss 0.12242013059766091\n",
      "2022-03-26 17:17:19.559559 Epoch 100, Training Loss 0.12367602996051769\n",
      "2022-03-26 17:17:19.581689 Epoch 100, Training Loss 0.1246019027498372\n",
      "2022-03-26 17:17:19.615120 Epoch 100, Training Loss 0.12531330064891855\n",
      "2022-03-26 17:17:19.644593 Epoch 100, Training Loss 0.12623170944278503\n",
      "2022-03-26 17:17:19.667638 Epoch 100, Training Loss 0.12721497624578987\n",
      "2022-03-26 17:17:19.690304 Epoch 100, Training Loss 0.12792137924515073\n",
      "2022-03-26 17:17:19.715733 Epoch 100, Training Loss 0.12913128028592796\n",
      "2022-03-26 17:17:19.738340 Epoch 100, Training Loss 0.1302566447145189\n",
      "2022-03-26 17:17:19.761470 Epoch 100, Training Loss 0.1311591300741791\n",
      "2022-03-26 17:17:19.784373 Epoch 100, Training Loss 0.13208583702364235\n",
      "2022-03-26 17:17:19.807686 Epoch 100, Training Loss 0.13290656470429257\n",
      "2022-03-26 17:17:19.831715 Epoch 100, Training Loss 0.1338136464052493\n",
      "2022-03-26 17:17:19.859695 Epoch 100, Training Loss 0.13458128037202693\n",
      "2022-03-26 17:17:19.884510 Epoch 100, Training Loss 0.1353432583000959\n",
      "2022-03-26 17:17:19.907139 Epoch 100, Training Loss 0.13597368637619117\n",
      "2022-03-26 17:17:19.928813 Epoch 100, Training Loss 0.13700441837005908\n",
      "2022-03-26 17:17:19.950730 Epoch 100, Training Loss 0.13777871685259788\n",
      "2022-03-26 17:17:19.977124 Epoch 100, Training Loss 0.13891934510082235\n",
      "2022-03-26 17:17:20.000028 Epoch 100, Training Loss 0.13979588338481191\n",
      "2022-03-26 17:17:20.022946 Epoch 100, Training Loss 0.14067941980288767\n",
      "2022-03-26 17:17:20.045144 Epoch 100, Training Loss 0.14144014992067577\n",
      "2022-03-26 17:17:20.078672 Epoch 100, Training Loss 0.1422619840220722\n",
      "2022-03-26 17:17:20.102786 Epoch 100, Training Loss 0.14287230479137977\n",
      "2022-03-26 17:17:20.125444 Epoch 100, Training Loss 0.14385538348151594\n",
      "2022-03-26 17:17:20.147952 Epoch 100, Training Loss 0.1445537193504441\n",
      "2022-03-26 17:17:20.179440 Epoch 100, Training Loss 0.1456061584108016\n",
      "2022-03-26 17:17:20.203063 Epoch 100, Training Loss 0.14646787167814992\n",
      "2022-03-26 17:17:20.226318 Epoch 100, Training Loss 0.14713800540360647\n",
      "2022-03-26 17:17:20.249852 Epoch 100, Training Loss 0.14823500686289404\n",
      "2022-03-26 17:17:20.272598 Epoch 100, Training Loss 0.14908618459006404\n",
      "2022-03-26 17:17:20.302493 Epoch 100, Training Loss 0.14975591350699324\n",
      "2022-03-26 17:17:20.325272 Epoch 100, Training Loss 0.15054017084333904\n",
      "2022-03-26 17:17:20.349589 Epoch 100, Training Loss 0.15156495304363768\n",
      "2022-03-26 17:17:20.375151 Epoch 100, Training Loss 0.1527501694534136\n",
      "2022-03-26 17:17:20.397953 Epoch 100, Training Loss 0.15359169153301308\n",
      "2022-03-26 17:17:20.420435 Epoch 100, Training Loss 0.15471318516584917\n",
      "2022-03-26 17:17:20.443075 Epoch 100, Training Loss 0.15552936604870554\n",
      "2022-03-26 17:17:20.465730 Epoch 100, Training Loss 0.1563709625960006\n",
      "2022-03-26 17:17:20.488766 Epoch 100, Training Loss 0.15725487592580067\n",
      "2022-03-26 17:17:20.517554 Epoch 100, Training Loss 0.1583135251498893\n",
      "2022-03-26 17:17:20.541961 Epoch 100, Training Loss 0.1589338615955904\n",
      "2022-03-26 17:17:20.565023 Epoch 100, Training Loss 0.1598515845549381\n",
      "2022-03-26 17:17:20.588494 Epoch 100, Training Loss 0.16061258518025087\n",
      "2022-03-26 17:17:20.616687 Epoch 100, Training Loss 0.16144858872341683\n",
      "2022-03-26 17:17:20.644793 Epoch 100, Training Loss 0.16250312347393817\n",
      "2022-03-26 17:17:20.667894 Epoch 100, Training Loss 0.16350765129946687\n",
      "2022-03-26 17:17:20.690607 Epoch 100, Training Loss 0.16452788456779002\n",
      "2022-03-26 17:17:20.713367 Epoch 100, Training Loss 0.1652758672947774\n",
      "2022-03-26 17:17:20.749398 Epoch 100, Training Loss 0.16626596126867377\n",
      "2022-03-26 17:17:20.773331 Epoch 100, Training Loss 0.16741628586636176\n",
      "2022-03-26 17:17:20.805394 Epoch 100, Training Loss 0.16838475623551538\n",
      "2022-03-26 17:17:20.841813 Epoch 100, Training Loss 0.16914207402550047\n",
      "2022-03-26 17:17:20.881154 Epoch 100, Training Loss 0.1701270035084556\n",
      "2022-03-26 17:17:20.913800 Epoch 100, Training Loss 0.17083699627757987\n",
      "2022-03-26 17:17:20.946814 Epoch 100, Training Loss 0.17167349312159105\n",
      "2022-03-26 17:17:21.004578 Epoch 100, Training Loss 0.17256522761738818\n",
      "2022-03-26 17:17:21.044787 Epoch 100, Training Loss 0.1735983816025507\n",
      "2022-03-26 17:17:21.079298 Epoch 100, Training Loss 0.17459521669408548\n",
      "2022-03-26 17:17:21.113074 Epoch 100, Training Loss 0.17555620789985218\n",
      "2022-03-26 17:17:21.144351 Epoch 100, Training Loss 0.1765782175695195\n",
      "2022-03-26 17:17:21.183838 Epoch 100, Training Loss 0.17736477532502634\n",
      "2022-03-26 17:17:21.225049 Epoch 100, Training Loss 0.1782512200991516\n",
      "2022-03-26 17:17:21.253998 Epoch 100, Training Loss 0.1791617119175089\n",
      "2022-03-26 17:17:21.279642 Epoch 100, Training Loss 0.1802091558494836\n",
      "2022-03-26 17:17:21.307835 Epoch 100, Training Loss 0.18112141454158842\n",
      "2022-03-26 17:17:21.332296 Epoch 100, Training Loss 0.18214270224809037\n",
      "2022-03-26 17:17:21.355507 Epoch 100, Training Loss 0.18294786724745465\n",
      "2022-03-26 17:17:21.378803 Epoch 100, Training Loss 0.18372058933195862\n",
      "2022-03-26 17:17:21.402854 Epoch 100, Training Loss 0.18485221445865338\n",
      "2022-03-26 17:17:21.425678 Epoch 100, Training Loss 0.18598748103279591\n",
      "2022-03-26 17:17:21.457596 Epoch 100, Training Loss 0.18686739566838345\n",
      "2022-03-26 17:17:21.480993 Epoch 100, Training Loss 0.18795788352904114\n",
      "2022-03-26 17:17:21.504258 Epoch 100, Training Loss 0.18895294458207573\n",
      "2022-03-26 17:17:21.527142 Epoch 100, Training Loss 0.19005595772620051\n",
      "2022-03-26 17:17:21.563303 Epoch 100, Training Loss 0.19083211576694722\n",
      "2022-03-26 17:17:21.586637 Epoch 100, Training Loss 0.19159350069739933\n",
      "2022-03-26 17:17:21.609705 Epoch 100, Training Loss 0.19256814433942976\n",
      "2022-03-26 17:17:21.634866 Epoch 100, Training Loss 0.19340286051373348\n",
      "2022-03-26 17:17:21.670847 Epoch 100, Training Loss 0.19415999880379728\n",
      "2022-03-26 17:17:21.694467 Epoch 100, Training Loss 0.1950218708390165\n",
      "2022-03-26 17:17:21.718504 Epoch 100, Training Loss 0.1959490133901996\n",
      "2022-03-26 17:17:21.742595 Epoch 100, Training Loss 0.19692948921714598\n",
      "2022-03-26 17:17:21.765830 Epoch 100, Training Loss 0.19800191039167098\n",
      "2022-03-26 17:17:21.789129 Epoch 100, Training Loss 0.1988981319281756\n",
      "2022-03-26 17:17:21.812611 Epoch 100, Training Loss 0.20017273518283044\n",
      "2022-03-26 17:17:21.837304 Epoch 100, Training Loss 0.20131838851420167\n",
      "2022-03-26 17:17:21.860374 Epoch 100, Training Loss 0.20223724037942375\n",
      "2022-03-26 17:17:21.895995 Epoch 100, Training Loss 0.2031680733117911\n",
      "2022-03-26 17:17:21.920132 Epoch 100, Training Loss 0.20405312012071194\n",
      "2022-03-26 17:17:21.942836 Epoch 100, Training Loss 0.2048883826455192\n",
      "2022-03-26 17:17:21.966233 Epoch 100, Training Loss 0.2057081712100207\n",
      "2022-03-26 17:17:21.993749 Epoch 100, Training Loss 0.20633170488850236\n",
      "2022-03-26 17:17:22.016258 Epoch 100, Training Loss 0.2073322449193891\n",
      "2022-03-26 17:17:22.038743 Epoch 100, Training Loss 0.2081816955600553\n",
      "2022-03-26 17:17:22.061605 Epoch 100, Training Loss 0.20871010232154671\n",
      "2022-03-26 17:17:22.084340 Epoch 100, Training Loss 0.20960650213844026\n",
      "2022-03-26 17:17:22.113962 Epoch 100, Training Loss 0.2103264079526867\n",
      "2022-03-26 17:17:22.140381 Epoch 100, Training Loss 0.21148628903471905\n",
      "2022-03-26 17:17:22.163311 Epoch 100, Training Loss 0.21259958207454827\n",
      "2022-03-26 17:17:22.189831 Epoch 100, Training Loss 0.21347503985285454\n",
      "2022-03-26 17:17:22.212839 Epoch 100, Training Loss 0.2141601885371196\n",
      "2022-03-26 17:17:22.237771 Epoch 100, Training Loss 0.21506073727937\n",
      "2022-03-26 17:17:22.261009 Epoch 100, Training Loss 0.21599144177973423\n",
      "2022-03-26 17:17:22.284435 Epoch 100, Training Loss 0.21687890333897622\n",
      "2022-03-26 17:17:22.308696 Epoch 100, Training Loss 0.21776871204071338\n",
      "2022-03-26 17:17:22.341066 Epoch 100, Training Loss 0.21869716063484818\n",
      "2022-03-26 17:17:22.365421 Epoch 100, Training Loss 0.21969859069570555\n",
      "2022-03-26 17:17:22.388878 Epoch 100, Training Loss 0.22050242701454845\n",
      "2022-03-26 17:17:22.416318 Epoch 100, Training Loss 0.22120728097913211\n",
      "2022-03-26 17:17:22.439792 Epoch 100, Training Loss 0.22187233116010877\n",
      "2022-03-26 17:17:22.462121 Epoch 100, Training Loss 0.22279415998007635\n",
      "2022-03-26 17:17:22.485077 Epoch 100, Training Loss 0.22412882566147144\n",
      "2022-03-26 17:17:22.508325 Epoch 100, Training Loss 0.22509926092594176\n",
      "2022-03-26 17:17:22.530962 Epoch 100, Training Loss 0.22633327707610168\n",
      "2022-03-26 17:17:22.560625 Epoch 100, Training Loss 0.22718110947352846\n",
      "2022-03-26 17:17:22.583461 Epoch 100, Training Loss 0.22809012154179156\n",
      "2022-03-26 17:17:22.606412 Epoch 100, Training Loss 0.22898599718842666\n",
      "2022-03-26 17:17:22.632979 Epoch 100, Training Loss 0.22971246157155928\n",
      "2022-03-26 17:17:22.655437 Epoch 100, Training Loss 0.2304373399528396\n",
      "2022-03-26 17:17:22.688305 Epoch 100, Training Loss 0.2313895982397182\n",
      "2022-03-26 17:17:22.710969 Epoch 100, Training Loss 0.2318587555452381\n",
      "2022-03-26 17:17:22.733362 Epoch 100, Training Loss 0.23253460399939885\n",
      "2022-03-26 17:17:22.755888 Epoch 100, Training Loss 0.23345390320433984\n",
      "2022-03-26 17:17:22.788361 Epoch 100, Training Loss 0.23448724644568264\n",
      "2022-03-26 17:17:22.812103 Epoch 100, Training Loss 0.2355024047062525\n",
      "2022-03-26 17:17:22.836873 Epoch 100, Training Loss 0.23648613759928652\n",
      "2022-03-26 17:17:22.860258 Epoch 100, Training Loss 0.2372975812086364\n",
      "2022-03-26 17:17:22.883560 Epoch 100, Training Loss 0.23835938338123625\n",
      "2022-03-26 17:17:22.910507 Epoch 100, Training Loss 0.23939956217775565\n",
      "2022-03-26 17:17:22.933373 Epoch 100, Training Loss 0.2405975751407311\n",
      "2022-03-26 17:17:22.956009 Epoch 100, Training Loss 0.24176475695331992\n",
      "2022-03-26 17:17:22.979027 Epoch 100, Training Loss 0.24241412112779934\n",
      "2022-03-26 17:17:23.008640 Epoch 100, Training Loss 0.24310619149671492\n",
      "2022-03-26 17:17:23.032483 Epoch 100, Training Loss 0.24375156719056543\n",
      "2022-03-26 17:17:23.055482 Epoch 100, Training Loss 0.2445113668051522\n",
      "2022-03-26 17:17:23.082458 Epoch 100, Training Loss 0.24545584302729048\n",
      "2022-03-26 17:17:23.112728 Epoch 100, Training Loss 0.24642187517012476\n",
      "2022-03-26 17:17:23.135817 Epoch 100, Training Loss 0.24751759512954966\n",
      "2022-03-26 17:17:23.159126 Epoch 100, Training Loss 0.2484797499216426\n",
      "2022-03-26 17:17:23.186523 Epoch 100, Training Loss 0.24955337187823126\n",
      "2022-03-26 17:17:23.213451 Epoch 100, Training Loss 0.250562623333748\n",
      "2022-03-26 17:17:23.239986 Epoch 100, Training Loss 0.25129372277833006\n",
      "2022-03-26 17:17:23.262852 Epoch 100, Training Loss 0.25235603265750134\n",
      "2022-03-26 17:17:23.285492 Epoch 100, Training Loss 0.25333983826515316\n",
      "2022-03-26 17:17:23.312149 Epoch 100, Training Loss 0.254125384037452\n",
      "2022-03-26 17:17:23.338017 Epoch 100, Training Loss 0.25491892292981255\n",
      "2022-03-26 17:17:23.360906 Epoch 100, Training Loss 0.2556836388605025\n",
      "2022-03-26 17:17:23.384071 Epoch 100, Training Loss 0.2565724304722398\n",
      "2022-03-26 17:17:23.407619 Epoch 100, Training Loss 0.25771643202323136\n",
      "2022-03-26 17:17:23.438653 Epoch 100, Training Loss 0.2584597083461254\n",
      "2022-03-26 17:17:23.462146 Epoch 100, Training Loss 0.2594534689203248\n",
      "2022-03-26 17:17:23.484786 Epoch 100, Training Loss 0.2607308782427512\n",
      "2022-03-26 17:17:23.510585 Epoch 100, Training Loss 0.2615892373387466\n",
      "2022-03-26 17:17:23.533217 Epoch 100, Training Loss 0.2624215164300426\n",
      "2022-03-26 17:17:23.555806 Epoch 100, Training Loss 0.2632382930544636\n",
      "2022-03-26 17:17:23.579366 Epoch 100, Training Loss 0.264329008357909\n",
      "2022-03-26 17:17:23.602369 Epoch 100, Training Loss 0.2656091870096943\n",
      "2022-03-26 17:17:23.626008 Epoch 100, Training Loss 0.2665005889542572\n",
      "2022-03-26 17:17:23.654681 Epoch 100, Training Loss 0.26709010663544736\n",
      "2022-03-26 17:17:23.679132 Epoch 100, Training Loss 0.2678788491832021\n",
      "2022-03-26 17:17:23.713259 Epoch 100, Training Loss 0.26875196858440215\n",
      "2022-03-26 17:17:23.736352 Epoch 100, Training Loss 0.2695040134212855\n",
      "2022-03-26 17:17:23.763190 Epoch 100, Training Loss 0.27042577409988167\n",
      "2022-03-26 17:17:23.785750 Epoch 100, Training Loss 0.27146872451238313\n",
      "2022-03-26 17:17:23.808805 Epoch 100, Training Loss 0.27244719024509423\n",
      "2022-03-26 17:17:23.832938 Epoch 100, Training Loss 0.2738110765319346\n",
      "2022-03-26 17:17:23.856568 Epoch 100, Training Loss 0.27444118756772307\n",
      "2022-03-26 17:17:23.885795 Epoch 100, Training Loss 0.275535214130226\n",
      "2022-03-26 17:17:23.908840 Epoch 100, Training Loss 0.27625034951492955\n",
      "2022-03-26 17:17:23.932314 Epoch 100, Training Loss 0.2769907948458591\n",
      "2022-03-26 17:17:23.959684 Epoch 100, Training Loss 0.2777772995517077\n",
      "2022-03-26 17:17:23.982711 Epoch 100, Training Loss 0.27854387336374853\n",
      "2022-03-26 17:17:24.009928 Epoch 100, Training Loss 0.27939078813928475\n",
      "2022-03-26 17:17:24.032954 Epoch 100, Training Loss 0.2801833944704831\n",
      "2022-03-26 17:17:24.058666 Epoch 100, Training Loss 0.28083027514350384\n",
      "2022-03-26 17:17:24.090083 Epoch 100, Training Loss 0.28167723877655576\n",
      "2022-03-26 17:17:24.114926 Epoch 100, Training Loss 0.28244137032257627\n",
      "2022-03-26 17:17:24.137959 Epoch 100, Training Loss 0.28358741367564483\n",
      "2022-03-26 17:17:24.162165 Epoch 100, Training Loss 0.2841359342226897\n",
      "2022-03-26 17:17:24.188697 Epoch 100, Training Loss 0.2848996819208955\n",
      "2022-03-26 17:17:24.212187 Epoch 100, Training Loss 0.28587245601979666\n",
      "2022-03-26 17:17:24.236053 Epoch 100, Training Loss 0.2867982822382237\n",
      "2022-03-26 17:17:24.258844 Epoch 100, Training Loss 0.2877163738012314\n",
      "2022-03-26 17:17:24.281564 Epoch 100, Training Loss 0.28862268430040317\n",
      "2022-03-26 17:17:24.313314 Epoch 100, Training Loss 0.28938369635883193\n",
      "2022-03-26 17:17:24.339508 Epoch 100, Training Loss 0.290186200795881\n",
      "2022-03-26 17:17:24.362025 Epoch 100, Training Loss 0.29091881794850233\n",
      "2022-03-26 17:17:24.388004 Epoch 100, Training Loss 0.291786105317228\n",
      "2022-03-26 17:17:24.410855 Epoch 100, Training Loss 0.29264665980015875\n",
      "2022-03-26 17:17:24.434773 Epoch 100, Training Loss 0.29367850461731787\n",
      "2022-03-26 17:17:24.457064 Epoch 100, Training Loss 0.29457067006536763\n",
      "2022-03-26 17:17:24.479608 Epoch 100, Training Loss 0.29562189950205175\n",
      "2022-03-26 17:17:24.501996 Epoch 100, Training Loss 0.2963606615734222\n",
      "2022-03-26 17:17:24.530464 Epoch 100, Training Loss 0.29720818390474296\n",
      "2022-03-26 17:17:24.555599 Epoch 100, Training Loss 0.2978906498464477\n",
      "2022-03-26 17:17:24.578314 Epoch 100, Training Loss 0.2986588767346214\n",
      "2022-03-26 17:17:24.600867 Epoch 100, Training Loss 0.29986956048652036\n",
      "2022-03-26 17:17:24.627161 Epoch 100, Training Loss 0.30076837833122827\n",
      "2022-03-26 17:17:24.650171 Epoch 100, Training Loss 0.30144752920283685\n",
      "2022-03-26 17:17:24.673295 Epoch 100, Training Loss 0.3022539320275607\n",
      "2022-03-26 17:17:24.695945 Epoch 100, Training Loss 0.30311561251997643\n",
      "2022-03-26 17:17:24.724683 Epoch 100, Training Loss 0.30396385422295624\n",
      "2022-03-26 17:17:24.755788 Epoch 100, Training Loss 0.30477150036093525\n",
      "2022-03-26 17:17:24.778501 Epoch 100, Training Loss 0.30591666115366895\n",
      "2022-03-26 17:17:24.801084 Epoch 100, Training Loss 0.30665411962115247\n",
      "2022-03-26 17:17:24.823371 Epoch 100, Training Loss 0.3075483934882352\n",
      "2022-03-26 17:17:24.850168 Epoch 100, Training Loss 0.3086915912911715\n",
      "2022-03-26 17:17:24.872458 Epoch 100, Training Loss 0.30983468147037585\n",
      "2022-03-26 17:17:24.894920 Epoch 100, Training Loss 0.3106224571576204\n",
      "2022-03-26 17:17:24.923677 Epoch 100, Training Loss 0.3114584327277625\n",
      "2022-03-26 17:17:24.946259 Epoch 100, Training Loss 0.3123255112515691\n",
      "2022-03-26 17:17:24.976749 Epoch 100, Training Loss 0.3134884701284301\n",
      "2022-03-26 17:17:25.004555 Epoch 100, Training Loss 0.31433095548616347\n",
      "2022-03-26 17:17:25.027873 Epoch 100, Training Loss 0.315041117312963\n",
      "2022-03-26 17:17:25.054218 Epoch 100, Training Loss 0.3159702404609422\n",
      "2022-03-26 17:17:25.076552 Epoch 100, Training Loss 0.31687112678499785\n",
      "2022-03-26 17:17:25.099182 Epoch 100, Training Loss 0.3175398111724488\n",
      "2022-03-26 17:17:25.122341 Epoch 100, Training Loss 0.31836488168410326\n",
      "2022-03-26 17:17:25.145195 Epoch 100, Training Loss 0.31962702329963677\n",
      "2022-03-26 17:17:25.167583 Epoch 100, Training Loss 0.32054006462664253\n",
      "2022-03-26 17:17:25.198419 Epoch 100, Training Loss 0.32139753186336867\n",
      "2022-03-26 17:17:25.223196 Epoch 100, Training Loss 0.32229390706095246\n",
      "2022-03-26 17:17:25.246032 Epoch 100, Training Loss 0.32333317250394455\n",
      "2022-03-26 17:17:25.272779 Epoch 100, Training Loss 0.32418720363198644\n",
      "2022-03-26 17:17:25.295732 Epoch 100, Training Loss 0.32476939786882963\n",
      "2022-03-26 17:17:25.319551 Epoch 100, Training Loss 0.32565272109740223\n",
      "2022-03-26 17:17:25.345892 Epoch 100, Training Loss 0.3264741149476117\n",
      "2022-03-26 17:17:25.368709 Epoch 100, Training Loss 0.3274978231972136\n",
      "2022-03-26 17:17:25.392088 Epoch 100, Training Loss 0.3284652421007986\n",
      "2022-03-26 17:17:25.422711 Epoch 100, Training Loss 0.32952870405695933\n",
      "2022-03-26 17:17:25.447037 Epoch 100, Training Loss 0.33055235605563044\n",
      "2022-03-26 17:17:25.470103 Epoch 100, Training Loss 0.3314576900142538\n",
      "2022-03-26 17:17:25.493705 Epoch 100, Training Loss 0.3325426059077158\n",
      "2022-03-26 17:17:25.518480 Epoch 100, Training Loss 0.33304333580119533\n",
      "2022-03-26 17:17:25.543133 Epoch 100, Training Loss 0.33383054356745745\n",
      "2022-03-26 17:17:25.566494 Epoch 100, Training Loss 0.33471911253831577\n",
      "2022-03-26 17:17:25.588927 Epoch 100, Training Loss 0.335535675515909\n",
      "2022-03-26 17:17:25.611891 Epoch 100, Training Loss 0.3363517409242937\n",
      "2022-03-26 17:17:25.640729 Epoch 100, Training Loss 0.33705160029403997\n",
      "2022-03-26 17:17:25.664996 Epoch 100, Training Loss 0.3377600052320134\n",
      "2022-03-26 17:17:25.688113 Epoch 100, Training Loss 0.3384149126384569\n",
      "2022-03-26 17:17:25.710923 Epoch 100, Training Loss 0.33901690270589746\n",
      "2022-03-26 17:17:25.736987 Epoch 100, Training Loss 0.3399714076000711\n",
      "2022-03-26 17:17:25.771044 Epoch 100, Training Loss 0.3407504866495157\n",
      "2022-03-26 17:17:25.794667 Epoch 100, Training Loss 0.3414401146761902\n",
      "2022-03-26 17:17:25.817953 Epoch 100, Training Loss 0.34204769397483153\n",
      "2022-03-26 17:17:25.842653 Epoch 100, Training Loss 0.34269305419586504\n",
      "2022-03-26 17:17:25.872306 Epoch 100, Training Loss 0.34358296625297086\n",
      "2022-03-26 17:17:25.895564 Epoch 100, Training Loss 0.34448435746342937\n",
      "2022-03-26 17:17:25.918720 Epoch 100, Training Loss 0.34534611798765713\n",
      "2022-03-26 17:17:25.944899 Epoch 100, Training Loss 0.34637698984664417\n",
      "2022-03-26 17:17:25.967859 Epoch 100, Training Loss 0.3469796826696152\n",
      "2022-03-26 17:17:25.990308 Epoch 100, Training Loss 0.34786520349552563\n",
      "2022-03-26 17:17:26.013266 Epoch 100, Training Loss 0.34867681501923925\n",
      "2022-03-26 17:17:26.036256 Epoch 100, Training Loss 0.34948002205937717\n",
      "2022-03-26 17:17:26.063534 Epoch 100, Training Loss 0.3505354196290531\n",
      "2022-03-26 17:17:26.089139 Epoch 100, Training Loss 0.3513381204108143\n",
      "2022-03-26 17:17:26.112913 Epoch 100, Training Loss 0.3522432241064813\n",
      "2022-03-26 17:17:26.139649 Epoch 100, Training Loss 0.35308455083223866\n",
      "2022-03-26 17:17:26.162263 Epoch 100, Training Loss 0.35396736612557755\n",
      "2022-03-26 17:17:26.189835 Epoch 100, Training Loss 0.3548395634078614\n",
      "2022-03-26 17:17:26.216895 Epoch 100, Training Loss 0.35597266035768993\n",
      "2022-03-26 17:17:26.240021 Epoch 100, Training Loss 0.35690395053847673\n",
      "2022-03-26 17:17:26.262799 Epoch 100, Training Loss 0.35769321855224306\n",
      "2022-03-26 17:17:26.292781 Epoch 100, Training Loss 0.35859256552155977\n",
      "2022-03-26 17:17:26.315658 Epoch 100, Training Loss 0.35953926883847515\n",
      "2022-03-26 17:17:26.340879 Epoch 100, Training Loss 0.36042876785521005\n",
      "2022-03-26 17:17:26.366784 Epoch 100, Training Loss 0.3614664413511296\n",
      "2022-03-26 17:17:26.389889 Epoch 100, Training Loss 0.3623176665638414\n",
      "2022-03-26 17:17:26.412539 Epoch 100, Training Loss 0.3636375172135165\n",
      "2022-03-26 17:17:26.435729 Epoch 100, Training Loss 0.36452040903251187\n",
      "2022-03-26 17:17:26.458293 Epoch 100, Training Loss 0.3654945010648054\n",
      "2022-03-26 17:17:26.482318 Epoch 100, Training Loss 0.3663248338586534\n",
      "2022-03-26 17:17:26.514241 Epoch 100, Training Loss 0.36728912641477707\n",
      "2022-03-26 17:17:26.538986 Epoch 100, Training Loss 0.36802606368461227\n",
      "2022-03-26 17:17:26.561896 Epoch 100, Training Loss 0.3687638878593664\n",
      "2022-03-26 17:17:26.588350 Epoch 100, Training Loss 0.36964473497989536\n",
      "2022-03-26 17:17:26.611663 Epoch 100, Training Loss 0.3705971290159713\n",
      "2022-03-26 17:17:26.635648 Epoch 100, Training Loss 0.371699667990665\n",
      "2022-03-26 17:17:26.659020 Epoch 100, Training Loss 0.37279612027928044\n",
      "2022-03-26 17:17:26.682297 Epoch 100, Training Loss 0.3734914759159698\n",
      "2022-03-26 17:17:26.705446 Epoch 100, Training Loss 0.3745536269510494\n",
      "2022-03-26 17:17:26.735832 Epoch 100, Training Loss 0.3752283382674922\n",
      "2022-03-26 17:17:26.760599 Epoch 100, Training Loss 0.3759806947711179\n",
      "2022-03-26 17:17:26.792597 Epoch 100, Training Loss 0.3768409951339902\n",
      "2022-03-26 17:17:26.815775 Epoch 100, Training Loss 0.37779649295617856\n",
      "2022-03-26 17:17:26.844651 Epoch 100, Training Loss 0.37872255637365226\n",
      "2022-03-26 17:17:26.867976 Epoch 100, Training Loss 0.3795396363567513\n",
      "2022-03-26 17:17:26.891228 Epoch 100, Training Loss 0.38018945945650723\n",
      "2022-03-26 17:17:26.914703 Epoch 100, Training Loss 0.38086409645769603\n",
      "2022-03-26 17:17:26.940493 Epoch 100, Training Loss 0.38200728137932166\n",
      "2022-03-26 17:17:26.967853 Epoch 100, Training Loss 0.3828972762121874\n",
      "2022-03-26 17:17:26.991442 Epoch 100, Training Loss 0.3838181489949946\n",
      "2022-03-26 17:17:27.015503 Epoch 100, Training Loss 0.38481000504072976\n",
      "2022-03-26 17:17:27.044289 Epoch 100, Training Loss 0.38548579808238825\n",
      "2022-03-26 17:17:27.067295 Epoch 100, Training Loss 0.3865011057356739\n",
      "2022-03-26 17:17:27.090574 Epoch 100, Training Loss 0.3877366807530908\n",
      "2022-03-26 17:17:27.114468 Epoch 100, Training Loss 0.38837548290067314\n",
      "2022-03-26 17:17:27.138363 Epoch 100, Training Loss 0.3893673336109542\n",
      "2022-03-26 17:17:27.172293 Epoch 100, Training Loss 0.39008314697943686\n",
      "2022-03-26 17:17:27.197427 Epoch 100, Training Loss 0.39107069060625627\n",
      "2022-03-26 17:17:27.220434 Epoch 100, Training Loss 0.39231184582271234\n",
      "2022-03-26 17:17:27.243530 Epoch 100, Training Loss 0.39320236330142105\n",
      "2022-03-26 17:17:27.267965 Epoch 100, Training Loss 0.39402223532766945\n",
      "2022-03-26 17:17:27.291488 Epoch 100, Training Loss 0.3950026163359737\n",
      "2022-03-26 17:17:27.313952 Epoch 100, Training Loss 0.39575079799917956\n",
      "2022-03-26 17:17:27.339708 Epoch 100, Training Loss 0.3968076951363507\n",
      "2022-03-26 17:17:27.362222 Epoch 100, Training Loss 0.39793971622996316\n",
      "2022-03-26 17:17:27.395165 Epoch 100, Training Loss 0.3990820127222544\n",
      "2022-03-26 17:17:27.420395 Epoch 100, Training Loss 0.3996332966153274\n",
      "2022-03-26 17:17:27.444493 Epoch 100, Training Loss 0.4006610612582673\n",
      "2022-03-26 17:17:27.474663 Epoch 100, Training Loss 0.40173566036517055\n",
      "2022-03-26 17:17:27.498256 Epoch 100, Training Loss 0.40266722486452067\n",
      "2022-03-26 17:17:27.521366 Epoch 100, Training Loss 0.40351960741345533\n",
      "2022-03-26 17:17:27.545189 Epoch 100, Training Loss 0.4042011211290384\n",
      "2022-03-26 17:17:27.568722 Epoch 100, Training Loss 0.40496612296384926\n",
      "2022-03-26 17:17:27.592308 Epoch 100, Training Loss 0.40602309319674207\n",
      "2022-03-26 17:17:27.623721 Epoch 100, Training Loss 0.40685296165363866\n",
      "2022-03-26 17:17:27.647758 Epoch 100, Training Loss 0.407648083041696\n",
      "2022-03-26 17:17:27.670803 Epoch 100, Training Loss 0.40859342741844296\n",
      "2022-03-26 17:17:27.697457 Epoch 100, Training Loss 0.4096427431801701\n",
      "2022-03-26 17:17:27.720372 Epoch 100, Training Loss 0.41021617343816\n",
      "2022-03-26 17:17:27.743603 Epoch 100, Training Loss 0.4111134587117778\n",
      "2022-03-26 17:17:27.766681 Epoch 100, Training Loss 0.41188106180914225\n",
      "2022-03-26 17:17:27.793937 Epoch 100, Training Loss 0.4125459499447547\n",
      "2022-03-26 17:17:27.823646 Epoch 100, Training Loss 0.4136069027130561\n",
      "2022-03-26 17:17:27.856282 Epoch 100, Training Loss 0.41462568778668524\n",
      "2022-03-26 17:17:27.879965 Epoch 100, Training Loss 0.41572757751283135\n",
      "2022-03-26 17:17:27.902976 Epoch 100, Training Loss 0.41673945606974383\n",
      "2022-03-26 17:17:27.936737 Epoch 100, Training Loss 0.4176346906996749\n",
      "2022-03-26 17:17:27.961002 Epoch 100, Training Loss 0.4184062866984731\n",
      "2022-03-26 17:17:27.984143 Epoch 100, Training Loss 0.4193060260523311\n",
      "2022-03-26 17:17:28.009916 Epoch 100, Training Loss 0.4198397766522434\n",
      "2022-03-26 17:17:28.033349 Epoch 100, Training Loss 0.42086258176190167\n",
      "2022-03-26 17:17:28.056398 Epoch 100, Training Loss 0.42178601045590225\n",
      "2022-03-26 17:17:28.085651 Epoch 100, Training Loss 0.42271411224551825\n",
      "2022-03-26 17:17:28.108680 Epoch 100, Training Loss 0.423396829410892\n",
      "2022-03-26 17:17:28.139277 Epoch 100, Training Loss 0.42420384779458153\n",
      "2022-03-26 17:17:28.162104 Epoch 100, Training Loss 0.42499108086614046\n",
      "2022-03-26 17:17:28.185996 Epoch 100, Training Loss 0.4259056472183798\n",
      "2022-03-26 17:17:28.209266 Epoch 100, Training Loss 0.42659894497040896\n",
      "2022-03-26 17:17:28.233474 Epoch 100, Training Loss 0.4277506657040028\n",
      "2022-03-26 17:17:28.256745 Epoch 100, Training Loss 0.4289895267514012\n",
      "2022-03-26 17:17:28.280995 Epoch 100, Training Loss 0.4298286131961876\n",
      "2022-03-26 17:17:28.311297 Epoch 100, Training Loss 0.4306749377180548\n",
      "2022-03-26 17:17:28.338223 Epoch 100, Training Loss 0.43141496391095163\n",
      "2022-03-26 17:17:28.360821 Epoch 100, Training Loss 0.43255486394590736\n",
      "2022-03-26 17:17:28.387018 Epoch 100, Training Loss 0.4336957857965508\n",
      "2022-03-26 17:17:28.409263 Epoch 100, Training Loss 0.4344643827365792\n",
      "2022-03-26 17:17:28.432300 Epoch 100, Training Loss 0.43526114218527706\n",
      "2022-03-26 17:17:28.455662 Epoch 100, Training Loss 0.43590717939921964\n",
      "2022-03-26 17:17:28.478558 Epoch 100, Training Loss 0.4366367647943594\n",
      "2022-03-26 17:17:28.501679 Epoch 100, Training Loss 0.43739500752342936\n",
      "2022-03-26 17:17:28.530651 Epoch 100, Training Loss 0.43822433229754953\n",
      "2022-03-26 17:17:28.555299 Epoch 100, Training Loss 0.4392914014780308\n",
      "2022-03-26 17:17:28.578307 Epoch 100, Training Loss 0.44018344954608957\n",
      "2022-03-26 17:17:28.601912 Epoch 100, Training Loss 0.44093159759593437\n",
      "2022-03-26 17:17:28.628150 Epoch 100, Training Loss 0.4421105640547355\n",
      "2022-03-26 17:17:28.651948 Epoch 100, Training Loss 0.44307380819412145\n",
      "2022-03-26 17:17:28.675142 Epoch 100, Training Loss 0.4439745817876533\n",
      "2022-03-26 17:17:28.698738 Epoch 100, Training Loss 0.44482055035850887\n",
      "2022-03-26 17:17:28.727840 Epoch 100, Training Loss 0.44562339367311626\n",
      "2022-03-26 17:17:28.756841 Epoch 100, Training Loss 0.4466065884474903\n",
      "2022-03-26 17:17:28.782427 Epoch 100, Training Loss 0.44762168199662355\n",
      "2022-03-26 17:17:28.806087 Epoch 100, Training Loss 0.4486409318645287\n",
      "2022-03-26 17:17:28.841710 Epoch 100, Training Loss 0.44939097209507245\n",
      "2022-03-26 17:17:28.865427 Epoch 100, Training Loss 0.450539146047419\n",
      "2022-03-26 17:17:28.892950 Epoch 100, Training Loss 0.4518151427702526\n",
      "2022-03-26 17:17:28.915878 Epoch 100, Training Loss 0.45259794116477525\n",
      "2022-03-26 17:17:28.938643 Epoch 100, Training Loss 0.45347490445579713\n",
      "2022-03-26 17:17:28.966759 Epoch 100, Training Loss 0.4544138804344875\n",
      "2022-03-26 17:17:28.993533 Epoch 100, Training Loss 0.45538089155693495\n",
      "2022-03-26 17:17:29.016991 Epoch 100, Training Loss 0.4564077763834878\n",
      "2022-03-26 17:17:29.041051 Epoch 100, Training Loss 0.4574655231536197\n",
      "2022-03-26 17:17:29.066499 Epoch 100, Training Loss 0.45820900298597866\n",
      "2022-03-26 17:17:29.089111 Epoch 100, Training Loss 0.45904327509805676\n",
      "2022-03-26 17:17:29.111946 Epoch 100, Training Loss 0.46000163051325954\n",
      "2022-03-26 17:17:29.141709 Epoch 100, Training Loss 0.4612622227891327\n",
      "2022-03-26 17:17:29.164963 Epoch 100, Training Loss 0.4622159243925758\n",
      "2022-03-26 17:17:29.196041 Epoch 100, Training Loss 0.4630629853595553\n",
      "2022-03-26 17:17:29.221274 Epoch 100, Training Loss 0.4635508978915641\n",
      "2022-03-26 17:17:29.244980 Epoch 100, Training Loss 0.4641903916283337\n",
      "2022-03-26 17:17:29.273847 Epoch 100, Training Loss 0.4651472419881455\n",
      "2022-03-26 17:17:29.297171 Epoch 100, Training Loss 0.4657587926558521\n",
      "2022-03-26 17:17:29.320360 Epoch 100, Training Loss 0.46660844162297066\n",
      "2022-03-26 17:17:29.346426 Epoch 100, Training Loss 0.46759095612694235\n",
      "2022-03-26 17:17:29.369634 Epoch 100, Training Loss 0.4684549387153762\n",
      "2022-03-26 17:17:29.392922 Epoch 100, Training Loss 0.46922457789825966\n",
      "2022-03-26 17:17:29.422691 Epoch 100, Training Loss 0.47005036092170366\n",
      "2022-03-26 17:17:29.446617 Epoch 100, Training Loss 0.4707194469164095\n",
      "2022-03-26 17:17:29.470253 Epoch 100, Training Loss 0.4716621809603308\n",
      "2022-03-26 17:17:29.495557 Epoch 100, Training Loss 0.4725468338602949\n",
      "2022-03-26 17:17:29.518778 Epoch 100, Training Loss 0.4732224678291994\n",
      "2022-03-26 17:17:29.542707 Epoch 100, Training Loss 0.4739894536907411\n",
      "2022-03-26 17:17:29.565659 Epoch 100, Training Loss 0.4748107786373714\n",
      "2022-03-26 17:17:29.588328 Epoch 100, Training Loss 0.47570162843865205\n",
      "2022-03-26 17:17:29.611042 Epoch 100, Training Loss 0.4766229561832555\n",
      "2022-03-26 17:17:29.641955 Epoch 100, Training Loss 0.4775037744161113\n",
      "2022-03-26 17:17:29.666263 Epoch 100, Training Loss 0.47848573441395675\n",
      "2022-03-26 17:17:29.689480 Epoch 100, Training Loss 0.47941234082821993\n",
      "2022-03-26 17:17:29.712608 Epoch 100, Training Loss 0.48022615452251777\n",
      "2022-03-26 17:17:29.738403 Epoch 100, Training Loss 0.4810615724614819\n",
      "2022-03-26 17:17:29.761547 Epoch 100, Training Loss 0.4820193080493556\n",
      "2022-03-26 17:17:29.784930 Epoch 100, Training Loss 0.4828206213843792\n",
      "2022-03-26 17:17:29.808367 Epoch 100, Training Loss 0.48386187519868623\n",
      "2022-03-26 17:17:29.831882 Epoch 100, Training Loss 0.4846167103256411\n",
      "2022-03-26 17:17:29.871455 Epoch 100, Training Loss 0.48544283291263046\n",
      "2022-03-26 17:17:29.895454 Epoch 100, Training Loss 0.4864480658565336\n",
      "2022-03-26 17:17:29.918642 Epoch 100, Training Loss 0.48731930039422894\n",
      "2022-03-26 17:17:29.945674 Epoch 100, Training Loss 0.4883485572112491\n",
      "2022-03-26 17:17:29.968583 Epoch 100, Training Loss 0.48937117489402554\n",
      "2022-03-26 17:17:29.991516 Epoch 100, Training Loss 0.4905577927751614\n",
      "2022-03-26 17:17:30.014361 Epoch 100, Training Loss 0.49130494812565384\n",
      "2022-03-26 17:17:30.044836 Epoch 100, Training Loss 0.4921829108996769\n",
      "2022-03-26 17:17:30.069130 Epoch 100, Training Loss 0.4931880636593265\n",
      "2022-03-26 17:17:30.101853 Epoch 100, Training Loss 0.4938606138119612\n",
      "2022-03-26 17:17:30.125806 Epoch 100, Training Loss 0.4951223249325667\n",
      "2022-03-26 17:17:30.149407 Epoch 100, Training Loss 0.49592689525745715\n",
      "2022-03-26 17:17:30.178826 Epoch 100, Training Loss 0.49649146496487395\n",
      "2022-03-26 17:17:30.202044 Epoch 100, Training Loss 0.49742764211676616\n",
      "2022-03-26 17:17:30.224827 Epoch 100, Training Loss 0.4980649502609697\n",
      "2022-03-26 17:17:30.248530 Epoch 100, Training Loss 0.49893371806577647\n",
      "2022-03-26 17:17:30.271906 Epoch 100, Training Loss 0.4997602042258548\n",
      "2022-03-26 17:17:30.295690 Epoch 100, Training Loss 0.5005419947530912\n",
      "2022-03-26 17:17:30.325885 Epoch 100, Training Loss 0.5013968897673785\n",
      "2022-03-26 17:17:30.351509 Epoch 100, Training Loss 0.5020791798677591\n",
      "2022-03-26 17:17:30.374726 Epoch 100, Training Loss 0.5029020293060776\n",
      "2022-03-26 17:17:30.400474 Epoch 100, Training Loss 0.5037962822124477\n",
      "2022-03-26 17:17:30.423397 Epoch 100, Training Loss 0.5046706187831776\n",
      "2022-03-26 17:17:30.446959 Epoch 100, Training Loss 0.5055101002040117\n",
      "2022-03-26 17:17:30.469884 Epoch 100, Training Loss 0.5062700209334073\n",
      "2022-03-26 17:17:30.492550 Epoch 100, Training Loss 0.5068955476326711\n",
      "2022-03-26 17:17:30.514593 Epoch 100, Training Loss 0.5078603054403954\n",
      "2022-03-26 17:17:30.543951 Epoch 100, Training Loss 0.5087712773734041\n",
      "2022-03-26 17:17:30.567429 Epoch 100, Training Loss 0.5098672295775255\n",
      "2022-03-26 17:17:30.589677 Epoch 100, Training Loss 0.5106253307646192\n",
      "2022-03-26 17:17:30.615207 Epoch 100, Training Loss 0.5113902454028654\n",
      "2022-03-26 17:17:30.637888 Epoch 100, Training Loss 0.5122177569610079\n",
      "2022-03-26 17:17:30.660587 Epoch 100, Training Loss 0.5131284295750396\n",
      "2022-03-26 17:17:30.683040 Epoch 100, Training Loss 0.5138484190796953\n",
      "2022-03-26 17:17:30.705876 Epoch 100, Training Loss 0.5146099102619054\n",
      "2022-03-26 17:17:30.728443 Epoch 100, Training Loss 0.515588053275862\n",
      "2022-03-26 17:17:30.757290 Epoch 100, Training Loss 0.5164578210209947\n",
      "2022-03-26 17:17:30.780826 Epoch 100, Training Loss 0.5172578659661285\n",
      "2022-03-26 17:17:30.803765 Epoch 100, Training Loss 0.518322181976055\n",
      "2022-03-26 17:17:30.827139 Epoch 100, Training Loss 0.5189739180647809\n",
      "2022-03-26 17:17:30.855037 Epoch 100, Training Loss 0.5198615540933731\n",
      "2022-03-26 17:17:30.880749 Epoch 100, Training Loss 0.5207168017049579\n",
      "2022-03-26 17:17:30.908757 Epoch 100, Training Loss 0.5215509617724992\n",
      "2022-03-26 17:17:30.930961 Epoch 100, Training Loss 0.5223149501759073\n",
      "2022-03-26 17:17:30.957625 Epoch 100, Training Loss 0.5232484324661362\n",
      "2022-03-26 17:17:30.989185 Epoch 100, Training Loss 0.5240604096208997\n",
      "2022-03-26 17:17:31.012751 Epoch 100, Training Loss 0.5249823041431739\n",
      "2022-03-26 17:17:31.035243 Epoch 100, Training Loss 0.5258619008619158\n",
      "2022-03-26 17:17:31.060284 Epoch 100, Training Loss 0.5267786532259353\n",
      "2022-03-26 17:17:31.082434 Epoch 100, Training Loss 0.5274593850688252\n",
      "2022-03-26 17:17:31.105055 Epoch 100, Training Loss 0.5281986618590782\n",
      "2022-03-26 17:17:31.127582 Epoch 100, Training Loss 0.5292212072845615\n",
      "2022-03-26 17:17:31.150705 Epoch 100, Training Loss 0.5301549555852895\n",
      "2022-03-26 17:17:31.173420 Epoch 100, Training Loss 0.5308505152649892\n",
      "2022-03-26 17:17:31.202152 Epoch 100, Training Loss 0.5315298361089223\n",
      "2022-03-26 17:17:31.226128 Epoch 100, Training Loss 0.5323311582093349\n",
      "2022-03-26 17:17:31.249469 Epoch 100, Training Loss 0.5330881492408646\n",
      "2022-03-26 17:17:31.271986 Epoch 100, Training Loss 0.5340563577916616\n",
      "2022-03-26 17:17:31.298670 Epoch 100, Training Loss 0.5349702275622531\n",
      "2022-03-26 17:17:31.321394 Epoch 100, Training Loss 0.5361535661970563\n",
      "2022-03-26 17:17:31.347375 Epoch 100, Training Loss 0.5371481428670761\n",
      "2022-03-26 17:17:31.370407 Epoch 100, Training Loss 0.5377999400086415\n",
      "2022-03-26 17:17:31.393666 Epoch 100, Training Loss 0.5386508048495369\n",
      "2022-03-26 17:17:31.423416 Epoch 100, Training Loss 0.5394048526159028\n",
      "2022-03-26 17:17:31.447571 Epoch 100, Training Loss 0.5404373038455349\n",
      "2022-03-26 17:17:31.470354 Epoch 100, Training Loss 0.5412965461878521\n",
      "2022-03-26 17:17:31.495167 Epoch 100, Training Loss 0.5422318743927704\n",
      "2022-03-26 17:17:31.520222 Epoch 100, Training Loss 0.5430262015603692\n",
      "2022-03-26 17:17:31.543268 Epoch 100, Training Loss 0.5440488718354793\n",
      "2022-03-26 17:17:31.566161 Epoch 100, Training Loss 0.5448923069802697\n",
      "2022-03-26 17:17:31.588622 Epoch 100, Training Loss 0.5456184748645938\n",
      "2022-03-26 17:17:31.611185 Epoch 100, Training Loss 0.5460696811871151\n",
      "2022-03-26 17:17:31.639679 Epoch 100, Training Loss 0.547023057632739\n",
      "2022-03-26 17:17:31.665839 Epoch 100, Training Loss 0.5478356822829722\n",
      "2022-03-26 17:17:31.688421 Epoch 100, Training Loss 0.548704264109092\n",
      "2022-03-26 17:17:31.713512 Epoch 100, Training Loss 0.5493658742941249\n",
      "2022-03-26 17:17:31.735854 Epoch 100, Training Loss 0.550457717360133\n",
      "2022-03-26 17:17:31.758336 Epoch 100, Training Loss 0.5514616816092635\n",
      "2022-03-26 17:17:31.780636 Epoch 100, Training Loss 0.5525661398230306\n",
      "2022-03-26 17:17:31.803260 Epoch 100, Training Loss 0.5538072711823846\n",
      "2022-03-26 17:17:31.826004 Epoch 100, Training Loss 0.5546226311675118\n",
      "2022-03-26 17:17:31.860804 Epoch 100, Training Loss 0.5553957734571393\n",
      "2022-03-26 17:17:31.884957 Epoch 100, Training Loss 0.5562077737830179\n",
      "2022-03-26 17:17:31.915691 Epoch 100, Training Loss 0.557008750298444\n",
      "2022-03-26 17:17:31.944560 Epoch 100, Training Loss 0.5579855759887744\n",
      "2022-03-26 17:17:31.967724 Epoch 100, Training Loss 0.5589340735426949\n",
      "2022-03-26 17:17:31.990298 Epoch 100, Training Loss 0.5598402935678087\n",
      "2022-03-26 17:17:32.013278 Epoch 100, Training Loss 0.5606054069136109\n",
      "2022-03-26 17:17:32.035575 Epoch 100, Training Loss 0.5612840122731445\n",
      "2022-03-26 17:17:32.058538 Epoch 100, Training Loss 0.561956806027371\n",
      "2022-03-26 17:17:32.088290 Epoch 100, Training Loss 0.5627307436807686\n",
      "2022-03-26 17:17:32.111269 Epoch 100, Training Loss 0.5635772673675167\n",
      "2022-03-26 17:17:32.134033 Epoch 100, Training Loss 0.5645005660288779\n",
      "2022-03-26 17:17:32.160661 Epoch 100, Training Loss 0.5655478043172061\n",
      "2022-03-26 17:17:32.183656 Epoch 100, Training Loss 0.5665198158272697\n",
      "2022-03-26 17:17:32.206628 Epoch 100, Training Loss 0.5678609371032861\n",
      "2022-03-26 17:17:32.229865 Epoch 100, Training Loss 0.5686149077342294\n",
      "2022-03-26 17:17:32.252731 Epoch 100, Training Loss 0.5692787666607391\n",
      "2022-03-26 17:17:32.275420 Epoch 100, Training Loss 0.5699876823541149\n",
      "2022-03-26 17:17:32.303182 Epoch 100, Training Loss 0.5707066716897823\n",
      "2022-03-26 17:17:32.331353 Epoch 100, Training Loss 0.5716696273335411\n",
      "2022-03-26 17:17:32.354687 Epoch 100, Training Loss 0.5723796253618987\n",
      "2022-03-26 17:17:32.376617 Epoch 100, Training Loss 0.5735518029126365\n",
      "2022-03-26 17:17:32.403331 Epoch 100, Training Loss 0.5745040842944094\n",
      "2022-03-26 17:17:32.426452 Epoch 100, Training Loss 0.5754408828742669\n",
      "2022-03-26 17:17:32.450567 Epoch 100, Training Loss 0.5763683445618281\n",
      "2022-03-26 17:17:32.473154 Epoch 100, Training Loss 0.5769205193614106\n",
      "2022-03-26 17:17:32.496260 Epoch 100, Training Loss 0.577772615472679\n",
      "2022-03-26 17:17:32.525670 Epoch 100, Training Loss 0.578725600067307\n",
      "2022-03-26 17:17:32.549605 Epoch 100, Training Loss 0.5795441351049696\n",
      "2022-03-26 17:17:32.572432 Epoch 100, Training Loss 0.580486301456571\n",
      "2022-03-26 17:17:32.599518 Epoch 100, Training Loss 0.5811256314711193\n",
      "2022-03-26 17:17:32.622793 Epoch 100, Training Loss 0.5820741109607165\n",
      "2022-03-26 17:17:32.646837 Epoch 100, Training Loss 0.5831747471600237\n",
      "2022-03-26 17:17:32.670400 Epoch 100, Training Loss 0.5841587908813716\n",
      "2022-03-26 17:17:32.693105 Epoch 100, Training Loss 0.5850177840579807\n",
      "2022-03-26 17:17:32.715647 Epoch 100, Training Loss 0.5859797758900601\n",
      "2022-03-26 17:17:32.744626 Epoch 100, Training Loss 0.5869878735917303\n",
      "2022-03-26 17:17:32.774149 Epoch 100, Training Loss 0.5880778443706615\n",
      "2022-03-26 17:17:32.797435 Epoch 100, Training Loss 0.5893527632555389\n",
      "2022-03-26 17:17:32.820815 Epoch 100, Training Loss 0.590698449546114\n",
      "2022-03-26 17:17:32.847474 Epoch 100, Training Loss 0.5915058770829149\n",
      "2022-03-26 17:17:32.870795 Epoch 100, Training Loss 0.5924099663563092\n",
      "2022-03-26 17:17:32.893505 Epoch 100, Training Loss 0.593174793645549\n",
      "2022-03-26 17:17:32.916592 Epoch 100, Training Loss 0.594069012938558\n",
      "2022-03-26 17:17:32.952859 Epoch 100, Training Loss 0.5949897014195352\n",
      "2022-03-26 17:17:32.978917 Epoch 100, Training Loss 0.5959343519585821\n",
      "2022-03-26 17:17:33.002011 Epoch 100, Training Loss 0.597025289445582\n",
      "2022-03-26 17:17:33.028707 Epoch 100, Training Loss 0.5977981688497621\n",
      "2022-03-26 17:17:33.051614 Epoch 100, Training Loss 0.598761389132046\n",
      "2022-03-26 17:17:33.083304 Epoch 100, Training Loss 0.5998261325880695\n",
      "2022-03-26 17:17:33.105984 Epoch 100, Training Loss 0.6007826506253093\n",
      "2022-03-26 17:17:33.130327 Epoch 100, Training Loss 0.6018634201849208\n",
      "2022-03-26 17:17:33.155501 Epoch 100, Training Loss 0.6029535578111248\n",
      "2022-03-26 17:17:33.185522 Epoch 100, Training Loss 0.603865254375026\n",
      "2022-03-26 17:17:33.209494 Epoch 100, Training Loss 0.6048387630135202\n",
      "2022-03-26 17:17:33.235797 Epoch 100, Training Loss 0.6056283356817177\n",
      "2022-03-26 17:17:33.258981 Epoch 100, Training Loss 0.6064951507103108\n",
      "2022-03-26 17:17:33.282826 Epoch 100, Training Loss 0.6074449511821313\n",
      "2022-03-26 17:17:33.306022 Epoch 100, Training Loss 0.6083338168804603\n",
      "2022-03-26 17:17:33.331252 Epoch 100, Training Loss 0.6093165141618465\n",
      "2022-03-26 17:17:33.355098 Epoch 100, Training Loss 0.6104088130280795\n",
      "2022-03-26 17:17:33.389037 Epoch 100, Training Loss 0.6114360387901516\n",
      "2022-03-26 17:17:33.414002 Epoch 100, Training Loss 0.6121354202556488\n",
      "2022-03-26 17:17:33.437793 Epoch 100, Training Loss 0.6131888475183331\n",
      "2022-03-26 17:17:33.465065 Epoch 100, Training Loss 0.614136096499765\n",
      "2022-03-26 17:17:33.488086 Epoch 100, Training Loss 0.6149293744884183\n",
      "2022-03-26 17:17:33.511456 Epoch 100, Training Loss 0.6161390954195081\n",
      "2022-03-26 17:17:33.535007 Epoch 100, Training Loss 0.6169511125901775\n",
      "2022-03-26 17:17:33.558541 Epoch 100, Training Loss 0.6180236660077444\n",
      "2022-03-26 17:17:33.581777 Epoch 100, Training Loss 0.6189804884707532\n",
      "2022-03-26 17:17:33.611925 Epoch 100, Training Loss 0.6196720161477624\n",
      "2022-03-26 17:17:33.635916 Epoch 100, Training Loss 0.6205375702561015\n",
      "2022-03-26 17:17:33.659325 Epoch 100, Training Loss 0.6214073139916906\n",
      "2022-03-26 17:17:33.682216 Epoch 100, Training Loss 0.622376383075019\n",
      "2022-03-26 17:17:33.712777 Epoch 100, Training Loss 0.6233485399380975\n",
      "2022-03-26 17:17:33.738478 Epoch 100, Training Loss 0.6241450653128002\n",
      "2022-03-26 17:17:33.762837 Epoch 100, Training Loss 0.625115418335056\n",
      "2022-03-26 17:17:33.787371 Epoch 100, Training Loss 0.6261133853050754\n",
      "2022-03-26 17:17:33.810829 Epoch 100, Training Loss 0.626692571641539\n",
      "2022-03-26 17:17:33.842953 Epoch 100, Training Loss 0.6277746768177622\n",
      "2022-03-26 17:17:33.868360 Epoch 100, Training Loss 0.6286637495317118\n",
      "2022-03-26 17:17:33.891936 Epoch 100, Training Loss 0.6295961714385415\n",
      "2022-03-26 17:17:33.918857 Epoch 100, Training Loss 0.6306254868693364\n",
      "2022-03-26 17:17:33.944305 Epoch 100, Training Loss 0.6319671595264273\n",
      "2022-03-26 17:17:33.984762 Epoch 100, Training Loss 0.6327461497024502\n",
      "2022-03-26 17:17:34.008713 Epoch 100, Training Loss 0.6336133925582442\n",
      "2022-03-26 17:17:34.032442 Epoch 100, Training Loss 0.634541954797552\n",
      "2022-03-26 17:17:34.063693 Epoch 100, Training Loss 0.6352703154010846\n",
      "2022-03-26 17:17:34.088700 Epoch 100, Training Loss 0.6359454337364573\n",
      "2022-03-26 17:17:34.112671 Epoch 100, Training Loss 0.6369262912008159\n",
      "2022-03-26 17:17:34.135725 Epoch 100, Training Loss 0.6375318927228298\n",
      "2022-03-26 17:17:34.162368 Epoch 100, Training Loss 0.638359452147618\n",
      "2022-03-26 17:17:34.185815 Epoch 100, Training Loss 0.6391646451962268\n",
      "2022-03-26 17:17:34.209068 Epoch 100, Training Loss 0.6405148219574442\n",
      "2022-03-26 17:17:34.232619 Epoch 100, Training Loss 0.6414286623068173\n",
      "2022-03-26 17:17:34.256209 Epoch 100, Training Loss 0.6424675374994497\n",
      "2022-03-26 17:17:34.288769 Epoch 100, Training Loss 0.6435630459462285\n",
      "2022-03-26 17:17:34.313655 Epoch 100, Training Loss 0.6443352482812789\n",
      "2022-03-26 17:17:34.340022 Epoch 100, Training Loss 0.645099649343954\n",
      "2022-03-26 17:17:34.363506 Epoch 100, Training Loss 0.6458975880042367\n",
      "2022-03-26 17:17:34.391563 Epoch 100, Training Loss 0.6469211314645265\n",
      "2022-03-26 17:17:34.416211 Epoch 100, Training Loss 0.6479097582647563\n",
      "2022-03-26 17:17:34.439579 Epoch 100, Training Loss 0.6485797227800959\n",
      "2022-03-26 17:17:34.463565 Epoch 100, Training Loss 0.64963332939026\n",
      "2022-03-26 17:17:34.487691 Epoch 100, Training Loss 0.6505425027416795\n",
      "2022-03-26 17:17:34.518877 Epoch 100, Training Loss 0.6514607241848851\n",
      "2022-03-26 17:17:34.542932 Epoch 100, Training Loss 0.6520781382118039\n",
      "2022-03-26 17:17:34.571128 Epoch 100, Training Loss 0.6529929999957609\n",
      "2022-03-26 17:17:34.594931 Epoch 100, Training Loss 0.6535789977635265\n",
      "2022-03-26 17:17:34.618881 Epoch 100, Training Loss 0.6540654797840606\n",
      "2022-03-26 17:17:34.642732 Epoch 100, Training Loss 0.654920188438557\n",
      "2022-03-26 17:17:34.666496 Epoch 100, Training Loss 0.655802986887105\n",
      "2022-03-26 17:17:34.689932 Epoch 100, Training Loss 0.6567300577907611\n",
      "2022-03-26 17:17:34.714035 Epoch 100, Training Loss 0.6576587098943608\n",
      "2022-03-26 17:17:34.745164 Epoch 100, Training Loss 0.6587717913453232\n",
      "2022-03-26 17:17:34.769638 Epoch 100, Training Loss 0.6597245955253805\n",
      "2022-03-26 17:17:34.793613 Epoch 100, Training Loss 0.6609127494837622\n",
      "2022-03-26 17:17:34.821517 Epoch 100, Training Loss 0.6617403711809222\n",
      "2022-03-26 17:17:34.845942 Epoch 100, Training Loss 0.6625467027392229\n",
      "2022-03-26 17:17:34.871243 Epoch 100, Training Loss 0.6635353884581104\n",
      "2022-03-26 17:17:34.902937 Epoch 100, Training Loss 0.6646016563296013\n",
      "2022-03-26 17:17:34.926368 Epoch 100, Training Loss 0.6654064881679652\n",
      "2022-03-26 17:17:34.956775 Epoch 100, Training Loss 0.6662147772281676\n",
      "2022-03-26 17:17:34.985310 Epoch 100, Training Loss 0.667085747051117\n",
      "2022-03-26 17:17:35.016884 Epoch 100, Training Loss 0.668086883661997\n",
      "2022-03-26 17:17:35.044882 Epoch 100, Training Loss 0.6691231480644791\n",
      "2022-03-26 17:17:35.068714 Epoch 100, Training Loss 0.6702241605657446\n",
      "2022-03-26 17:17:35.092157 Epoch 100, Training Loss 0.6710350778706543\n",
      "2022-03-26 17:17:35.115430 Epoch 100, Training Loss 0.6721272587471301\n",
      "2022-03-26 17:17:35.138646 Epoch 100, Training Loss 0.6732460806131972\n",
      "2022-03-26 17:17:35.162224 Epoch 100, Training Loss 0.6743408408006439\n",
      "2022-03-26 17:17:35.194380 Epoch 100, Training Loss 0.6753206587660953\n",
      "2022-03-26 17:17:35.219352 Epoch 100, Training Loss 0.67665390125321\n",
      "2022-03-26 17:17:35.247443 Epoch 100, Training Loss 0.6776370685118849\n",
      "2022-03-26 17:17:35.271562 Epoch 100, Training Loss 0.6784858864439113\n",
      "2022-03-26 17:17:35.295370 Epoch 100, Training Loss 0.6794292918404045\n",
      "2022-03-26 17:17:35.319101 Epoch 100, Training Loss 0.6801274245047508\n",
      "2022-03-26 17:17:35.345606 Epoch 100, Training Loss 0.6808024122739387\n",
      "2022-03-26 17:17:35.369775 Epoch 100, Training Loss 0.6817978479520744\n",
      "2022-03-26 17:17:35.392596 Epoch 100, Training Loss 0.6826706501986365\n",
      "2022-03-26 17:17:35.425065 Epoch 100, Training Loss 0.6837661087208087\n",
      "2022-03-26 17:17:35.449423 Epoch 100, Training Loss 0.6847612689370695\n",
      "2022-03-26 17:17:35.473825 Epoch 100, Training Loss 0.6858143945179327\n",
      "2022-03-26 17:17:35.499368 Epoch 100, Training Loss 0.6868599634951033\n",
      "2022-03-26 17:17:35.523181 Epoch 100, Training Loss 0.6876620105312913\n",
      "2022-03-26 17:17:35.547291 Epoch 100, Training Loss 0.6884994365065299\n",
      "2022-03-26 17:17:35.570906 Epoch 100, Training Loss 0.6895780053437518\n",
      "2022-03-26 17:17:35.593830 Epoch 100, Training Loss 0.6905927033070713\n",
      "2022-03-26 17:17:35.616203 Epoch 100, Training Loss 0.6914937802592812\n",
      "2022-03-26 17:17:35.645418 Epoch 100, Training Loss 0.692271682764868\n",
      "2022-03-26 17:17:35.669062 Epoch 100, Training Loss 0.6934311868589552\n",
      "2022-03-26 17:17:35.691965 Epoch 100, Training Loss 0.6943671316137094\n",
      "2022-03-26 17:17:35.714537 Epoch 100, Training Loss 0.6953155247451704\n",
      "2022-03-26 17:17:35.738703 Epoch 100, Training Loss 0.6961901060608037\n",
      "2022-03-26 17:17:35.761328 Epoch 100, Training Loss 0.6969901863723764\n",
      "2022-03-26 17:17:35.784307 Epoch 100, Training Loss 0.6979694190385092\n",
      "2022-03-26 17:17:35.807514 Epoch 100, Training Loss 0.6990482648620215\n",
      "2022-03-26 17:17:35.816457 Epoch 100, Training Loss 0.6999214182576865\n",
      "2022-03-26 17:32:16.481502 Epoch 150, Training Loss 0.0006688706710210542\n",
      "2022-03-26 17:32:16.505752 Epoch 150, Training Loss 0.001659768545414176\n",
      "2022-03-26 17:32:16.529143 Epoch 150, Training Loss 0.0026383803933477767\n",
      "2022-03-26 17:32:16.552101 Epoch 150, Training Loss 0.0034301527930647516\n",
      "2022-03-26 17:32:16.575015 Epoch 150, Training Loss 0.0044459477257545645\n",
      "2022-03-26 17:32:16.599331 Epoch 150, Training Loss 0.005404509089486983\n",
      "2022-03-26 17:32:16.623048 Epoch 150, Training Loss 0.005949965447111203\n",
      "2022-03-26 17:32:16.646986 Epoch 150, Training Loss 0.00666319378806502\n",
      "2022-03-26 17:32:16.670431 Epoch 150, Training Loss 0.007505528000004761\n",
      "2022-03-26 17:32:16.708670 Epoch 150, Training Loss 0.008410119644516264\n",
      "2022-03-26 17:32:16.732576 Epoch 150, Training Loss 0.008900213767500484\n",
      "2022-03-26 17:32:16.757319 Epoch 150, Training Loss 0.009652067595125768\n",
      "2022-03-26 17:32:16.787222 Epoch 150, Training Loss 0.010346857826118274\n",
      "2022-03-26 17:32:16.816093 Epoch 150, Training Loss 0.011273193885298335\n",
      "2022-03-26 17:32:16.839195 Epoch 150, Training Loss 0.01191118965520883\n",
      "2022-03-26 17:32:16.861945 Epoch 150, Training Loss 0.01271881597578678\n",
      "2022-03-26 17:32:16.884534 Epoch 150, Training Loss 0.013471451218780654\n",
      "2022-03-26 17:32:16.907874 Epoch 150, Training Loss 0.014141718178149075\n",
      "2022-03-26 17:32:16.940353 Epoch 150, Training Loss 0.014787620938647434\n",
      "2022-03-26 17:32:16.963649 Epoch 150, Training Loss 0.015560997912036183\n",
      "2022-03-26 17:32:16.986470 Epoch 150, Training Loss 0.016371689755898304\n",
      "2022-03-26 17:32:17.009394 Epoch 150, Training Loss 0.01739682020891048\n",
      "2022-03-26 17:32:17.042550 Epoch 150, Training Loss 0.0182368395197422\n",
      "2022-03-26 17:32:17.065788 Epoch 150, Training Loss 0.018884291543680078\n",
      "2022-03-26 17:32:17.090350 Epoch 150, Training Loss 0.019578864240585386\n",
      "2022-03-26 17:32:17.113096 Epoch 150, Training Loss 0.02048122154934632\n",
      "2022-03-26 17:32:17.135982 Epoch 150, Training Loss 0.021092616459902597\n",
      "2022-03-26 17:32:17.166146 Epoch 150, Training Loss 0.021640346521306832\n",
      "2022-03-26 17:32:17.189470 Epoch 150, Training Loss 0.02260002497669376\n",
      "2022-03-26 17:32:17.212371 Epoch 150, Training Loss 0.02331129131872026\n",
      "2022-03-26 17:32:17.240288 Epoch 150, Training Loss 0.02419008726201704\n",
      "2022-03-26 17:32:17.268099 Epoch 150, Training Loss 0.02476033743690042\n",
      "2022-03-26 17:32:17.291916 Epoch 150, Training Loss 0.025808980321640248\n",
      "2022-03-26 17:32:17.314400 Epoch 150, Training Loss 0.02654092703633906\n",
      "2022-03-26 17:32:17.340661 Epoch 150, Training Loss 0.027331560087935698\n",
      "2022-03-26 17:32:17.363387 Epoch 150, Training Loss 0.028148801277970415\n",
      "2022-03-26 17:32:17.393089 Epoch 150, Training Loss 0.028782254061125733\n",
      "2022-03-26 17:32:17.416032 Epoch 150, Training Loss 0.029575177165858276\n",
      "2022-03-26 17:32:17.438398 Epoch 150, Training Loss 0.030404725190623642\n",
      "2022-03-26 17:32:17.463303 Epoch 150, Training Loss 0.031005515238208235\n",
      "2022-03-26 17:32:17.486866 Epoch 150, Training Loss 0.031734248789984855\n",
      "2022-03-26 17:32:17.509728 Epoch 150, Training Loss 0.032311144082442574\n",
      "2022-03-26 17:32:17.533659 Epoch 150, Training Loss 0.032917379342076725\n",
      "2022-03-26 17:32:17.556594 Epoch 150, Training Loss 0.03362073831241149\n",
      "2022-03-26 17:32:17.579368 Epoch 150, Training Loss 0.03427627309204063\n",
      "2022-03-26 17:32:17.608789 Epoch 150, Training Loss 0.03495388773396192\n",
      "2022-03-26 17:32:17.632890 Epoch 150, Training Loss 0.035859969951917446\n",
      "2022-03-26 17:32:17.655385 Epoch 150, Training Loss 0.036562185610651665\n",
      "2022-03-26 17:32:17.680649 Epoch 150, Training Loss 0.03713540294591118\n",
      "2022-03-26 17:32:17.703637 Epoch 150, Training Loss 0.03784938320479429\n",
      "2022-03-26 17:32:17.726015 Epoch 150, Training Loss 0.038847089880872564\n",
      "2022-03-26 17:32:17.748487 Epoch 150, Training Loss 0.03947869198554007\n",
      "2022-03-26 17:32:17.770828 Epoch 150, Training Loss 0.04022086425053189\n",
      "2022-03-26 17:32:17.795064 Epoch 150, Training Loss 0.0407882429602201\n",
      "2022-03-26 17:32:17.829733 Epoch 150, Training Loss 0.04148627272652238\n",
      "2022-03-26 17:32:17.856087 Epoch 150, Training Loss 0.04236262419339641\n",
      "2022-03-26 17:32:17.879222 Epoch 150, Training Loss 0.04305409950673428\n",
      "2022-03-26 17:32:17.902242 Epoch 150, Training Loss 0.04379666415626741\n",
      "2022-03-26 17:32:17.934975 Epoch 150, Training Loss 0.044554898882156135\n",
      "2022-03-26 17:32:17.958613 Epoch 150, Training Loss 0.045408399437394595\n",
      "2022-03-26 17:32:17.981276 Epoch 150, Training Loss 0.0460018243859796\n",
      "2022-03-26 17:32:18.007613 Epoch 150, Training Loss 0.04704900970087027\n",
      "2022-03-26 17:32:18.030083 Epoch 150, Training Loss 0.04788625526153828\n",
      "2022-03-26 17:32:18.059862 Epoch 150, Training Loss 0.04847671674645465\n",
      "2022-03-26 17:32:18.083000 Epoch 150, Training Loss 0.04910080691280268\n",
      "2022-03-26 17:32:18.106329 Epoch 150, Training Loss 0.04997614853065032\n",
      "2022-03-26 17:32:18.132345 Epoch 150, Training Loss 0.050718554717195614\n",
      "2022-03-26 17:32:18.154649 Epoch 150, Training Loss 0.051580082539402314\n",
      "2022-03-26 17:32:18.176728 Epoch 150, Training Loss 0.05257867436732173\n",
      "2022-03-26 17:32:18.199386 Epoch 150, Training Loss 0.05329937005744261\n",
      "2022-03-26 17:32:18.222446 Epoch 150, Training Loss 0.05392492027081492\n",
      "2022-03-26 17:32:18.245667 Epoch 150, Training Loss 0.05479867794477116\n",
      "2022-03-26 17:32:18.272325 Epoch 150, Training Loss 0.05556252343422922\n",
      "2022-03-26 17:32:18.297719 Epoch 150, Training Loss 0.05636152362122255\n",
      "2022-03-26 17:32:18.320285 Epoch 150, Training Loss 0.057467447331799267\n",
      "2022-03-26 17:32:18.348637 Epoch 150, Training Loss 0.05848307739895628\n",
      "2022-03-26 17:32:18.372798 Epoch 150, Training Loss 0.05924491149842587\n",
      "2022-03-26 17:32:18.396703 Epoch 150, Training Loss 0.05991983737634576\n",
      "2022-03-26 17:32:18.419421 Epoch 150, Training Loss 0.06073635706053975\n",
      "2022-03-26 17:32:18.442245 Epoch 150, Training Loss 0.06158009266761868\n",
      "2022-03-26 17:32:18.466003 Epoch 150, Training Loss 0.062263003586198365\n",
      "2022-03-26 17:32:18.501724 Epoch 150, Training Loss 0.0631660583912564\n",
      "2022-03-26 17:32:18.525811 Epoch 150, Training Loss 0.06405734623332157\n",
      "2022-03-26 17:32:18.549543 Epoch 150, Training Loss 0.06464514220157243\n",
      "2022-03-26 17:32:18.576633 Epoch 150, Training Loss 0.06533578335476653\n",
      "2022-03-26 17:32:18.600592 Epoch 150, Training Loss 0.06615739915986805\n",
      "2022-03-26 17:32:18.624121 Epoch 150, Training Loss 0.06683658288262995\n",
      "2022-03-26 17:32:18.648238 Epoch 150, Training Loss 0.06755656057306568\n",
      "2022-03-26 17:32:18.671879 Epoch 150, Training Loss 0.06838324140099918\n",
      "2022-03-26 17:32:18.695825 Epoch 150, Training Loss 0.06922714621819498\n",
      "2022-03-26 17:32:18.726497 Epoch 150, Training Loss 0.07004603065188279\n",
      "2022-03-26 17:32:18.751471 Epoch 150, Training Loss 0.07098162052271616\n",
      "2022-03-26 17:32:18.775542 Epoch 150, Training Loss 0.07179976706309697\n",
      "2022-03-26 17:32:18.805503 Epoch 150, Training Loss 0.07264098204920054\n",
      "2022-03-26 17:32:18.828772 Epoch 150, Training Loss 0.07344752153777101\n",
      "2022-03-26 17:32:18.859581 Epoch 150, Training Loss 0.07451786798284486\n",
      "2022-03-26 17:32:18.884507 Epoch 150, Training Loss 0.0754871254077043\n",
      "2022-03-26 17:32:18.908389 Epoch 150, Training Loss 0.0762659828071399\n",
      "2022-03-26 17:32:18.935477 Epoch 150, Training Loss 0.07746210084546862\n",
      "2022-03-26 17:32:18.961930 Epoch 150, Training Loss 0.07828639253326085\n",
      "2022-03-26 17:32:18.985291 Epoch 150, Training Loss 0.07909919767428542\n",
      "2022-03-26 17:32:19.010102 Epoch 150, Training Loss 0.0798850252347834\n",
      "2022-03-26 17:32:19.036394 Epoch 150, Training Loss 0.08114360543468115\n",
      "2022-03-26 17:32:19.059764 Epoch 150, Training Loss 0.08177149070955603\n",
      "2022-03-26 17:32:19.083407 Epoch 150, Training Loss 0.08268691843275523\n",
      "2022-03-26 17:32:19.107281 Epoch 150, Training Loss 0.08347263021389846\n",
      "2022-03-26 17:32:19.134826 Epoch 150, Training Loss 0.08430611900508861\n",
      "2022-03-26 17:32:19.167084 Epoch 150, Training Loss 0.08523825996214776\n",
      "2022-03-26 17:32:19.192622 Epoch 150, Training Loss 0.08617272976871647\n",
      "2022-03-26 17:32:19.215936 Epoch 150, Training Loss 0.08670585421497559\n",
      "2022-03-26 17:32:19.243799 Epoch 150, Training Loss 0.08743307516550469\n",
      "2022-03-26 17:32:19.267782 Epoch 150, Training Loss 0.0881665534604236\n",
      "2022-03-26 17:32:19.292688 Epoch 150, Training Loss 0.0888623909648422\n",
      "2022-03-26 17:32:19.316454 Epoch 150, Training Loss 0.08968285408318805\n",
      "2022-03-26 17:32:19.343512 Epoch 150, Training Loss 0.0904376228980701\n",
      "2022-03-26 17:32:19.367314 Epoch 150, Training Loss 0.09141367342313537\n",
      "2022-03-26 17:32:19.398292 Epoch 150, Training Loss 0.09213706275538715\n",
      "2022-03-26 17:32:19.421675 Epoch 150, Training Loss 0.09277544690824835\n",
      "2022-03-26 17:32:19.445311 Epoch 150, Training Loss 0.09355033259562519\n",
      "2022-03-26 17:32:19.471162 Epoch 150, Training Loss 0.09437053336206909\n",
      "2022-03-26 17:32:19.494052 Epoch 150, Training Loss 0.09518193969946079\n",
      "2022-03-26 17:32:19.516798 Epoch 150, Training Loss 0.09602031751971721\n",
      "2022-03-26 17:32:19.539291 Epoch 150, Training Loss 0.0968236850045831\n",
      "2022-03-26 17:32:19.561783 Epoch 150, Training Loss 0.09818656106129327\n",
      "2022-03-26 17:32:19.584353 Epoch 150, Training Loss 0.09883966790440747\n",
      "2022-03-26 17:32:19.612841 Epoch 150, Training Loss 0.09965210756682374\n",
      "2022-03-26 17:32:19.637872 Epoch 150, Training Loss 0.10038582313701015\n",
      "2022-03-26 17:32:19.660212 Epoch 150, Training Loss 0.10108876335041603\n",
      "2022-03-26 17:32:19.683350 Epoch 150, Training Loss 0.10192778919968763\n",
      "2022-03-26 17:32:19.706580 Epoch 150, Training Loss 0.10255365847321728\n",
      "2022-03-26 17:32:19.735788 Epoch 150, Training Loss 0.10379049662129043\n",
      "2022-03-26 17:32:19.758262 Epoch 150, Training Loss 0.10456664330514191\n",
      "2022-03-26 17:32:19.780824 Epoch 150, Training Loss 0.10528825364454324\n",
      "2022-03-26 17:32:19.804814 Epoch 150, Training Loss 0.10643779072920075\n",
      "2022-03-26 17:32:19.832416 Epoch 150, Training Loss 0.1070275959913688\n",
      "2022-03-26 17:32:19.855843 Epoch 150, Training Loss 0.10772012985880723\n",
      "2022-03-26 17:32:19.888282 Epoch 150, Training Loss 0.10843626495517428\n",
      "2022-03-26 17:32:19.913227 Epoch 150, Training Loss 0.10903781492386938\n",
      "2022-03-26 17:32:19.937624 Epoch 150, Training Loss 0.10968298809912504\n",
      "2022-03-26 17:32:19.960490 Epoch 150, Training Loss 0.1103184514719507\n",
      "2022-03-26 17:32:19.982787 Epoch 150, Training Loss 0.11106042251410082\n",
      "2022-03-26 17:32:20.005559 Epoch 150, Training Loss 0.11174531578255431\n",
      "2022-03-26 17:32:20.027713 Epoch 150, Training Loss 0.11312295961410493\n",
      "2022-03-26 17:32:20.064795 Epoch 150, Training Loss 0.11391351697847361\n",
      "2022-03-26 17:32:20.087558 Epoch 150, Training Loss 0.11497178852863019\n",
      "2022-03-26 17:32:20.111463 Epoch 150, Training Loss 0.11595329211648468\n",
      "2022-03-26 17:32:20.137148 Epoch 150, Training Loss 0.11653655332982388\n",
      "2022-03-26 17:32:20.159642 Epoch 150, Training Loss 0.11734807628499883\n",
      "2022-03-26 17:32:20.181839 Epoch 150, Training Loss 0.11832979626362891\n",
      "2022-03-26 17:32:20.204519 Epoch 150, Training Loss 0.11915935000495227\n",
      "2022-03-26 17:32:20.227318 Epoch 150, Training Loss 0.11983776290703307\n",
      "2022-03-26 17:32:20.249811 Epoch 150, Training Loss 0.12037264530920921\n",
      "2022-03-26 17:32:20.276897 Epoch 150, Training Loss 0.12099773720707126\n",
      "2022-03-26 17:32:20.300898 Epoch 150, Training Loss 0.12171591143778827\n",
      "2022-03-26 17:32:20.323793 Epoch 150, Training Loss 0.12237950374403268\n",
      "2022-03-26 17:32:20.349091 Epoch 150, Training Loss 0.12319287017483235\n",
      "2022-03-26 17:32:20.372098 Epoch 150, Training Loss 0.1241508332817146\n",
      "2022-03-26 17:32:20.396199 Epoch 150, Training Loss 0.1252406167099848\n",
      "2022-03-26 17:32:20.418913 Epoch 150, Training Loss 0.12592933023982036\n",
      "2022-03-26 17:32:20.442551 Epoch 150, Training Loss 0.1267081709469066\n",
      "2022-03-26 17:32:20.465698 Epoch 150, Training Loss 0.12771672315304847\n",
      "2022-03-26 17:32:20.496521 Epoch 150, Training Loss 0.12849519761931866\n",
      "2022-03-26 17:32:20.520633 Epoch 150, Training Loss 0.12934144973145117\n",
      "2022-03-26 17:32:20.543596 Epoch 150, Training Loss 0.13024726067967426\n",
      "2022-03-26 17:32:20.571579 Epoch 150, Training Loss 0.13118168665929827\n",
      "2022-03-26 17:32:20.594044 Epoch 150, Training Loss 0.13179576980031055\n",
      "2022-03-26 17:32:20.616505 Epoch 150, Training Loss 0.13264544597824515\n",
      "2022-03-26 17:32:20.639632 Epoch 150, Training Loss 0.1332779596452518\n",
      "2022-03-26 17:32:20.662117 Epoch 150, Training Loss 0.13402371664943596\n",
      "2022-03-26 17:32:20.684946 Epoch 150, Training Loss 0.13474717991583793\n",
      "2022-03-26 17:32:20.717084 Epoch 150, Training Loss 0.1356832257393376\n",
      "2022-03-26 17:32:20.741891 Epoch 150, Training Loss 0.1365315602029986\n",
      "2022-03-26 17:32:20.765638 Epoch 150, Training Loss 0.13708591861340702\n",
      "2022-03-26 17:32:20.792433 Epoch 150, Training Loss 0.13777254369405226\n",
      "2022-03-26 17:32:20.816479 Epoch 150, Training Loss 0.1389334408752144\n",
      "2022-03-26 17:32:20.840027 Epoch 150, Training Loss 0.13977187936720642\n",
      "2022-03-26 17:32:20.863336 Epoch 150, Training Loss 0.14067735250496194\n",
      "2022-03-26 17:32:20.886895 Epoch 150, Training Loss 0.141463778017427\n",
      "2022-03-26 17:32:20.924877 Epoch 150, Training Loss 0.14223832071132367\n",
      "2022-03-26 17:32:20.957384 Epoch 150, Training Loss 0.14331846884296984\n",
      "2022-03-26 17:32:20.981627 Epoch 150, Training Loss 0.14412169432853494\n",
      "2022-03-26 17:32:21.006599 Epoch 150, Training Loss 0.14488337026989978\n",
      "2022-03-26 17:32:21.034337 Epoch 150, Training Loss 0.1458592623319772\n",
      "2022-03-26 17:32:21.057462 Epoch 150, Training Loss 0.14661518977883528\n",
      "2022-03-26 17:32:21.080637 Epoch 150, Training Loss 0.1472248812694379\n",
      "2022-03-26 17:32:21.103881 Epoch 150, Training Loss 0.14814180352956133\n",
      "2022-03-26 17:32:21.130928 Epoch 150, Training Loss 0.1487792194499384\n",
      "2022-03-26 17:32:21.158945 Epoch 150, Training Loss 0.14960877997491062\n",
      "2022-03-26 17:32:21.182025 Epoch 150, Training Loss 0.1504297152047267\n",
      "2022-03-26 17:32:21.205462 Epoch 150, Training Loss 0.15110888337845083\n",
      "2022-03-26 17:32:21.229238 Epoch 150, Training Loss 0.1518020920283959\n",
      "2022-03-26 17:32:21.255106 Epoch 150, Training Loss 0.1525676232164778\n",
      "2022-03-26 17:32:21.278284 Epoch 150, Training Loss 0.1533114667743673\n",
      "2022-03-26 17:32:21.301671 Epoch 150, Training Loss 0.15402445944068988\n",
      "2022-03-26 17:32:21.324998 Epoch 150, Training Loss 0.15466744035406185\n",
      "2022-03-26 17:32:21.357816 Epoch 150, Training Loss 0.15580759435663444\n",
      "2022-03-26 17:32:21.382160 Epoch 150, Training Loss 0.15653092736173468\n",
      "2022-03-26 17:32:21.406153 Epoch 150, Training Loss 0.15721705921775544\n",
      "2022-03-26 17:32:21.431402 Epoch 150, Training Loss 0.15802046816672205\n",
      "2022-03-26 17:32:21.455380 Epoch 150, Training Loss 0.1588043223713975\n",
      "2022-03-26 17:32:21.479154 Epoch 150, Training Loss 0.1595397122833125\n",
      "2022-03-26 17:32:21.502564 Epoch 150, Training Loss 0.1604498741419419\n",
      "2022-03-26 17:32:21.525752 Epoch 150, Training Loss 0.1611595207925343\n",
      "2022-03-26 17:32:21.549496 Epoch 150, Training Loss 0.1619955298235959\n",
      "2022-03-26 17:32:21.581069 Epoch 150, Training Loss 0.16294129646342734\n",
      "2022-03-26 17:32:21.605675 Epoch 150, Training Loss 0.16374291536753136\n",
      "2022-03-26 17:32:21.629126 Epoch 150, Training Loss 0.16434622904681184\n",
      "2022-03-26 17:32:21.657360 Epoch 150, Training Loss 0.16501517540506086\n",
      "2022-03-26 17:32:21.680348 Epoch 150, Training Loss 0.16565339899886294\n",
      "2022-03-26 17:32:21.704277 Epoch 150, Training Loss 0.1664996998922904\n",
      "2022-03-26 17:32:21.728145 Epoch 150, Training Loss 0.1670787796721129\n",
      "2022-03-26 17:32:21.751753 Epoch 150, Training Loss 0.16779277037324197\n",
      "2022-03-26 17:32:21.774127 Epoch 150, Training Loss 0.16839086651192298\n",
      "2022-03-26 17:32:21.805704 Epoch 150, Training Loss 0.16907002043236247\n",
      "2022-03-26 17:32:21.829683 Epoch 150, Training Loss 0.16982971074636025\n",
      "2022-03-26 17:32:21.859686 Epoch 150, Training Loss 0.17087500830135688\n",
      "2022-03-26 17:32:21.886234 Epoch 150, Training Loss 0.171898462202238\n",
      "2022-03-26 17:32:21.910439 Epoch 150, Training Loss 0.17275834693323316\n",
      "2022-03-26 17:32:21.946412 Epoch 150, Training Loss 0.1736949993978681\n",
      "2022-03-26 17:32:21.970435 Epoch 150, Training Loss 0.17460787806974348\n",
      "2022-03-26 17:32:21.994050 Epoch 150, Training Loss 0.17542841230207087\n",
      "2022-03-26 17:32:22.025295 Epoch 150, Training Loss 0.17626600573434853\n",
      "2022-03-26 17:32:22.049439 Epoch 150, Training Loss 0.17713829120406716\n",
      "2022-03-26 17:32:22.072556 Epoch 150, Training Loss 0.178073136626607\n",
      "2022-03-26 17:32:22.099434 Epoch 150, Training Loss 0.1786771420856266\n",
      "2022-03-26 17:32:22.122669 Epoch 150, Training Loss 0.17946133745448364\n",
      "2022-03-26 17:32:22.145819 Epoch 150, Training Loss 0.18048462892889672\n",
      "2022-03-26 17:32:22.169069 Epoch 150, Training Loss 0.18140708580803688\n",
      "2022-03-26 17:32:22.192282 Epoch 150, Training Loss 0.18252678375567316\n",
      "2022-03-26 17:32:22.215892 Epoch 150, Training Loss 0.18317184008448326\n",
      "2022-03-26 17:32:22.246870 Epoch 150, Training Loss 0.18437407613562806\n",
      "2022-03-26 17:32:22.272421 Epoch 150, Training Loss 0.18497246736303316\n",
      "2022-03-26 17:32:22.295676 Epoch 150, Training Loss 0.18571520381418946\n",
      "2022-03-26 17:32:22.323065 Epoch 150, Training Loss 0.1863510034730672\n",
      "2022-03-26 17:32:22.349183 Epoch 150, Training Loss 0.18712383973628968\n",
      "2022-03-26 17:32:22.373053 Epoch 150, Training Loss 0.18795678789353432\n",
      "2022-03-26 17:32:22.396354 Epoch 150, Training Loss 0.18894034776541277\n",
      "2022-03-26 17:32:22.420095 Epoch 150, Training Loss 0.18952877255504394\n",
      "2022-03-26 17:32:22.442751 Epoch 150, Training Loss 0.19026193148492243\n",
      "2022-03-26 17:32:22.473839 Epoch 150, Training Loss 0.19125020469698456\n",
      "2022-03-26 17:32:22.497275 Epoch 150, Training Loss 0.19206188840177052\n",
      "2022-03-26 17:32:22.521194 Epoch 150, Training Loss 0.19276293940708766\n",
      "2022-03-26 17:32:22.548036 Epoch 150, Training Loss 0.193442370847363\n",
      "2022-03-26 17:32:22.570417 Epoch 150, Training Loss 0.1943558714045283\n",
      "2022-03-26 17:32:22.593839 Epoch 150, Training Loss 0.195090583408885\n",
      "2022-03-26 17:32:22.616807 Epoch 150, Training Loss 0.19591692978006495\n",
      "2022-03-26 17:32:22.639957 Epoch 150, Training Loss 0.19682224475971574\n",
      "2022-03-26 17:32:22.663001 Epoch 150, Training Loss 0.19776942255094532\n",
      "2022-03-26 17:32:22.691706 Epoch 150, Training Loss 0.19839888300432268\n",
      "2022-03-26 17:32:22.715676 Epoch 150, Training Loss 0.19919692235224692\n",
      "2022-03-26 17:32:22.738339 Epoch 150, Training Loss 0.2002127171324952\n",
      "2022-03-26 17:32:22.761575 Epoch 150, Training Loss 0.20090920518121452\n",
      "2022-03-26 17:32:22.784162 Epoch 150, Training Loss 0.20149289868066989\n",
      "2022-03-26 17:32:22.813062 Epoch 150, Training Loss 0.2021679964364337\n",
      "2022-03-26 17:32:22.835451 Epoch 150, Training Loss 0.20306545168237614\n",
      "2022-03-26 17:32:22.857793 Epoch 150, Training Loss 0.2036588200751473\n",
      "2022-03-26 17:32:22.880912 Epoch 150, Training Loss 0.20435061940299276\n",
      "2022-03-26 17:32:22.908872 Epoch 150, Training Loss 0.2052543917504113\n",
      "2022-03-26 17:32:22.934744 Epoch 150, Training Loss 0.2060248468766737\n",
      "2022-03-26 17:32:22.967843 Epoch 150, Training Loss 0.20674693298614238\n",
      "2022-03-26 17:32:22.993077 Epoch 150, Training Loss 0.2077419562336734\n",
      "2022-03-26 17:32:23.017442 Epoch 150, Training Loss 0.20841087644819714\n",
      "2022-03-26 17:32:23.040736 Epoch 150, Training Loss 0.20914375137947405\n",
      "2022-03-26 17:32:23.068240 Epoch 150, Training Loss 0.20994202685935417\n",
      "2022-03-26 17:32:23.090646 Epoch 150, Training Loss 0.21057320277556738\n",
      "2022-03-26 17:32:23.121098 Epoch 150, Training Loss 0.21119082332267175\n",
      "2022-03-26 17:32:23.144533 Epoch 150, Training Loss 0.21203371089742618\n",
      "2022-03-26 17:32:23.167185 Epoch 150, Training Loss 0.21265129905070185\n",
      "2022-03-26 17:32:23.189639 Epoch 150, Training Loss 0.2136141547690267\n",
      "2022-03-26 17:32:23.215129 Epoch 150, Training Loss 0.21436531026192637\n",
      "2022-03-26 17:32:23.237704 Epoch 150, Training Loss 0.21505666674707857\n",
      "2022-03-26 17:32:23.260534 Epoch 150, Training Loss 0.21573328061024552\n",
      "2022-03-26 17:32:23.283325 Epoch 150, Training Loss 0.21657065010589102\n",
      "2022-03-26 17:32:23.306127 Epoch 150, Training Loss 0.21768195385975606\n",
      "2022-03-26 17:32:23.340047 Epoch 150, Training Loss 0.21847644383492676\n",
      "2022-03-26 17:32:23.372317 Epoch 150, Training Loss 0.21937938392772088\n",
      "2022-03-26 17:32:23.395646 Epoch 150, Training Loss 0.22025268244773835\n",
      "2022-03-26 17:32:23.424258 Epoch 150, Training Loss 0.2210170508879225\n",
      "2022-03-26 17:32:23.448396 Epoch 150, Training Loss 0.22215662942365613\n",
      "2022-03-26 17:32:23.471927 Epoch 150, Training Loss 0.22275414018679762\n",
      "2022-03-26 17:32:23.495060 Epoch 150, Training Loss 0.22350611612010185\n",
      "2022-03-26 17:32:23.518364 Epoch 150, Training Loss 0.22424809348857616\n",
      "2022-03-26 17:32:23.542447 Epoch 150, Training Loss 0.2252072315386799\n",
      "2022-03-26 17:32:23.573446 Epoch 150, Training Loss 0.22614789009094238\n",
      "2022-03-26 17:32:23.597174 Epoch 150, Training Loss 0.22682302649063832\n",
      "2022-03-26 17:32:23.621727 Epoch 150, Training Loss 0.227533054092656\n",
      "2022-03-26 17:32:23.647752 Epoch 150, Training Loss 0.22838583314205374\n",
      "2022-03-26 17:32:23.671923 Epoch 150, Training Loss 0.2292428262093488\n",
      "2022-03-26 17:32:23.695549 Epoch 150, Training Loss 0.22998367063224773\n",
      "2022-03-26 17:32:23.719521 Epoch 150, Training Loss 0.230713741858597\n",
      "2022-03-26 17:32:23.743549 Epoch 150, Training Loss 0.23140384367359873\n",
      "2022-03-26 17:32:23.774007 Epoch 150, Training Loss 0.23231921522208795\n",
      "2022-03-26 17:32:23.797786 Epoch 150, Training Loss 0.2330567008241668\n",
      "2022-03-26 17:32:23.821973 Epoch 150, Training Loss 0.23402090214402474\n",
      "2022-03-26 17:32:23.845826 Epoch 150, Training Loss 0.23458694969601643\n",
      "2022-03-26 17:32:23.871316 Epoch 150, Training Loss 0.23561136496951207\n",
      "2022-03-26 17:32:23.895019 Epoch 150, Training Loss 0.23636101373016377\n",
      "2022-03-26 17:32:23.918491 Epoch 150, Training Loss 0.23745996400218486\n",
      "2022-03-26 17:32:23.943462 Epoch 150, Training Loss 0.2379380865093997\n",
      "2022-03-26 17:32:23.983244 Epoch 150, Training Loss 0.23864098815509424\n",
      "2022-03-26 17:32:24.013890 Epoch 150, Training Loss 0.23945187680099322\n",
      "2022-03-26 17:32:24.037161 Epoch 150, Training Loss 0.24017491224019424\n",
      "2022-03-26 17:32:24.062449 Epoch 150, Training Loss 0.2409223265126538\n",
      "2022-03-26 17:32:24.087187 Epoch 150, Training Loss 0.24186217628629006\n",
      "2022-03-26 17:32:24.110149 Epoch 150, Training Loss 0.24271875715164273\n",
      "2022-03-26 17:32:24.133643 Epoch 150, Training Loss 0.24374392411440535\n",
      "2022-03-26 17:32:24.156672 Epoch 150, Training Loss 0.24457381307469\n",
      "2022-03-26 17:32:24.180192 Epoch 150, Training Loss 0.24539546264559411\n",
      "2022-03-26 17:32:24.210992 Epoch 150, Training Loss 0.24637720823440407\n",
      "2022-03-26 17:32:24.234225 Epoch 150, Training Loss 0.2471138727100914\n",
      "2022-03-26 17:32:24.258837 Epoch 150, Training Loss 0.24803984047049452\n",
      "2022-03-26 17:32:24.285388 Epoch 150, Training Loss 0.24879742655760187\n",
      "2022-03-26 17:32:24.308852 Epoch 150, Training Loss 0.24960075997178208\n",
      "2022-03-26 17:32:24.334383 Epoch 150, Training Loss 0.2503079609264193\n",
      "2022-03-26 17:32:24.358075 Epoch 150, Training Loss 0.2511797038566731\n",
      "2022-03-26 17:32:24.381841 Epoch 150, Training Loss 0.25214859351630103\n",
      "2022-03-26 17:32:24.405586 Epoch 150, Training Loss 0.25293510531068153\n",
      "2022-03-26 17:32:24.441137 Epoch 150, Training Loss 0.253836989898206\n",
      "2022-03-26 17:32:24.465574 Epoch 150, Training Loss 0.2546104158053313\n",
      "2022-03-26 17:32:24.488559 Epoch 150, Training Loss 0.25566371082497374\n",
      "2022-03-26 17:32:24.515323 Epoch 150, Training Loss 0.25662901101972135\n",
      "2022-03-26 17:32:24.537687 Epoch 150, Training Loss 0.25742123731414374\n",
      "2022-03-26 17:32:24.560046 Epoch 150, Training Loss 0.25843224657313596\n",
      "2022-03-26 17:32:24.583173 Epoch 150, Training Loss 0.2591162032407263\n",
      "2022-03-26 17:32:24.606273 Epoch 150, Training Loss 0.25986386157210223\n",
      "2022-03-26 17:32:24.629991 Epoch 150, Training Loss 0.26036582155453275\n",
      "2022-03-26 17:32:24.658370 Epoch 150, Training Loss 0.26095162091962515\n",
      "2022-03-26 17:32:24.681691 Epoch 150, Training Loss 0.26196689968523773\n",
      "2022-03-26 17:32:24.704716 Epoch 150, Training Loss 0.2628174452373134\n",
      "2022-03-26 17:32:24.727626 Epoch 150, Training Loss 0.2636576508317152\n",
      "2022-03-26 17:32:24.751655 Epoch 150, Training Loss 0.2645453441021083\n",
      "2022-03-26 17:32:24.774530 Epoch 150, Training Loss 0.2655550985385085\n",
      "2022-03-26 17:32:24.796666 Epoch 150, Training Loss 0.2662921866492542\n",
      "2022-03-26 17:32:24.820633 Epoch 150, Training Loss 0.26713298958585696\n",
      "2022-03-26 17:32:24.842920 Epoch 150, Training Loss 0.268052942917475\n",
      "2022-03-26 17:32:24.870762 Epoch 150, Training Loss 0.2691625509877949\n",
      "2022-03-26 17:32:24.898120 Epoch 150, Training Loss 0.2699843087921972\n",
      "2022-03-26 17:32:24.921008 Epoch 150, Training Loss 0.2707971927455014\n",
      "2022-03-26 17:32:24.945125 Epoch 150, Training Loss 0.2714281008981378\n",
      "2022-03-26 17:32:24.968508 Epoch 150, Training Loss 0.272145163525096\n",
      "2022-03-26 17:32:24.998312 Epoch 150, Training Loss 0.2731184954076167\n",
      "2022-03-26 17:32:25.025026 Epoch 150, Training Loss 0.2742369626184254\n",
      "2022-03-26 17:32:25.046992 Epoch 150, Training Loss 0.27504979672334384\n",
      "2022-03-26 17:32:25.069336 Epoch 150, Training Loss 0.2760290374688785\n",
      "2022-03-26 17:32:25.099339 Epoch 150, Training Loss 0.27690676098589395\n",
      "2022-03-26 17:32:25.122544 Epoch 150, Training Loss 0.2775949104057858\n",
      "2022-03-26 17:32:25.144922 Epoch 150, Training Loss 0.27823259248910354\n",
      "2022-03-26 17:32:25.168354 Epoch 150, Training Loss 0.27909929227188723\n",
      "2022-03-26 17:32:25.193377 Epoch 150, Training Loss 0.27999189888577325\n",
      "2022-03-26 17:32:25.216914 Epoch 150, Training Loss 0.2808342857662674\n",
      "2022-03-26 17:32:25.239982 Epoch 150, Training Loss 0.28148424636829844\n",
      "2022-03-26 17:32:25.263295 Epoch 150, Training Loss 0.2821944904373125\n",
      "2022-03-26 17:32:25.287172 Epoch 150, Training Loss 0.28304579461474555\n",
      "2022-03-26 17:32:25.318825 Epoch 150, Training Loss 0.2837790769460561\n",
      "2022-03-26 17:32:25.346333 Epoch 150, Training Loss 0.2848773786555166\n",
      "2022-03-26 17:32:25.370470 Epoch 150, Training Loss 0.2856247237195139\n",
      "2022-03-26 17:32:25.395139 Epoch 150, Training Loss 0.28619474789980426\n",
      "2022-03-26 17:32:25.422713 Epoch 150, Training Loss 0.28669114384199956\n",
      "2022-03-26 17:32:25.446063 Epoch 150, Training Loss 0.2874898264170303\n",
      "2022-03-26 17:32:25.469770 Epoch 150, Training Loss 0.28836971742417805\n",
      "2022-03-26 17:32:25.493985 Epoch 150, Training Loss 0.2892380588499786\n",
      "2022-03-26 17:32:25.517762 Epoch 150, Training Loss 0.2899885740121612\n",
      "2022-03-26 17:32:25.549555 Epoch 150, Training Loss 0.29095224292991717\n",
      "2022-03-26 17:32:25.573563 Epoch 150, Training Loss 0.29184483567162245\n",
      "2022-03-26 17:32:25.600201 Epoch 150, Training Loss 0.2928204535675781\n",
      "2022-03-26 17:32:25.624526 Epoch 150, Training Loss 0.2935776818743752\n",
      "2022-03-26 17:32:25.648647 Epoch 150, Training Loss 0.2945274328789138\n",
      "2022-03-26 17:32:25.672443 Epoch 150, Training Loss 0.2955174259365062\n",
      "2022-03-26 17:32:25.696047 Epoch 150, Training Loss 0.29620338728665696\n",
      "2022-03-26 17:32:25.720084 Epoch 150, Training Loss 0.2968836259049223\n",
      "2022-03-26 17:32:25.743818 Epoch 150, Training Loss 0.29790574533250325\n",
      "2022-03-26 17:32:25.778388 Epoch 150, Training Loss 0.2985272323307784\n",
      "2022-03-26 17:32:25.803976 Epoch 150, Training Loss 0.29938362188198986\n",
      "2022-03-26 17:32:25.828399 Epoch 150, Training Loss 0.30021133329100014\n",
      "2022-03-26 17:32:25.851564 Epoch 150, Training Loss 0.3010611669410525\n",
      "2022-03-26 17:32:25.877149 Epoch 150, Training Loss 0.30174025439697766\n",
      "2022-03-26 17:32:25.902865 Epoch 150, Training Loss 0.3027167947929534\n",
      "2022-03-26 17:32:25.926412 Epoch 150, Training Loss 0.3037310345932041\n",
      "2022-03-26 17:32:25.949546 Epoch 150, Training Loss 0.3046125478832923\n",
      "2022-03-26 17:32:25.972417 Epoch 150, Training Loss 0.30531691494957564\n",
      "2022-03-26 17:32:26.003687 Epoch 150, Training Loss 0.3058889322649792\n",
      "2022-03-26 17:32:26.037715 Epoch 150, Training Loss 0.30655839235124077\n",
      "2022-03-26 17:32:26.060386 Epoch 150, Training Loss 0.30714493681250327\n",
      "2022-03-26 17:32:26.092311 Epoch 150, Training Loss 0.3076391515067166\n",
      "2022-03-26 17:32:26.115721 Epoch 150, Training Loss 0.3084448396854693\n",
      "2022-03-26 17:32:26.139292 Epoch 150, Training Loss 0.30928922987654994\n",
      "2022-03-26 17:32:26.164009 Epoch 150, Training Loss 0.3099738900618785\n",
      "2022-03-26 17:32:26.187010 Epoch 150, Training Loss 0.31069791027347143\n",
      "2022-03-26 17:32:26.212300 Epoch 150, Training Loss 0.3114500240901547\n",
      "2022-03-26 17:32:26.237311 Epoch 150, Training Loss 0.31195790215831276\n",
      "2022-03-26 17:32:26.259708 Epoch 150, Training Loss 0.31259710214022174\n",
      "2022-03-26 17:32:26.284213 Epoch 150, Training Loss 0.3134249070721209\n",
      "2022-03-26 17:32:26.306628 Epoch 150, Training Loss 0.31409537296770784\n",
      "2022-03-26 17:32:26.330475 Epoch 150, Training Loss 0.3147125239567379\n",
      "2022-03-26 17:32:26.352930 Epoch 150, Training Loss 0.3155470395179661\n",
      "2022-03-26 17:32:26.375881 Epoch 150, Training Loss 0.3165421709227745\n",
      "2022-03-26 17:32:26.398941 Epoch 150, Training Loss 0.3171621117826618\n",
      "2022-03-26 17:32:26.428580 Epoch 150, Training Loss 0.3179601261301724\n",
      "2022-03-26 17:32:26.453472 Epoch 150, Training Loss 0.3188077096667741\n",
      "2022-03-26 17:32:26.477181 Epoch 150, Training Loss 0.3195207341933799\n",
      "2022-03-26 17:32:26.502843 Epoch 150, Training Loss 0.3204086918736358\n",
      "2022-03-26 17:32:26.526874 Epoch 150, Training Loss 0.321535119658236\n",
      "2022-03-26 17:32:26.550351 Epoch 150, Training Loss 0.3222342797023866\n",
      "2022-03-26 17:32:26.573086 Epoch 150, Training Loss 0.3230538061436485\n",
      "2022-03-26 17:32:26.596882 Epoch 150, Training Loss 0.3237890324857839\n",
      "2022-03-26 17:32:26.620346 Epoch 150, Training Loss 0.32451592751628605\n",
      "2022-03-26 17:32:26.651417 Epoch 150, Training Loss 0.32547231872215904\n",
      "2022-03-26 17:32:26.675576 Epoch 150, Training Loss 0.3262506517988946\n",
      "2022-03-26 17:32:26.699174 Epoch 150, Training Loss 0.3271836202848903\n",
      "2022-03-26 17:32:26.721875 Epoch 150, Training Loss 0.32794019987668527\n",
      "2022-03-26 17:32:26.745100 Epoch 150, Training Loss 0.32851143093670115\n",
      "2022-03-26 17:32:26.771443 Epoch 150, Training Loss 0.32939539076117297\n",
      "2022-03-26 17:32:26.794074 Epoch 150, Training Loss 0.3304783480094217\n",
      "2022-03-26 17:32:26.818691 Epoch 150, Training Loss 0.3312226137541749\n",
      "2022-03-26 17:32:26.842046 Epoch 150, Training Loss 0.33178892647823716\n",
      "2022-03-26 17:32:26.872112 Epoch 150, Training Loss 0.3327661496599007\n",
      "2022-03-26 17:32:26.896123 Epoch 150, Training Loss 0.33359293795912465\n",
      "2022-03-26 17:32:26.918570 Epoch 150, Training Loss 0.33480655887852545\n",
      "2022-03-26 17:32:26.946751 Epoch 150, Training Loss 0.33575169616343115\n",
      "2022-03-26 17:32:26.969349 Epoch 150, Training Loss 0.3365084729383669\n",
      "2022-03-26 17:32:26.998590 Epoch 150, Training Loss 0.33735862747787515\n",
      "2022-03-26 17:32:27.021305 Epoch 150, Training Loss 0.3382440735312069\n",
      "2022-03-26 17:32:27.050731 Epoch 150, Training Loss 0.3389941413536706\n",
      "2022-03-26 17:32:27.081377 Epoch 150, Training Loss 0.33960739445046084\n",
      "2022-03-26 17:32:27.109754 Epoch 150, Training Loss 0.3403084941608522\n",
      "2022-03-26 17:32:27.132539 Epoch 150, Training Loss 0.3411229090846103\n",
      "2022-03-26 17:32:27.154873 Epoch 150, Training Loss 0.34178212776665795\n",
      "2022-03-26 17:32:27.179770 Epoch 150, Training Loss 0.34276146836140575\n",
      "2022-03-26 17:32:27.202277 Epoch 150, Training Loss 0.3433881123428759\n",
      "2022-03-26 17:32:27.225122 Epoch 150, Training Loss 0.3441433884832255\n",
      "2022-03-26 17:32:27.247927 Epoch 150, Training Loss 0.3446827461499997\n",
      "2022-03-26 17:32:27.270492 Epoch 150, Training Loss 0.34545226856265837\n",
      "2022-03-26 17:32:27.299129 Epoch 150, Training Loss 0.3460271665278603\n",
      "2022-03-26 17:32:27.322550 Epoch 150, Training Loss 0.34698842572586613\n",
      "2022-03-26 17:32:27.347586 Epoch 150, Training Loss 0.3479133650393742\n",
      "2022-03-26 17:32:27.371789 Epoch 150, Training Loss 0.34869566052923423\n",
      "2022-03-26 17:32:27.395370 Epoch 150, Training Loss 0.3494936374523451\n",
      "2022-03-26 17:32:27.418299 Epoch 150, Training Loss 0.35037419706811684\n",
      "2022-03-26 17:32:27.441121 Epoch 150, Training Loss 0.35134941975936257\n",
      "2022-03-26 17:32:27.464073 Epoch 150, Training Loss 0.3524529666013425\n",
      "2022-03-26 17:32:27.486349 Epoch 150, Training Loss 0.35320622784554806\n",
      "2022-03-26 17:32:27.513869 Epoch 150, Training Loss 0.35413475258423543\n",
      "2022-03-26 17:32:27.541127 Epoch 150, Training Loss 0.3547796483920968\n",
      "2022-03-26 17:32:27.563307 Epoch 150, Training Loss 0.355416584845699\n",
      "2022-03-26 17:32:27.586771 Epoch 150, Training Loss 0.35613652983742294\n",
      "2022-03-26 17:32:27.611437 Epoch 150, Training Loss 0.35687890881315215\n",
      "2022-03-26 17:32:27.634711 Epoch 150, Training Loss 0.3576350067658802\n",
      "2022-03-26 17:32:27.656511 Epoch 150, Training Loss 0.35853256026039954\n",
      "2022-03-26 17:32:27.678908 Epoch 150, Training Loss 0.3592788151386754\n",
      "2022-03-26 17:32:27.701252 Epoch 150, Training Loss 0.36012701331959357\n",
      "2022-03-26 17:32:27.728550 Epoch 150, Training Loss 0.3612608963342579\n",
      "2022-03-26 17:32:27.751541 Epoch 150, Training Loss 0.36204568721601726\n",
      "2022-03-26 17:32:27.773306 Epoch 150, Training Loss 0.3631108696274745\n",
      "2022-03-26 17:32:27.795571 Epoch 150, Training Loss 0.36388618840128567\n",
      "2022-03-26 17:32:27.819961 Epoch 150, Training Loss 0.3648840121524718\n",
      "2022-03-26 17:32:27.846851 Epoch 150, Training Loss 0.36581719329442514\n",
      "2022-03-26 17:32:27.869170 Epoch 150, Training Loss 0.36653652760531286\n",
      "2022-03-26 17:32:27.897994 Epoch 150, Training Loss 0.3676706142056629\n",
      "2022-03-26 17:32:27.921354 Epoch 150, Training Loss 0.3686647365998734\n",
      "2022-03-26 17:32:27.954036 Epoch 150, Training Loss 0.36933182240904444\n",
      "2022-03-26 17:32:27.982307 Epoch 150, Training Loss 0.37003114952913024\n",
      "2022-03-26 17:32:28.004764 Epoch 150, Training Loss 0.3708487524629554\n",
      "2022-03-26 17:32:28.027990 Epoch 150, Training Loss 0.37154243429146155\n",
      "2022-03-26 17:32:28.051857 Epoch 150, Training Loss 0.37232755536161116\n",
      "2022-03-26 17:32:28.080018 Epoch 150, Training Loss 0.3734686329312947\n",
      "2022-03-26 17:32:28.105392 Epoch 150, Training Loss 0.37423697735190087\n",
      "2022-03-26 17:32:28.130247 Epoch 150, Training Loss 0.37490308296192637\n",
      "2022-03-26 17:32:28.152604 Epoch 150, Training Loss 0.37584900280551226\n",
      "2022-03-26 17:32:28.183419 Epoch 150, Training Loss 0.37700166982000743\n",
      "2022-03-26 17:32:28.206417 Epoch 150, Training Loss 0.3775920512731118\n",
      "2022-03-26 17:32:28.228726 Epoch 150, Training Loss 0.37852753763613495\n",
      "2022-03-26 17:32:28.255119 Epoch 150, Training Loss 0.3793136464512866\n",
      "2022-03-26 17:32:28.277883 Epoch 150, Training Loss 0.3798925334687733\n",
      "2022-03-26 17:32:28.300545 Epoch 150, Training Loss 0.3809604329221389\n",
      "2022-03-26 17:32:28.323315 Epoch 150, Training Loss 0.3816818219926351\n",
      "2022-03-26 17:32:28.348101 Epoch 150, Training Loss 0.3823751529007007\n",
      "2022-03-26 17:32:28.371084 Epoch 150, Training Loss 0.38320687184553315\n",
      "2022-03-26 17:32:28.403176 Epoch 150, Training Loss 0.383972235576576\n",
      "2022-03-26 17:32:28.428022 Epoch 150, Training Loss 0.38496869688143814\n",
      "2022-03-26 17:32:28.450793 Epoch 150, Training Loss 0.3856448950365071\n",
      "2022-03-26 17:32:28.478980 Epoch 150, Training Loss 0.38639574846648195\n",
      "2022-03-26 17:32:28.503655 Epoch 150, Training Loss 0.38719430878339217\n",
      "2022-03-26 17:32:28.526654 Epoch 150, Training Loss 0.3880479673442938\n",
      "2022-03-26 17:32:28.549348 Epoch 150, Training Loss 0.3887889796053357\n",
      "2022-03-26 17:32:28.571793 Epoch 150, Training Loss 0.38962377207663357\n",
      "2022-03-26 17:32:28.594348 Epoch 150, Training Loss 0.3902764427082618\n",
      "2022-03-26 17:32:28.625592 Epoch 150, Training Loss 0.3912047561629654\n",
      "2022-03-26 17:32:28.650260 Epoch 150, Training Loss 0.39198826104783646\n",
      "2022-03-26 17:32:28.674092 Epoch 150, Training Loss 0.3928429722176183\n",
      "2022-03-26 17:32:28.698409 Epoch 150, Training Loss 0.39343543350696564\n",
      "2022-03-26 17:32:28.722023 Epoch 150, Training Loss 0.3943923235015796\n",
      "2022-03-26 17:32:28.749815 Epoch 150, Training Loss 0.39530879583047784\n",
      "2022-03-26 17:32:28.772775 Epoch 150, Training Loss 0.39581789404077605\n",
      "2022-03-26 17:32:28.796368 Epoch 150, Training Loss 0.3966956533053342\n",
      "2022-03-26 17:32:28.821246 Epoch 150, Training Loss 0.3974354131066281\n",
      "2022-03-26 17:32:28.853000 Epoch 150, Training Loss 0.398322472448849\n",
      "2022-03-26 17:32:28.877336 Epoch 150, Training Loss 0.39892517746714373\n",
      "2022-03-26 17:32:28.900811 Epoch 150, Training Loss 0.39976837190673176\n",
      "2022-03-26 17:32:28.930303 Epoch 150, Training Loss 0.4006250046022103\n",
      "2022-03-26 17:32:28.954708 Epoch 150, Training Loss 0.40150860001516464\n",
      "2022-03-26 17:32:28.978585 Epoch 150, Training Loss 0.4023073396795546\n",
      "2022-03-26 17:32:29.002811 Epoch 150, Training Loss 0.40300077333322265\n",
      "2022-03-26 17:32:29.026030 Epoch 150, Training Loss 0.4037674665832154\n",
      "2022-03-26 17:32:29.050141 Epoch 150, Training Loss 0.404572008935082\n",
      "2022-03-26 17:32:29.082384 Epoch 150, Training Loss 0.4053226676972016\n",
      "2022-03-26 17:32:29.121147 Epoch 150, Training Loss 0.4060836504868534\n",
      "2022-03-26 17:32:29.143845 Epoch 150, Training Loss 0.40675835105616726\n",
      "2022-03-26 17:32:29.169398 Epoch 150, Training Loss 0.40770881815487164\n",
      "2022-03-26 17:32:29.192130 Epoch 150, Training Loss 0.4083617232415987\n",
      "2022-03-26 17:32:29.214663 Epoch 150, Training Loss 0.40917556894862134\n",
      "2022-03-26 17:32:29.237236 Epoch 150, Training Loss 0.4099534109349141\n",
      "2022-03-26 17:32:29.259215 Epoch 150, Training Loss 0.41082285977233096\n",
      "2022-03-26 17:32:29.281425 Epoch 150, Training Loss 0.4115199675721586\n",
      "2022-03-26 17:32:29.312352 Epoch 150, Training Loss 0.4120810714066791\n",
      "2022-03-26 17:32:29.338304 Epoch 150, Training Loss 0.41272807807263817\n",
      "2022-03-26 17:32:29.362641 Epoch 150, Training Loss 0.4135791399442326\n",
      "2022-03-26 17:32:29.390624 Epoch 150, Training Loss 0.4145653620552834\n",
      "2022-03-26 17:32:29.414502 Epoch 150, Training Loss 0.41543429884154476\n",
      "2022-03-26 17:32:29.438574 Epoch 150, Training Loss 0.4162162211544983\n",
      "2022-03-26 17:32:29.463102 Epoch 150, Training Loss 0.41718884875707307\n",
      "2022-03-26 17:32:29.486814 Epoch 150, Training Loss 0.418036619568115\n",
      "2022-03-26 17:32:29.510187 Epoch 150, Training Loss 0.41860986880176815\n",
      "2022-03-26 17:32:29.542835 Epoch 150, Training Loss 0.419202819085487\n",
      "2022-03-26 17:32:29.565781 Epoch 150, Training Loss 0.41990341665342334\n",
      "2022-03-26 17:32:29.590969 Epoch 150, Training Loss 0.42111353217945685\n",
      "2022-03-26 17:32:29.614972 Epoch 150, Training Loss 0.4219999213886383\n",
      "2022-03-26 17:32:29.639205 Epoch 150, Training Loss 0.42305446444722394\n",
      "2022-03-26 17:32:29.662830 Epoch 150, Training Loss 0.4238509237385162\n",
      "2022-03-26 17:32:29.687352 Epoch 150, Training Loss 0.4245504371421721\n",
      "2022-03-26 17:32:29.711179 Epoch 150, Training Loss 0.42521184587570104\n",
      "2022-03-26 17:32:29.734769 Epoch 150, Training Loss 0.4259298413687045\n",
      "2022-03-26 17:32:29.764968 Epoch 150, Training Loss 0.42661148016257666\n",
      "2022-03-26 17:32:29.789420 Epoch 150, Training Loss 0.4275521333031642\n",
      "2022-03-26 17:32:29.812511 Epoch 150, Training Loss 0.4285113515375215\n",
      "2022-03-26 17:32:29.836832 Epoch 150, Training Loss 0.4293349704245472\n",
      "2022-03-26 17:32:29.860002 Epoch 150, Training Loss 0.4299673935031647\n",
      "2022-03-26 17:32:29.887014 Epoch 150, Training Loss 0.43088621373676583\n",
      "2022-03-26 17:32:29.910241 Epoch 150, Training Loss 0.4314946890105982\n",
      "2022-03-26 17:32:29.934187 Epoch 150, Training Loss 0.4321512731978351\n",
      "2022-03-26 17:32:29.957810 Epoch 150, Training Loss 0.4328548525224256\n",
      "2022-03-26 17:32:29.996132 Epoch 150, Training Loss 0.4335098370642918\n",
      "2022-03-26 17:32:30.021741 Epoch 150, Training Loss 0.43421839078521485\n",
      "2022-03-26 17:32:30.045660 Epoch 150, Training Loss 0.4351813074039376\n",
      "2022-03-26 17:32:30.069797 Epoch 150, Training Loss 0.43598566575885733\n",
      "2022-03-26 17:32:30.100612 Epoch 150, Training Loss 0.4367988007071683\n",
      "2022-03-26 17:32:30.129322 Epoch 150, Training Loss 0.43777099545197107\n",
      "2022-03-26 17:32:30.156069 Epoch 150, Training Loss 0.4386943346627838\n",
      "2022-03-26 17:32:30.179109 Epoch 150, Training Loss 0.4394462316313668\n",
      "2022-03-26 17:32:30.206011 Epoch 150, Training Loss 0.4401917607354386\n",
      "2022-03-26 17:32:30.233879 Epoch 150, Training Loss 0.4411370801879927\n",
      "2022-03-26 17:32:30.256850 Epoch 150, Training Loss 0.4418249005628059\n",
      "2022-03-26 17:32:30.283149 Epoch 150, Training Loss 0.4427546589347103\n",
      "2022-03-26 17:32:30.308353 Epoch 150, Training Loss 0.4438377193858861\n",
      "2022-03-26 17:32:30.335388 Epoch 150, Training Loss 0.4448276434636787\n",
      "2022-03-26 17:32:30.358365 Epoch 150, Training Loss 0.4454456817768419\n",
      "2022-03-26 17:32:30.382443 Epoch 150, Training Loss 0.4461934740281166\n",
      "2022-03-26 17:32:30.405814 Epoch 150, Training Loss 0.4468680531777384\n",
      "2022-03-26 17:32:30.438395 Epoch 150, Training Loss 0.44752151863959133\n",
      "2022-03-26 17:32:30.461919 Epoch 150, Training Loss 0.44813965638275344\n",
      "2022-03-26 17:32:30.485451 Epoch 150, Training Loss 0.44930029883409095\n",
      "2022-03-26 17:32:30.512622 Epoch 150, Training Loss 0.4502408544120886\n",
      "2022-03-26 17:32:30.536665 Epoch 150, Training Loss 0.4509837103774176\n",
      "2022-03-26 17:32:30.559884 Epoch 150, Training Loss 0.45167468979840386\n",
      "2022-03-26 17:32:30.583525 Epoch 150, Training Loss 0.4524050823715337\n",
      "2022-03-26 17:32:30.607593 Epoch 150, Training Loss 0.4536321845353412\n",
      "2022-03-26 17:32:30.631443 Epoch 150, Training Loss 0.45466868491733775\n",
      "2022-03-26 17:32:30.662543 Epoch 150, Training Loss 0.4556908054882303\n",
      "2022-03-26 17:32:30.686332 Epoch 150, Training Loss 0.45650857763217234\n",
      "2022-03-26 17:32:30.709310 Epoch 150, Training Loss 0.4570603408014683\n",
      "2022-03-26 17:32:30.734338 Epoch 150, Training Loss 0.45795713368889013\n",
      "2022-03-26 17:32:30.760770 Epoch 150, Training Loss 0.4587785943847178\n",
      "2022-03-26 17:32:30.783821 Epoch 150, Training Loss 0.4595135147766689\n",
      "2022-03-26 17:32:30.806834 Epoch 150, Training Loss 0.46042639177168726\n",
      "2022-03-26 17:32:30.831440 Epoch 150, Training Loss 0.46146540766786737\n",
      "2022-03-26 17:32:30.854528 Epoch 150, Training Loss 0.4620379342523682\n",
      "2022-03-26 17:32:30.885568 Epoch 150, Training Loss 0.4630678166132754\n",
      "2022-03-26 17:32:30.916080 Epoch 150, Training Loss 0.4639513614156362\n",
      "2022-03-26 17:32:30.939983 Epoch 150, Training Loss 0.4648397758869869\n",
      "2022-03-26 17:32:30.964842 Epoch 150, Training Loss 0.4656979442786073\n",
      "2022-03-26 17:32:30.992912 Epoch 150, Training Loss 0.466626856676148\n",
      "2022-03-26 17:32:31.015607 Epoch 150, Training Loss 0.46725580279174667\n",
      "2022-03-26 17:32:31.038731 Epoch 150, Training Loss 0.46812916792872006\n",
      "2022-03-26 17:32:31.061280 Epoch 150, Training Loss 0.4689477865805711\n",
      "2022-03-26 17:32:31.083750 Epoch 150, Training Loss 0.46950476961520016\n",
      "2022-03-26 17:32:31.113996 Epoch 150, Training Loss 0.47037033530909694\n",
      "2022-03-26 17:32:31.136865 Epoch 150, Training Loss 0.4711710195178571\n",
      "2022-03-26 17:32:31.169216 Epoch 150, Training Loss 0.47211309409964725\n",
      "2022-03-26 17:32:31.197517 Epoch 150, Training Loss 0.47286403937565397\n",
      "2022-03-26 17:32:31.220511 Epoch 150, Training Loss 0.4737639963398199\n",
      "2022-03-26 17:32:31.243548 Epoch 150, Training Loss 0.4745758345441135\n",
      "2022-03-26 17:32:31.266703 Epoch 150, Training Loss 0.47534501586881134\n",
      "2022-03-26 17:32:31.289594 Epoch 150, Training Loss 0.4760694866976165\n",
      "2022-03-26 17:32:31.313280 Epoch 150, Training Loss 0.47664945867970165\n",
      "2022-03-26 17:32:31.348356 Epoch 150, Training Loss 0.4773314544916763\n",
      "2022-03-26 17:32:31.371937 Epoch 150, Training Loss 0.4780237901088832\n",
      "2022-03-26 17:32:31.398425 Epoch 150, Training Loss 0.4789103032530421\n",
      "2022-03-26 17:32:31.423382 Epoch 150, Training Loss 0.4796388159170175\n",
      "2022-03-26 17:32:31.447322 Epoch 150, Training Loss 0.4804294382214851\n",
      "2022-03-26 17:32:31.470799 Epoch 150, Training Loss 0.48142594510636977\n",
      "2022-03-26 17:32:31.494419 Epoch 150, Training Loss 0.48200643020670125\n",
      "2022-03-26 17:32:31.518343 Epoch 150, Training Loss 0.48304365742999267\n",
      "2022-03-26 17:32:31.542441 Epoch 150, Training Loss 0.4839128103783673\n",
      "2022-03-26 17:32:31.574426 Epoch 150, Training Loss 0.4848641245185262\n",
      "2022-03-26 17:32:31.598467 Epoch 150, Training Loss 0.4855856278439617\n",
      "2022-03-26 17:32:31.623250 Epoch 150, Training Loss 0.48645138500444113\n",
      "2022-03-26 17:32:31.650834 Epoch 150, Training Loss 0.48743082941188226\n",
      "2022-03-26 17:32:31.674472 Epoch 150, Training Loss 0.4883626252412796\n",
      "2022-03-26 17:32:31.698222 Epoch 150, Training Loss 0.48926750728693763\n",
      "2022-03-26 17:32:31.722344 Epoch 150, Training Loss 0.49022283772830766\n",
      "2022-03-26 17:32:31.745645 Epoch 150, Training Loss 0.490743235172823\n",
      "2022-03-26 17:32:31.769630 Epoch 150, Training Loss 0.4915998222120583\n",
      "2022-03-26 17:32:31.802208 Epoch 150, Training Loss 0.49243712455720245\n",
      "2022-03-26 17:32:31.828250 Epoch 150, Training Loss 0.49340103113132977\n",
      "2022-03-26 17:32:31.853811 Epoch 150, Training Loss 0.4944160609598965\n",
      "2022-03-26 17:32:31.877571 Epoch 150, Training Loss 0.495406519116648\n",
      "2022-03-26 17:32:31.901721 Epoch 150, Training Loss 0.49625606808211187\n",
      "2022-03-26 17:32:31.925756 Epoch 150, Training Loss 0.4969432847884\n",
      "2022-03-26 17:32:31.949628 Epoch 150, Training Loss 0.49778722954528104\n",
      "2022-03-26 17:32:31.973746 Epoch 150, Training Loss 0.49871748762057566\n",
      "2022-03-26 17:32:31.998046 Epoch 150, Training Loss 0.4995783081140055\n",
      "2022-03-26 17:32:32.029706 Epoch 150, Training Loss 0.5003002603797961\n",
      "2022-03-26 17:32:32.054044 Epoch 150, Training Loss 0.5010740423141538\n",
      "2022-03-26 17:32:32.078859 Epoch 150, Training Loss 0.5017085394362355\n",
      "2022-03-26 17:32:32.108799 Epoch 150, Training Loss 0.5022926037116429\n",
      "2022-03-26 17:32:32.132421 Epoch 150, Training Loss 0.5031873120371338\n",
      "2022-03-26 17:32:32.158615 Epoch 150, Training Loss 0.5040511387540861\n",
      "2022-03-26 17:32:32.193877 Epoch 150, Training Loss 0.5049250924678714\n",
      "2022-03-26 17:32:32.219227 Epoch 150, Training Loss 0.5058735202035636\n",
      "2022-03-26 17:32:32.257737 Epoch 150, Training Loss 0.5067223597060689\n",
      "2022-03-26 17:32:32.282225 Epoch 150, Training Loss 0.5075665689490335\n",
      "2022-03-26 17:32:32.306200 Epoch 150, Training Loss 0.5084683501049686\n",
      "2022-03-26 17:32:32.335396 Epoch 150, Training Loss 0.5090592195996848\n",
      "2022-03-26 17:32:32.358813 Epoch 150, Training Loss 0.5100265633495872\n",
      "2022-03-26 17:32:32.382064 Epoch 150, Training Loss 0.5109922902282241\n",
      "2022-03-26 17:32:32.405667 Epoch 150, Training Loss 0.5116293827819702\n",
      "2022-03-26 17:32:32.429512 Epoch 150, Training Loss 0.5123315890655493\n",
      "2022-03-26 17:32:32.453244 Epoch 150, Training Loss 0.5128996850889357\n",
      "2022-03-26 17:32:32.487186 Epoch 150, Training Loss 0.5136307691369215\n",
      "2022-03-26 17:32:32.511024 Epoch 150, Training Loss 0.5144594209578336\n",
      "2022-03-26 17:32:32.534123 Epoch 150, Training Loss 0.5152000465508922\n",
      "2022-03-26 17:32:32.561089 Epoch 150, Training Loss 0.5160277570452532\n",
      "2022-03-26 17:32:32.583571 Epoch 150, Training Loss 0.5168446666749237\n",
      "2022-03-26 17:32:32.606038 Epoch 150, Training Loss 0.5175187226451571\n",
      "2022-03-26 17:32:32.628847 Epoch 150, Training Loss 0.5184782036887411\n",
      "2022-03-26 17:32:32.651555 Epoch 150, Training Loss 0.5193854136692594\n",
      "2022-03-26 17:32:32.674560 Epoch 150, Training Loss 0.5202491259788309\n",
      "2022-03-26 17:32:32.701473 Epoch 150, Training Loss 0.5210393268586425\n",
      "2022-03-26 17:32:32.726439 Epoch 150, Training Loss 0.5218349875086714\n",
      "2022-03-26 17:32:32.749544 Epoch 150, Training Loss 0.5224833746090569\n",
      "2022-03-26 17:32:32.772124 Epoch 150, Training Loss 0.5232047542281772\n",
      "2022-03-26 17:32:32.794640 Epoch 150, Training Loss 0.5237531403979987\n",
      "2022-03-26 17:32:32.821568 Epoch 150, Training Loss 0.524575558731623\n",
      "2022-03-26 17:32:32.844410 Epoch 150, Training Loss 0.5253468606706775\n",
      "2022-03-26 17:32:32.866867 Epoch 150, Training Loss 0.5260785890890814\n",
      "2022-03-26 17:32:32.890108 Epoch 150, Training Loss 0.5266463474925521\n",
      "2022-03-26 17:32:32.919104 Epoch 150, Training Loss 0.5275816101476055\n",
      "2022-03-26 17:32:32.943563 Epoch 150, Training Loss 0.5282881923420045\n",
      "2022-03-26 17:32:32.966668 Epoch 150, Training Loss 0.5290999562310441\n",
      "2022-03-26 17:32:32.989898 Epoch 150, Training Loss 0.5301057250832047\n",
      "2022-03-26 17:32:33.021178 Epoch 150, Training Loss 0.5307933879859003\n",
      "2022-03-26 17:32:33.045378 Epoch 150, Training Loss 0.5318435207580972\n",
      "2022-03-26 17:32:33.068277 Epoch 150, Training Loss 0.5330218245534946\n",
      "2022-03-26 17:32:33.094502 Epoch 150, Training Loss 0.5338345560652521\n",
      "2022-03-26 17:32:33.117385 Epoch 150, Training Loss 0.5346492466033267\n",
      "2022-03-26 17:32:33.147809 Epoch 150, Training Loss 0.5354605708509454\n",
      "2022-03-26 17:32:33.170910 Epoch 150, Training Loss 0.5360665450925413\n",
      "2022-03-26 17:32:33.201613 Epoch 150, Training Loss 0.5368484599358591\n",
      "2022-03-26 17:32:33.228418 Epoch 150, Training Loss 0.5375831008262342\n",
      "2022-03-26 17:32:33.251173 Epoch 150, Training Loss 0.5383103368684764\n",
      "2022-03-26 17:32:33.275150 Epoch 150, Training Loss 0.5389282441200198\n",
      "2022-03-26 17:32:33.298259 Epoch 150, Training Loss 0.5399194710394916\n",
      "2022-03-26 17:32:33.321495 Epoch 150, Training Loss 0.5408841664986233\n",
      "2022-03-26 17:32:33.346981 Epoch 150, Training Loss 0.5419350590394891\n",
      "2022-03-26 17:32:33.378562 Epoch 150, Training Loss 0.5426188042706541\n",
      "2022-03-26 17:32:33.402422 Epoch 150, Training Loss 0.5434712647172191\n",
      "2022-03-26 17:32:33.426423 Epoch 150, Training Loss 0.5441244850530649\n",
      "2022-03-26 17:32:33.452324 Epoch 150, Training Loss 0.5450925102929021\n",
      "2022-03-26 17:32:33.474871 Epoch 150, Training Loss 0.5457228888254946\n",
      "2022-03-26 17:32:33.498097 Epoch 150, Training Loss 0.5467155721715039\n",
      "2022-03-26 17:32:33.520859 Epoch 150, Training Loss 0.5474544676292278\n",
      "2022-03-26 17:32:33.544116 Epoch 150, Training Loss 0.5484099018832912\n",
      "2022-03-26 17:32:33.566945 Epoch 150, Training Loss 0.5491576939821243\n",
      "2022-03-26 17:32:33.599444 Epoch 150, Training Loss 0.5501203231723107\n",
      "2022-03-26 17:32:33.623088 Epoch 150, Training Loss 0.5509334010312624\n",
      "2022-03-26 17:32:33.645632 Epoch 150, Training Loss 0.5515219300909115\n",
      "2022-03-26 17:32:33.669096 Epoch 150, Training Loss 0.5521731952114788\n",
      "2022-03-26 17:32:33.694740 Epoch 150, Training Loss 0.552910863667193\n",
      "2022-03-26 17:32:33.717931 Epoch 150, Training Loss 0.5537722163340625\n",
      "2022-03-26 17:32:33.741012 Epoch 150, Training Loss 0.5545054288471446\n",
      "2022-03-26 17:32:33.764153 Epoch 150, Training Loss 0.5553490662056467\n",
      "2022-03-26 17:32:33.786789 Epoch 150, Training Loss 0.5560803223601387\n",
      "2022-03-26 17:32:33.816466 Epoch 150, Training Loss 0.5567155547459107\n",
      "2022-03-26 17:32:33.841791 Epoch 150, Training Loss 0.5578290669204634\n",
      "2022-03-26 17:32:33.864491 Epoch 150, Training Loss 0.5589332555413551\n",
      "2022-03-26 17:32:33.889072 Epoch 150, Training Loss 0.5596589267711201\n",
      "2022-03-26 17:32:33.915161 Epoch 150, Training Loss 0.5603732475844185\n",
      "2022-03-26 17:32:33.940863 Epoch 150, Training Loss 0.5613842792523182\n",
      "2022-03-26 17:32:33.964138 Epoch 150, Training Loss 0.5620241507392405\n",
      "2022-03-26 17:32:33.989304 Epoch 150, Training Loss 0.5631795443232407\n",
      "2022-03-26 17:32:34.012332 Epoch 150, Training Loss 0.5642488715441331\n",
      "2022-03-26 17:32:34.043110 Epoch 150, Training Loss 0.5653161347064826\n",
      "2022-03-26 17:32:34.066443 Epoch 150, Training Loss 0.565983781348104\n",
      "2022-03-26 17:32:34.089236 Epoch 150, Training Loss 0.566686596788104\n",
      "2022-03-26 17:32:34.115618 Epoch 150, Training Loss 0.5673101502839867\n",
      "2022-03-26 17:32:34.138541 Epoch 150, Training Loss 0.5681189213643598\n",
      "2022-03-26 17:32:34.161621 Epoch 150, Training Loss 0.568697809906262\n",
      "2022-03-26 17:32:34.184537 Epoch 150, Training Loss 0.569766028724668\n",
      "2022-03-26 17:32:34.206996 Epoch 150, Training Loss 0.5704405181624396\n",
      "2022-03-26 17:32:34.240441 Epoch 150, Training Loss 0.571268036809114\n",
      "2022-03-26 17:32:34.271247 Epoch 150, Training Loss 0.5722890668131811\n",
      "2022-03-26 17:32:34.294708 Epoch 150, Training Loss 0.5731799085350597\n",
      "2022-03-26 17:32:34.317503 Epoch 150, Training Loss 0.574178885506547\n",
      "2022-03-26 17:32:34.348542 Epoch 150, Training Loss 0.5755682014824485\n",
      "2022-03-26 17:32:34.372677 Epoch 150, Training Loss 0.5763345078357955\n",
      "2022-03-26 17:32:34.396906 Epoch 150, Training Loss 0.5770365804281381\n",
      "2022-03-26 17:32:34.420763 Epoch 150, Training Loss 0.5781372566433514\n",
      "2022-03-26 17:32:34.444723 Epoch 150, Training Loss 0.5789150025914697\n",
      "2022-03-26 17:32:34.468333 Epoch 150, Training Loss 0.5793847330772054\n",
      "2022-03-26 17:32:34.500458 Epoch 150, Training Loss 0.5803939454314654\n",
      "2022-03-26 17:32:34.525150 Epoch 150, Training Loss 0.5811180728094657\n",
      "2022-03-26 17:32:34.548979 Epoch 150, Training Loss 0.5820339830101603\n",
      "2022-03-26 17:32:34.576344 Epoch 150, Training Loss 0.5827551814525024\n",
      "2022-03-26 17:32:34.600828 Epoch 150, Training Loss 0.5834302221189069\n",
      "2022-03-26 17:32:34.624262 Epoch 150, Training Loss 0.5842860557157975\n",
      "2022-03-26 17:32:34.648426 Epoch 150, Training Loss 0.5852400266834538\n",
      "2022-03-26 17:32:34.671821 Epoch 150, Training Loss 0.5859359593113975\n",
      "2022-03-26 17:32:34.695396 Epoch 150, Training Loss 0.5869656389250475\n",
      "2022-03-26 17:32:34.726837 Epoch 150, Training Loss 0.5876396135677158\n",
      "2022-03-26 17:32:34.751586 Epoch 150, Training Loss 0.5886778273164769\n",
      "2022-03-26 17:32:34.775156 Epoch 150, Training Loss 0.5894325584020761\n",
      "2022-03-26 17:32:34.798631 Epoch 150, Training Loss 0.590072425010869\n",
      "2022-03-26 17:32:34.823892 Epoch 150, Training Loss 0.591140244584864\n",
      "2022-03-26 17:32:34.850308 Epoch 150, Training Loss 0.5920209086993161\n",
      "2022-03-26 17:32:34.874059 Epoch 150, Training Loss 0.5930126972134461\n",
      "2022-03-26 17:32:34.898706 Epoch 150, Training Loss 0.5940881248401559\n",
      "2022-03-26 17:32:34.921948 Epoch 150, Training Loss 0.5950096888691568\n",
      "2022-03-26 17:32:34.952699 Epoch 150, Training Loss 0.5958769109929004\n",
      "2022-03-26 17:32:34.976718 Epoch 150, Training Loss 0.5967047945846378\n",
      "2022-03-26 17:32:35.001191 Epoch 150, Training Loss 0.5975856433058029\n",
      "2022-03-26 17:32:35.025435 Epoch 150, Training Loss 0.5982883388505262\n",
      "2022-03-26 17:32:35.050575 Epoch 150, Training Loss 0.598965734738828\n",
      "2022-03-26 17:32:35.073862 Epoch 150, Training Loss 0.5999362187845932\n",
      "2022-03-26 17:32:35.097393 Epoch 150, Training Loss 0.6007188538379986\n",
      "2022-03-26 17:32:35.126093 Epoch 150, Training Loss 0.6013240599266404\n",
      "2022-03-26 17:32:35.150560 Epoch 150, Training Loss 0.6022435347442432\n",
      "2022-03-26 17:32:35.186077 Epoch 150, Training Loss 0.6030610449936079\n",
      "2022-03-26 17:32:35.209390 Epoch 150, Training Loss 0.6037420739450723\n",
      "2022-03-26 17:32:35.241115 Epoch 150, Training Loss 0.6046941252163304\n",
      "2022-03-26 17:32:35.270650 Epoch 150, Training Loss 0.6053208426746262\n",
      "2022-03-26 17:32:35.294154 Epoch 150, Training Loss 0.6061697131989862\n",
      "2022-03-26 17:32:35.317449 Epoch 150, Training Loss 0.6069393580222069\n",
      "2022-03-26 17:32:35.344177 Epoch 150, Training Loss 0.607704208101458\n",
      "2022-03-26 17:32:35.366949 Epoch 150, Training Loss 0.6084444858991277\n",
      "2022-03-26 17:32:35.395436 Epoch 150, Training Loss 0.6092715686391992\n",
      "2022-03-26 17:32:35.420016 Epoch 150, Training Loss 0.6101955168539911\n",
      "2022-03-26 17:32:35.442636 Epoch 150, Training Loss 0.6110935238621119\n",
      "2022-03-26 17:32:35.470585 Epoch 150, Training Loss 0.6118148065284085\n",
      "2022-03-26 17:32:35.494091 Epoch 150, Training Loss 0.6127737984632897\n",
      "2022-03-26 17:32:35.517511 Epoch 150, Training Loss 0.6135312434657455\n",
      "2022-03-26 17:32:35.541181 Epoch 150, Training Loss 0.614568209709109\n",
      "2022-03-26 17:32:35.564701 Epoch 150, Training Loss 0.6152509002734328\n",
      "2022-03-26 17:32:35.587686 Epoch 150, Training Loss 0.6161065465958832\n",
      "2022-03-26 17:32:35.617558 Epoch 150, Training Loss 0.6169924013450018\n",
      "2022-03-26 17:32:35.641731 Epoch 150, Training Loss 0.6176939003760248\n",
      "2022-03-26 17:32:35.664224 Epoch 150, Training Loss 0.6186462214688206\n",
      "2022-03-26 17:32:35.687161 Epoch 150, Training Loss 0.6194106057629256\n",
      "2022-03-26 17:32:35.710191 Epoch 150, Training Loss 0.6203300657174776\n",
      "2022-03-26 17:32:35.737049 Epoch 150, Training Loss 0.6213749411618313\n",
      "2022-03-26 17:32:35.759784 Epoch 150, Training Loss 0.6221447402558973\n",
      "2022-03-26 17:32:35.782269 Epoch 150, Training Loss 0.6228745084284516\n",
      "2022-03-26 17:32:35.805258 Epoch 150, Training Loss 0.623681753568942\n",
      "2022-03-26 17:32:35.836972 Epoch 150, Training Loss 0.6243706570409447\n",
      "2022-03-26 17:32:35.861345 Epoch 150, Training Loss 0.6251347073356209\n",
      "2022-03-26 17:32:35.884083 Epoch 150, Training Loss 0.6259979240577239\n",
      "2022-03-26 17:32:35.906723 Epoch 150, Training Loss 0.6268154997045122\n",
      "2022-03-26 17:32:35.933743 Epoch 150, Training Loss 0.6274350521814488\n",
      "2022-03-26 17:32:35.956648 Epoch 150, Training Loss 0.6283307180685156\n",
      "2022-03-26 17:32:35.979089 Epoch 150, Training Loss 0.6292550688814325\n",
      "2022-03-26 17:32:36.001628 Epoch 150, Training Loss 0.6300968995027225\n",
      "2022-03-26 17:32:36.029434 Epoch 150, Training Loss 0.6307907859840052\n",
      "2022-03-26 17:32:36.062678 Epoch 150, Training Loss 0.6314854428286443\n",
      "2022-03-26 17:32:36.071824 Epoch 150, Training Loss 0.6318909059781248\n",
      "2022-03-26 17:47:11.297642 Epoch 200, Training Loss 0.0005275366632529842\n",
      "2022-03-26 17:47:11.320761 Epoch 200, Training Loss 0.00127918583810177\n",
      "2022-03-26 17:47:11.345945 Epoch 200, Training Loss 0.0022841509422072974\n",
      "2022-03-26 17:47:11.370098 Epoch 200, Training Loss 0.0028087387761801404\n",
      "2022-03-26 17:47:11.393883 Epoch 200, Training Loss 0.0032093494063448113\n",
      "2022-03-26 17:47:11.417100 Epoch 200, Training Loss 0.004005769977484213\n",
      "2022-03-26 17:47:11.440656 Epoch 200, Training Loss 0.0048813971183489045\n",
      "2022-03-26 17:47:11.463272 Epoch 200, Training Loss 0.005582074946759607\n",
      "2022-03-26 17:47:11.486534 Epoch 200, Training Loss 0.00631373446158436\n",
      "2022-03-26 17:47:11.517755 Epoch 200, Training Loss 0.007046937447069856\n",
      "2022-03-26 17:47:11.541867 Epoch 200, Training Loss 0.007903546514108663\n",
      "2022-03-26 17:47:11.565380 Epoch 200, Training Loss 0.008798252476755615\n",
      "2022-03-26 17:47:11.588898 Epoch 200, Training Loss 0.00961427078070238\n",
      "2022-03-26 17:47:11.611825 Epoch 200, Training Loss 0.010487088881185293\n",
      "2022-03-26 17:47:11.635411 Epoch 200, Training Loss 0.011305632608016128\n",
      "2022-03-26 17:47:11.658482 Epoch 200, Training Loss 0.011876980628808745\n",
      "2022-03-26 17:47:11.684949 Epoch 200, Training Loss 0.0124834319743354\n",
      "2022-03-26 17:47:11.707863 Epoch 200, Training Loss 0.013014939701770577\n",
      "2022-03-26 17:47:11.739311 Epoch 200, Training Loss 0.01363079550930911\n",
      "2022-03-26 17:47:11.763069 Epoch 200, Training Loss 0.014380016137876778\n",
      "2022-03-26 17:47:11.786088 Epoch 200, Training Loss 0.015164241766380837\n",
      "2022-03-26 17:47:11.809376 Epoch 200, Training Loss 0.015972653694469912\n",
      "2022-03-26 17:47:11.833191 Epoch 200, Training Loss 0.01659673837292225\n",
      "2022-03-26 17:47:11.856641 Epoch 200, Training Loss 0.01744008975108261\n",
      "2022-03-26 17:47:11.880701 Epoch 200, Training Loss 0.018186355757591366\n",
      "2022-03-26 17:47:11.904262 Epoch 200, Training Loss 0.01876916174236161\n",
      "2022-03-26 17:47:11.936566 Epoch 200, Training Loss 0.0195570766849591\n",
      "2022-03-26 17:47:11.969673 Epoch 200, Training Loss 0.020394439320735005\n",
      "2022-03-26 17:47:11.993539 Epoch 200, Training Loss 0.020980812552030128\n",
      "2022-03-26 17:47:12.016397 Epoch 200, Training Loss 0.021848266234483254\n",
      "2022-03-26 17:47:12.040581 Epoch 200, Training Loss 0.02253111129831475\n",
      "2022-03-26 17:47:12.066603 Epoch 200, Training Loss 0.02309134107111665\n",
      "2022-03-26 17:47:12.093424 Epoch 200, Training Loss 0.023924063005105918\n",
      "2022-03-26 17:47:12.116089 Epoch 200, Training Loss 0.02475849411371724\n",
      "2022-03-26 17:47:12.139317 Epoch 200, Training Loss 0.025586285706981064\n",
      "2022-03-26 17:47:12.162447 Epoch 200, Training Loss 0.026444071241656836\n",
      "2022-03-26 17:47:12.195382 Epoch 200, Training Loss 0.027238074470968807\n",
      "2022-03-26 17:47:12.219240 Epoch 200, Training Loss 0.027901870210457334\n",
      "2022-03-26 17:47:12.242926 Epoch 200, Training Loss 0.02879716673165636\n",
      "2022-03-26 17:47:12.271925 Epoch 200, Training Loss 0.029280658001485077\n",
      "2022-03-26 17:47:12.296015 Epoch 200, Training Loss 0.029865114204109172\n",
      "2022-03-26 17:47:12.320392 Epoch 200, Training Loss 0.030530675978916684\n",
      "2022-03-26 17:47:12.346835 Epoch 200, Training Loss 0.031147764238250226\n",
      "2022-03-26 17:47:12.371370 Epoch 200, Training Loss 0.03205220923399377\n",
      "2022-03-26 17:47:12.395622 Epoch 200, Training Loss 0.033083280867627816\n",
      "2022-03-26 17:47:12.426659 Epoch 200, Training Loss 0.03386343493485999\n",
      "2022-03-26 17:47:12.451005 Epoch 200, Training Loss 0.03445488530809007\n",
      "2022-03-26 17:47:12.474661 Epoch 200, Training Loss 0.035006246755799976\n",
      "2022-03-26 17:47:12.498660 Epoch 200, Training Loss 0.0359084303574184\n",
      "2022-03-26 17:47:12.522751 Epoch 200, Training Loss 0.03665720715242274\n",
      "2022-03-26 17:47:12.546634 Epoch 200, Training Loss 0.03728879004945535\n",
      "2022-03-26 17:47:12.570576 Epoch 200, Training Loss 0.03812325431410309\n",
      "2022-03-26 17:47:12.596544 Epoch 200, Training Loss 0.03887958024316432\n",
      "2022-03-26 17:47:12.620826 Epoch 200, Training Loss 0.03971077616104994\n",
      "2022-03-26 17:47:12.654840 Epoch 200, Training Loss 0.04033830418916005\n",
      "2022-03-26 17:47:12.679002 Epoch 200, Training Loss 0.04088359507148528\n",
      "2022-03-26 17:47:12.703020 Epoch 200, Training Loss 0.041475944697399576\n",
      "2022-03-26 17:47:12.726737 Epoch 200, Training Loss 0.042403915615947654\n",
      "2022-03-26 17:47:12.752011 Epoch 200, Training Loss 0.043125628777172256\n",
      "2022-03-26 17:47:12.775811 Epoch 200, Training Loss 0.043790479488385\n",
      "2022-03-26 17:47:12.800317 Epoch 200, Training Loss 0.04430414217969646\n",
      "2022-03-26 17:47:12.824351 Epoch 200, Training Loss 0.045030319515396565\n",
      "2022-03-26 17:47:12.852818 Epoch 200, Training Loss 0.045945321850459596\n",
      "2022-03-26 17:47:12.884270 Epoch 200, Training Loss 0.04682688453160893\n",
      "2022-03-26 17:47:12.908238 Epoch 200, Training Loss 0.047812851226848106\n",
      "2022-03-26 17:47:12.930892 Epoch 200, Training Loss 0.04896882911929694\n",
      "2022-03-26 17:47:12.961379 Epoch 200, Training Loss 0.04975529030308394\n",
      "2022-03-26 17:47:12.987372 Epoch 200, Training Loss 0.05041354303927068\n",
      "2022-03-26 17:47:13.018378 Epoch 200, Training Loss 0.05122366703837119\n",
      "2022-03-26 17:47:13.041966 Epoch 200, Training Loss 0.05185710735943006\n",
      "2022-03-26 17:47:13.065557 Epoch 200, Training Loss 0.052852181217554584\n",
      "2022-03-26 17:47:13.093360 Epoch 200, Training Loss 0.05354054748554669\n",
      "2022-03-26 17:47:13.120274 Epoch 200, Training Loss 0.05435945448058341\n",
      "2022-03-26 17:47:13.144036 Epoch 200, Training Loss 0.055310538281565125\n",
      "2022-03-26 17:47:13.167070 Epoch 200, Training Loss 0.05601999887724972\n",
      "2022-03-26 17:47:13.190091 Epoch 200, Training Loss 0.05683316560962316\n",
      "2022-03-26 17:47:13.212572 Epoch 200, Training Loss 0.057611260115338105\n",
      "2022-03-26 17:47:13.235811 Epoch 200, Training Loss 0.058272491178244276\n",
      "2022-03-26 17:47:13.263206 Epoch 200, Training Loss 0.058885660089190356\n",
      "2022-03-26 17:47:13.286515 Epoch 200, Training Loss 0.05950367103909592\n",
      "2022-03-26 17:47:13.315678 Epoch 200, Training Loss 0.060267304406141683\n",
      "2022-03-26 17:47:13.346454 Epoch 200, Training Loss 0.0610069725138452\n",
      "2022-03-26 17:47:13.371098 Epoch 200, Training Loss 0.06154991846407771\n",
      "2022-03-26 17:47:13.395623 Epoch 200, Training Loss 0.06240902757248305\n",
      "2022-03-26 17:47:13.420395 Epoch 200, Training Loss 0.06306487920186708\n",
      "2022-03-26 17:47:13.444084 Epoch 200, Training Loss 0.0636654645204544\n",
      "2022-03-26 17:47:13.466824 Epoch 200, Training Loss 0.06425274432162799\n",
      "2022-03-26 17:47:13.489910 Epoch 200, Training Loss 0.06494708713668082\n",
      "2022-03-26 17:47:13.513575 Epoch 200, Training Loss 0.06592172940673731\n",
      "2022-03-26 17:47:13.547691 Epoch 200, Training Loss 0.06664705276489258\n",
      "2022-03-26 17:47:13.572666 Epoch 200, Training Loss 0.06730608508714934\n",
      "2022-03-26 17:47:13.596622 Epoch 200, Training Loss 0.06810448518799395\n",
      "2022-03-26 17:47:13.621412 Epoch 200, Training Loss 0.0687879900950605\n",
      "2022-03-26 17:47:13.649838 Epoch 200, Training Loss 0.06947481754185904\n",
      "2022-03-26 17:47:13.673859 Epoch 200, Training Loss 0.07016934633559888\n",
      "2022-03-26 17:47:13.698499 Epoch 200, Training Loss 0.0709238143833092\n",
      "2022-03-26 17:47:13.722561 Epoch 200, Training Loss 0.07194788811151939\n",
      "2022-03-26 17:47:13.748100 Epoch 200, Training Loss 0.072588883168862\n",
      "2022-03-26 17:47:13.781535 Epoch 200, Training Loss 0.07340829893756096\n",
      "2022-03-26 17:47:13.805648 Epoch 200, Training Loss 0.0741232430843441\n",
      "2022-03-26 17:47:13.829726 Epoch 200, Training Loss 0.0750140044695276\n",
      "2022-03-26 17:47:13.854303 Epoch 200, Training Loss 0.07560243135523004\n",
      "2022-03-26 17:47:13.878230 Epoch 200, Training Loss 0.07637205689459506\n",
      "2022-03-26 17:47:13.908076 Epoch 200, Training Loss 0.07694734358574118\n",
      "2022-03-26 17:47:13.932914 Epoch 200, Training Loss 0.07775674272528695\n",
      "2022-03-26 17:47:13.956734 Epoch 200, Training Loss 0.07828208118143594\n",
      "2022-03-26 17:47:13.990853 Epoch 200, Training Loss 0.07892622331829022\n",
      "2022-03-26 17:47:14.023367 Epoch 200, Training Loss 0.07973093846265007\n",
      "2022-03-26 17:47:14.047818 Epoch 200, Training Loss 0.08038674222538843\n",
      "2022-03-26 17:47:14.072131 Epoch 200, Training Loss 0.08118964483975755\n",
      "2022-03-26 17:47:14.102248 Epoch 200, Training Loss 0.08205365502011136\n",
      "2022-03-26 17:47:14.126625 Epoch 200, Training Loss 0.08289459606875545\n",
      "2022-03-26 17:47:14.150250 Epoch 200, Training Loss 0.08379775621092228\n",
      "2022-03-26 17:47:14.174368 Epoch 200, Training Loss 0.08423681012199968\n",
      "2022-03-26 17:47:14.204647 Epoch 200, Training Loss 0.08483917645328795\n",
      "2022-03-26 17:47:14.229272 Epoch 200, Training Loss 0.08573583595435638\n",
      "2022-03-26 17:47:14.258659 Epoch 200, Training Loss 0.08639807587541888\n",
      "2022-03-26 17:47:14.282621 Epoch 200, Training Loss 0.0870489266598621\n",
      "2022-03-26 17:47:14.307564 Epoch 200, Training Loss 0.08773566942538141\n",
      "2022-03-26 17:47:14.333884 Epoch 200, Training Loss 0.0883372512162494\n",
      "2022-03-26 17:47:14.357683 Epoch 200, Training Loss 0.08922181623366178\n",
      "2022-03-26 17:47:14.385512 Epoch 200, Training Loss 0.08989269349276258\n",
      "2022-03-26 17:47:14.413636 Epoch 200, Training Loss 0.0907700212715227\n",
      "2022-03-26 17:47:14.444469 Epoch 200, Training Loss 0.09168599641231624\n",
      "2022-03-26 17:47:14.467570 Epoch 200, Training Loss 0.09259910656668036\n",
      "2022-03-26 17:47:14.491310 Epoch 200, Training Loss 0.09334228212571205\n",
      "2022-03-26 17:47:14.514515 Epoch 200, Training Loss 0.09414124298278632\n",
      "2022-03-26 17:47:14.541706 Epoch 200, Training Loss 0.09492598988515946\n",
      "2022-03-26 17:47:14.564752 Epoch 200, Training Loss 0.0957094931693943\n",
      "2022-03-26 17:47:14.587144 Epoch 200, Training Loss 0.09652763574629489\n",
      "2022-03-26 17:47:14.609759 Epoch 200, Training Loss 0.09743015586262774\n",
      "2022-03-26 17:47:14.640961 Epoch 200, Training Loss 0.0980281430437132\n",
      "2022-03-26 17:47:14.664753 Epoch 200, Training Loss 0.09885163235542414\n",
      "2022-03-26 17:47:14.697200 Epoch 200, Training Loss 0.0997818756438887\n",
      "2022-03-26 17:47:14.721057 Epoch 200, Training Loss 0.10055220386256343\n",
      "2022-03-26 17:47:14.746108 Epoch 200, Training Loss 0.1015265988724311\n",
      "2022-03-26 17:47:14.769088 Epoch 200, Training Loss 0.10217915006610744\n",
      "2022-03-26 17:47:14.792114 Epoch 200, Training Loss 0.10275422947486039\n",
      "2022-03-26 17:47:14.815531 Epoch 200, Training Loss 0.1035209928479646\n",
      "2022-03-26 17:47:14.840143 Epoch 200, Training Loss 0.10427749423724611\n",
      "2022-03-26 17:47:14.874360 Epoch 200, Training Loss 0.10489183241296607\n",
      "2022-03-26 17:47:14.898148 Epoch 200, Training Loss 0.10582275703892378\n",
      "2022-03-26 17:47:14.920579 Epoch 200, Training Loss 0.10644652601093282\n",
      "2022-03-26 17:47:14.943643 Epoch 200, Training Loss 0.10718171530977234\n",
      "2022-03-26 17:47:14.966353 Epoch 200, Training Loss 0.10785600459179305\n",
      "2022-03-26 17:47:14.989382 Epoch 200, Training Loss 0.10854764118828737\n",
      "2022-03-26 17:47:15.018605 Epoch 200, Training Loss 0.10929064944272152\n",
      "2022-03-26 17:47:15.049203 Epoch 200, Training Loss 0.10999017534658427\n",
      "2022-03-26 17:47:15.073051 Epoch 200, Training Loss 0.1108795338884339\n",
      "2022-03-26 17:47:15.108720 Epoch 200, Training Loss 0.11153399357405465\n",
      "2022-03-26 17:47:15.132606 Epoch 200, Training Loss 0.11248990230243225\n",
      "2022-03-26 17:47:15.155322 Epoch 200, Training Loss 0.1135135224408201\n",
      "2022-03-26 17:47:15.178593 Epoch 200, Training Loss 0.11439674399088105\n",
      "2022-03-26 17:47:15.207733 Epoch 200, Training Loss 0.11495117328660873\n",
      "2022-03-26 17:47:15.230923 Epoch 200, Training Loss 0.11552968655553315\n",
      "2022-03-26 17:47:15.254798 Epoch 200, Training Loss 0.11662626628528165\n",
      "2022-03-26 17:47:15.277793 Epoch 200, Training Loss 0.11731527852432808\n",
      "2022-03-26 17:47:15.301411 Epoch 200, Training Loss 0.11798088016260005\n",
      "2022-03-26 17:47:15.334248 Epoch 200, Training Loss 0.1187854232766744\n",
      "2022-03-26 17:47:15.359459 Epoch 200, Training Loss 0.11977515093353398\n",
      "2022-03-26 17:47:15.383604 Epoch 200, Training Loss 0.12026809487501373\n",
      "2022-03-26 17:47:15.407635 Epoch 200, Training Loss 0.12079922306110792\n",
      "2022-03-26 17:47:15.437299 Epoch 200, Training Loss 0.12145758342102665\n",
      "2022-03-26 17:47:15.461621 Epoch 200, Training Loss 0.12238396978591715\n",
      "2022-03-26 17:47:15.484835 Epoch 200, Training Loss 0.12302972391590743\n",
      "2022-03-26 17:47:15.507644 Epoch 200, Training Loss 0.1237420001069603\n",
      "2022-03-26 17:47:15.530354 Epoch 200, Training Loss 0.12453983221060175\n",
      "2022-03-26 17:47:15.561707 Epoch 200, Training Loss 0.12519049648281252\n",
      "2022-03-26 17:47:15.585354 Epoch 200, Training Loss 0.1259614963589422\n",
      "2022-03-26 17:47:15.608035 Epoch 200, Training Loss 0.12664437031044679\n",
      "2022-03-26 17:47:15.630470 Epoch 200, Training Loss 0.12746784902747024\n",
      "2022-03-26 17:47:15.653767 Epoch 200, Training Loss 0.12811042345545787\n",
      "2022-03-26 17:47:15.676576 Epoch 200, Training Loss 0.12910405689340723\n",
      "2022-03-26 17:47:15.699733 Epoch 200, Training Loss 0.12995286697469405\n",
      "2022-03-26 17:47:15.723631 Epoch 200, Training Loss 0.13066791889765073\n",
      "2022-03-26 17:47:15.748454 Epoch 200, Training Loss 0.1313226753107422\n",
      "2022-03-26 17:47:15.777021 Epoch 200, Training Loss 0.13188605117218574\n",
      "2022-03-26 17:47:15.803415 Epoch 200, Training Loss 0.13292746931848015\n",
      "2022-03-26 17:47:15.830752 Epoch 200, Training Loss 0.1336017483869172\n",
      "2022-03-26 17:47:15.854764 Epoch 200, Training Loss 0.134527224218449\n",
      "2022-03-26 17:47:15.878062 Epoch 200, Training Loss 0.13524146248465\n",
      "2022-03-26 17:47:15.900740 Epoch 200, Training Loss 0.13577817117466645\n",
      "2022-03-26 17:47:15.923550 Epoch 200, Training Loss 0.13660454003097455\n",
      "2022-03-26 17:47:15.949763 Epoch 200, Training Loss 0.13737943783745438\n",
      "2022-03-26 17:47:15.972845 Epoch 200, Training Loss 0.13824551344832495\n",
      "2022-03-26 17:47:16.004919 Epoch 200, Training Loss 0.13875599609464026\n",
      "2022-03-26 17:47:16.033697 Epoch 200, Training Loss 0.13934423349550007\n",
      "2022-03-26 17:47:16.068409 Epoch 200, Training Loss 0.1400740638642055\n",
      "2022-03-26 17:47:16.093969 Epoch 200, Training Loss 0.14072103432529723\n",
      "2022-03-26 17:47:16.119780 Epoch 200, Training Loss 0.14134562366149006\n",
      "2022-03-26 17:47:16.143674 Epoch 200, Training Loss 0.14186659321913023\n",
      "2022-03-26 17:47:16.167653 Epoch 200, Training Loss 0.14265032554678905\n",
      "2022-03-26 17:47:16.193435 Epoch 200, Training Loss 0.14324322823063493\n",
      "2022-03-26 17:47:16.228101 Epoch 200, Training Loss 0.14388610632218363\n",
      "2022-03-26 17:47:16.254077 Epoch 200, Training Loss 0.14472742153860418\n",
      "2022-03-26 17:47:16.276948 Epoch 200, Training Loss 0.14563372883650347\n",
      "2022-03-26 17:47:16.300021 Epoch 200, Training Loss 0.14630602570750828\n",
      "2022-03-26 17:47:16.326943 Epoch 200, Training Loss 0.14717142630720992\n",
      "2022-03-26 17:47:16.351083 Epoch 200, Training Loss 0.14796997869715972\n",
      "2022-03-26 17:47:16.374235 Epoch 200, Training Loss 0.14875019046351734\n",
      "2022-03-26 17:47:16.397275 Epoch 200, Training Loss 0.14949217034727716\n",
      "2022-03-26 17:47:16.419508 Epoch 200, Training Loss 0.1501466724116479\n",
      "2022-03-26 17:47:16.450574 Epoch 200, Training Loss 0.15070914486637504\n",
      "2022-03-26 17:47:16.476231 Epoch 200, Training Loss 0.1514393166279244\n",
      "2022-03-26 17:47:16.500295 Epoch 200, Training Loss 0.15229011614761695\n",
      "2022-03-26 17:47:16.523892 Epoch 200, Training Loss 0.15330301880684044\n",
      "2022-03-26 17:47:16.547830 Epoch 200, Training Loss 0.1543877396513434\n",
      "2022-03-26 17:47:16.571498 Epoch 200, Training Loss 0.15508298248128818\n",
      "2022-03-26 17:47:16.595589 Epoch 200, Training Loss 0.15578337653975963\n",
      "2022-03-26 17:47:16.619705 Epoch 200, Training Loss 0.1567900765811086\n",
      "2022-03-26 17:47:16.644314 Epoch 200, Training Loss 0.15734722554836128\n",
      "2022-03-26 17:47:16.675095 Epoch 200, Training Loss 0.15812566083715396\n",
      "2022-03-26 17:47:16.700048 Epoch 200, Training Loss 0.159116339119499\n",
      "2022-03-26 17:47:16.724112 Epoch 200, Training Loss 0.1601255594769402\n",
      "2022-03-26 17:47:16.750186 Epoch 200, Training Loss 0.16059407443189255\n",
      "2022-03-26 17:47:16.774372 Epoch 200, Training Loss 0.16145729694677435\n",
      "2022-03-26 17:47:16.798274 Epoch 200, Training Loss 0.16220507761249153\n",
      "2022-03-26 17:47:16.822233 Epoch 200, Training Loss 0.16279101436552795\n",
      "2022-03-26 17:47:16.846378 Epoch 200, Training Loss 0.1634329872591721\n",
      "2022-03-26 17:47:16.877496 Epoch 200, Training Loss 0.16404569434845234\n",
      "2022-03-26 17:47:16.911342 Epoch 200, Training Loss 0.16497019897488988\n",
      "2022-03-26 17:47:16.939693 Epoch 200, Training Loss 0.16579305733103886\n",
      "2022-03-26 17:47:16.963349 Epoch 200, Training Loss 0.16638993466144328\n",
      "2022-03-26 17:47:16.989215 Epoch 200, Training Loss 0.16727201190903362\n",
      "2022-03-26 17:47:17.013245 Epoch 200, Training Loss 0.16807848424710276\n",
      "2022-03-26 17:47:17.037447 Epoch 200, Training Loss 0.16879082164343664\n",
      "2022-03-26 17:47:17.062823 Epoch 200, Training Loss 0.16957416013835946\n",
      "2022-03-26 17:47:17.101650 Epoch 200, Training Loss 0.17035132459819774\n",
      "2022-03-26 17:47:17.127334 Epoch 200, Training Loss 0.17095994884553162\n",
      "2022-03-26 17:47:17.154040 Epoch 200, Training Loss 0.1719877148604454\n",
      "2022-03-26 17:47:17.180601 Epoch 200, Training Loss 0.1726493828970453\n",
      "2022-03-26 17:47:17.204688 Epoch 200, Training Loss 0.17318030395318784\n",
      "2022-03-26 17:47:17.228749 Epoch 200, Training Loss 0.1741893136364115\n",
      "2022-03-26 17:47:17.252739 Epoch 200, Training Loss 0.17483270835236212\n",
      "2022-03-26 17:47:17.275692 Epoch 200, Training Loss 0.17566841600648583\n",
      "2022-03-26 17:47:17.298603 Epoch 200, Training Loss 0.17647959124249266\n",
      "2022-03-26 17:47:17.334848 Epoch 200, Training Loss 0.17709793798301532\n",
      "2022-03-26 17:47:17.359044 Epoch 200, Training Loss 0.1776882503419886\n",
      "2022-03-26 17:47:17.383599 Epoch 200, Training Loss 0.1785435298900775\n",
      "2022-03-26 17:47:17.407218 Epoch 200, Training Loss 0.1792595197096505\n",
      "2022-03-26 17:47:17.431555 Epoch 200, Training Loss 0.17982944144922144\n",
      "2022-03-26 17:47:17.456614 Epoch 200, Training Loss 0.1805132526113554\n",
      "2022-03-26 17:47:17.480732 Epoch 200, Training Loss 0.18137007258127413\n",
      "2022-03-26 17:47:17.505007 Epoch 200, Training Loss 0.182117195156834\n",
      "2022-03-26 17:47:17.529393 Epoch 200, Training Loss 0.1829886486005905\n",
      "2022-03-26 17:47:17.560721 Epoch 200, Training Loss 0.18361027778871833\n",
      "2022-03-26 17:47:17.583657 Epoch 200, Training Loss 0.18431304475230634\n",
      "2022-03-26 17:47:17.606576 Epoch 200, Training Loss 0.1850008533128997\n",
      "2022-03-26 17:47:17.631492 Epoch 200, Training Loss 0.18570653777902998\n",
      "2022-03-26 17:47:17.657241 Epoch 200, Training Loss 0.1865393076558857\n",
      "2022-03-26 17:47:17.680250 Epoch 200, Training Loss 0.18735026939750632\n",
      "2022-03-26 17:47:17.703382 Epoch 200, Training Loss 0.18798771431988767\n",
      "2022-03-26 17:47:17.726395 Epoch 200, Training Loss 0.1885223079596639\n",
      "2022-03-26 17:47:17.750682 Epoch 200, Training Loss 0.18929257604014843\n",
      "2022-03-26 17:47:17.782176 Epoch 200, Training Loss 0.18987075896824107\n",
      "2022-03-26 17:47:17.806731 Epoch 200, Training Loss 0.19070504510494143\n",
      "2022-03-26 17:47:17.830356 Epoch 200, Training Loss 0.19136398344698463\n",
      "2022-03-26 17:47:17.855103 Epoch 200, Training Loss 0.19188286676583693\n",
      "2022-03-26 17:47:17.878567 Epoch 200, Training Loss 0.19269484430170425\n",
      "2022-03-26 17:47:17.901789 Epoch 200, Training Loss 0.1935660545249729\n",
      "2022-03-26 17:47:17.924801 Epoch 200, Training Loss 0.19445373712445768\n",
      "2022-03-26 17:47:17.947999 Epoch 200, Training Loss 0.19527159234904268\n",
      "2022-03-26 17:47:17.970915 Epoch 200, Training Loss 0.19603154600581243\n",
      "2022-03-26 17:47:18.000540 Epoch 200, Training Loss 0.19679591680884057\n",
      "2022-03-26 17:47:18.024260 Epoch 200, Training Loss 0.19756299253467405\n",
      "2022-03-26 17:47:18.047950 Epoch 200, Training Loss 0.19821071864851295\n",
      "2022-03-26 17:47:18.071141 Epoch 200, Training Loss 0.1991994735758628\n",
      "2022-03-26 17:47:18.106640 Epoch 200, Training Loss 0.200116849273367\n",
      "2022-03-26 17:47:18.137798 Epoch 200, Training Loss 0.20069698162395935\n",
      "2022-03-26 17:47:18.161301 Epoch 200, Training Loss 0.2014061583734839\n",
      "2022-03-26 17:47:18.184486 Epoch 200, Training Loss 0.20184974844955728\n",
      "2022-03-26 17:47:18.214488 Epoch 200, Training Loss 0.2024687836542154\n",
      "2022-03-26 17:47:18.241102 Epoch 200, Training Loss 0.20310944288283053\n",
      "2022-03-26 17:47:18.266186 Epoch 200, Training Loss 0.20369650728409858\n",
      "2022-03-26 17:47:18.293036 Epoch 200, Training Loss 0.20435155798559604\n",
      "2022-03-26 17:47:18.316720 Epoch 200, Training Loss 0.204891630069679\n",
      "2022-03-26 17:47:18.342908 Epoch 200, Training Loss 0.20594754868456164\n",
      "2022-03-26 17:47:18.367768 Epoch 200, Training Loss 0.20689552016270435\n",
      "2022-03-26 17:47:18.391815 Epoch 200, Training Loss 0.20768567881620753\n",
      "2022-03-26 17:47:18.414581 Epoch 200, Training Loss 0.20858544560954395\n",
      "2022-03-26 17:47:18.446838 Epoch 200, Training Loss 0.20939180697016704\n",
      "2022-03-26 17:47:18.470822 Epoch 200, Training Loss 0.21020076319079875\n",
      "2022-03-26 17:47:18.495057 Epoch 200, Training Loss 0.21120981258504531\n",
      "2022-03-26 17:47:18.518093 Epoch 200, Training Loss 0.2120079104705235\n",
      "2022-03-26 17:47:18.543039 Epoch 200, Training Loss 0.21277894335024802\n",
      "2022-03-26 17:47:18.568477 Epoch 200, Training Loss 0.21336270308555544\n",
      "2022-03-26 17:47:18.591799 Epoch 200, Training Loss 0.21393349332272854\n",
      "2022-03-26 17:47:18.614692 Epoch 200, Training Loss 0.214852478574304\n",
      "2022-03-26 17:47:18.638736 Epoch 200, Training Loss 0.21562549609052556\n",
      "2022-03-26 17:47:18.669814 Epoch 200, Training Loss 0.21648406159237524\n",
      "2022-03-26 17:47:18.694015 Epoch 200, Training Loss 0.21728638797769767\n",
      "2022-03-26 17:47:18.717399 Epoch 200, Training Loss 0.21799774936702856\n",
      "2022-03-26 17:47:18.740805 Epoch 200, Training Loss 0.2187286323446142\n",
      "2022-03-26 17:47:18.764958 Epoch 200, Training Loss 0.21926409314812906\n",
      "2022-03-26 17:47:18.788325 Epoch 200, Training Loss 0.22015697385191613\n",
      "2022-03-26 17:47:18.810875 Epoch 200, Training Loss 0.22069202207238472\n",
      "2022-03-26 17:47:18.833980 Epoch 200, Training Loss 0.22140137702607743\n",
      "2022-03-26 17:47:18.857364 Epoch 200, Training Loss 0.22203565078318271\n",
      "2022-03-26 17:47:18.885484 Epoch 200, Training Loss 0.2226002618022587\n",
      "2022-03-26 17:47:18.909113 Epoch 200, Training Loss 0.22329754376655345\n",
      "2022-03-26 17:47:18.932159 Epoch 200, Training Loss 0.2238631717994085\n",
      "2022-03-26 17:47:18.954836 Epoch 200, Training Loss 0.22471502690059145\n",
      "2022-03-26 17:47:18.980375 Epoch 200, Training Loss 0.22559847650320633\n",
      "2022-03-26 17:47:19.005742 Epoch 200, Training Loss 0.22643096535407065\n",
      "2022-03-26 17:47:19.033084 Epoch 200, Training Loss 0.2271409326654566\n",
      "2022-03-26 17:47:19.056006 Epoch 200, Training Loss 0.22776401866122584\n",
      "2022-03-26 17:47:19.082573 Epoch 200, Training Loss 0.22830513557967017\n",
      "2022-03-26 17:47:19.118285 Epoch 200, Training Loss 0.22898847226748992\n",
      "2022-03-26 17:47:19.148377 Epoch 200, Training Loss 0.22969371472935543\n",
      "2022-03-26 17:47:19.171768 Epoch 200, Training Loss 0.23030400927871694\n",
      "2022-03-26 17:47:19.195349 Epoch 200, Training Loss 0.23080631991481537\n",
      "2022-03-26 17:47:19.218064 Epoch 200, Training Loss 0.23159424438500953\n",
      "2022-03-26 17:47:19.242724 Epoch 200, Training Loss 0.23198714760868142\n",
      "2022-03-26 17:47:19.266552 Epoch 200, Training Loss 0.2325441013364231\n",
      "2022-03-26 17:47:19.290209 Epoch 200, Training Loss 0.23326390204222305\n",
      "2022-03-26 17:47:19.313380 Epoch 200, Training Loss 0.23385856847476472\n",
      "2022-03-26 17:47:19.346011 Epoch 200, Training Loss 0.2346613724594531\n",
      "2022-03-26 17:47:19.369778 Epoch 200, Training Loss 0.2355464652295003\n",
      "2022-03-26 17:47:19.392591 Epoch 200, Training Loss 0.2363366664904158\n",
      "2022-03-26 17:47:19.422147 Epoch 200, Training Loss 0.23693786687253382\n",
      "2022-03-26 17:47:19.445130 Epoch 200, Training Loss 0.23770234118337216\n",
      "2022-03-26 17:47:19.467365 Epoch 200, Training Loss 0.2386687661680724\n",
      "2022-03-26 17:47:19.490588 Epoch 200, Training Loss 0.23952160962402363\n",
      "2022-03-26 17:47:19.515091 Epoch 200, Training Loss 0.24033516912204225\n",
      "2022-03-26 17:47:19.540067 Epoch 200, Training Loss 0.24116162761397983\n",
      "2022-03-26 17:47:19.571963 Epoch 200, Training Loss 0.24156353536926573\n",
      "2022-03-26 17:47:19.596096 Epoch 200, Training Loss 0.24231566564963603\n",
      "2022-03-26 17:47:19.618654 Epoch 200, Training Loss 0.2429372165590296\n",
      "2022-03-26 17:47:19.641765 Epoch 200, Training Loss 0.24358374326277876\n",
      "2022-03-26 17:47:19.664448 Epoch 200, Training Loss 0.24437448157526342\n",
      "2022-03-26 17:47:19.690557 Epoch 200, Training Loss 0.2450448201821588\n",
      "2022-03-26 17:47:19.713080 Epoch 200, Training Loss 0.2458192698104912\n",
      "2022-03-26 17:47:19.735592 Epoch 200, Training Loss 0.2465233222374221\n",
      "2022-03-26 17:47:19.759485 Epoch 200, Training Loss 0.247195734430457\n",
      "2022-03-26 17:47:19.785973 Epoch 200, Training Loss 0.24796103047745308\n",
      "2022-03-26 17:47:19.813856 Epoch 200, Training Loss 0.24871482381887752\n",
      "2022-03-26 17:47:19.836823 Epoch 200, Training Loss 0.2494404293463358\n",
      "2022-03-26 17:47:19.860105 Epoch 200, Training Loss 0.25020087287401604\n",
      "2022-03-26 17:47:19.883306 Epoch 200, Training Loss 0.250936601815931\n",
      "2022-03-26 17:47:19.906560 Epoch 200, Training Loss 0.25190621656377604\n",
      "2022-03-26 17:47:19.934942 Epoch 200, Training Loss 0.2526315098147258\n",
      "2022-03-26 17:47:19.958631 Epoch 200, Training Loss 0.253333712363487\n",
      "2022-03-26 17:47:19.981575 Epoch 200, Training Loss 0.25417510143783695\n",
      "2022-03-26 17:47:20.013116 Epoch 200, Training Loss 0.25465107089875605\n",
      "2022-03-26 17:47:20.036022 Epoch 200, Training Loss 0.2552213473316958\n",
      "2022-03-26 17:47:20.058515 Epoch 200, Training Loss 0.2559153187991408\n",
      "2022-03-26 17:47:20.081571 Epoch 200, Training Loss 0.25667634846456827\n",
      "2022-03-26 17:47:20.106090 Epoch 200, Training Loss 0.2574111253328031\n",
      "2022-03-26 17:47:20.134861 Epoch 200, Training Loss 0.25802435583013406\n",
      "2022-03-26 17:47:20.167462 Epoch 200, Training Loss 0.2589028398780262\n",
      "2022-03-26 17:47:20.190306 Epoch 200, Training Loss 0.2596990782815172\n",
      "2022-03-26 17:47:20.213100 Epoch 200, Training Loss 0.2608194789084632\n",
      "2022-03-26 17:47:20.242997 Epoch 200, Training Loss 0.2615397153684245\n",
      "2022-03-26 17:47:20.266370 Epoch 200, Training Loss 0.26230671022401747\n",
      "2022-03-26 17:47:20.290148 Epoch 200, Training Loss 0.2629922881074574\n",
      "2022-03-26 17:47:20.313611 Epoch 200, Training Loss 0.2635970238377066\n",
      "2022-03-26 17:47:20.338858 Epoch 200, Training Loss 0.2644526423395747\n",
      "2022-03-26 17:47:20.361354 Epoch 200, Training Loss 0.26529920131654083\n",
      "2022-03-26 17:47:20.384236 Epoch 200, Training Loss 0.2661005096972141\n",
      "2022-03-26 17:47:20.412321 Epoch 200, Training Loss 0.2667845239114883\n",
      "2022-03-26 17:47:20.435322 Epoch 200, Training Loss 0.2674770132659951\n",
      "2022-03-26 17:47:20.465933 Epoch 200, Training Loss 0.26858777204133055\n",
      "2022-03-26 17:47:20.490541 Epoch 200, Training Loss 0.2692114609053068\n",
      "2022-03-26 17:47:20.514423 Epoch 200, Training Loss 0.2697427750319776\n",
      "2022-03-26 17:47:20.543952 Epoch 200, Training Loss 0.2704909339051722\n",
      "2022-03-26 17:47:20.568734 Epoch 200, Training Loss 0.27140999636839114\n",
      "2022-03-26 17:47:20.593976 Epoch 200, Training Loss 0.27225402862672\n",
      "2022-03-26 17:47:20.618668 Epoch 200, Training Loss 0.27320413032303686\n",
      "2022-03-26 17:47:20.643683 Epoch 200, Training Loss 0.27392586909444133\n",
      "2022-03-26 17:47:20.674720 Epoch 200, Training Loss 0.2747205909332046\n",
      "2022-03-26 17:47:20.700609 Epoch 200, Training Loss 0.2755401480914382\n",
      "2022-03-26 17:47:20.724424 Epoch 200, Training Loss 0.27605823692305925\n",
      "2022-03-26 17:47:20.747732 Epoch 200, Training Loss 0.27683965709355784\n",
      "2022-03-26 17:47:20.771996 Epoch 200, Training Loss 0.27759012095915997\n",
      "2022-03-26 17:47:20.795084 Epoch 200, Training Loss 0.27813077960020444\n",
      "2022-03-26 17:47:20.818144 Epoch 200, Training Loss 0.2785996764593417\n",
      "2022-03-26 17:47:20.848345 Epoch 200, Training Loss 0.2793416624407634\n",
      "2022-03-26 17:47:20.872476 Epoch 200, Training Loss 0.28019180989173975\n",
      "2022-03-26 17:47:20.904274 Epoch 200, Training Loss 0.28124787873776674\n",
      "2022-03-26 17:47:20.928902 Epoch 200, Training Loss 0.2819171691565867\n",
      "2022-03-26 17:47:20.956614 Epoch 200, Training Loss 0.2826899741692921\n",
      "2022-03-26 17:47:20.980206 Epoch 200, Training Loss 0.28344757458590486\n",
      "2022-03-26 17:47:21.004046 Epoch 200, Training Loss 0.2844931130747661\n",
      "2022-03-26 17:47:21.027006 Epoch 200, Training Loss 0.2853148741566617\n",
      "2022-03-26 17:47:21.050020 Epoch 200, Training Loss 0.2861056930345038\n",
      "2022-03-26 17:47:21.078549 Epoch 200, Training Loss 0.2869640437462141\n",
      "2022-03-26 17:47:21.101417 Epoch 200, Training Loss 0.28784270607449514\n",
      "2022-03-26 17:47:21.132621 Epoch 200, Training Loss 0.28890644814199806\n",
      "2022-03-26 17:47:21.156838 Epoch 200, Training Loss 0.2896781497827881\n",
      "2022-03-26 17:47:21.190137 Epoch 200, Training Loss 0.290503869619211\n",
      "2022-03-26 17:47:21.213365 Epoch 200, Training Loss 0.2912554438690395\n",
      "2022-03-26 17:47:21.236221 Epoch 200, Training Loss 0.29189976299052955\n",
      "2022-03-26 17:47:21.259243 Epoch 200, Training Loss 0.29267035150314535\n",
      "2022-03-26 17:47:21.282482 Epoch 200, Training Loss 0.2935329067813771\n",
      "2022-03-26 17:47:21.305974 Epoch 200, Training Loss 0.2944680903954884\n",
      "2022-03-26 17:47:21.330623 Epoch 200, Training Loss 0.295010557564933\n",
      "2022-03-26 17:47:21.362140 Epoch 200, Training Loss 0.295565191894541\n",
      "2022-03-26 17:47:21.386390 Epoch 200, Training Loss 0.2964188926817511\n",
      "2022-03-26 17:47:21.410088 Epoch 200, Training Loss 0.29738239421868873\n",
      "2022-03-26 17:47:21.437112 Epoch 200, Training Loss 0.29831784849276627\n",
      "2022-03-26 17:47:21.460872 Epoch 200, Training Loss 0.2990237261023363\n",
      "2022-03-26 17:47:21.484207 Epoch 200, Training Loss 0.29998327170491523\n",
      "2022-03-26 17:47:21.507571 Epoch 200, Training Loss 0.30089217835984877\n",
      "2022-03-26 17:47:21.530552 Epoch 200, Training Loss 0.3018681485482189\n",
      "2022-03-26 17:47:21.555588 Epoch 200, Training Loss 0.30255969550908374\n",
      "2022-03-26 17:47:21.587451 Epoch 200, Training Loss 0.3033194130338976\n",
      "2022-03-26 17:47:21.612003 Epoch 200, Training Loss 0.3040855696896458\n",
      "2022-03-26 17:47:21.635434 Epoch 200, Training Loss 0.3047127024749356\n",
      "2022-03-26 17:47:21.662130 Epoch 200, Training Loss 0.3056777544948451\n",
      "2022-03-26 17:47:21.691055 Epoch 200, Training Loss 0.30639645358180756\n",
      "2022-03-26 17:47:21.714169 Epoch 200, Training Loss 0.3072547918695318\n",
      "2022-03-26 17:47:21.737666 Epoch 200, Training Loss 0.3081563246219664\n",
      "2022-03-26 17:47:21.761449 Epoch 200, Training Loss 0.3089215335486185\n",
      "2022-03-26 17:47:21.787963 Epoch 200, Training Loss 0.3095930310161522\n",
      "2022-03-26 17:47:21.817586 Epoch 200, Training Loss 0.31033527477622946\n",
      "2022-03-26 17:47:21.841489 Epoch 200, Training Loss 0.3110030017545461\n",
      "2022-03-26 17:47:21.864345 Epoch 200, Training Loss 0.3116242939325245\n",
      "2022-03-26 17:47:21.887224 Epoch 200, Training Loss 0.31235307192101197\n",
      "2022-03-26 17:47:21.910051 Epoch 200, Training Loss 0.3131810151936148\n",
      "2022-03-26 17:47:21.933042 Epoch 200, Training Loss 0.3138643757385366\n",
      "2022-03-26 17:47:21.956420 Epoch 200, Training Loss 0.31450750589218285\n",
      "2022-03-26 17:47:21.979628 Epoch 200, Training Loss 0.31529524628920935\n",
      "2022-03-26 17:47:22.003422 Epoch 200, Training Loss 0.3166176219806647\n",
      "2022-03-26 17:47:22.032862 Epoch 200, Training Loss 0.31721275812372224\n",
      "2022-03-26 17:47:22.067986 Epoch 200, Training Loss 0.31787662345277684\n",
      "2022-03-26 17:47:22.091047 Epoch 200, Training Loss 0.31868805402837447\n",
      "2022-03-26 17:47:22.113822 Epoch 200, Training Loss 0.31937344562824427\n",
      "2022-03-26 17:47:22.138738 Epoch 200, Training Loss 0.32025979924232456\n",
      "2022-03-26 17:47:22.161766 Epoch 200, Training Loss 0.32085911518015214\n",
      "2022-03-26 17:47:22.192975 Epoch 200, Training Loss 0.32156248329697973\n",
      "2022-03-26 17:47:22.221587 Epoch 200, Training Loss 0.32236759745708815\n",
      "2022-03-26 17:47:22.249979 Epoch 200, Training Loss 0.3230975317909285\n",
      "2022-03-26 17:47:22.276774 Epoch 200, Training Loss 0.32389313497049427\n",
      "2022-03-26 17:47:22.300281 Epoch 200, Training Loss 0.32475932933332974\n",
      "2022-03-26 17:47:22.323261 Epoch 200, Training Loss 0.3254594198806816\n",
      "2022-03-26 17:47:22.348617 Epoch 200, Training Loss 0.32630792218248555\n",
      "2022-03-26 17:47:22.372636 Epoch 200, Training Loss 0.326992524897351\n",
      "2022-03-26 17:47:22.395531 Epoch 200, Training Loss 0.3277124525107386\n",
      "2022-03-26 17:47:22.418100 Epoch 200, Training Loss 0.32862801522092744\n",
      "2022-03-26 17:47:22.442358 Epoch 200, Training Loss 0.3292514719545384\n",
      "2022-03-26 17:47:22.478005 Epoch 200, Training Loss 0.329962349799283\n",
      "2022-03-26 17:47:22.503076 Epoch 200, Training Loss 0.3307292593638306\n",
      "2022-03-26 17:47:22.526719 Epoch 200, Training Loss 0.33144091206895726\n",
      "2022-03-26 17:47:22.550316 Epoch 200, Training Loss 0.33213295423618666\n",
      "2022-03-26 17:47:22.577709 Epoch 200, Training Loss 0.3330044095854625\n",
      "2022-03-26 17:47:22.601335 Epoch 200, Training Loss 0.3336949170474201\n",
      "2022-03-26 17:47:22.625086 Epoch 200, Training Loss 0.33453878642195634\n",
      "2022-03-26 17:47:22.649131 Epoch 200, Training Loss 0.3351797451982108\n",
      "2022-03-26 17:47:22.672564 Epoch 200, Training Loss 0.3357109573795972\n",
      "2022-03-26 17:47:22.704601 Epoch 200, Training Loss 0.33628075587017764\n",
      "2022-03-26 17:47:22.728980 Epoch 200, Training Loss 0.3368594000482803\n",
      "2022-03-26 17:47:22.752470 Epoch 200, Training Loss 0.33749655849488497\n",
      "2022-03-26 17:47:22.777137 Epoch 200, Training Loss 0.33817864554312527\n",
      "2022-03-26 17:47:22.800560 Epoch 200, Training Loss 0.33911284324153307\n",
      "2022-03-26 17:47:22.824380 Epoch 200, Training Loss 0.3401806488671266\n",
      "2022-03-26 17:47:22.848636 Epoch 200, Training Loss 0.34127983161250647\n",
      "2022-03-26 17:47:22.871941 Epoch 200, Training Loss 0.3420402207185545\n",
      "2022-03-26 17:47:22.895926 Epoch 200, Training Loss 0.3430883542960867\n",
      "2022-03-26 17:47:22.928300 Epoch 200, Training Loss 0.3437474607811559\n",
      "2022-03-26 17:47:22.958275 Epoch 200, Training Loss 0.3443656186847126\n",
      "2022-03-26 17:47:22.982109 Epoch 200, Training Loss 0.3451571840688091\n",
      "2022-03-26 17:47:23.005762 Epoch 200, Training Loss 0.34575653704993253\n",
      "2022-03-26 17:47:23.029092 Epoch 200, Training Loss 0.34664653432186304\n",
      "2022-03-26 17:47:23.060016 Epoch 200, Training Loss 0.34741755660690005\n",
      "2022-03-26 17:47:23.083464 Epoch 200, Training Loss 0.3483502622074483\n",
      "2022-03-26 17:47:23.106415 Epoch 200, Training Loss 0.34916406271555234\n",
      "2022-03-26 17:47:23.129184 Epoch 200, Training Loss 0.34976822579913125\n",
      "2022-03-26 17:47:23.161763 Epoch 200, Training Loss 0.35038553837620084\n",
      "2022-03-26 17:47:23.184965 Epoch 200, Training Loss 0.3511364396728213\n",
      "2022-03-26 17:47:23.210342 Epoch 200, Training Loss 0.35178905809321975\n",
      "2022-03-26 17:47:23.240501 Epoch 200, Training Loss 0.35269535540619773\n",
      "2022-03-26 17:47:23.263663 Epoch 200, Training Loss 0.3532796207520053\n",
      "2022-03-26 17:47:23.289663 Epoch 200, Training Loss 0.35411610753487444\n",
      "2022-03-26 17:47:23.312959 Epoch 200, Training Loss 0.3548465994998927\n",
      "2022-03-26 17:47:23.339135 Epoch 200, Training Loss 0.35599419851894576\n",
      "2022-03-26 17:47:23.362450 Epoch 200, Training Loss 0.35659840562002126\n",
      "2022-03-26 17:47:23.394817 Epoch 200, Training Loss 0.35749224571468274\n",
      "2022-03-26 17:47:23.418888 Epoch 200, Training Loss 0.35827620411315536\n",
      "2022-03-26 17:47:23.442197 Epoch 200, Training Loss 0.35874051072865804\n",
      "2022-03-26 17:47:23.465569 Epoch 200, Training Loss 0.35950440408476175\n",
      "2022-03-26 17:47:23.488335 Epoch 200, Training Loss 0.3603520545432025\n",
      "2022-03-26 17:47:23.512459 Epoch 200, Training Loss 0.36136103800648006\n",
      "2022-03-26 17:47:23.537394 Epoch 200, Training Loss 0.36200364063615387\n",
      "2022-03-26 17:47:23.560864 Epoch 200, Training Loss 0.3627780417880744\n",
      "2022-03-26 17:47:23.583405 Epoch 200, Training Loss 0.3635820202586596\n",
      "2022-03-26 17:47:23.613755 Epoch 200, Training Loss 0.36445444086780937\n",
      "2022-03-26 17:47:23.638836 Epoch 200, Training Loss 0.36527118822345345\n",
      "2022-03-26 17:47:23.662708 Epoch 200, Training Loss 0.36592450993292774\n",
      "2022-03-26 17:47:23.686924 Epoch 200, Training Loss 0.36661515969906927\n",
      "2022-03-26 17:47:23.711177 Epoch 200, Training Loss 0.3675646036863327\n",
      "2022-03-26 17:47:23.736058 Epoch 200, Training Loss 0.3682382584304151\n",
      "2022-03-26 17:47:23.761320 Epoch 200, Training Loss 0.3687975909703833\n",
      "2022-03-26 17:47:23.785502 Epoch 200, Training Loss 0.36948771473696773\n",
      "2022-03-26 17:47:23.809350 Epoch 200, Training Loss 0.3701568466927999\n",
      "2022-03-26 17:47:23.840814 Epoch 200, Training Loss 0.3711161697307206\n",
      "2022-03-26 17:47:23.873758 Epoch 200, Training Loss 0.3716362539459677\n",
      "2022-03-26 17:47:23.898016 Epoch 200, Training Loss 0.37230662281251015\n",
      "2022-03-26 17:47:23.921880 Epoch 200, Training Loss 0.37306167600709766\n",
      "2022-03-26 17:47:23.945540 Epoch 200, Training Loss 0.37414999614896066\n",
      "2022-03-26 17:47:23.970008 Epoch 200, Training Loss 0.3748897512245666\n",
      "2022-03-26 17:47:23.993722 Epoch 200, Training Loss 0.37566915977641446\n",
      "2022-03-26 17:47:24.018064 Epoch 200, Training Loss 0.3761987839742085\n",
      "2022-03-26 17:47:24.043001 Epoch 200, Training Loss 0.3771676832376539\n",
      "2022-03-26 17:47:24.074453 Epoch 200, Training Loss 0.3779150213655608\n",
      "2022-03-26 17:47:24.099977 Epoch 200, Training Loss 0.37865604277309556\n",
      "2022-03-26 17:47:24.124110 Epoch 200, Training Loss 0.37930644454096285\n",
      "2022-03-26 17:47:24.154270 Epoch 200, Training Loss 0.38013074171665073\n",
      "2022-03-26 17:47:24.178260 Epoch 200, Training Loss 0.3809722518295888\n",
      "2022-03-26 17:47:24.201929 Epoch 200, Training Loss 0.3818443892785655\n",
      "2022-03-26 17:47:24.227306 Epoch 200, Training Loss 0.3823973735808716\n",
      "2022-03-26 17:47:24.269746 Epoch 200, Training Loss 0.38315629322662986\n",
      "2022-03-26 17:47:24.294506 Epoch 200, Training Loss 0.3839675088596466\n",
      "2022-03-26 17:47:24.318404 Epoch 200, Training Loss 0.38469257352449704\n",
      "2022-03-26 17:47:24.345525 Epoch 200, Training Loss 0.38549884159089354\n",
      "2022-03-26 17:47:24.369898 Epoch 200, Training Loss 0.3865357791752462\n",
      "2022-03-26 17:47:24.397956 Epoch 200, Training Loss 0.38715538782689274\n",
      "2022-03-26 17:47:24.421986 Epoch 200, Training Loss 0.3877918204230726\n",
      "2022-03-26 17:47:24.446011 Epoch 200, Training Loss 0.38838526656103256\n",
      "2022-03-26 17:47:24.469797 Epoch 200, Training Loss 0.38897151246552575\n",
      "2022-03-26 17:47:24.501189 Epoch 200, Training Loss 0.38946472138852417\n",
      "2022-03-26 17:47:24.524760 Epoch 200, Training Loss 0.39049737109705007\n",
      "2022-03-26 17:47:24.548124 Epoch 200, Training Loss 0.3916691728031544\n",
      "2022-03-26 17:47:24.570854 Epoch 200, Training Loss 0.3924334212718412\n",
      "2022-03-26 17:47:24.596630 Epoch 200, Training Loss 0.393046268759786\n",
      "2022-03-26 17:47:24.619473 Epoch 200, Training Loss 0.39371562991148373\n",
      "2022-03-26 17:47:24.642352 Epoch 200, Training Loss 0.39443996499109146\n",
      "2022-03-26 17:47:24.664914 Epoch 200, Training Loss 0.3949903114829832\n",
      "2022-03-26 17:47:24.687584 Epoch 200, Training Loss 0.39581949974569824\n",
      "2022-03-26 17:47:24.716269 Epoch 200, Training Loss 0.3965556895946298\n",
      "2022-03-26 17:47:24.742950 Epoch 200, Training Loss 0.3972330854829315\n",
      "2022-03-26 17:47:24.767197 Epoch 200, Training Loss 0.3978319750036425\n",
      "2022-03-26 17:47:24.790223 Epoch 200, Training Loss 0.398682627569684\n",
      "2022-03-26 17:47:24.813093 Epoch 200, Training Loss 0.39933000428749776\n",
      "2022-03-26 17:47:24.835439 Epoch 200, Training Loss 0.40056979675274673\n",
      "2022-03-26 17:47:24.858261 Epoch 200, Training Loss 0.4017373042567002\n",
      "2022-03-26 17:47:24.881095 Epoch 200, Training Loss 0.40249015589046966\n",
      "2022-03-26 17:47:24.904258 Epoch 200, Training Loss 0.40305373164089137\n",
      "2022-03-26 17:47:24.932793 Epoch 200, Training Loss 0.40388510850689296\n",
      "2022-03-26 17:47:24.955676 Epoch 200, Training Loss 0.40485068980385275\n",
      "2022-03-26 17:47:24.978027 Epoch 200, Training Loss 0.40562625103594396\n",
      "2022-03-26 17:47:25.000485 Epoch 200, Training Loss 0.4063889113685969\n",
      "2022-03-26 17:47:25.026968 Epoch 200, Training Loss 0.4071348022164591\n",
      "2022-03-26 17:47:25.050035 Epoch 200, Training Loss 0.4076506509195508\n",
      "2022-03-26 17:47:25.080408 Epoch 200, Training Loss 0.4084683565227577\n",
      "2022-03-26 17:47:25.103411 Epoch 200, Training Loss 0.40913242261732935\n",
      "2022-03-26 17:47:25.127020 Epoch 200, Training Loss 0.4097903847237072\n",
      "2022-03-26 17:47:25.158930 Epoch 200, Training Loss 0.41045048840515447\n",
      "2022-03-26 17:47:25.194115 Epoch 200, Training Loss 0.4114165348774942\n",
      "2022-03-26 17:47:25.219792 Epoch 200, Training Loss 0.4122409325883821\n",
      "2022-03-26 17:47:25.242757 Epoch 200, Training Loss 0.4128504921027157\n",
      "2022-03-26 17:47:25.273008 Epoch 200, Training Loss 0.4136605075634349\n",
      "2022-03-26 17:47:25.300350 Epoch 200, Training Loss 0.4144354644410141\n",
      "2022-03-26 17:47:25.323803 Epoch 200, Training Loss 0.41511473368348367\n",
      "2022-03-26 17:47:25.348839 Epoch 200, Training Loss 0.4161372500688524\n",
      "2022-03-26 17:47:25.380007 Epoch 200, Training Loss 0.41691284933510947\n",
      "2022-03-26 17:47:25.404347 Epoch 200, Training Loss 0.4176460559029713\n",
      "2022-03-26 17:47:25.427327 Epoch 200, Training Loss 0.4185470785097698\n",
      "2022-03-26 17:47:25.451028 Epoch 200, Training Loss 0.41935960049062126\n",
      "2022-03-26 17:47:25.474117 Epoch 200, Training Loss 0.4202587902927033\n",
      "2022-03-26 17:47:25.497261 Epoch 200, Training Loss 0.4208148079531272\n",
      "2022-03-26 17:47:25.520328 Epoch 200, Training Loss 0.4217245598964374\n",
      "2022-03-26 17:47:25.543305 Epoch 200, Training Loss 0.42240582825735096\n",
      "2022-03-26 17:47:25.565815 Epoch 200, Training Loss 0.4229372836211148\n",
      "2022-03-26 17:47:25.594606 Epoch 200, Training Loss 0.4236488210042114\n",
      "2022-03-26 17:47:25.618307 Epoch 200, Training Loss 0.4245051838781523\n",
      "2022-03-26 17:47:25.641237 Epoch 200, Training Loss 0.4252540718030442\n",
      "2022-03-26 17:47:25.668554 Epoch 200, Training Loss 0.4259076516722779\n",
      "2022-03-26 17:47:25.691440 Epoch 200, Training Loss 0.4267897337979978\n",
      "2022-03-26 17:47:25.713964 Epoch 200, Training Loss 0.4274182632908492\n",
      "2022-03-26 17:47:25.736721 Epoch 200, Training Loss 0.42815539614318887\n",
      "2022-03-26 17:47:25.759283 Epoch 200, Training Loss 0.42897474171255556\n",
      "2022-03-26 17:47:25.783743 Epoch 200, Training Loss 0.4298235523273878\n",
      "2022-03-26 17:47:25.812379 Epoch 200, Training Loss 0.4308178395871311\n",
      "2022-03-26 17:47:25.836336 Epoch 200, Training Loss 0.43130994856814897\n",
      "2022-03-26 17:47:25.858656 Epoch 200, Training Loss 0.4318229348000968\n",
      "2022-03-26 17:47:25.881386 Epoch 200, Training Loss 0.43274972742170936\n",
      "2022-03-26 17:47:25.908533 Epoch 200, Training Loss 0.4335156079295956\n",
      "2022-03-26 17:47:25.930782 Epoch 200, Training Loss 0.43420669886157337\n",
      "2022-03-26 17:47:25.953010 Epoch 200, Training Loss 0.43519785436218045\n",
      "2022-03-26 17:47:25.981937 Epoch 200, Training Loss 0.4362000413715382\n",
      "2022-03-26 17:47:26.004514 Epoch 200, Training Loss 0.43706287706599517\n",
      "2022-03-26 17:47:26.032822 Epoch 200, Training Loss 0.4380241485355455\n",
      "2022-03-26 17:47:26.058396 Epoch 200, Training Loss 0.4387333500568214\n",
      "2022-03-26 17:47:26.083648 Epoch 200, Training Loss 0.4394500323421205\n",
      "2022-03-26 17:47:26.106411 Epoch 200, Training Loss 0.4401171752406508\n",
      "2022-03-26 17:47:26.128974 Epoch 200, Training Loss 0.4408141951579267\n",
      "2022-03-26 17:47:26.151633 Epoch 200, Training Loss 0.4418652304603011\n",
      "2022-03-26 17:47:26.179634 Epoch 200, Training Loss 0.4425918494953829\n",
      "2022-03-26 17:47:26.202414 Epoch 200, Training Loss 0.4433947483749341\n",
      "2022-03-26 17:47:26.225187 Epoch 200, Training Loss 0.44434267976095\n",
      "2022-03-26 17:47:26.257555 Epoch 200, Training Loss 0.4450176333070106\n",
      "2022-03-26 17:47:26.282123 Epoch 200, Training Loss 0.44577662490517894\n",
      "2022-03-26 17:47:26.316322 Epoch 200, Training Loss 0.4464335417198708\n",
      "2022-03-26 17:47:26.344734 Epoch 200, Training Loss 0.44717185142095134\n",
      "2022-03-26 17:47:26.368306 Epoch 200, Training Loss 0.44788022282178447\n",
      "2022-03-26 17:47:26.391901 Epoch 200, Training Loss 0.4485781743093525\n",
      "2022-03-26 17:47:26.415589 Epoch 200, Training Loss 0.44943091570568816\n",
      "2022-03-26 17:47:26.439060 Epoch 200, Training Loss 0.45022324558414156\n",
      "2022-03-26 17:47:26.468048 Epoch 200, Training Loss 0.4509372418493871\n",
      "2022-03-26 17:47:26.493862 Epoch 200, Training Loss 0.4519725829896415\n",
      "2022-03-26 17:47:26.517617 Epoch 200, Training Loss 0.4526469651085641\n",
      "2022-03-26 17:47:26.541007 Epoch 200, Training Loss 0.4535129446050395\n",
      "2022-03-26 17:47:26.564124 Epoch 200, Training Loss 0.4542499565712326\n",
      "2022-03-26 17:47:26.587105 Epoch 200, Training Loss 0.45511726977880046\n",
      "2022-03-26 17:47:26.611785 Epoch 200, Training Loss 0.4558825363283572\n",
      "2022-03-26 17:47:26.636600 Epoch 200, Training Loss 0.4565219671829887\n",
      "2022-03-26 17:47:26.659490 Epoch 200, Training Loss 0.4572607079887634\n",
      "2022-03-26 17:47:26.689431 Epoch 200, Training Loss 0.45808537887490314\n",
      "2022-03-26 17:47:26.720743 Epoch 200, Training Loss 0.45933566762663214\n",
      "2022-03-26 17:47:26.743976 Epoch 200, Training Loss 0.46032885906031673\n",
      "2022-03-26 17:47:26.768305 Epoch 200, Training Loss 0.4611171910829861\n",
      "2022-03-26 17:47:26.793517 Epoch 200, Training Loss 0.4616634633839893\n",
      "2022-03-26 17:47:26.815995 Epoch 200, Training Loss 0.46233910337433487\n",
      "2022-03-26 17:47:26.841353 Epoch 200, Training Loss 0.46304698223653046\n",
      "2022-03-26 17:47:26.864793 Epoch 200, Training Loss 0.46392517534973066\n",
      "2022-03-26 17:47:26.899496 Epoch 200, Training Loss 0.46490683206511885\n",
      "2022-03-26 17:47:26.929918 Epoch 200, Training Loss 0.46557473054017556\n",
      "2022-03-26 17:47:26.958693 Epoch 200, Training Loss 0.4663215916784828\n",
      "2022-03-26 17:47:26.981933 Epoch 200, Training Loss 0.4672435355155974\n",
      "2022-03-26 17:47:27.004557 Epoch 200, Training Loss 0.46779843200655546\n",
      "2022-03-26 17:47:27.027984 Epoch 200, Training Loss 0.4683635343447366\n",
      "2022-03-26 17:47:27.052594 Epoch 200, Training Loss 0.4689442003931841\n",
      "2022-03-26 17:47:27.075515 Epoch 200, Training Loss 0.46953628473269665\n",
      "2022-03-26 17:47:27.098380 Epoch 200, Training Loss 0.470252230039338\n",
      "2022-03-26 17:47:27.135766 Epoch 200, Training Loss 0.4711002814190467\n",
      "2022-03-26 17:47:27.159172 Epoch 200, Training Loss 0.47158934423685683\n",
      "2022-03-26 17:47:27.182319 Epoch 200, Training Loss 0.4723384177593319\n",
      "2022-03-26 17:47:27.204879 Epoch 200, Training Loss 0.47323354270757007\n",
      "2022-03-26 17:47:27.227589 Epoch 200, Training Loss 0.4737562259749683\n",
      "2022-03-26 17:47:27.253879 Epoch 200, Training Loss 0.4745069694183672\n",
      "2022-03-26 17:47:27.276016 Epoch 200, Training Loss 0.4752176368175565\n",
      "2022-03-26 17:47:27.298097 Epoch 200, Training Loss 0.4759541339124255\n",
      "2022-03-26 17:47:27.327139 Epoch 200, Training Loss 0.47663518794052434\n",
      "2022-03-26 17:47:27.362014 Epoch 200, Training Loss 0.47747922965022915\n",
      "2022-03-26 17:47:27.388478 Epoch 200, Training Loss 0.47823852704614017\n",
      "2022-03-26 17:47:27.411150 Epoch 200, Training Loss 0.4791793189847561\n",
      "2022-03-26 17:47:27.434273 Epoch 200, Training Loss 0.47970063325084383\n",
      "2022-03-26 17:47:27.459998 Epoch 200, Training Loss 0.48049562757887193\n",
      "2022-03-26 17:47:27.483239 Epoch 200, Training Loss 0.48124074013641727\n",
      "2022-03-26 17:47:27.506191 Epoch 200, Training Loss 0.48229372181246044\n",
      "2022-03-26 17:47:27.530771 Epoch 200, Training Loss 0.4830176121438556\n",
      "2022-03-26 17:47:27.554622 Epoch 200, Training Loss 0.4839591608023095\n",
      "2022-03-26 17:47:27.587829 Epoch 200, Training Loss 0.48478105176440284\n",
      "2022-03-26 17:47:27.612059 Epoch 200, Training Loss 0.4854739591898516\n",
      "2022-03-26 17:47:27.636250 Epoch 200, Training Loss 0.48622705328190113\n",
      "2022-03-26 17:47:27.664767 Epoch 200, Training Loss 0.4872320829450017\n",
      "2022-03-26 17:47:27.688102 Epoch 200, Training Loss 0.4877940183100493\n",
      "2022-03-26 17:47:27.711735 Epoch 200, Training Loss 0.4884994177104872\n",
      "2022-03-26 17:47:27.734570 Epoch 200, Training Loss 0.4891358174174033\n",
      "2022-03-26 17:47:27.758196 Epoch 200, Training Loss 0.48995056634059037\n",
      "2022-03-26 17:47:27.783116 Epoch 200, Training Loss 0.49048980129191944\n",
      "2022-03-26 17:47:27.815446 Epoch 200, Training Loss 0.4911491547322944\n",
      "2022-03-26 17:47:27.839892 Epoch 200, Training Loss 0.49220664608661474\n",
      "2022-03-26 17:47:27.863532 Epoch 200, Training Loss 0.4929516243431574\n",
      "2022-03-26 17:47:27.887664 Epoch 200, Training Loss 0.4938175426343518\n",
      "2022-03-26 17:47:27.911208 Epoch 200, Training Loss 0.49441870688782324\n",
      "2022-03-26 17:47:27.934867 Epoch 200, Training Loss 0.49531846659262774\n",
      "2022-03-26 17:47:27.962054 Epoch 200, Training Loss 0.49588075588883646\n",
      "2022-03-26 17:47:27.985222 Epoch 200, Training Loss 0.49685390400307256\n",
      "2022-03-26 17:47:28.008881 Epoch 200, Training Loss 0.49772865422393964\n",
      "2022-03-26 17:47:28.041576 Epoch 200, Training Loss 0.4984461170861788\n",
      "2022-03-26 17:47:28.069674 Epoch 200, Training Loss 0.499380833924274\n",
      "2022-03-26 17:47:28.098042 Epoch 200, Training Loss 0.499888266215239\n",
      "2022-03-26 17:47:28.123982 Epoch 200, Training Loss 0.5005971995156134\n",
      "2022-03-26 17:47:28.148235 Epoch 200, Training Loss 0.5014276650098278\n",
      "2022-03-26 17:47:28.173525 Epoch 200, Training Loss 0.5020737721182197\n",
      "2022-03-26 17:47:28.201912 Epoch 200, Training Loss 0.5027783340810205\n",
      "2022-03-26 17:47:28.226017 Epoch 200, Training Loss 0.5036513111780366\n",
      "2022-03-26 17:47:28.257113 Epoch 200, Training Loss 0.5044212773480379\n",
      "2022-03-26 17:47:28.284524 Epoch 200, Training Loss 0.5053142858740619\n",
      "2022-03-26 17:47:28.307720 Epoch 200, Training Loss 0.506240230768233\n",
      "2022-03-26 17:47:28.332619 Epoch 200, Training Loss 0.5070015489292876\n",
      "2022-03-26 17:47:28.369459 Epoch 200, Training Loss 0.5077187641502341\n",
      "2022-03-26 17:47:28.392792 Epoch 200, Training Loss 0.5085645808130884\n",
      "2022-03-26 17:47:28.416089 Epoch 200, Training Loss 0.5091057464366069\n",
      "2022-03-26 17:47:28.439479 Epoch 200, Training Loss 0.5100359838560719\n",
      "2022-03-26 17:47:28.468895 Epoch 200, Training Loss 0.5108579476852246\n",
      "2022-03-26 17:47:28.496437 Epoch 200, Training Loss 0.5116674757140982\n",
      "2022-03-26 17:47:28.520273 Epoch 200, Training Loss 0.5122140015635039\n",
      "2022-03-26 17:47:28.543417 Epoch 200, Training Loss 0.5130726834544745\n",
      "2022-03-26 17:47:28.566501 Epoch 200, Training Loss 0.5140120030364113\n",
      "2022-03-26 17:47:28.589628 Epoch 200, Training Loss 0.5149022142600526\n",
      "2022-03-26 17:47:28.611937 Epoch 200, Training Loss 0.5156874862473334\n",
      "2022-03-26 17:47:28.635326 Epoch 200, Training Loss 0.5161999834849097\n",
      "2022-03-26 17:47:28.661545 Epoch 200, Training Loss 0.5166838922159142\n",
      "2022-03-26 17:47:28.693267 Epoch 200, Training Loss 0.5176342152573569\n",
      "2022-03-26 17:47:28.716552 Epoch 200, Training Loss 0.5182390067049915\n",
      "2022-03-26 17:47:28.739641 Epoch 200, Training Loss 0.518806064258451\n",
      "2022-03-26 17:47:28.762556 Epoch 200, Training Loss 0.519710200026517\n",
      "2022-03-26 17:47:28.786333 Epoch 200, Training Loss 0.5202734129660574\n",
      "2022-03-26 17:47:28.808695 Epoch 200, Training Loss 0.5208904729855944\n",
      "2022-03-26 17:47:28.831777 Epoch 200, Training Loss 0.521487556607522\n",
      "2022-03-26 17:47:28.855326 Epoch 200, Training Loss 0.522103481547302\n",
      "2022-03-26 17:47:28.878143 Epoch 200, Training Loss 0.5228039621163512\n",
      "2022-03-26 17:47:28.906634 Epoch 200, Training Loss 0.5236792820874993\n",
      "2022-03-26 17:47:28.939483 Epoch 200, Training Loss 0.5242954709035966\n",
      "2022-03-26 17:47:28.963071 Epoch 200, Training Loss 0.5248865128478126\n",
      "2022-03-26 17:47:28.992836 Epoch 200, Training Loss 0.5256238073644126\n",
      "2022-03-26 17:47:29.016706 Epoch 200, Training Loss 0.52652279960225\n",
      "2022-03-26 17:47:29.040980 Epoch 200, Training Loss 0.5276472800985321\n",
      "2022-03-26 17:47:29.064328 Epoch 200, Training Loss 0.5282395776275479\n",
      "2022-03-26 17:47:29.090175 Epoch 200, Training Loss 0.5285760024776849\n",
      "2022-03-26 17:47:29.112909 Epoch 200, Training Loss 0.5296801952907192\n",
      "2022-03-26 17:47:29.144093 Epoch 200, Training Loss 0.5302334362283692\n",
      "2022-03-26 17:47:29.166885 Epoch 200, Training Loss 0.5311954889608466\n",
      "2022-03-26 17:47:29.189979 Epoch 200, Training Loss 0.5320680605633484\n",
      "2022-03-26 17:47:29.212391 Epoch 200, Training Loss 0.5327162590935407\n",
      "2022-03-26 17:47:29.235197 Epoch 200, Training Loss 0.5333887311961035\n",
      "2022-03-26 17:47:29.258532 Epoch 200, Training Loss 0.5347052936816155\n",
      "2022-03-26 17:47:29.284358 Epoch 200, Training Loss 0.5352592445014382\n",
      "2022-03-26 17:47:29.307354 Epoch 200, Training Loss 0.5359265863361871\n",
      "2022-03-26 17:47:29.335231 Epoch 200, Training Loss 0.5365047779534479\n",
      "2022-03-26 17:47:29.361881 Epoch 200, Training Loss 0.5372153597567088\n",
      "2022-03-26 17:47:29.396002 Epoch 200, Training Loss 0.5377586775880945\n",
      "2022-03-26 17:47:29.422868 Epoch 200, Training Loss 0.5386126009399629\n",
      "2022-03-26 17:47:29.446897 Epoch 200, Training Loss 0.5395848410361258\n",
      "2022-03-26 17:47:29.470402 Epoch 200, Training Loss 0.5402690721747211\n",
      "2022-03-26 17:47:29.494077 Epoch 200, Training Loss 0.5411029673750748\n",
      "2022-03-26 17:47:29.518078 Epoch 200, Training Loss 0.5420138178121708\n",
      "2022-03-26 17:47:29.545700 Epoch 200, Training Loss 0.542713920707288\n",
      "2022-03-26 17:47:29.575237 Epoch 200, Training Loss 0.5433857357105636\n",
      "2022-03-26 17:47:29.599220 Epoch 200, Training Loss 0.5444981541932391\n",
      "2022-03-26 17:47:29.623757 Epoch 200, Training Loss 0.5452152666685831\n",
      "2022-03-26 17:47:29.647637 Epoch 200, Training Loss 0.5462089989648755\n",
      "2022-03-26 17:47:29.670825 Epoch 200, Training Loss 0.5471351934058587\n",
      "2022-03-26 17:47:29.694016 Epoch 200, Training Loss 0.547939022178845\n",
      "2022-03-26 17:47:29.717384 Epoch 200, Training Loss 0.5486872087964012\n",
      "2022-03-26 17:47:29.741087 Epoch 200, Training Loss 0.5496204310975721\n",
      "2022-03-26 17:47:29.770774 Epoch 200, Training Loss 0.5504132491700789\n",
      "2022-03-26 17:47:29.796397 Epoch 200, Training Loss 0.5510619970996057\n",
      "2022-03-26 17:47:29.820139 Epoch 200, Training Loss 0.5515646991293753\n",
      "2022-03-26 17:47:29.844019 Epoch 200, Training Loss 0.5522030942961383\n",
      "2022-03-26 17:47:29.875741 Epoch 200, Training Loss 0.5528037340363579\n",
      "2022-03-26 17:47:29.906300 Epoch 200, Training Loss 0.5535342753162165\n",
      "2022-03-26 17:47:29.930066 Epoch 200, Training Loss 0.5542597146061681\n",
      "2022-03-26 17:47:29.954223 Epoch 200, Training Loss 0.5549593596430995\n",
      "2022-03-26 17:47:29.981424 Epoch 200, Training Loss 0.5558892508678119\n",
      "2022-03-26 17:47:30.012251 Epoch 200, Training Loss 0.5565209404358169\n",
      "2022-03-26 17:47:30.035709 Epoch 200, Training Loss 0.5572565146495619\n",
      "2022-03-26 17:47:30.059591 Epoch 200, Training Loss 0.5581576954906858\n",
      "2022-03-26 17:47:30.083613 Epoch 200, Training Loss 0.5588110041664079\n",
      "2022-03-26 17:47:30.109795 Epoch 200, Training Loss 0.5595677470993203\n",
      "2022-03-26 17:47:30.134874 Epoch 200, Training Loss 0.5601982101226402\n",
      "2022-03-26 17:47:30.160692 Epoch 200, Training Loss 0.5608528363506508\n",
      "2022-03-26 17:47:30.184136 Epoch 200, Training Loss 0.5614563445453449\n",
      "2022-03-26 17:47:30.218755 Epoch 200, Training Loss 0.5620844546333909\n",
      "2022-03-26 17:47:30.242389 Epoch 200, Training Loss 0.5627434858885567\n",
      "2022-03-26 17:47:30.266526 Epoch 200, Training Loss 0.5634314540554496\n",
      "2022-03-26 17:47:30.290124 Epoch 200, Training Loss 0.5645950514337291\n",
      "2022-03-26 17:47:30.314738 Epoch 200, Training Loss 0.5651944522433878\n",
      "2022-03-26 17:47:30.342548 Epoch 200, Training Loss 0.565966243977132\n",
      "2022-03-26 17:47:30.365586 Epoch 200, Training Loss 0.5666452522396737\n",
      "2022-03-26 17:47:30.393698 Epoch 200, Training Loss 0.5676632889015291\n",
      "2022-03-26 17:47:30.430477 Epoch 200, Training Loss 0.5684529667543938\n",
      "2022-03-26 17:47:30.455202 Epoch 200, Training Loss 0.5690823562462312\n",
      "2022-03-26 17:47:30.479400 Epoch 200, Training Loss 0.569853365116412\n",
      "2022-03-26 17:47:30.503348 Epoch 200, Training Loss 0.5705327242231735\n",
      "2022-03-26 17:47:30.530816 Epoch 200, Training Loss 0.5713820076354629\n",
      "2022-03-26 17:47:30.554271 Epoch 200, Training Loss 0.5719137971510972\n",
      "2022-03-26 17:47:30.577537 Epoch 200, Training Loss 0.572625445816523\n",
      "2022-03-26 17:47:30.600452 Epoch 200, Training Loss 0.5734621738381398\n",
      "2022-03-26 17:47:30.623728 Epoch 200, Training Loss 0.5742857793103093\n",
      "2022-03-26 17:47:30.657831 Epoch 200, Training Loss 0.5749174014991506\n",
      "2022-03-26 17:47:30.681584 Epoch 200, Training Loss 0.5758239401270971\n",
      "2022-03-26 17:47:30.704218 Epoch 200, Training Loss 0.5765156272579642\n",
      "2022-03-26 17:47:30.727368 Epoch 200, Training Loss 0.5773034588912563\n",
      "2022-03-26 17:47:30.750061 Epoch 200, Training Loss 0.5782793447032304\n",
      "2022-03-26 17:47:30.774321 Epoch 200, Training Loss 0.578980403635508\n",
      "2022-03-26 17:47:30.797885 Epoch 200, Training Loss 0.5796624345090383\n",
      "2022-03-26 17:47:30.821480 Epoch 200, Training Loss 0.5803471947722423\n",
      "2022-03-26 17:47:30.849839 Epoch 200, Training Loss 0.5809970811352401\n",
      "2022-03-26 17:47:30.880757 Epoch 200, Training Loss 0.581856210899475\n",
      "2022-03-26 17:47:30.904625 Epoch 200, Training Loss 0.5826070671496184\n",
      "2022-03-26 17:47:30.928093 Epoch 200, Training Loss 0.5832181422378097\n",
      "2022-03-26 17:47:30.954380 Epoch 200, Training Loss 0.5840506284209468\n",
      "2022-03-26 17:47:30.977681 Epoch 200, Training Loss 0.5846114240567702\n",
      "2022-03-26 17:47:31.001648 Epoch 200, Training Loss 0.5855018552535635\n",
      "2022-03-26 17:47:31.025128 Epoch 200, Training Loss 0.5861341967378431\n",
      "2022-03-26 17:47:31.049217 Epoch 200, Training Loss 0.5866068330262323\n",
      "2022-03-26 17:47:31.072794 Epoch 200, Training Loss 0.5874735514831055\n",
      "2022-03-26 17:47:31.110972 Epoch 200, Training Loss 0.5883616516199868\n",
      "2022-03-26 17:47:31.122093 Epoch 200, Training Loss 0.5892011260285097\n",
      "2022-03-26 18:02:10.960410 Epoch 250, Training Loss 0.00042163719758963036\n",
      "2022-03-26 18:02:10.985145 Epoch 250, Training Loss 0.0014227145849286442\n",
      "2022-03-26 18:02:11.008978 Epoch 250, Training Loss 0.0022388823578119886\n",
      "2022-03-26 18:02:11.033382 Epoch 250, Training Loss 0.003069854339065454\n",
      "2022-03-26 18:02:11.057584 Epoch 250, Training Loss 0.003836603382664263\n",
      "2022-03-26 18:02:11.082854 Epoch 250, Training Loss 0.00438488916972714\n",
      "2022-03-26 18:02:11.105147 Epoch 250, Training Loss 0.0049474014498083795\n",
      "2022-03-26 18:02:11.128118 Epoch 250, Training Loss 0.0056933502254583645\n",
      "2022-03-26 18:02:11.150747 Epoch 250, Training Loss 0.006594913923526968\n",
      "2022-03-26 18:02:11.180791 Epoch 250, Training Loss 0.006989911998934149\n",
      "2022-03-26 18:02:11.205610 Epoch 250, Training Loss 0.007637596953555446\n",
      "2022-03-26 18:02:11.228941 Epoch 250, Training Loss 0.00837626755999787\n",
      "2022-03-26 18:02:11.252635 Epoch 250, Training Loss 0.009241283625897849\n",
      "2022-03-26 18:02:11.275390 Epoch 250, Training Loss 0.010031117807568796\n",
      "2022-03-26 18:02:11.301692 Epoch 250, Training Loss 0.01065241394902739\n",
      "2022-03-26 18:02:11.324381 Epoch 250, Training Loss 0.011271537531672232\n",
      "2022-03-26 18:02:11.351318 Epoch 250, Training Loss 0.011903616587829102\n",
      "2022-03-26 18:02:11.375580 Epoch 250, Training Loss 0.012820290368231362\n",
      "2022-03-26 18:02:11.408440 Epoch 250, Training Loss 0.013361726804157656\n",
      "2022-03-26 18:02:11.433333 Epoch 250, Training Loss 0.013986249561504939\n",
      "2022-03-26 18:02:11.457668 Epoch 250, Training Loss 0.01465574005985504\n",
      "2022-03-26 18:02:11.480786 Epoch 250, Training Loss 0.015138861582712139\n",
      "2022-03-26 18:02:11.506279 Epoch 250, Training Loss 0.0156284856140766\n",
      "2022-03-26 18:02:11.529449 Epoch 250, Training Loss 0.016251367414393997\n",
      "2022-03-26 18:02:11.564044 Epoch 250, Training Loss 0.016984031366570222\n",
      "2022-03-26 18:02:11.587381 Epoch 250, Training Loss 0.017741400986681203\n",
      "2022-03-26 18:02:11.610287 Epoch 250, Training Loss 0.01849673063401371\n",
      "2022-03-26 18:02:11.641514 Epoch 250, Training Loss 0.01911608531804341\n",
      "2022-03-26 18:02:11.664516 Epoch 250, Training Loss 0.019814913115842873\n",
      "2022-03-26 18:02:11.687439 Epoch 250, Training Loss 0.020360187229597964\n",
      "2022-03-26 18:02:11.710392 Epoch 250, Training Loss 0.021058562580886703\n",
      "2022-03-26 18:02:11.734160 Epoch 250, Training Loss 0.021655219900028784\n",
      "2022-03-26 18:02:11.756324 Epoch 250, Training Loss 0.02244951276827956\n",
      "2022-03-26 18:02:11.779780 Epoch 250, Training Loss 0.023220178568759537\n",
      "2022-03-26 18:02:11.803458 Epoch 250, Training Loss 0.024021283211305623\n",
      "2022-03-26 18:02:11.832342 Epoch 250, Training Loss 0.024671656091499815\n",
      "2022-03-26 18:02:11.860537 Epoch 250, Training Loss 0.025657216301354605\n",
      "2022-03-26 18:02:11.890379 Epoch 250, Training Loss 0.026378000636234918\n",
      "2022-03-26 18:02:11.912795 Epoch 250, Training Loss 0.027048311422548026\n",
      "2022-03-26 18:02:11.935887 Epoch 250, Training Loss 0.027979156321576795\n",
      "2022-03-26 18:02:11.958143 Epoch 250, Training Loss 0.028843900584198934\n",
      "2022-03-26 18:02:11.980911 Epoch 250, Training Loss 0.029978360742559215\n",
      "2022-03-26 18:02:12.007740 Epoch 250, Training Loss 0.03071593742846223\n",
      "2022-03-26 18:02:12.030615 Epoch 250, Training Loss 0.03137150795563408\n",
      "2022-03-26 18:02:12.062088 Epoch 250, Training Loss 0.03203548113708301\n",
      "2022-03-26 18:02:12.085614 Epoch 250, Training Loss 0.03267881647705117\n",
      "2022-03-26 18:02:12.108180 Epoch 250, Training Loss 0.03338638832197165\n",
      "2022-03-26 18:02:12.137893 Epoch 250, Training Loss 0.03401898141102413\n",
      "2022-03-26 18:02:12.160676 Epoch 250, Training Loss 0.03463469670556695\n",
      "2022-03-26 18:02:12.182898 Epoch 250, Training Loss 0.03541058050397107\n",
      "2022-03-26 18:02:12.212678 Epoch 250, Training Loss 0.03593790622623375\n",
      "2022-03-26 18:02:12.236588 Epoch 250, Training Loss 0.03673762029699047\n",
      "2022-03-26 18:02:12.260113 Epoch 250, Training Loss 0.03733385843998941\n",
      "2022-03-26 18:02:12.294752 Epoch 250, Training Loss 0.03798257740562224\n",
      "2022-03-26 18:02:12.318583 Epoch 250, Training Loss 0.03844133648268707\n",
      "2022-03-26 18:02:12.345267 Epoch 250, Training Loss 0.03906645582002752\n",
      "2022-03-26 18:02:12.372495 Epoch 250, Training Loss 0.03967701138742744\n",
      "2022-03-26 18:02:12.395286 Epoch 250, Training Loss 0.040495807862342775\n",
      "2022-03-26 18:02:12.418783 Epoch 250, Training Loss 0.041181926151065876\n",
      "2022-03-26 18:02:12.443357 Epoch 250, Training Loss 0.04176816698687766\n",
      "2022-03-26 18:02:12.465970 Epoch 250, Training Loss 0.04269637350383622\n",
      "2022-03-26 18:02:12.489803 Epoch 250, Training Loss 0.04324294546680987\n",
      "2022-03-26 18:02:12.522267 Epoch 250, Training Loss 0.04400424045674941\n",
      "2022-03-26 18:02:12.545845 Epoch 250, Training Loss 0.04465361263441003\n",
      "2022-03-26 18:02:12.576559 Epoch 250, Training Loss 0.04522180641093827\n",
      "2022-03-26 18:02:12.601427 Epoch 250, Training Loss 0.04584064214583248\n",
      "2022-03-26 18:02:12.626401 Epoch 250, Training Loss 0.04671275322242161\n",
      "2022-03-26 18:02:12.649592 Epoch 250, Training Loss 0.047243482735760685\n",
      "2022-03-26 18:02:12.672555 Epoch 250, Training Loss 0.048103852764419884\n",
      "2022-03-26 18:02:12.695754 Epoch 250, Training Loss 0.048565480067297016\n",
      "2022-03-26 18:02:12.718583 Epoch 250, Training Loss 0.049279569550548366\n",
      "2022-03-26 18:02:12.750418 Epoch 250, Training Loss 0.04980723563667454\n",
      "2022-03-26 18:02:12.774014 Epoch 250, Training Loss 0.05037829821067088\n",
      "2022-03-26 18:02:12.797740 Epoch 250, Training Loss 0.05103313427447053\n",
      "2022-03-26 18:02:12.820667 Epoch 250, Training Loss 0.05153060412925223\n",
      "2022-03-26 18:02:12.844200 Epoch 250, Training Loss 0.052166748610908725\n",
      "2022-03-26 18:02:12.866929 Epoch 250, Training Loss 0.05291712512750455\n",
      "2022-03-26 18:02:12.889895 Epoch 250, Training Loss 0.053780439488418263\n",
      "2022-03-26 18:02:12.912946 Epoch 250, Training Loss 0.05446212538672835\n",
      "2022-03-26 18:02:12.935238 Epoch 250, Training Loss 0.055109802643051535\n",
      "2022-03-26 18:02:12.961982 Epoch 250, Training Loss 0.05570130088292729\n",
      "2022-03-26 18:02:12.991712 Epoch 250, Training Loss 0.05654415850291777\n",
      "2022-03-26 18:02:13.014974 Epoch 250, Training Loss 0.057274228281072335\n",
      "2022-03-26 18:02:13.041470 Epoch 250, Training Loss 0.05800060089439382\n",
      "2022-03-26 18:02:13.064133 Epoch 250, Training Loss 0.058674991702484657\n",
      "2022-03-26 18:02:13.091181 Epoch 250, Training Loss 0.05931063038308907\n",
      "2022-03-26 18:02:13.113785 Epoch 250, Training Loss 0.05985171025823754\n",
      "2022-03-26 18:02:13.136367 Epoch 250, Training Loss 0.06052651464024468\n",
      "2022-03-26 18:02:13.164080 Epoch 250, Training Loss 0.061281766008842935\n",
      "2022-03-26 18:02:13.195183 Epoch 250, Training Loss 0.06189811340225932\n",
      "2022-03-26 18:02:13.218735 Epoch 250, Training Loss 0.06274274483208767\n",
      "2022-03-26 18:02:13.242178 Epoch 250, Training Loss 0.06340156411728286\n",
      "2022-03-26 18:02:13.266645 Epoch 250, Training Loss 0.0641496860234024\n",
      "2022-03-26 18:02:13.292265 Epoch 250, Training Loss 0.06491652325443599\n",
      "2022-03-26 18:02:13.315448 Epoch 250, Training Loss 0.06572981094917678\n",
      "2022-03-26 18:02:13.341177 Epoch 250, Training Loss 0.06645549086810988\n",
      "2022-03-26 18:02:13.363733 Epoch 250, Training Loss 0.06708385747716859\n",
      "2022-03-26 18:02:13.387300 Epoch 250, Training Loss 0.06776980183008686\n",
      "2022-03-26 18:02:13.419340 Epoch 250, Training Loss 0.06835253677709634\n",
      "2022-03-26 18:02:13.444341 Epoch 250, Training Loss 0.0691425237051971\n",
      "2022-03-26 18:02:13.467483 Epoch 250, Training Loss 0.06996342265392508\n",
      "2022-03-26 18:02:13.490648 Epoch 250, Training Loss 0.0706239572114042\n",
      "2022-03-26 18:02:13.518754 Epoch 250, Training Loss 0.07130170600188662\n",
      "2022-03-26 18:02:13.542332 Epoch 250, Training Loss 0.07216856943067078\n",
      "2022-03-26 18:02:13.565072 Epoch 250, Training Loss 0.07295878852724724\n",
      "2022-03-26 18:02:13.591672 Epoch 250, Training Loss 0.0737539859074156\n",
      "2022-03-26 18:02:13.620209 Epoch 250, Training Loss 0.07428360503652821\n",
      "2022-03-26 18:02:13.656610 Epoch 250, Training Loss 0.07485776343156615\n",
      "2022-03-26 18:02:13.680347 Epoch 250, Training Loss 0.07581499253240083\n",
      "2022-03-26 18:02:13.703748 Epoch 250, Training Loss 0.07647207124001534\n",
      "2022-03-26 18:02:13.727801 Epoch 250, Training Loss 0.07704481714979157\n",
      "2022-03-26 18:02:13.750271 Epoch 250, Training Loss 0.0778461243109325\n",
      "2022-03-26 18:02:13.772521 Epoch 250, Training Loss 0.07845892172182917\n",
      "2022-03-26 18:02:13.795321 Epoch 250, Training Loss 0.07921590055803508\n",
      "2022-03-26 18:02:13.818182 Epoch 250, Training Loss 0.07998081016571015\n",
      "2022-03-26 18:02:13.841394 Epoch 250, Training Loss 0.08060285216554657\n",
      "2022-03-26 18:02:13.870599 Epoch 250, Training Loss 0.08130500296040265\n",
      "2022-03-26 18:02:13.894891 Epoch 250, Training Loss 0.08200926720486272\n",
      "2022-03-26 18:02:13.920302 Epoch 250, Training Loss 0.08263259611623672\n",
      "2022-03-26 18:02:13.948154 Epoch 250, Training Loss 0.08349915496681047\n",
      "2022-03-26 18:02:13.970839 Epoch 250, Training Loss 0.08441482103236801\n",
      "2022-03-26 18:02:13.993361 Epoch 250, Training Loss 0.0851329524270104\n",
      "2022-03-26 18:02:14.016132 Epoch 250, Training Loss 0.08559150044875377\n",
      "2022-03-26 18:02:14.038949 Epoch 250, Training Loss 0.08605777096870305\n",
      "2022-03-26 18:02:14.062953 Epoch 250, Training Loss 0.08681322508455848\n",
      "2022-03-26 18:02:14.093102 Epoch 250, Training Loss 0.0874869555920896\n",
      "2022-03-26 18:02:14.116861 Epoch 250, Training Loss 0.0880306577667251\n",
      "2022-03-26 18:02:14.139650 Epoch 250, Training Loss 0.08864677809845761\n",
      "2022-03-26 18:02:14.162727 Epoch 250, Training Loss 0.0893281491287529\n",
      "2022-03-26 18:02:14.185544 Epoch 250, Training Loss 0.09006916802100208\n",
      "2022-03-26 18:02:14.207907 Epoch 250, Training Loss 0.09075928614724932\n",
      "2022-03-26 18:02:14.230259 Epoch 250, Training Loss 0.0915069966136342\n",
      "2022-03-26 18:02:14.259779 Epoch 250, Training Loss 0.09212533283569015\n",
      "2022-03-26 18:02:14.282561 Epoch 250, Training Loss 0.09289473390487758\n",
      "2022-03-26 18:02:14.313976 Epoch 250, Training Loss 0.0936502278079767\n",
      "2022-03-26 18:02:14.340514 Epoch 250, Training Loss 0.09421416278690328\n",
      "2022-03-26 18:02:14.362732 Epoch 250, Training Loss 0.09491238417222982\n",
      "2022-03-26 18:02:14.389371 Epoch 250, Training Loss 0.09559229984307838\n",
      "2022-03-26 18:02:14.411676 Epoch 250, Training Loss 0.09620309680166757\n",
      "2022-03-26 18:02:14.434402 Epoch 250, Training Loss 0.09701814214744227\n",
      "2022-03-26 18:02:14.457861 Epoch 250, Training Loss 0.09770038049391773\n",
      "2022-03-26 18:02:14.482259 Epoch 250, Training Loss 0.09843507466261345\n",
      "2022-03-26 18:02:14.506198 Epoch 250, Training Loss 0.09898499687157018\n",
      "2022-03-26 18:02:14.536305 Epoch 250, Training Loss 0.09961830674077543\n",
      "2022-03-26 18:02:14.562185 Epoch 250, Training Loss 0.10044309111964672\n",
      "2022-03-26 18:02:14.584427 Epoch 250, Training Loss 0.10134618136736438\n",
      "2022-03-26 18:02:14.610415 Epoch 250, Training Loss 0.10212446261397408\n",
      "2022-03-26 18:02:14.640606 Epoch 250, Training Loss 0.10278835271478004\n",
      "2022-03-26 18:02:14.667366 Epoch 250, Training Loss 0.10342235626924373\n",
      "2022-03-26 18:02:14.690361 Epoch 250, Training Loss 0.10392738192739999\n",
      "2022-03-26 18:02:14.713253 Epoch 250, Training Loss 0.10481035819901224\n",
      "2022-03-26 18:02:14.737830 Epoch 250, Training Loss 0.1053791186769905\n",
      "2022-03-26 18:02:14.768202 Epoch 250, Training Loss 0.10603619845169585\n",
      "2022-03-26 18:02:14.791563 Epoch 250, Training Loss 0.10678355205241981\n",
      "2022-03-26 18:02:14.814389 Epoch 250, Training Loss 0.1074263131450814\n",
      "2022-03-26 18:02:14.838680 Epoch 250, Training Loss 0.10816306405512573\n",
      "2022-03-26 18:02:14.863094 Epoch 250, Training Loss 0.10896782973385832\n",
      "2022-03-26 18:02:14.885621 Epoch 250, Training Loss 0.10954241576554526\n",
      "2022-03-26 18:02:14.907968 Epoch 250, Training Loss 0.11031722538459027\n",
      "2022-03-26 18:02:14.930343 Epoch 250, Training Loss 0.11093851032159517\n",
      "2022-03-26 18:02:14.954007 Epoch 250, Training Loss 0.1115182162931813\n",
      "2022-03-26 18:02:14.985370 Epoch 250, Training Loss 0.11203857638951763\n",
      "2022-03-26 18:02:15.008655 Epoch 250, Training Loss 0.11256650143572132\n",
      "2022-03-26 18:02:15.030836 Epoch 250, Training Loss 0.11313947307331787\n",
      "2022-03-26 18:02:15.056093 Epoch 250, Training Loss 0.11400498769929647\n",
      "2022-03-26 18:02:15.081997 Epoch 250, Training Loss 0.11468250279688774\n",
      "2022-03-26 18:02:15.105122 Epoch 250, Training Loss 0.11552435651307216\n",
      "2022-03-26 18:02:15.127859 Epoch 250, Training Loss 0.11602384743788054\n",
      "2022-03-26 18:02:15.151358 Epoch 250, Training Loss 0.11665275563364444\n",
      "2022-03-26 18:02:15.178872 Epoch 250, Training Loss 0.11741919651665651\n",
      "2022-03-26 18:02:15.211295 Epoch 250, Training Loss 0.11801768595452809\n",
      "2022-03-26 18:02:15.234417 Epoch 250, Training Loss 0.11887459616984247\n",
      "2022-03-26 18:02:15.257449 Epoch 250, Training Loss 0.11956959494087092\n",
      "2022-03-26 18:02:15.280090 Epoch 250, Training Loss 0.12016422848414887\n",
      "2022-03-26 18:02:15.302715 Epoch 250, Training Loss 0.1207662171033947\n",
      "2022-03-26 18:02:15.325102 Epoch 250, Training Loss 0.12127242970954427\n",
      "2022-03-26 18:02:15.351086 Epoch 250, Training Loss 0.12217475965504757\n",
      "2022-03-26 18:02:15.378316 Epoch 250, Training Loss 0.12283228029070607\n",
      "2022-03-26 18:02:15.400755 Epoch 250, Training Loss 0.12343334008360762\n",
      "2022-03-26 18:02:15.430355 Epoch 250, Training Loss 0.12390140960436039\n",
      "2022-03-26 18:02:15.455780 Epoch 250, Training Loss 0.12465820772111263\n",
      "2022-03-26 18:02:15.478767 Epoch 250, Training Loss 0.1252992907372277\n",
      "2022-03-26 18:02:15.501985 Epoch 250, Training Loss 0.12591297093712156\n",
      "2022-03-26 18:02:15.524830 Epoch 250, Training Loss 0.12655246368302103\n",
      "2022-03-26 18:02:15.548450 Epoch 250, Training Loss 0.12704045632306268\n",
      "2022-03-26 18:02:15.573634 Epoch 250, Training Loss 0.1275402163834218\n",
      "2022-03-26 18:02:15.596715 Epoch 250, Training Loss 0.12820955565975756\n",
      "2022-03-26 18:02:15.619825 Epoch 250, Training Loss 0.12900028825568421\n",
      "2022-03-26 18:02:15.655124 Epoch 250, Training Loss 0.1295487810202572\n",
      "2022-03-26 18:02:15.683881 Epoch 250, Training Loss 0.13014457563457588\n",
      "2022-03-26 18:02:15.706464 Epoch 250, Training Loss 0.13068485694468174\n",
      "2022-03-26 18:02:15.729499 Epoch 250, Training Loss 0.1313883584478627\n",
      "2022-03-26 18:02:15.753646 Epoch 250, Training Loss 0.1323079273218999\n",
      "2022-03-26 18:02:15.776734 Epoch 250, Training Loss 0.13326955916326674\n",
      "2022-03-26 18:02:15.799800 Epoch 250, Training Loss 0.1341544385151485\n",
      "2022-03-26 18:02:15.822636 Epoch 250, Training Loss 0.13472083653025616\n",
      "2022-03-26 18:02:15.846707 Epoch 250, Training Loss 0.13534379039731476\n",
      "2022-03-26 18:02:15.877645 Epoch 250, Training Loss 0.13595308809329176\n",
      "2022-03-26 18:02:15.901058 Epoch 250, Training Loss 0.13655483581678338\n",
      "2022-03-26 18:02:15.924867 Epoch 250, Training Loss 0.1370387690146561\n",
      "2022-03-26 18:02:15.951070 Epoch 250, Training Loss 0.13781216809206911\n",
      "2022-03-26 18:02:15.974573 Epoch 250, Training Loss 0.13839693321748767\n",
      "2022-03-26 18:02:15.998136 Epoch 250, Training Loss 0.13892545610132728\n",
      "2022-03-26 18:02:16.021495 Epoch 250, Training Loss 0.13960532222867317\n",
      "2022-03-26 18:02:16.053853 Epoch 250, Training Loss 0.1404624284075959\n",
      "2022-03-26 18:02:16.077891 Epoch 250, Training Loss 0.14130393768210545\n",
      "2022-03-26 18:02:16.110079 Epoch 250, Training Loss 0.14209519071347268\n",
      "2022-03-26 18:02:16.134341 Epoch 250, Training Loss 0.14266937479491124\n",
      "2022-03-26 18:02:16.158775 Epoch 250, Training Loss 0.1437000663155485\n",
      "2022-03-26 18:02:16.189706 Epoch 250, Training Loss 0.14448320343518806\n",
      "2022-03-26 18:02:16.212927 Epoch 250, Training Loss 0.14547404665928668\n",
      "2022-03-26 18:02:16.235615 Epoch 250, Training Loss 0.14611648632894697\n",
      "2022-03-26 18:02:16.259248 Epoch 250, Training Loss 0.1468485526340392\n",
      "2022-03-26 18:02:16.282617 Epoch 250, Training Loss 0.14759752341091176\n",
      "2022-03-26 18:02:16.305371 Epoch 250, Training Loss 0.14830300154740853\n",
      "2022-03-26 18:02:16.339174 Epoch 250, Training Loss 0.14895511058437855\n",
      "2022-03-26 18:02:16.363639 Epoch 250, Training Loss 0.14954762503771526\n",
      "2022-03-26 18:02:16.387034 Epoch 250, Training Loss 0.15002502066552487\n",
      "2022-03-26 18:02:16.409750 Epoch 250, Training Loss 0.15071045289106685\n",
      "2022-03-26 18:02:16.433313 Epoch 250, Training Loss 0.15129528486210367\n",
      "2022-03-26 18:02:16.458857 Epoch 250, Training Loss 0.15198029718740516\n",
      "2022-03-26 18:02:16.482441 Epoch 250, Training Loss 0.15249084999494236\n",
      "2022-03-26 18:02:16.509522 Epoch 250, Training Loss 0.1532993799127886\n",
      "2022-03-26 18:02:16.533411 Epoch 250, Training Loss 0.1540961585691213\n",
      "2022-03-26 18:02:16.566407 Epoch 250, Training Loss 0.1546602028105265\n",
      "2022-03-26 18:02:16.591930 Epoch 250, Training Loss 0.155165712013269\n",
      "2022-03-26 18:02:16.616172 Epoch 250, Training Loss 0.15594326626614233\n",
      "2022-03-26 18:02:16.641592 Epoch 250, Training Loss 0.15720516519473338\n",
      "2022-03-26 18:02:16.668288 Epoch 250, Training Loss 0.1578182769782098\n",
      "2022-03-26 18:02:16.703979 Epoch 250, Training Loss 0.1587247660626536\n",
      "2022-03-26 18:02:16.730426 Epoch 250, Training Loss 0.15969071405775406\n",
      "2022-03-26 18:02:16.754135 Epoch 250, Training Loss 0.16058996544622095\n",
      "2022-03-26 18:02:16.784536 Epoch 250, Training Loss 0.16116015430149214\n",
      "2022-03-26 18:02:16.811347 Epoch 250, Training Loss 0.16217005592020575\n",
      "2022-03-26 18:02:16.834989 Epoch 250, Training Loss 0.16301646573311837\n",
      "2022-03-26 18:02:16.858225 Epoch 250, Training Loss 0.16356643653281813\n",
      "2022-03-26 18:02:16.880961 Epoch 250, Training Loss 0.16455510830330422\n",
      "2022-03-26 18:02:16.907147 Epoch 250, Training Loss 0.16510210470165437\n",
      "2022-03-26 18:02:16.929499 Epoch 250, Training Loss 0.1660830114046326\n",
      "2022-03-26 18:02:16.959399 Epoch 250, Training Loss 0.16666175916676632\n",
      "2022-03-26 18:02:16.983239 Epoch 250, Training Loss 0.167382190904349\n",
      "2022-03-26 18:02:17.014853 Epoch 250, Training Loss 0.1678936244048121\n",
      "2022-03-26 18:02:17.038869 Epoch 250, Training Loss 0.16843688640448137\n",
      "2022-03-26 18:02:17.063206 Epoch 250, Training Loss 0.16936417385135466\n",
      "2022-03-26 18:02:17.086778 Epoch 250, Training Loss 0.17012092455878586\n",
      "2022-03-26 18:02:17.110671 Epoch 250, Training Loss 0.17063271355293597\n",
      "2022-03-26 18:02:17.141079 Epoch 250, Training Loss 0.17133226803958873\n",
      "2022-03-26 18:02:17.165557 Epoch 250, Training Loss 0.17205925849849915\n",
      "2022-03-26 18:02:17.189420 Epoch 250, Training Loss 0.1727714852984909\n",
      "2022-03-26 18:02:17.213393 Epoch 250, Training Loss 0.1735644425501299\n",
      "2022-03-26 18:02:17.245193 Epoch 250, Training Loss 0.17424914846792244\n",
      "2022-03-26 18:02:17.270061 Epoch 250, Training Loss 0.17503765724656528\n",
      "2022-03-26 18:02:17.293866 Epoch 250, Training Loss 0.17572011968211446\n",
      "2022-03-26 18:02:17.317468 Epoch 250, Training Loss 0.1762549477007688\n",
      "2022-03-26 18:02:17.347775 Epoch 250, Training Loss 0.1771026635566331\n",
      "2022-03-26 18:02:17.371413 Epoch 250, Training Loss 0.17767862232444842\n",
      "2022-03-26 18:02:17.394827 Epoch 250, Training Loss 0.17861350700068657\n",
      "2022-03-26 18:02:17.418145 Epoch 250, Training Loss 0.17933505140911893\n",
      "2022-03-26 18:02:17.440811 Epoch 250, Training Loss 0.17983313236395113\n",
      "2022-03-26 18:02:17.473235 Epoch 250, Training Loss 0.1806242055142932\n",
      "2022-03-26 18:02:17.498204 Epoch 250, Training Loss 0.18140524237052255\n",
      "2022-03-26 18:02:17.521710 Epoch 250, Training Loss 0.18210956492387426\n",
      "2022-03-26 18:02:17.550083 Epoch 250, Training Loss 0.18268747814476033\n",
      "2022-03-26 18:02:17.574122 Epoch 250, Training Loss 0.1833245872003038\n",
      "2022-03-26 18:02:17.598121 Epoch 250, Training Loss 0.1838455714304429\n",
      "2022-03-26 18:02:17.621900 Epoch 250, Training Loss 0.18435888499250191\n",
      "2022-03-26 18:02:17.645538 Epoch 250, Training Loss 0.18519490911527667\n",
      "2022-03-26 18:02:17.669156 Epoch 250, Training Loss 0.1856001904019919\n",
      "2022-03-26 18:02:17.702616 Epoch 250, Training Loss 0.18615542039694383\n",
      "2022-03-26 18:02:17.735704 Epoch 250, Training Loss 0.18687767342990622\n",
      "2022-03-26 18:02:17.759869 Epoch 250, Training Loss 0.18751521195139725\n",
      "2022-03-26 18:02:17.783208 Epoch 250, Training Loss 0.18828355073166625\n",
      "2022-03-26 18:02:17.806249 Epoch 250, Training Loss 0.18889464163566794\n",
      "2022-03-26 18:02:17.829429 Epoch 250, Training Loss 0.18953563974184148\n",
      "2022-03-26 18:02:17.855953 Epoch 250, Training Loss 0.1903612239815085\n",
      "2022-03-26 18:02:17.880162 Epoch 250, Training Loss 0.19113898829883322\n",
      "2022-03-26 18:02:17.903151 Epoch 250, Training Loss 0.19173655657054822\n",
      "2022-03-26 18:02:17.936353 Epoch 250, Training Loss 0.19237879501736682\n",
      "2022-03-26 18:02:17.962471 Epoch 250, Training Loss 0.1928899380404626\n",
      "2022-03-26 18:02:17.988559 Epoch 250, Training Loss 0.1935703576830647\n",
      "2022-03-26 18:02:18.012589 Epoch 250, Training Loss 0.19445606585963607\n",
      "2022-03-26 18:02:18.035759 Epoch 250, Training Loss 0.19528361827211307\n",
      "2022-03-26 18:02:18.058438 Epoch 250, Training Loss 0.1959214407922057\n",
      "2022-03-26 18:02:18.081487 Epoch 250, Training Loss 0.19651376473171936\n",
      "2022-03-26 18:02:18.106114 Epoch 250, Training Loss 0.19736969573875826\n",
      "2022-03-26 18:02:18.128684 Epoch 250, Training Loss 0.19794464057973585\n",
      "2022-03-26 18:02:18.159100 Epoch 250, Training Loss 0.19909845052472772\n",
      "2022-03-26 18:02:18.183623 Epoch 250, Training Loss 0.19967196641675652\n",
      "2022-03-26 18:02:18.206851 Epoch 250, Training Loss 0.20044874718122166\n",
      "2022-03-26 18:02:18.230124 Epoch 250, Training Loss 0.20091876764888958\n",
      "2022-03-26 18:02:18.252907 Epoch 250, Training Loss 0.2015780537101009\n",
      "2022-03-26 18:02:18.275587 Epoch 250, Training Loss 0.20230075842736628\n",
      "2022-03-26 18:02:18.298042 Epoch 250, Training Loss 0.20303193546469558\n",
      "2022-03-26 18:02:18.320192 Epoch 250, Training Loss 0.20363795364756718\n",
      "2022-03-26 18:02:18.351113 Epoch 250, Training Loss 0.20460058466705214\n",
      "2022-03-26 18:02:18.382014 Epoch 250, Training Loss 0.20521385357965288\n",
      "2022-03-26 18:02:18.405356 Epoch 250, Training Loss 0.2059685489177094\n",
      "2022-03-26 18:02:18.428418 Epoch 250, Training Loss 0.2065033015540189\n",
      "2022-03-26 18:02:18.452136 Epoch 250, Training Loss 0.2073896714031239\n",
      "2022-03-26 18:02:18.478608 Epoch 250, Training Loss 0.2079472590590377\n",
      "2022-03-26 18:02:18.501522 Epoch 250, Training Loss 0.20875063927277274\n",
      "2022-03-26 18:02:18.523695 Epoch 250, Training Loss 0.20932032644291362\n",
      "2022-03-26 18:02:18.546529 Epoch 250, Training Loss 0.21004782232177227\n",
      "2022-03-26 18:02:18.569557 Epoch 250, Training Loss 0.21085545146251883\n",
      "2022-03-26 18:02:18.598211 Epoch 250, Training Loss 0.21137434388975354\n",
      "2022-03-26 18:02:18.623474 Epoch 250, Training Loss 0.2121756374835968\n",
      "2022-03-26 18:02:18.647141 Epoch 250, Training Loss 0.213066043146431\n",
      "2022-03-26 18:02:18.670215 Epoch 250, Training Loss 0.21370936408067298\n",
      "2022-03-26 18:02:18.692847 Epoch 250, Training Loss 0.21437434078482412\n",
      "2022-03-26 18:02:18.715590 Epoch 250, Training Loss 0.21496878507192177\n",
      "2022-03-26 18:02:18.748825 Epoch 250, Training Loss 0.2156051054330128\n",
      "2022-03-26 18:02:18.773381 Epoch 250, Training Loss 0.2162239424255498\n",
      "2022-03-26 18:02:18.797327 Epoch 250, Training Loss 0.21673334925375937\n",
      "2022-03-26 18:02:18.828827 Epoch 250, Training Loss 0.21741678167487044\n",
      "2022-03-26 18:02:18.858120 Epoch 250, Training Loss 0.2178958655928102\n",
      "2022-03-26 18:02:18.881023 Epoch 250, Training Loss 0.21857389357998547\n",
      "2022-03-26 18:02:18.903405 Epoch 250, Training Loss 0.21939173935319456\n",
      "2022-03-26 18:02:18.925801 Epoch 250, Training Loss 0.21991539287292744\n",
      "2022-03-26 18:02:18.952355 Epoch 250, Training Loss 0.22066850868789742\n",
      "2022-03-26 18:02:18.975220 Epoch 250, Training Loss 0.2212577436662391\n",
      "2022-03-26 18:02:18.997635 Epoch 250, Training Loss 0.2219173646415286\n",
      "2022-03-26 18:02:19.020361 Epoch 250, Training Loss 0.22272512099474592\n",
      "2022-03-26 18:02:19.054122 Epoch 250, Training Loss 0.22338963541990656\n",
      "2022-03-26 18:02:19.081638 Epoch 250, Training Loss 0.2242280993696369\n",
      "2022-03-26 18:02:19.104704 Epoch 250, Training Loss 0.22495797714766333\n",
      "2022-03-26 18:02:19.127651 Epoch 250, Training Loss 0.2255688342444427\n",
      "2022-03-26 18:02:19.150027 Epoch 250, Training Loss 0.22612675696687626\n",
      "2022-03-26 18:02:19.178155 Epoch 250, Training Loss 0.22674521655225388\n",
      "2022-03-26 18:02:19.200376 Epoch 250, Training Loss 0.2272280926823311\n",
      "2022-03-26 18:02:19.223479 Epoch 250, Training Loss 0.22808850234579248\n",
      "2022-03-26 18:02:19.246577 Epoch 250, Training Loss 0.22870878734247155\n",
      "2022-03-26 18:02:19.277633 Epoch 250, Training Loss 0.22930992488056193\n",
      "2022-03-26 18:02:19.302563 Epoch 250, Training Loss 0.22993737851719723\n",
      "2022-03-26 18:02:19.324885 Epoch 250, Training Loss 0.23069429149865495\n",
      "2022-03-26 18:02:19.350558 Epoch 250, Training Loss 0.23142437713073039\n",
      "2022-03-26 18:02:19.376975 Epoch 250, Training Loss 0.23201645518202915\n",
      "2022-03-26 18:02:19.400469 Epoch 250, Training Loss 0.23256862133055392\n",
      "2022-03-26 18:02:19.423252 Epoch 250, Training Loss 0.23317131636392735\n",
      "2022-03-26 18:02:19.446835 Epoch 250, Training Loss 0.23366401266411443\n",
      "2022-03-26 18:02:19.470513 Epoch 250, Training Loss 0.23430659105558224\n",
      "2022-03-26 18:02:19.500524 Epoch 250, Training Loss 0.23483111086251485\n",
      "2022-03-26 18:02:19.524650 Epoch 250, Training Loss 0.2355470897062965\n",
      "2022-03-26 18:02:19.547833 Epoch 250, Training Loss 0.23616783843016076\n",
      "2022-03-26 18:02:19.571480 Epoch 250, Training Loss 0.2366213847685348\n",
      "2022-03-26 18:02:19.594993 Epoch 250, Training Loss 0.2371281542436546\n",
      "2022-03-26 18:02:19.618821 Epoch 250, Training Loss 0.23776408976606092\n",
      "2022-03-26 18:02:19.642068 Epoch 250, Training Loss 0.23864109231078107\n",
      "2022-03-26 18:02:19.667674 Epoch 250, Training Loss 0.23941389214047384\n",
      "2022-03-26 18:02:19.690921 Epoch 250, Training Loss 0.24010916248611783\n",
      "2022-03-26 18:02:19.720583 Epoch 250, Training Loss 0.24086749843319358\n",
      "2022-03-26 18:02:19.746699 Epoch 250, Training Loss 0.24198199781920293\n",
      "2022-03-26 18:02:19.780867 Epoch 250, Training Loss 0.24256590500359645\n",
      "2022-03-26 18:02:19.804383 Epoch 250, Training Loss 0.24324897636690407\n",
      "2022-03-26 18:02:19.827636 Epoch 250, Training Loss 0.24392307265792662\n",
      "2022-03-26 18:02:19.851451 Epoch 250, Training Loss 0.24451666605442077\n",
      "2022-03-26 18:02:19.875469 Epoch 250, Training Loss 0.24520600619523422\n",
      "2022-03-26 18:02:19.898672 Epoch 250, Training Loss 0.24585241536655084\n",
      "2022-03-26 18:02:19.930249 Epoch 250, Training Loss 0.24650483759467864\n",
      "2022-03-26 18:02:19.954456 Epoch 250, Training Loss 0.24734082757054693\n",
      "2022-03-26 18:02:19.984641 Epoch 250, Training Loss 0.2477370472743993\n",
      "2022-03-26 18:02:20.007919 Epoch 250, Training Loss 0.24830693841132973\n",
      "2022-03-26 18:02:20.032864 Epoch 250, Training Loss 0.24904517699842868\n",
      "2022-03-26 18:02:20.061465 Epoch 250, Training Loss 0.2498470835597314\n",
      "2022-03-26 18:02:20.085886 Epoch 250, Training Loss 0.2505357203352482\n",
      "2022-03-26 18:02:20.108869 Epoch 250, Training Loss 0.2515710716128654\n",
      "2022-03-26 18:02:20.131852 Epoch 250, Training Loss 0.2521980915914106\n",
      "2022-03-26 18:02:20.168251 Epoch 250, Training Loss 0.2526828506794732\n",
      "2022-03-26 18:02:20.191775 Epoch 250, Training Loss 0.253402362203659\n",
      "2022-03-26 18:02:20.215119 Epoch 250, Training Loss 0.2542608775522398\n",
      "2022-03-26 18:02:20.238046 Epoch 250, Training Loss 0.2549916660160665\n",
      "2022-03-26 18:02:20.269829 Epoch 250, Training Loss 0.25561620020652975\n",
      "2022-03-26 18:02:20.293219 Epoch 250, Training Loss 0.2562821188469982\n",
      "2022-03-26 18:02:20.315951 Epoch 250, Training Loss 0.2572154542597968\n",
      "2022-03-26 18:02:20.342089 Epoch 250, Training Loss 0.25764327711613894\n",
      "2022-03-26 18:02:20.364492 Epoch 250, Training Loss 0.25832541073527177\n",
      "2022-03-26 18:02:20.395555 Epoch 250, Training Loss 0.25925450880661643\n",
      "2022-03-26 18:02:20.418683 Epoch 250, Training Loss 0.259824000013149\n",
      "2022-03-26 18:02:20.441899 Epoch 250, Training Loss 0.26048455972348333\n",
      "2022-03-26 18:02:20.464698 Epoch 250, Training Loss 0.26118667088353725\n",
      "2022-03-26 18:02:20.488197 Epoch 250, Training Loss 0.2621272609133245\n",
      "2022-03-26 18:02:20.514511 Epoch 250, Training Loss 0.2626667944595332\n",
      "2022-03-26 18:02:20.537290 Epoch 250, Training Loss 0.263434359751394\n",
      "2022-03-26 18:02:20.559918 Epoch 250, Training Loss 0.2643178763520687\n",
      "2022-03-26 18:02:20.582728 Epoch 250, Training Loss 0.2652491682859333\n",
      "2022-03-26 18:02:20.615453 Epoch 250, Training Loss 0.2664672292940452\n",
      "2022-03-26 18:02:20.641992 Epoch 250, Training Loss 0.2672067134048018\n",
      "2022-03-26 18:02:20.664964 Epoch 250, Training Loss 0.26785357265978516\n",
      "2022-03-26 18:02:20.688338 Epoch 250, Training Loss 0.26864575413639286\n",
      "2022-03-26 18:02:20.710889 Epoch 250, Training Loss 0.2693793061367996\n",
      "2022-03-26 18:02:20.733126 Epoch 250, Training Loss 0.27019305900692026\n",
      "2022-03-26 18:02:20.757110 Epoch 250, Training Loss 0.27094179243230454\n",
      "2022-03-26 18:02:20.784126 Epoch 250, Training Loss 0.2719338621248675\n",
      "2022-03-26 18:02:20.811355 Epoch 250, Training Loss 0.2727558043835413\n",
      "2022-03-26 18:02:20.844670 Epoch 250, Training Loss 0.2733869581957302\n",
      "2022-03-26 18:02:20.874912 Epoch 250, Training Loss 0.2740239614187299\n",
      "2022-03-26 18:02:20.898661 Epoch 250, Training Loss 0.2748309882629253\n",
      "2022-03-26 18:02:20.920990 Epoch 250, Training Loss 0.275215572789502\n",
      "2022-03-26 18:02:20.943195 Epoch 250, Training Loss 0.27581998073231534\n",
      "2022-03-26 18:02:20.968304 Epoch 250, Training Loss 0.2763434116111692\n",
      "2022-03-26 18:02:20.995878 Epoch 250, Training Loss 0.2774231969319341\n",
      "2022-03-26 18:02:21.018711 Epoch 250, Training Loss 0.2779413755897366\n",
      "2022-03-26 18:02:21.041923 Epoch 250, Training Loss 0.278907532124873\n",
      "2022-03-26 18:02:21.074285 Epoch 250, Training Loss 0.28000523572992486\n",
      "2022-03-26 18:02:21.102007 Epoch 250, Training Loss 0.2807299941397079\n",
      "2022-03-26 18:02:21.124812 Epoch 250, Training Loss 0.28151890147677466\n",
      "2022-03-26 18:02:21.147656 Epoch 250, Training Loss 0.28229247555708337\n",
      "2022-03-26 18:02:21.170002 Epoch 250, Training Loss 0.28289520389893474\n",
      "2022-03-26 18:02:21.196024 Epoch 250, Training Loss 0.2834471847166491\n",
      "2022-03-26 18:02:21.219392 Epoch 250, Training Loss 0.2841758820635583\n",
      "2022-03-26 18:02:21.242364 Epoch 250, Training Loss 0.2848120515075181\n",
      "2022-03-26 18:02:21.265138 Epoch 250, Training Loss 0.28556795951807895\n",
      "2022-03-26 18:02:21.295216 Epoch 250, Training Loss 0.28614818039909956\n",
      "2022-03-26 18:02:21.319631 Epoch 250, Training Loss 0.28689844060279523\n",
      "2022-03-26 18:02:21.345776 Epoch 250, Training Loss 0.28772276925766255\n",
      "2022-03-26 18:02:21.369548 Epoch 250, Training Loss 0.2883453901542727\n",
      "2022-03-26 18:02:21.399276 Epoch 250, Training Loss 0.2889030991155473\n",
      "2022-03-26 18:02:21.422921 Epoch 250, Training Loss 0.2896130136821581\n",
      "2022-03-26 18:02:21.446828 Epoch 250, Training Loss 0.2901689145342468\n",
      "2022-03-26 18:02:21.470546 Epoch 250, Training Loss 0.2910217354288491\n",
      "2022-03-26 18:02:21.495012 Epoch 250, Training Loss 0.2918290200517001\n",
      "2022-03-26 18:02:21.529367 Epoch 250, Training Loss 0.2924648481409263\n",
      "2022-03-26 18:02:21.553656 Epoch 250, Training Loss 0.29350032045713165\n",
      "2022-03-26 18:02:21.577377 Epoch 250, Training Loss 0.29429702502687266\n",
      "2022-03-26 18:02:21.601111 Epoch 250, Training Loss 0.2950747039007104\n",
      "2022-03-26 18:02:21.628611 Epoch 250, Training Loss 0.2957930352224413\n",
      "2022-03-26 18:02:21.653146 Epoch 250, Training Loss 0.29655480377204585\n",
      "2022-03-26 18:02:21.675947 Epoch 250, Training Loss 0.29751931424336053\n",
      "2022-03-26 18:02:21.699412 Epoch 250, Training Loss 0.2985758111452508\n",
      "2022-03-26 18:02:21.722705 Epoch 250, Training Loss 0.29963256193853705\n",
      "2022-03-26 18:02:21.754397 Epoch 250, Training Loss 0.3005402838177693\n",
      "2022-03-26 18:02:21.779437 Epoch 250, Training Loss 0.30110984206047203\n",
      "2022-03-26 18:02:21.807158 Epoch 250, Training Loss 0.30236119592128813\n",
      "2022-03-26 18:02:21.836518 Epoch 250, Training Loss 0.3029484170705766\n",
      "2022-03-26 18:02:21.860756 Epoch 250, Training Loss 0.30377642192956433\n",
      "2022-03-26 18:02:21.886418 Epoch 250, Training Loss 0.3047548567928621\n",
      "2022-03-26 18:02:21.909793 Epoch 250, Training Loss 0.30561633156541057\n",
      "2022-03-26 18:02:21.933199 Epoch 250, Training Loss 0.306382784110201\n",
      "2022-03-26 18:02:21.956459 Epoch 250, Training Loss 0.3070516592782477\n",
      "2022-03-26 18:02:21.989370 Epoch 250, Training Loss 0.3076054840670217\n",
      "2022-03-26 18:02:22.012838 Epoch 250, Training Loss 0.30843778312816034\n",
      "2022-03-26 18:02:22.035741 Epoch 250, Training Loss 0.3092955967883015\n",
      "2022-03-26 18:02:22.059301 Epoch 250, Training Loss 0.3097259596638057\n",
      "2022-03-26 18:02:22.090514 Epoch 250, Training Loss 0.3105374382584906\n",
      "2022-03-26 18:02:22.116134 Epoch 250, Training Loss 0.31118342913020297\n",
      "2022-03-26 18:02:22.139237 Epoch 250, Training Loss 0.3119183413665313\n",
      "2022-03-26 18:02:22.169865 Epoch 250, Training Loss 0.31248610575333274\n",
      "2022-03-26 18:02:22.194565 Epoch 250, Training Loss 0.31295472128159557\n",
      "2022-03-26 18:02:22.223112 Epoch 250, Training Loss 0.313683243015843\n",
      "2022-03-26 18:02:22.245976 Epoch 250, Training Loss 0.31456369996223305\n",
      "2022-03-26 18:02:22.269804 Epoch 250, Training Loss 0.3152811273055918\n",
      "2022-03-26 18:02:22.292672 Epoch 250, Training Loss 0.3160960472681943\n",
      "2022-03-26 18:02:22.316092 Epoch 250, Training Loss 0.31663851539039856\n",
      "2022-03-26 18:02:22.345210 Epoch 250, Training Loss 0.3171774561295424\n",
      "2022-03-26 18:02:22.371811 Epoch 250, Training Loss 0.31780768816580857\n",
      "2022-03-26 18:02:22.406574 Epoch 250, Training Loss 0.3183733748124384\n",
      "2022-03-26 18:02:22.431094 Epoch 250, Training Loss 0.3191414732686089\n",
      "2022-03-26 18:02:22.456074 Epoch 250, Training Loss 0.31977723489332077\n",
      "2022-03-26 18:02:22.480406 Epoch 250, Training Loss 0.3208501193377063\n",
      "2022-03-26 18:02:22.504276 Epoch 250, Training Loss 0.3214842233892597\n",
      "2022-03-26 18:02:22.528581 Epoch 250, Training Loss 0.32221656614709693\n",
      "2022-03-26 18:02:22.556927 Epoch 250, Training Loss 0.3234554133223146\n",
      "2022-03-26 18:02:22.581145 Epoch 250, Training Loss 0.32407244060502943\n",
      "2022-03-26 18:02:22.615516 Epoch 250, Training Loss 0.32490249626014545\n",
      "2022-03-26 18:02:22.639468 Epoch 250, Training Loss 0.32619305816300387\n",
      "2022-03-26 18:02:22.663722 Epoch 250, Training Loss 0.32684465613968844\n",
      "2022-03-26 18:02:22.688618 Epoch 250, Training Loss 0.3277216704224077\n",
      "2022-03-26 18:02:22.712984 Epoch 250, Training Loss 0.32850116982941735\n",
      "2022-03-26 18:02:22.737463 Epoch 250, Training Loss 0.3292735502161943\n",
      "2022-03-26 18:02:22.762657 Epoch 250, Training Loss 0.33017197453304936\n",
      "2022-03-26 18:02:22.786729 Epoch 250, Training Loss 0.3310788572977876\n",
      "2022-03-26 18:02:22.818699 Epoch 250, Training Loss 0.33178118134246154\n",
      "2022-03-26 18:02:22.854093 Epoch 250, Training Loss 0.3324568651597518\n",
      "2022-03-26 18:02:22.877970 Epoch 250, Training Loss 0.3333906681107743\n",
      "2022-03-26 18:02:22.906912 Epoch 250, Training Loss 0.33417348304520483\n",
      "2022-03-26 18:02:22.931566 Epoch 250, Training Loss 0.3349351254494294\n",
      "2022-03-26 18:02:22.955629 Epoch 250, Training Loss 0.33556824526213624\n",
      "2022-03-26 18:02:22.984587 Epoch 250, Training Loss 0.3364083314185862\n",
      "2022-03-26 18:02:23.010518 Epoch 250, Training Loss 0.3372485532480128\n",
      "2022-03-26 18:02:23.042728 Epoch 250, Training Loss 0.3380107419265201\n",
      "2022-03-26 18:02:23.072345 Epoch 250, Training Loss 0.3386313965177292\n",
      "2022-03-26 18:02:23.096594 Epoch 250, Training Loss 0.3394476030869862\n",
      "2022-03-26 18:02:23.123646 Epoch 250, Training Loss 0.34004083721686507\n",
      "2022-03-26 18:02:23.147585 Epoch 250, Training Loss 0.34086451281214614\n",
      "2022-03-26 18:02:23.170733 Epoch 250, Training Loss 0.34138811354899345\n",
      "2022-03-26 18:02:23.194414 Epoch 250, Training Loss 0.3421546062621314\n",
      "2022-03-26 18:02:23.218186 Epoch 250, Training Loss 0.342803050528097\n",
      "2022-03-26 18:02:23.241470 Epoch 250, Training Loss 0.34352371664455783\n",
      "2022-03-26 18:02:23.275644 Epoch 250, Training Loss 0.34434123200071437\n",
      "2022-03-26 18:02:23.301372 Epoch 250, Training Loss 0.3450992927145775\n",
      "2022-03-26 18:02:23.324574 Epoch 250, Training Loss 0.3458062983915934\n",
      "2022-03-26 18:02:23.350479 Epoch 250, Training Loss 0.346574001643054\n",
      "2022-03-26 18:02:23.378396 Epoch 250, Training Loss 0.347236336230317\n",
      "2022-03-26 18:02:23.400909 Epoch 250, Training Loss 0.3480622343471288\n",
      "2022-03-26 18:02:23.423454 Epoch 250, Training Loss 0.3490735300437874\n",
      "2022-03-26 18:02:23.446625 Epoch 250, Training Loss 0.3498698699733485\n",
      "2022-03-26 18:02:23.469828 Epoch 250, Training Loss 0.35045972192074026\n",
      "2022-03-26 18:02:23.502196 Epoch 250, Training Loss 0.3509534828345794\n",
      "2022-03-26 18:02:23.526466 Epoch 250, Training Loss 0.351740351113517\n",
      "2022-03-26 18:02:23.550214 Epoch 250, Training Loss 0.35251411833726537\n",
      "2022-03-26 18:02:23.573682 Epoch 250, Training Loss 0.35307024747057036\n",
      "2022-03-26 18:02:23.601105 Epoch 250, Training Loss 0.35381408302527867\n",
      "2022-03-26 18:02:23.625108 Epoch 250, Training Loss 0.35438280222970814\n",
      "2022-03-26 18:02:23.648254 Epoch 250, Training Loss 0.35506848224898435\n",
      "2022-03-26 18:02:23.672030 Epoch 250, Training Loss 0.3558559274429555\n",
      "2022-03-26 18:02:23.696602 Epoch 250, Training Loss 0.35680555245455575\n",
      "2022-03-26 18:02:23.727744 Epoch 250, Training Loss 0.35742254711477955\n",
      "2022-03-26 18:02:23.753734 Epoch 250, Training Loss 0.3581000173945561\n",
      "2022-03-26 18:02:23.777235 Epoch 250, Training Loss 0.3587727156441535\n",
      "2022-03-26 18:02:23.800540 Epoch 250, Training Loss 0.35938739749934057\n",
      "2022-03-26 18:02:23.823444 Epoch 250, Training Loss 0.36026930805209956\n",
      "2022-03-26 18:02:23.849897 Epoch 250, Training Loss 0.36094723679982793\n",
      "2022-03-26 18:02:23.880192 Epoch 250, Training Loss 0.3616067711883189\n",
      "2022-03-26 18:02:23.905477 Epoch 250, Training Loss 0.3624464247156592\n",
      "2022-03-26 18:02:23.931140 Epoch 250, Training Loss 0.3630782610848737\n",
      "2022-03-26 18:02:23.965297 Epoch 250, Training Loss 0.36379527599762773\n",
      "2022-03-26 18:02:23.988115 Epoch 250, Training Loss 0.36446174731492387\n",
      "2022-03-26 18:02:24.014858 Epoch 250, Training Loss 0.36523715054135186\n",
      "2022-03-26 18:02:24.037999 Epoch 250, Training Loss 0.3658658160501734\n",
      "2022-03-26 18:02:24.061803 Epoch 250, Training Loss 0.36654459930899197\n",
      "2022-03-26 18:02:24.086989 Epoch 250, Training Loss 0.367272705487583\n",
      "2022-03-26 18:02:24.110545 Epoch 250, Training Loss 0.3679489100451969\n",
      "2022-03-26 18:02:24.134401 Epoch 250, Training Loss 0.3686885311246833\n",
      "2022-03-26 18:02:24.168169 Epoch 250, Training Loss 0.36931647116418387\n",
      "2022-03-26 18:02:24.192903 Epoch 250, Training Loss 0.36997537231048966\n",
      "2022-03-26 18:02:24.216838 Epoch 250, Training Loss 0.3704732823783479\n",
      "2022-03-26 18:02:24.239585 Epoch 250, Training Loss 0.3713501079384323\n",
      "2022-03-26 18:02:24.262052 Epoch 250, Training Loss 0.37219117459891093\n",
      "2022-03-26 18:02:24.288672 Epoch 250, Training Loss 0.37310040160975494\n",
      "2022-03-26 18:02:24.311574 Epoch 250, Training Loss 0.37375436387860866\n",
      "2022-03-26 18:02:24.337617 Epoch 250, Training Loss 0.3744861723288246\n",
      "2022-03-26 18:02:24.360500 Epoch 250, Training Loss 0.3751571554585796\n",
      "2022-03-26 18:02:24.390134 Epoch 250, Training Loss 0.3758235907234499\n",
      "2022-03-26 18:02:24.418057 Epoch 250, Training Loss 0.37648080335096323\n",
      "2022-03-26 18:02:24.441262 Epoch 250, Training Loss 0.3772057904230664\n",
      "2022-03-26 18:02:24.465283 Epoch 250, Training Loss 0.3777236283740119\n",
      "2022-03-26 18:02:24.489184 Epoch 250, Training Loss 0.3786519204106782\n",
      "2022-03-26 18:02:24.513049 Epoch 250, Training Loss 0.37940978874330933\n",
      "2022-03-26 18:02:24.536845 Epoch 250, Training Loss 0.3800992096781426\n",
      "2022-03-26 18:02:24.560640 Epoch 250, Training Loss 0.3811052551354899\n",
      "2022-03-26 18:02:24.585125 Epoch 250, Training Loss 0.3820933436646181\n",
      "2022-03-26 18:02:24.622229 Epoch 250, Training Loss 0.3829224484655863\n",
      "2022-03-26 18:02:24.645955 Epoch 250, Training Loss 0.3836725323706332\n",
      "2022-03-26 18:02:24.670866 Epoch 250, Training Loss 0.3843123604879355\n",
      "2022-03-26 18:02:24.695115 Epoch 250, Training Loss 0.3851348641126052\n",
      "2022-03-26 18:02:24.719193 Epoch 250, Training Loss 0.385712759269168\n",
      "2022-03-26 18:02:24.742525 Epoch 250, Training Loss 0.38643252567562\n",
      "2022-03-26 18:02:24.766665 Epoch 250, Training Loss 0.38728422429555515\n",
      "2022-03-26 18:02:24.791118 Epoch 250, Training Loss 0.38804026554002785\n",
      "2022-03-26 18:02:24.815092 Epoch 250, Training Loss 0.3887233669343202\n",
      "2022-03-26 18:02:24.847004 Epoch 250, Training Loss 0.3894660151980417\n",
      "2022-03-26 18:02:24.871234 Epoch 250, Training Loss 0.3903682443796826\n",
      "2022-03-26 18:02:24.902817 Epoch 250, Training Loss 0.3907905062446204\n",
      "2022-03-26 18:02:24.925585 Epoch 250, Training Loss 0.39140029003857957\n",
      "2022-03-26 18:02:24.948756 Epoch 250, Training Loss 0.3920787748930704\n",
      "2022-03-26 18:02:24.976268 Epoch 250, Training Loss 0.3925219004797509\n",
      "2022-03-26 18:02:24.999569 Epoch 250, Training Loss 0.3934821996771161\n",
      "2022-03-26 18:02:25.022821 Epoch 250, Training Loss 0.394034663086657\n",
      "2022-03-26 18:02:25.046061 Epoch 250, Training Loss 0.39488878491741924\n",
      "2022-03-26 18:02:25.080417 Epoch 250, Training Loss 0.3953623813207802\n",
      "2022-03-26 18:02:25.111854 Epoch 250, Training Loss 0.39608049457487854\n",
      "2022-03-26 18:02:25.135169 Epoch 250, Training Loss 0.39689416665097943\n",
      "2022-03-26 18:02:25.158376 Epoch 250, Training Loss 0.3979114859991366\n",
      "2022-03-26 18:02:25.184957 Epoch 250, Training Loss 0.3989675854859145\n",
      "2022-03-26 18:02:25.208143 Epoch 250, Training Loss 0.39961677747766683\n",
      "2022-03-26 18:02:25.230656 Epoch 250, Training Loss 0.4006622761792844\n",
      "2022-03-26 18:02:25.254193 Epoch 250, Training Loss 0.4014364349491456\n",
      "2022-03-26 18:02:25.277534 Epoch 250, Training Loss 0.40242495709825354\n",
      "2022-03-26 18:02:25.310332 Epoch 250, Training Loss 0.40320980179187893\n",
      "2022-03-26 18:02:25.337322 Epoch 250, Training Loss 0.4038141020347395\n",
      "2022-03-26 18:02:25.361428 Epoch 250, Training Loss 0.40451627493361986\n",
      "2022-03-26 18:02:25.388680 Epoch 250, Training Loss 0.4052881567603182\n",
      "2022-03-26 18:02:25.412174 Epoch 250, Training Loss 0.4060695002527188\n",
      "2022-03-26 18:02:25.435183 Epoch 250, Training Loss 0.40697939541486217\n",
      "2022-03-26 18:02:25.458825 Epoch 250, Training Loss 0.4078142140298853\n",
      "2022-03-26 18:02:25.482040 Epoch 250, Training Loss 0.4086799467997173\n",
      "2022-03-26 18:02:25.505798 Epoch 250, Training Loss 0.4092818274522376\n",
      "2022-03-26 18:02:25.537918 Epoch 250, Training Loss 0.4097867909904636\n",
      "2022-03-26 18:02:25.562628 Epoch 250, Training Loss 0.4104723557639305\n",
      "2022-03-26 18:02:25.586581 Epoch 250, Training Loss 0.4113786240367938\n",
      "2022-03-26 18:02:25.610500 Epoch 250, Training Loss 0.412127803811027\n",
      "2022-03-26 18:02:25.634196 Epoch 250, Training Loss 0.41279548856303516\n",
      "2022-03-26 18:02:25.657684 Epoch 250, Training Loss 0.413267266186302\n",
      "2022-03-26 18:02:25.681075 Epoch 250, Training Loss 0.4139188218604573\n",
      "2022-03-26 18:02:25.705751 Epoch 250, Training Loss 0.4146003169781717\n",
      "2022-03-26 18:02:25.729297 Epoch 250, Training Loss 0.41533850221072927\n",
      "2022-03-26 18:02:25.764076 Epoch 250, Training Loss 0.416298895083425\n",
      "2022-03-26 18:02:25.789510 Epoch 250, Training Loss 0.4169822438141269\n",
      "2022-03-26 18:02:25.813196 Epoch 250, Training Loss 0.417639656978495\n",
      "2022-03-26 18:02:25.836237 Epoch 250, Training Loss 0.4184417476129654\n",
      "2022-03-26 18:02:25.859618 Epoch 250, Training Loss 0.41929385256584345\n",
      "2022-03-26 18:02:25.882576 Epoch 250, Training Loss 0.4201622352270824\n",
      "2022-03-26 18:02:25.913732 Epoch 250, Training Loss 0.4210861688837066\n",
      "2022-03-26 18:02:25.938074 Epoch 250, Training Loss 0.421693329120536\n",
      "2022-03-26 18:02:25.960794 Epoch 250, Training Loss 0.42246985340209875\n",
      "2022-03-26 18:02:25.997668 Epoch 250, Training Loss 0.4230477442140774\n",
      "2022-03-26 18:02:26.022505 Epoch 250, Training Loss 0.42381065866678874\n",
      "2022-03-26 18:02:26.044739 Epoch 250, Training Loss 0.42439171679489446\n",
      "2022-03-26 18:02:26.067754 Epoch 250, Training Loss 0.4251685725606006\n",
      "2022-03-26 18:02:26.097196 Epoch 250, Training Loss 0.4257606690954369\n",
      "2022-03-26 18:02:26.120140 Epoch 250, Training Loss 0.42651247269357256\n",
      "2022-03-26 18:02:26.142992 Epoch 250, Training Loss 0.42729194748127247\n",
      "2022-03-26 18:02:26.166081 Epoch 250, Training Loss 0.42796193082314316\n",
      "2022-03-26 18:02:26.192613 Epoch 250, Training Loss 0.42889878611125604\n",
      "2022-03-26 18:02:26.224845 Epoch 250, Training Loss 0.4295408266127262\n",
      "2022-03-26 18:02:26.250709 Epoch 250, Training Loss 0.43010665297203354\n",
      "2022-03-26 18:02:26.273237 Epoch 250, Training Loss 0.4306512486446849\n",
      "2022-03-26 18:02:26.296045 Epoch 250, Training Loss 0.4313159980584898\n",
      "2022-03-26 18:02:26.323392 Epoch 250, Training Loss 0.43209087780064637\n",
      "2022-03-26 18:02:26.349384 Epoch 250, Training Loss 0.4328091003553337\n",
      "2022-03-26 18:02:26.373205 Epoch 250, Training Loss 0.43382415808070346\n",
      "2022-03-26 18:02:26.397979 Epoch 250, Training Loss 0.4346370620800711\n",
      "2022-03-26 18:02:26.422403 Epoch 250, Training Loss 0.4355471804928597\n",
      "2022-03-26 18:02:26.455943 Epoch 250, Training Loss 0.4361950694142705\n",
      "2022-03-26 18:02:26.480171 Epoch 250, Training Loss 0.4369946171712997\n",
      "2022-03-26 18:02:26.502927 Epoch 250, Training Loss 0.43801918541988755\n",
      "2022-03-26 18:02:26.525629 Epoch 250, Training Loss 0.43864153508487563\n",
      "2022-03-26 18:02:26.548518 Epoch 250, Training Loss 0.4392763495140368\n",
      "2022-03-26 18:02:26.571741 Epoch 250, Training Loss 0.44019624888134734\n",
      "2022-03-26 18:02:26.595478 Epoch 250, Training Loss 0.4407775298789944\n",
      "2022-03-26 18:02:26.620362 Epoch 250, Training Loss 0.44146678236591846\n",
      "2022-03-26 18:02:26.650634 Epoch 250, Training Loss 0.4421462865208116\n",
      "2022-03-26 18:02:26.680711 Epoch 250, Training Loss 0.4427575691200583\n",
      "2022-03-26 18:02:26.704857 Epoch 250, Training Loss 0.44339637305882884\n",
      "2022-03-26 18:02:26.727738 Epoch 250, Training Loss 0.4441645164090349\n",
      "2022-03-26 18:02:26.751140 Epoch 250, Training Loss 0.4449016043673391\n",
      "2022-03-26 18:02:26.774898 Epoch 250, Training Loss 0.4458375671864166\n",
      "2022-03-26 18:02:26.797896 Epoch 250, Training Loss 0.4465795078164781\n",
      "2022-03-26 18:02:26.821052 Epoch 250, Training Loss 0.44731893049329136\n",
      "2022-03-26 18:02:26.844094 Epoch 250, Training Loss 0.44798997757227527\n",
      "2022-03-26 18:02:26.867050 Epoch 250, Training Loss 0.4487155660262803\n",
      "2022-03-26 18:02:26.898970 Epoch 250, Training Loss 0.4494719597918298\n",
      "2022-03-26 18:02:26.926055 Epoch 250, Training Loss 0.45014792829370864\n",
      "2022-03-26 18:02:26.957192 Epoch 250, Training Loss 0.45097090189566696\n",
      "2022-03-26 18:02:26.984334 Epoch 250, Training Loss 0.45173156333853826\n",
      "2022-03-26 18:02:27.008085 Epoch 250, Training Loss 0.4525327872665947\n",
      "2022-03-26 18:02:27.037447 Epoch 250, Training Loss 0.453260895350705\n",
      "2022-03-26 18:02:27.060899 Epoch 250, Training Loss 0.4540689408855365\n",
      "2022-03-26 18:02:27.083837 Epoch 250, Training Loss 0.4548529431109538\n",
      "2022-03-26 18:02:27.112269 Epoch 250, Training Loss 0.4557840819172847\n",
      "2022-03-26 18:02:27.147973 Epoch 250, Training Loss 0.45645743261670213\n",
      "2022-03-26 18:02:27.171334 Epoch 250, Training Loss 0.45714910961020633\n",
      "2022-03-26 18:02:27.194630 Epoch 250, Training Loss 0.4578381334348103\n",
      "2022-03-26 18:02:27.218477 Epoch 250, Training Loss 0.45831069521739354\n",
      "2022-03-26 18:02:27.243790 Epoch 250, Training Loss 0.4589304905718245\n",
      "2022-03-26 18:02:27.267008 Epoch 250, Training Loss 0.45958275197412046\n",
      "2022-03-26 18:02:27.289937 Epoch 250, Training Loss 0.4602044784961759\n",
      "2022-03-26 18:02:27.312675 Epoch 250, Training Loss 0.4609888302700599\n",
      "2022-03-26 18:02:27.349348 Epoch 250, Training Loss 0.4616379528246877\n",
      "2022-03-26 18:02:27.373442 Epoch 250, Training Loss 0.462436262222812\n",
      "2022-03-26 18:02:27.396799 Epoch 250, Training Loss 0.4630105681431568\n",
      "2022-03-26 18:02:27.419551 Epoch 250, Training Loss 0.4635982522955331\n",
      "2022-03-26 18:02:27.444787 Epoch 250, Training Loss 0.464237610549878\n",
      "2022-03-26 18:02:27.469493 Epoch 250, Training Loss 0.4651781767987839\n",
      "2022-03-26 18:02:27.493630 Epoch 250, Training Loss 0.46595881402949846\n",
      "2022-03-26 18:02:27.517399 Epoch 250, Training Loss 0.46671901448913244\n",
      "2022-03-26 18:02:27.541503 Epoch 250, Training Loss 0.46719639635909244\n",
      "2022-03-26 18:02:27.572685 Epoch 250, Training Loss 0.4679722950205474\n",
      "2022-03-26 18:02:27.598068 Epoch 250, Training Loss 0.4685331828072858\n",
      "2022-03-26 18:02:27.621486 Epoch 250, Training Loss 0.4691469881997999\n",
      "2022-03-26 18:02:27.644688 Epoch 250, Training Loss 0.4700993600556308\n",
      "2022-03-26 18:02:27.667929 Epoch 250, Training Loss 0.47092760882109325\n",
      "2022-03-26 18:02:27.690333 Epoch 250, Training Loss 0.4716964870157754\n",
      "2022-03-26 18:02:27.713241 Epoch 250, Training Loss 0.47231892597339953\n",
      "2022-03-26 18:02:27.736269 Epoch 250, Training Loss 0.47294938625277155\n",
      "2022-03-26 18:02:27.759796 Epoch 250, Training Loss 0.4736124801513789\n",
      "2022-03-26 18:02:27.787233 Epoch 250, Training Loss 0.4743627170315179\n",
      "2022-03-26 18:02:27.812720 Epoch 250, Training Loss 0.47511139740724395\n",
      "2022-03-26 18:02:27.835639 Epoch 250, Training Loss 0.4758041652724566\n",
      "2022-03-26 18:02:27.859594 Epoch 250, Training Loss 0.47642237717843117\n",
      "2022-03-26 18:02:27.882885 Epoch 250, Training Loss 0.47739052056046705\n",
      "2022-03-26 18:02:27.905358 Epoch 250, Training Loss 0.47804904067912674\n",
      "2022-03-26 18:02:27.928092 Epoch 250, Training Loss 0.47902138916122944\n",
      "2022-03-26 18:02:27.953750 Epoch 250, Training Loss 0.4796950099108469\n",
      "2022-03-26 18:02:27.981820 Epoch 250, Training Loss 0.48011574610267455\n",
      "2022-03-26 18:02:28.011881 Epoch 250, Training Loss 0.4809423933934678\n",
      "2022-03-26 18:02:28.041518 Epoch 250, Training Loss 0.4818786518729251\n",
      "2022-03-26 18:02:28.064888 Epoch 250, Training Loss 0.482862461825161\n",
      "2022-03-26 18:02:28.088040 Epoch 250, Training Loss 0.48365416562618196\n",
      "2022-03-26 18:02:28.117122 Epoch 250, Training Loss 0.48451956553989667\n",
      "2022-03-26 18:02:28.141882 Epoch 250, Training Loss 0.4851283681819506\n",
      "2022-03-26 18:02:28.164749 Epoch 250, Training Loss 0.48580972442541587\n",
      "2022-03-26 18:02:28.191253 Epoch 250, Training Loss 0.4866406747599697\n",
      "2022-03-26 18:02:28.218913 Epoch 250, Training Loss 0.48763317410903206\n",
      "2022-03-26 18:02:28.259831 Epoch 250, Training Loss 0.4881476495805604\n",
      "2022-03-26 18:02:28.282439 Epoch 250, Training Loss 0.4887847880954328\n",
      "2022-03-26 18:02:28.304779 Epoch 250, Training Loss 0.4894380729140528\n",
      "2022-03-26 18:02:28.327857 Epoch 250, Training Loss 0.4900890913079767\n",
      "2022-03-26 18:02:28.350609 Epoch 250, Training Loss 0.4910296568708956\n",
      "2022-03-26 18:02:28.375017 Epoch 250, Training Loss 0.4918379401383193\n",
      "2022-03-26 18:02:28.399557 Epoch 250, Training Loss 0.49263329567659236\n",
      "2022-03-26 18:02:28.428257 Epoch 250, Training Loss 0.49329758494558845\n",
      "2022-03-26 18:02:28.454274 Epoch 250, Training Loss 0.49386898883620794\n",
      "2022-03-26 18:02:28.486102 Epoch 250, Training Loss 0.49462297501618907\n",
      "2022-03-26 18:02:28.509331 Epoch 250, Training Loss 0.495317769134441\n",
      "2022-03-26 18:02:28.532798 Epoch 250, Training Loss 0.4959995325492776\n",
      "2022-03-26 18:02:28.555784 Epoch 250, Training Loss 0.4968757532213045\n",
      "2022-03-26 18:02:28.582332 Epoch 250, Training Loss 0.4976113447371651\n",
      "2022-03-26 18:02:28.605757 Epoch 250, Training Loss 0.49831795048378313\n",
      "2022-03-26 18:02:28.628588 Epoch 250, Training Loss 0.4990574319268127\n",
      "2022-03-26 18:02:28.660845 Epoch 250, Training Loss 0.4997900233930334\n",
      "2022-03-26 18:02:28.685177 Epoch 250, Training Loss 0.5003937696252028\n",
      "2022-03-26 18:02:28.709419 Epoch 250, Training Loss 0.5011575908764548\n",
      "2022-03-26 18:02:28.732650 Epoch 250, Training Loss 0.5016409112974201\n",
      "2022-03-26 18:02:28.755742 Epoch 250, Training Loss 0.5021386848538733\n",
      "2022-03-26 18:02:28.778833 Epoch 250, Training Loss 0.5028425631925578\n",
      "2022-03-26 18:02:28.802219 Epoch 250, Training Loss 0.503523095108359\n",
      "2022-03-26 18:02:28.825721 Epoch 250, Training Loss 0.5045750115990943\n",
      "2022-03-26 18:02:28.849126 Epoch 250, Training Loss 0.5053204810985213\n",
      "2022-03-26 18:02:28.877578 Epoch 250, Training Loss 0.5059949418010614\n",
      "2022-03-26 18:02:28.903606 Epoch 250, Training Loss 0.5068451794212127\n",
      "2022-03-26 18:02:28.928006 Epoch 250, Training Loss 0.5074416717223804\n",
      "2022-03-26 18:02:28.955626 Epoch 250, Training Loss 0.5079564375569449\n",
      "2022-03-26 18:02:28.984227 Epoch 250, Training Loss 0.5087622640001804\n",
      "2022-03-26 18:02:29.015758 Epoch 250, Training Loss 0.5092069118300362\n",
      "2022-03-26 18:02:29.039509 Epoch 250, Training Loss 0.509721142937765\n",
      "2022-03-26 18:02:29.068378 Epoch 250, Training Loss 0.5102814076196812\n",
      "2022-03-26 18:02:29.098893 Epoch 250, Training Loss 0.510942382504568\n",
      "2022-03-26 18:02:29.127861 Epoch 250, Training Loss 0.5115711507589921\n",
      "2022-03-26 18:02:29.150417 Epoch 250, Training Loss 0.5123285655018008\n",
      "2022-03-26 18:02:29.172850 Epoch 250, Training Loss 0.5128711569111061\n",
      "2022-03-26 18:02:29.200466 Epoch 250, Training Loss 0.5139303842697607\n",
      "2022-03-26 18:02:29.223553 Epoch 250, Training Loss 0.5146152744893833\n",
      "2022-03-26 18:02:29.246112 Epoch 250, Training Loss 0.5153079637709785\n",
      "2022-03-26 18:02:29.269015 Epoch 250, Training Loss 0.5161500261796405\n",
      "2022-03-26 18:02:29.292331 Epoch 250, Training Loss 0.5170697610625221\n",
      "2022-03-26 18:02:29.331541 Epoch 250, Training Loss 0.5176937677290129\n",
      "2022-03-26 18:02:29.356085 Epoch 250, Training Loss 0.5186291853408984\n",
      "2022-03-26 18:02:29.379072 Epoch 250, Training Loss 0.5193455026811346\n",
      "2022-03-26 18:02:29.402548 Epoch 250, Training Loss 0.519967527035862\n",
      "2022-03-26 18:02:29.429468 Epoch 250, Training Loss 0.520787743580006\n",
      "2022-03-26 18:02:29.453455 Epoch 250, Training Loss 0.5214012727484374\n",
      "2022-03-26 18:02:29.477571 Epoch 250, Training Loss 0.5219931357047137\n",
      "2022-03-26 18:02:29.500970 Epoch 250, Training Loss 0.5225355769972058\n",
      "2022-03-26 18:02:29.523963 Epoch 250, Training Loss 0.5233636578483045\n",
      "2022-03-26 18:02:29.553053 Epoch 250, Training Loss 0.5242191430400399\n",
      "2022-03-26 18:02:29.576642 Epoch 250, Training Loss 0.5251628517952112\n",
      "2022-03-26 18:02:29.599342 Epoch 250, Training Loss 0.5258000006379984\n",
      "2022-03-26 18:02:29.623637 Epoch 250, Training Loss 0.5264956565845348\n",
      "2022-03-26 18:02:29.650560 Epoch 250, Training Loss 0.5274264748825137\n",
      "2022-03-26 18:02:29.681624 Epoch 250, Training Loss 0.528227828995651\n",
      "2022-03-26 18:02:29.705238 Epoch 250, Training Loss 0.5291661323260164\n",
      "2022-03-26 18:02:29.728517 Epoch 250, Training Loss 0.530071250960955\n",
      "2022-03-26 18:02:29.751215 Epoch 250, Training Loss 0.5306749065666247\n",
      "2022-03-26 18:02:29.784020 Epoch 250, Training Loss 0.5312785630488335\n",
      "2022-03-26 18:02:29.807757 Epoch 250, Training Loss 0.5319153554070636\n",
      "2022-03-26 18:02:29.831149 Epoch 250, Training Loss 0.5327494445892856\n",
      "2022-03-26 18:02:29.855184 Epoch 250, Training Loss 0.5335489457754223\n",
      "2022-03-26 18:02:29.878451 Epoch 250, Training Loss 0.5344267699800794\n",
      "2022-03-26 18:02:29.901593 Epoch 250, Training Loss 0.5352290664868586\n",
      "2022-03-26 18:02:29.931692 Epoch 250, Training Loss 0.5359542035614439\n",
      "2022-03-26 18:02:29.954332 Epoch 250, Training Loss 0.536581975229256\n",
      "2022-03-26 18:02:29.977384 Epoch 250, Training Loss 0.5372298237536569\n",
      "2022-03-26 18:02:30.014143 Epoch 250, Training Loss 0.537813840695964\n",
      "2022-03-26 18:02:30.043063 Epoch 250, Training Loss 0.5385182281131939\n",
      "2022-03-26 18:02:30.071306 Epoch 250, Training Loss 0.5394370865334025\n",
      "2022-03-26 18:02:30.094439 Epoch 250, Training Loss 0.5401128889502161\n",
      "2022-03-26 18:02:30.119402 Epoch 250, Training Loss 0.5407102073321257\n",
      "2022-03-26 18:02:30.142473 Epoch 250, Training Loss 0.5412789218489776\n",
      "2022-03-26 18:02:30.164935 Epoch 250, Training Loss 0.5420854460171727\n",
      "2022-03-26 18:02:30.187439 Epoch 250, Training Loss 0.5428528850874328\n",
      "2022-03-26 18:02:30.210335 Epoch 250, Training Loss 0.5434902620208842\n",
      "2022-03-26 18:02:30.244426 Epoch 250, Training Loss 0.5443582952098773\n",
      "2022-03-26 18:02:30.268035 Epoch 250, Training Loss 0.5450519060768435\n",
      "2022-03-26 18:02:30.291426 Epoch 250, Training Loss 0.5456729151327592\n",
      "2022-03-26 18:02:30.314976 Epoch 250, Training Loss 0.5460409487757232\n",
      "2022-03-26 18:02:30.340391 Epoch 250, Training Loss 0.5465244238486375\n",
      "2022-03-26 18:02:30.362532 Epoch 250, Training Loss 0.5471712062730814\n",
      "2022-03-26 18:02:30.387419 Epoch 250, Training Loss 0.5478054235505936\n",
      "2022-03-26 18:02:30.409762 Epoch 250, Training Loss 0.5484666730589269\n",
      "2022-03-26 18:02:30.435553 Epoch 250, Training Loss 0.5491270752209226\n",
      "2022-03-26 18:02:30.464566 Epoch 250, Training Loss 0.5498702157183987\n",
      "2022-03-26 18:02:30.488858 Epoch 250, Training Loss 0.5508027879325935\n",
      "2022-03-26 18:02:30.512213 Epoch 250, Training Loss 0.551450400172597\n",
      "2022-03-26 18:02:30.535244 Epoch 250, Training Loss 0.5520402060826416\n",
      "2022-03-26 18:02:30.561555 Epoch 250, Training Loss 0.5525598566397987\n",
      "2022-03-26 18:02:30.584092 Epoch 250, Training Loss 0.5530842114287569\n",
      "2022-03-26 18:02:30.606630 Epoch 250, Training Loss 0.5536220364863306\n",
      "2022-03-26 18:02:30.629466 Epoch 250, Training Loss 0.5544429621123292\n",
      "2022-03-26 18:02:30.652243 Epoch 250, Training Loss 0.5552378042274729\n",
      "2022-03-26 18:02:30.679603 Epoch 250, Training Loss 0.5558555516821649\n",
      "2022-03-26 18:02:30.709206 Epoch 250, Training Loss 0.556333206460604\n",
      "2022-03-26 18:02:30.718892 Epoch 250, Training Loss 0.5571895290137557\n",
      "2022-03-26 18:17:09.782934 Epoch 300, Training Loss 0.0005527866618407657\n",
      "2022-03-26 18:17:09.807517 Epoch 300, Training Loss 0.0014450095803536418\n",
      "2022-03-26 18:17:09.831495 Epoch 300, Training Loss 0.0020014399762653635\n",
      "2022-03-26 18:17:09.855340 Epoch 300, Training Loss 0.002563435022178513\n",
      "2022-03-26 18:17:09.878380 Epoch 300, Training Loss 0.0034148538356546855\n",
      "2022-03-26 18:17:09.902655 Epoch 300, Training Loss 0.003941103663590863\n",
      "2022-03-26 18:17:09.927150 Epoch 300, Training Loss 0.004451647972511818\n",
      "2022-03-26 18:17:09.951321 Epoch 300, Training Loss 0.005086859969226905\n",
      "2022-03-26 18:17:09.974740 Epoch 300, Training Loss 0.005833839249732854\n",
      "2022-03-26 18:17:10.013322 Epoch 300, Training Loss 0.006494869852005063\n",
      "2022-03-26 18:17:10.037809 Epoch 300, Training Loss 0.007096860719763715\n",
      "2022-03-26 18:17:10.061261 Epoch 300, Training Loss 0.007664028793344717\n",
      "2022-03-26 18:17:10.085031 Epoch 300, Training Loss 0.008243608581440529\n",
      "2022-03-26 18:17:10.108590 Epoch 300, Training Loss 0.00909935040851993\n",
      "2022-03-26 18:17:10.131718 Epoch 300, Training Loss 0.009598413201244285\n",
      "2022-03-26 18:17:10.155213 Epoch 300, Training Loss 0.010109563541534307\n",
      "2022-03-26 18:17:10.184764 Epoch 300, Training Loss 0.011045323003588431\n",
      "2022-03-26 18:17:10.219443 Epoch 300, Training Loss 0.011683440726736317\n",
      "2022-03-26 18:17:10.251711 Epoch 300, Training Loss 0.01251640359459021\n",
      "2022-03-26 18:17:10.274496 Epoch 300, Training Loss 0.013354439031132651\n",
      "2022-03-26 18:17:10.297560 Epoch 300, Training Loss 0.014120911164661807\n",
      "2022-03-26 18:17:10.319989 Epoch 300, Training Loss 0.015174815340725052\n",
      "2022-03-26 18:17:10.346180 Epoch 300, Training Loss 0.015783825379503354\n",
      "2022-03-26 18:17:10.369145 Epoch 300, Training Loss 0.016551993425239993\n",
      "2022-03-26 18:17:10.392205 Epoch 300, Training Loss 0.017589495957964826\n",
      "2022-03-26 18:17:10.415531 Epoch 300, Training Loss 0.01800855319670704\n",
      "2022-03-26 18:17:10.447238 Epoch 300, Training Loss 0.018568502515173325\n",
      "2022-03-26 18:17:10.471815 Epoch 300, Training Loss 0.01935536797394228\n",
      "2022-03-26 18:17:10.495266 Epoch 300, Training Loss 0.020056366082042684\n",
      "2022-03-26 18:17:10.519135 Epoch 300, Training Loss 0.020922766317187064\n",
      "2022-03-26 18:17:10.544366 Epoch 300, Training Loss 0.021600656802087184\n",
      "2022-03-26 18:17:10.570530 Epoch 300, Training Loss 0.02227196806227155\n",
      "2022-03-26 18:17:10.595626 Epoch 300, Training Loss 0.022953714251213366\n",
      "2022-03-26 18:17:10.619618 Epoch 300, Training Loss 0.023631586900452518\n",
      "2022-03-26 18:17:10.642938 Epoch 300, Training Loss 0.024250885066778763\n",
      "2022-03-26 18:17:10.675613 Epoch 300, Training Loss 0.024968732996364994\n",
      "2022-03-26 18:17:10.700592 Epoch 300, Training Loss 0.025444164757838336\n",
      "2022-03-26 18:17:10.724026 Epoch 300, Training Loss 0.02604718266240776\n",
      "2022-03-26 18:17:10.747102 Epoch 300, Training Loss 0.02655129123221883\n",
      "2022-03-26 18:17:10.773461 Epoch 300, Training Loss 0.027263344706171918\n",
      "2022-03-26 18:17:10.798225 Epoch 300, Training Loss 0.02781899460136433\n",
      "2022-03-26 18:17:10.822486 Epoch 300, Training Loss 0.0286720351642355\n",
      "2022-03-26 18:17:10.846190 Epoch 300, Training Loss 0.029312100023259898\n",
      "2022-03-26 18:17:10.870396 Epoch 300, Training Loss 0.029833910791465388\n",
      "2022-03-26 18:17:10.904923 Epoch 300, Training Loss 0.030418394700340603\n",
      "2022-03-26 18:17:10.929654 Epoch 300, Training Loss 0.031114335712569448\n",
      "2022-03-26 18:17:10.956344 Epoch 300, Training Loss 0.031939200885460504\n",
      "2022-03-26 18:17:10.981341 Epoch 300, Training Loss 0.03255349005122319\n",
      "2022-03-26 18:17:11.004718 Epoch 300, Training Loss 0.03345904253480379\n",
      "2022-03-26 18:17:11.028732 Epoch 300, Training Loss 0.034243959333280775\n",
      "2022-03-26 18:17:11.051817 Epoch 300, Training Loss 0.034898807211300294\n",
      "2022-03-26 18:17:11.074942 Epoch 300, Training Loss 0.03562909116982804\n",
      "2022-03-26 18:17:11.098180 Epoch 300, Training Loss 0.03632497425426912\n",
      "2022-03-26 18:17:11.131554 Epoch 300, Training Loss 0.037016518471186116\n",
      "2022-03-26 18:17:11.155554 Epoch 300, Training Loss 0.037848804574793254\n",
      "2022-03-26 18:17:11.179092 Epoch 300, Training Loss 0.03847095869538729\n",
      "2022-03-26 18:17:11.202919 Epoch 300, Training Loss 0.039118812021696966\n",
      "2022-03-26 18:17:11.242088 Epoch 300, Training Loss 0.03961173294450316\n",
      "2022-03-26 18:17:11.268762 Epoch 300, Training Loss 0.04038117006611641\n",
      "2022-03-26 18:17:11.293702 Epoch 300, Training Loss 0.041283402067925926\n",
      "2022-03-26 18:17:11.318073 Epoch 300, Training Loss 0.04206328509408799\n",
      "2022-03-26 18:17:11.349614 Epoch 300, Training Loss 0.043254927448604416\n",
      "2022-03-26 18:17:11.375825 Epoch 300, Training Loss 0.043762582304227686\n",
      "2022-03-26 18:17:11.398559 Epoch 300, Training Loss 0.044543720991410254\n",
      "2022-03-26 18:17:11.420981 Epoch 300, Training Loss 0.045028042114909045\n",
      "2022-03-26 18:17:11.444511 Epoch 300, Training Loss 0.04567469046701251\n",
      "2022-03-26 18:17:11.471336 Epoch 300, Training Loss 0.046275278262774965\n",
      "2022-03-26 18:17:11.494746 Epoch 300, Training Loss 0.04697807464758148\n",
      "2022-03-26 18:17:11.518634 Epoch 300, Training Loss 0.04775998293591277\n",
      "2022-03-26 18:17:11.542538 Epoch 300, Training Loss 0.04847584302772951\n",
      "2022-03-26 18:17:11.573594 Epoch 300, Training Loss 0.04926724049746228\n",
      "2022-03-26 18:17:11.597540 Epoch 300, Training Loss 0.05004345974349\n",
      "2022-03-26 18:17:11.621583 Epoch 300, Training Loss 0.050682903631873756\n",
      "2022-03-26 18:17:11.645156 Epoch 300, Training Loss 0.051406682757160545\n",
      "2022-03-26 18:17:11.668537 Epoch 300, Training Loss 0.052081983565064646\n",
      "2022-03-26 18:17:11.692782 Epoch 300, Training Loss 0.0527245876429331\n",
      "2022-03-26 18:17:11.718073 Epoch 300, Training Loss 0.05328429358847001\n",
      "2022-03-26 18:17:11.744972 Epoch 300, Training Loss 0.0539651176584956\n",
      "2022-03-26 18:17:11.769054 Epoch 300, Training Loss 0.05450990159645715\n",
      "2022-03-26 18:17:11.800516 Epoch 300, Training Loss 0.055246196401393624\n",
      "2022-03-26 18:17:11.824566 Epoch 300, Training Loss 0.05583857274268899\n",
      "2022-03-26 18:17:11.848597 Epoch 300, Training Loss 0.0563903394562509\n",
      "2022-03-26 18:17:11.872604 Epoch 300, Training Loss 0.057101130028210025\n",
      "2022-03-26 18:17:11.896391 Epoch 300, Training Loss 0.05801282247618946\n",
      "2022-03-26 18:17:11.920817 Epoch 300, Training Loss 0.058617391930821605\n",
      "2022-03-26 18:17:11.946228 Epoch 300, Training Loss 0.059282709418050465\n",
      "2022-03-26 18:17:11.973022 Epoch 300, Training Loss 0.06000473920036765\n",
      "2022-03-26 18:17:11.997369 Epoch 300, Training Loss 0.06083482000833887\n",
      "2022-03-26 18:17:12.031456 Epoch 300, Training Loss 0.061273734473511386\n",
      "2022-03-26 18:17:12.055881 Epoch 300, Training Loss 0.061893096322293784\n",
      "2022-03-26 18:17:12.079282 Epoch 300, Training Loss 0.06260040246159829\n",
      "2022-03-26 18:17:12.110199 Epoch 300, Training Loss 0.06324976473055838\n",
      "2022-03-26 18:17:12.135399 Epoch 300, Training Loss 0.06408150082506488\n",
      "2022-03-26 18:17:12.159042 Epoch 300, Training Loss 0.06489689930168259\n",
      "2022-03-26 18:17:12.182767 Epoch 300, Training Loss 0.06544116726311881\n",
      "2022-03-26 18:17:12.207740 Epoch 300, Training Loss 0.06599249288706524\n",
      "2022-03-26 18:17:12.234464 Epoch 300, Training Loss 0.06660489429293386\n",
      "2022-03-26 18:17:12.272443 Epoch 300, Training Loss 0.06719505428658117\n",
      "2022-03-26 18:17:12.295356 Epoch 300, Training Loss 0.0677301731255963\n",
      "2022-03-26 18:17:12.318815 Epoch 300, Training Loss 0.06843376395952366\n",
      "2022-03-26 18:17:12.344504 Epoch 300, Training Loss 0.06907341859834579\n",
      "2022-03-26 18:17:12.368089 Epoch 300, Training Loss 0.06990213718865533\n",
      "2022-03-26 18:17:12.390325 Epoch 300, Training Loss 0.07059463767139502\n",
      "2022-03-26 18:17:12.412816 Epoch 300, Training Loss 0.07131850826160988\n",
      "2022-03-26 18:17:12.435053 Epoch 300, Training Loss 0.0720596792905227\n",
      "2022-03-26 18:17:12.470442 Epoch 300, Training Loss 0.07267242792012442\n",
      "2022-03-26 18:17:12.493348 Epoch 300, Training Loss 0.07336333798020697\n",
      "2022-03-26 18:17:12.515824 Epoch 300, Training Loss 0.073933163834045\n",
      "2022-03-26 18:17:12.538600 Epoch 300, Training Loss 0.07465056625321088\n",
      "2022-03-26 18:17:12.561141 Epoch 300, Training Loss 0.07520980957676382\n",
      "2022-03-26 18:17:12.583757 Epoch 300, Training Loss 0.07588100277097024\n",
      "2022-03-26 18:17:12.606170 Epoch 300, Training Loss 0.07631520804999124\n",
      "2022-03-26 18:17:12.634045 Epoch 300, Training Loss 0.07682554400942819\n",
      "2022-03-26 18:17:12.659570 Epoch 300, Training Loss 0.07729537445870811\n",
      "2022-03-26 18:17:12.694932 Epoch 300, Training Loss 0.07786217983573904\n",
      "2022-03-26 18:17:12.718798 Epoch 300, Training Loss 0.07854310985263961\n",
      "2022-03-26 18:17:12.741179 Epoch 300, Training Loss 0.07896460596557774\n",
      "2022-03-26 18:17:12.763781 Epoch 300, Training Loss 0.07959029692060807\n",
      "2022-03-26 18:17:12.786037 Epoch 300, Training Loss 0.08005903402100438\n",
      "2022-03-26 18:17:12.808150 Epoch 300, Training Loss 0.08062603905835115\n",
      "2022-03-26 18:17:12.830408 Epoch 300, Training Loss 0.08127945993105164\n",
      "2022-03-26 18:17:12.852881 Epoch 300, Training Loss 0.08211006872031999\n",
      "2022-03-26 18:17:12.875148 Epoch 300, Training Loss 0.08256550876380843\n",
      "2022-03-26 18:17:12.897262 Epoch 300, Training Loss 0.08335695585326465\n",
      "2022-03-26 18:17:12.928067 Epoch 300, Training Loss 0.0838732606995746\n",
      "2022-03-26 18:17:12.950544 Epoch 300, Training Loss 0.08472123341944517\n",
      "2022-03-26 18:17:12.973399 Epoch 300, Training Loss 0.08563721618231605\n",
      "2022-03-26 18:17:12.996275 Epoch 300, Training Loss 0.08669266138997529\n",
      "2022-03-26 18:17:13.025499 Epoch 300, Training Loss 0.08721651262639428\n",
      "2022-03-26 18:17:13.048051 Epoch 300, Training Loss 0.08771265441042078\n",
      "2022-03-26 18:17:13.071592 Epoch 300, Training Loss 0.0884354088236304\n",
      "2022-03-26 18:17:13.094251 Epoch 300, Training Loss 0.08907163169835229\n",
      "2022-03-26 18:17:13.131901 Epoch 300, Training Loss 0.08987782422996238\n",
      "2022-03-26 18:17:13.154837 Epoch 300, Training Loss 0.09064706919900596\n",
      "2022-03-26 18:17:13.177345 Epoch 300, Training Loss 0.09149113339383888\n",
      "2022-03-26 18:17:13.199487 Epoch 300, Training Loss 0.09222772004811661\n",
      "2022-03-26 18:17:13.226028 Epoch 300, Training Loss 0.09296048781298616\n",
      "2022-03-26 18:17:13.248293 Epoch 300, Training Loss 0.09360217678424952\n",
      "2022-03-26 18:17:13.270741 Epoch 300, Training Loss 0.09421224267129093\n",
      "2022-03-26 18:17:13.303781 Epoch 300, Training Loss 0.09474382200814269\n",
      "2022-03-26 18:17:13.327148 Epoch 300, Training Loss 0.09538460372354063\n",
      "2022-03-26 18:17:13.359590 Epoch 300, Training Loss 0.09628573883220058\n",
      "2022-03-26 18:17:13.383074 Epoch 300, Training Loss 0.0968925898413524\n",
      "2022-03-26 18:17:13.405550 Epoch 300, Training Loss 0.09760048387148192\n",
      "2022-03-26 18:17:13.427957 Epoch 300, Training Loss 0.09830699861049652\n",
      "2022-03-26 18:17:13.451363 Epoch 300, Training Loss 0.09903024434281127\n",
      "2022-03-26 18:17:13.474990 Epoch 300, Training Loss 0.09941791905009228\n",
      "2022-03-26 18:17:13.498677 Epoch 300, Training Loss 0.10002904234792265\n",
      "2022-03-26 18:17:13.522091 Epoch 300, Training Loss 0.10074168394136307\n",
      "2022-03-26 18:17:13.552601 Epoch 300, Training Loss 0.10142113961984435\n",
      "2022-03-26 18:17:13.586348 Epoch 300, Training Loss 0.10209326819538156\n",
      "2022-03-26 18:17:13.610805 Epoch 300, Training Loss 0.10269573990188902\n",
      "2022-03-26 18:17:13.635144 Epoch 300, Training Loss 0.10338433193581184\n",
      "2022-03-26 18:17:13.658658 Epoch 300, Training Loss 0.1042199060511406\n",
      "2022-03-26 18:17:13.682480 Epoch 300, Training Loss 0.1051673612097645\n",
      "2022-03-26 18:17:13.707591 Epoch 300, Training Loss 0.10586307691338727\n",
      "2022-03-26 18:17:13.731053 Epoch 300, Training Loss 0.10637910507828988\n",
      "2022-03-26 18:17:13.755154 Epoch 300, Training Loss 0.10722562388690841\n",
      "2022-03-26 18:17:13.778789 Epoch 300, Training Loss 0.10783076061464636\n",
      "2022-03-26 18:17:13.810984 Epoch 300, Training Loss 0.10874169344639839\n",
      "2022-03-26 18:17:13.835443 Epoch 300, Training Loss 0.1095931929014528\n",
      "2022-03-26 18:17:13.859115 Epoch 300, Training Loss 0.11011916368513766\n",
      "2022-03-26 18:17:13.882471 Epoch 300, Training Loss 0.11074547789743185\n",
      "2022-03-26 18:17:13.905820 Epoch 300, Training Loss 0.11130746425417683\n",
      "2022-03-26 18:17:13.937376 Epoch 300, Training Loss 0.11180379719990294\n",
      "2022-03-26 18:17:13.963342 Epoch 300, Training Loss 0.11245409149647978\n",
      "2022-03-26 18:17:13.987483 Epoch 300, Training Loss 0.11312646908528359\n",
      "2022-03-26 18:17:14.011167 Epoch 300, Training Loss 0.1135456840629163\n",
      "2022-03-26 18:17:14.047009 Epoch 300, Training Loss 0.11419471759168083\n",
      "2022-03-26 18:17:14.071516 Epoch 300, Training Loss 0.11468567621067662\n",
      "2022-03-26 18:17:14.095150 Epoch 300, Training Loss 0.11537059493686842\n",
      "2022-03-26 18:17:14.119470 Epoch 300, Training Loss 0.116138977239199\n",
      "2022-03-26 18:17:14.143730 Epoch 300, Training Loss 0.11691818548285443\n",
      "2022-03-26 18:17:14.167377 Epoch 300, Training Loss 0.11777566095142414\n",
      "2022-03-26 18:17:14.199590 Epoch 300, Training Loss 0.11823708196277813\n",
      "2022-03-26 18:17:14.223608 Epoch 300, Training Loss 0.11875580788573341\n",
      "2022-03-26 18:17:14.248104 Epoch 300, Training Loss 0.11947399072939782\n",
      "2022-03-26 18:17:14.281494 Epoch 300, Training Loss 0.12000916452359056\n",
      "2022-03-26 18:17:14.310099 Epoch 300, Training Loss 0.12073073172203415\n",
      "2022-03-26 18:17:14.341264 Epoch 300, Training Loss 0.12151588785373951\n",
      "2022-03-26 18:17:14.366075 Epoch 300, Training Loss 0.12223615442090632\n",
      "2022-03-26 18:17:14.390190 Epoch 300, Training Loss 0.12315196561081634\n",
      "2022-03-26 18:17:14.414296 Epoch 300, Training Loss 0.1236508521048919\n",
      "2022-03-26 18:17:14.437584 Epoch 300, Training Loss 0.12446901686203754\n",
      "2022-03-26 18:17:14.464818 Epoch 300, Training Loss 0.12498532102236053\n",
      "2022-03-26 18:17:14.493002 Epoch 300, Training Loss 0.1256028345554991\n",
      "2022-03-26 18:17:14.519749 Epoch 300, Training Loss 0.1264757989236461\n",
      "2022-03-26 18:17:14.543457 Epoch 300, Training Loss 0.12714759624370223\n",
      "2022-03-26 18:17:14.567148 Epoch 300, Training Loss 0.12759019381097517\n",
      "2022-03-26 18:17:14.591483 Epoch 300, Training Loss 0.12821089405842753\n",
      "2022-03-26 18:17:14.615270 Epoch 300, Training Loss 0.12890964632144059\n",
      "2022-03-26 18:17:14.639132 Epoch 300, Training Loss 0.12964048547208157\n",
      "2022-03-26 18:17:14.663121 Epoch 300, Training Loss 0.13013368985994392\n",
      "2022-03-26 18:17:14.692450 Epoch 300, Training Loss 0.13090583266657027\n",
      "2022-03-26 18:17:14.730076 Epoch 300, Training Loss 0.1314537594156802\n",
      "2022-03-26 18:17:14.754600 Epoch 300, Training Loss 0.13237271600824488\n",
      "2022-03-26 18:17:14.778084 Epoch 300, Training Loss 0.1330856128650553\n",
      "2022-03-26 18:17:14.801761 Epoch 300, Training Loss 0.13384850780524865\n",
      "2022-03-26 18:17:14.825633 Epoch 300, Training Loss 0.13453795313072936\n",
      "2022-03-26 18:17:14.849146 Epoch 300, Training Loss 0.1352514888700622\n",
      "2022-03-26 18:17:14.873088 Epoch 300, Training Loss 0.13611619265945366\n",
      "2022-03-26 18:17:14.896524 Epoch 300, Training Loss 0.13669828220706462\n",
      "2022-03-26 18:17:14.922399 Epoch 300, Training Loss 0.1374352840358949\n",
      "2022-03-26 18:17:14.956191 Epoch 300, Training Loss 0.1379875562075154\n",
      "2022-03-26 18:17:14.982170 Epoch 300, Training Loss 0.13867379004693092\n",
      "2022-03-26 18:17:15.005851 Epoch 300, Training Loss 0.13917350094489125\n",
      "2022-03-26 18:17:15.029001 Epoch 300, Training Loss 0.13971784146850372\n",
      "2022-03-26 18:17:15.052899 Epoch 300, Training Loss 0.14029429837718338\n",
      "2022-03-26 18:17:15.076064 Epoch 300, Training Loss 0.14082391601999092\n",
      "2022-03-26 18:17:15.099852 Epoch 300, Training Loss 0.14143416296947947\n",
      "2022-03-26 18:17:15.125251 Epoch 300, Training Loss 0.14191076437683056\n",
      "2022-03-26 18:17:15.149223 Epoch 300, Training Loss 0.14260057487603647\n",
      "2022-03-26 18:17:15.189609 Epoch 300, Training Loss 0.14299998171341693\n",
      "2022-03-26 18:17:15.213067 Epoch 300, Training Loss 0.14377988257523996\n",
      "2022-03-26 18:17:15.236615 Epoch 300, Training Loss 0.14442177444620205\n",
      "2022-03-26 18:17:15.259593 Epoch 300, Training Loss 0.14540262802330126\n",
      "2022-03-26 18:17:15.282318 Epoch 300, Training Loss 0.14589914866268178\n",
      "2022-03-26 18:17:15.305273 Epoch 300, Training Loss 0.1466737495892493\n",
      "2022-03-26 18:17:15.336359 Epoch 300, Training Loss 0.1470869196116772\n",
      "2022-03-26 18:17:15.362727 Epoch 300, Training Loss 0.1477923392868408\n",
      "2022-03-26 18:17:15.385986 Epoch 300, Training Loss 0.14833268218333154\n",
      "2022-03-26 18:17:15.417682 Epoch 300, Training Loss 0.1490412660876808\n",
      "2022-03-26 18:17:15.442688 Epoch 300, Training Loss 0.14982970260903047\n",
      "2022-03-26 18:17:15.468320 Epoch 300, Training Loss 0.15045940079499998\n",
      "2022-03-26 18:17:15.492323 Epoch 300, Training Loss 0.15103306817581585\n",
      "2022-03-26 18:17:15.515368 Epoch 300, Training Loss 0.15156919945536368\n",
      "2022-03-26 18:17:15.538149 Epoch 300, Training Loss 0.15225571005240732\n",
      "2022-03-26 18:17:15.560589 Epoch 300, Training Loss 0.15282420295736063\n",
      "2022-03-26 18:17:15.583014 Epoch 300, Training Loss 0.1536797197044963\n",
      "2022-03-26 18:17:15.605408 Epoch 300, Training Loss 0.15422141673924672\n",
      "2022-03-26 18:17:15.638083 Epoch 300, Training Loss 0.15516902144302797\n",
      "2022-03-26 18:17:15.662122 Epoch 300, Training Loss 0.1559804642901701\n",
      "2022-03-26 18:17:15.684758 Epoch 300, Training Loss 0.1571436573934677\n",
      "2022-03-26 18:17:15.708813 Epoch 300, Training Loss 0.15796028104279658\n",
      "2022-03-26 18:17:15.731867 Epoch 300, Training Loss 0.1587682245942333\n",
      "2022-03-26 18:17:15.754667 Epoch 300, Training Loss 0.1595655653025488\n",
      "2022-03-26 18:17:15.777385 Epoch 300, Training Loss 0.16006978953738346\n",
      "2022-03-26 18:17:15.799965 Epoch 300, Training Loss 0.1606174635384089\n",
      "2022-03-26 18:17:15.822424 Epoch 300, Training Loss 0.16121758333862285\n",
      "2022-03-26 18:17:15.853692 Epoch 300, Training Loss 0.1619205685985058\n",
      "2022-03-26 18:17:15.880103 Epoch 300, Training Loss 0.16263673166789666\n",
      "2022-03-26 18:17:15.903197 Epoch 300, Training Loss 0.1632510218245294\n",
      "2022-03-26 18:17:15.926072 Epoch 300, Training Loss 0.16382626320242577\n",
      "2022-03-26 18:17:15.949416 Epoch 300, Training Loss 0.16447811499428566\n",
      "2022-03-26 18:17:15.972227 Epoch 300, Training Loss 0.1652117054099622\n",
      "2022-03-26 18:17:15.998966 Epoch 300, Training Loss 0.16612827743563202\n",
      "2022-03-26 18:17:16.022164 Epoch 300, Training Loss 0.16690702347651773\n",
      "2022-03-26 18:17:16.051670 Epoch 300, Training Loss 0.1676332086629575\n",
      "2022-03-26 18:17:16.087931 Epoch 300, Training Loss 0.16820411940517327\n",
      "2022-03-26 18:17:16.111923 Epoch 300, Training Loss 0.16880301521409807\n",
      "2022-03-26 18:17:16.136472 Epoch 300, Training Loss 0.16938781124704025\n",
      "2022-03-26 18:17:16.159424 Epoch 300, Training Loss 0.1700084171712856\n",
      "2022-03-26 18:17:16.182592 Epoch 300, Training Loss 0.17074354564594796\n",
      "2022-03-26 18:17:16.208646 Epoch 300, Training Loss 0.1712855311763256\n",
      "2022-03-26 18:17:16.231126 Epoch 300, Training Loss 0.1717699733003021\n",
      "2022-03-26 18:17:16.254076 Epoch 300, Training Loss 0.17257002541018873\n",
      "2022-03-26 18:17:16.276681 Epoch 300, Training Loss 0.17312303147352565\n",
      "2022-03-26 18:17:16.307169 Epoch 300, Training Loss 0.1738855674900972\n",
      "2022-03-26 18:17:16.332616 Epoch 300, Training Loss 0.17479470128293537\n",
      "2022-03-26 18:17:16.360260 Epoch 300, Training Loss 0.17560049838117323\n",
      "2022-03-26 18:17:16.387459 Epoch 300, Training Loss 0.17611533910264748\n",
      "2022-03-26 18:17:16.409804 Epoch 300, Training Loss 0.17663600198600604\n",
      "2022-03-26 18:17:16.432623 Epoch 300, Training Loss 0.1772301500791784\n",
      "2022-03-26 18:17:16.455598 Epoch 300, Training Loss 0.17781732660120406\n",
      "2022-03-26 18:17:16.480045 Epoch 300, Training Loss 0.1784405955649398\n",
      "2022-03-26 18:17:16.502372 Epoch 300, Training Loss 0.1793943025800578\n",
      "2022-03-26 18:17:16.538222 Epoch 300, Training Loss 0.17983359997839574\n",
      "2022-03-26 18:17:16.560891 Epoch 300, Training Loss 0.18064934282046755\n",
      "2022-03-26 18:17:16.583130 Epoch 300, Training Loss 0.18144935552421435\n",
      "2022-03-26 18:17:16.606424 Epoch 300, Training Loss 0.18238554266102783\n",
      "2022-03-26 18:17:16.629779 Epoch 300, Training Loss 0.18271233229076161\n",
      "2022-03-26 18:17:16.652600 Epoch 300, Training Loss 0.18361777333957155\n",
      "2022-03-26 18:17:16.675437 Epoch 300, Training Loss 0.18453492952124848\n",
      "2022-03-26 18:17:16.699578 Epoch 300, Training Loss 0.18514554183501417\n",
      "2022-03-26 18:17:16.727332 Epoch 300, Training Loss 0.18572881173752154\n",
      "2022-03-26 18:17:16.756466 Epoch 300, Training Loss 0.18628108352803818\n",
      "2022-03-26 18:17:16.786276 Epoch 300, Training Loss 0.18706216985154944\n",
      "2022-03-26 18:17:16.809229 Epoch 300, Training Loss 0.1877319077625299\n",
      "2022-03-26 18:17:16.831772 Epoch 300, Training Loss 0.18810440032073603\n",
      "2022-03-26 18:17:16.854436 Epoch 300, Training Loss 0.18868182077432227\n",
      "2022-03-26 18:17:16.877061 Epoch 300, Training Loss 0.18912062220408787\n",
      "2022-03-26 18:17:16.899626 Epoch 300, Training Loss 0.18994620690107955\n",
      "2022-03-26 18:17:16.922152 Epoch 300, Training Loss 0.19072450293451929\n",
      "2022-03-26 18:17:16.952569 Epoch 300, Training Loss 0.19131465450577115\n",
      "2022-03-26 18:17:16.984876 Epoch 300, Training Loss 0.1917223483324051\n",
      "2022-03-26 18:17:17.009664 Epoch 300, Training Loss 0.19240045574162623\n",
      "2022-03-26 18:17:17.032022 Epoch 300, Training Loss 0.19308329821395143\n",
      "2022-03-26 18:17:17.055191 Epoch 300, Training Loss 0.1937075694046362\n",
      "2022-03-26 18:17:17.077475 Epoch 300, Training Loss 0.1944163553702557\n",
      "2022-03-26 18:17:17.099689 Epoch 300, Training Loss 0.19486567823935652\n",
      "2022-03-26 18:17:17.122850 Epoch 300, Training Loss 0.1954411534244752\n",
      "2022-03-26 18:17:17.145847 Epoch 300, Training Loss 0.19634487043561227\n",
      "2022-03-26 18:17:17.167986 Epoch 300, Training Loss 0.19698791087740827\n",
      "2022-03-26 18:17:17.197099 Epoch 300, Training Loss 0.19793471876922472\n",
      "2022-03-26 18:17:17.222640 Epoch 300, Training Loss 0.1987743851778757\n",
      "2022-03-26 18:17:17.250196 Epoch 300, Training Loss 0.1993691508117539\n",
      "2022-03-26 18:17:17.272331 Epoch 300, Training Loss 0.20005434217965207\n",
      "2022-03-26 18:17:17.295122 Epoch 300, Training Loss 0.20074911731893144\n",
      "2022-03-26 18:17:17.317211 Epoch 300, Training Loss 0.20139953006259012\n",
      "2022-03-26 18:17:17.342546 Epoch 300, Training Loss 0.20202477112450562\n",
      "2022-03-26 18:17:17.365076 Epoch 300, Training Loss 0.20257240392820305\n",
      "2022-03-26 18:17:17.398193 Epoch 300, Training Loss 0.20300984241620965\n",
      "2022-03-26 18:17:17.433023 Epoch 300, Training Loss 0.20378559656307826\n",
      "2022-03-26 18:17:17.456701 Epoch 300, Training Loss 0.20436608677020157\n",
      "2022-03-26 18:17:17.479622 Epoch 300, Training Loss 0.20498775913739753\n",
      "2022-03-26 18:17:17.502882 Epoch 300, Training Loss 0.2055067830454663\n",
      "2022-03-26 18:17:17.525348 Epoch 300, Training Loss 0.20605760969011985\n",
      "2022-03-26 18:17:17.548543 Epoch 300, Training Loss 0.20675005033955246\n",
      "2022-03-26 18:17:17.570920 Epoch 300, Training Loss 0.2074986038839116\n",
      "2022-03-26 18:17:17.593504 Epoch 300, Training Loss 0.20835763943927063\n",
      "2022-03-26 18:17:17.615743 Epoch 300, Training Loss 0.20880011730181897\n",
      "2022-03-26 18:17:17.641994 Epoch 300, Training Loss 0.20934595137148562\n",
      "2022-03-26 18:17:17.667890 Epoch 300, Training Loss 0.20996760956161772\n",
      "2022-03-26 18:17:17.690667 Epoch 300, Training Loss 0.21075312469316565\n",
      "2022-03-26 18:17:17.717356 Epoch 300, Training Loss 0.2114276626073491\n",
      "2022-03-26 18:17:17.740699 Epoch 300, Training Loss 0.21184795645191845\n",
      "2022-03-26 18:17:17.763372 Epoch 300, Training Loss 0.21249385113301483\n",
      "2022-03-26 18:17:17.786167 Epoch 300, Training Loss 0.2131528345215351\n",
      "2022-03-26 18:17:17.809197 Epoch 300, Training Loss 0.21360755915684468\n",
      "2022-03-26 18:17:17.831893 Epoch 300, Training Loss 0.21423207966567914\n",
      "2022-03-26 18:17:17.866735 Epoch 300, Training Loss 0.2148359927451214\n",
      "2022-03-26 18:17:17.891088 Epoch 300, Training Loss 0.21571687142105053\n",
      "2022-03-26 18:17:17.919658 Epoch 300, Training Loss 0.21663539325032394\n",
      "2022-03-26 18:17:17.943340 Epoch 300, Training Loss 0.21728917746745108\n",
      "2022-03-26 18:17:17.967844 Epoch 300, Training Loss 0.21805079662433977\n",
      "2022-03-26 18:17:17.991703 Epoch 300, Training Loss 0.2186067996503752\n",
      "2022-03-26 18:17:18.015339 Epoch 300, Training Loss 0.21956249236908104\n",
      "2022-03-26 18:17:18.039060 Epoch 300, Training Loss 0.22010630197689662\n",
      "2022-03-26 18:17:18.062711 Epoch 300, Training Loss 0.22084431213033778\n",
      "2022-03-26 18:17:18.095612 Epoch 300, Training Loss 0.22147970800966862\n",
      "2022-03-26 18:17:18.119925 Epoch 300, Training Loss 0.22205561811052016\n",
      "2022-03-26 18:17:18.144472 Epoch 300, Training Loss 0.2226943147304418\n",
      "2022-03-26 18:17:18.169760 Epoch 300, Training Loss 0.22341272852304953\n",
      "2022-03-26 18:17:18.193310 Epoch 300, Training Loss 0.2237613022403644\n",
      "2022-03-26 18:17:18.216831 Epoch 300, Training Loss 0.22462343540795318\n",
      "2022-03-26 18:17:18.239334 Epoch 300, Training Loss 0.22516090550538523\n",
      "2022-03-26 18:17:18.262323 Epoch 300, Training Loss 0.22561344511978462\n",
      "2022-03-26 18:17:18.284702 Epoch 300, Training Loss 0.2262478024910783\n",
      "2022-03-26 18:17:18.316273 Epoch 300, Training Loss 0.22693838480183537\n",
      "2022-03-26 18:17:18.343079 Epoch 300, Training Loss 0.22744834567884656\n",
      "2022-03-26 18:17:18.366271 Epoch 300, Training Loss 0.22838927809234774\n",
      "2022-03-26 18:17:18.393864 Epoch 300, Training Loss 0.2290185467742593\n",
      "2022-03-26 18:17:18.428654 Epoch 300, Training Loss 0.22967760158164421\n",
      "2022-03-26 18:17:18.453545 Epoch 300, Training Loss 0.23031942683565038\n",
      "2022-03-26 18:17:18.477445 Epoch 300, Training Loss 0.23092940343005577\n",
      "2022-03-26 18:17:18.501548 Epoch 300, Training Loss 0.23149425374424976\n",
      "2022-03-26 18:17:18.535477 Epoch 300, Training Loss 0.23192743130047302\n",
      "2022-03-26 18:17:18.559556 Epoch 300, Training Loss 0.2327736863852157\n",
      "2022-03-26 18:17:18.583198 Epoch 300, Training Loss 0.23326216348449288\n",
      "2022-03-26 18:17:18.606317 Epoch 300, Training Loss 0.2338723538400572\n",
      "2022-03-26 18:17:18.630098 Epoch 300, Training Loss 0.23462560456579604\n",
      "2022-03-26 18:17:18.653228 Epoch 300, Training Loss 0.23512662028717568\n",
      "2022-03-26 18:17:18.679283 Epoch 300, Training Loss 0.23571791379805415\n",
      "2022-03-26 18:17:18.703770 Epoch 300, Training Loss 0.23640348123924812\n",
      "2022-03-26 18:17:18.727475 Epoch 300, Training Loss 0.2372520189074909\n",
      "2022-03-26 18:17:18.759811 Epoch 300, Training Loss 0.23776840633900878\n",
      "2022-03-26 18:17:18.783423 Epoch 300, Training Loss 0.23836752467448144\n",
      "2022-03-26 18:17:18.807072 Epoch 300, Training Loss 0.23923075145772657\n",
      "2022-03-26 18:17:18.832569 Epoch 300, Training Loss 0.24022493612430895\n",
      "2022-03-26 18:17:18.863634 Epoch 300, Training Loss 0.240922416415056\n",
      "2022-03-26 18:17:18.887926 Epoch 300, Training Loss 0.24169484855573806\n",
      "2022-03-26 18:17:18.911022 Epoch 300, Training Loss 0.24231002378799116\n",
      "2022-03-26 18:17:18.933891 Epoch 300, Training Loss 0.2428596606263724\n",
      "2022-03-26 18:17:18.960750 Epoch 300, Training Loss 0.24365404290159035\n",
      "2022-03-26 18:17:18.994621 Epoch 300, Training Loss 0.24416976343945165\n",
      "2022-03-26 18:17:19.018666 Epoch 300, Training Loss 0.2451117072263947\n",
      "2022-03-26 18:17:19.041910 Epoch 300, Training Loss 0.24583159665317486\n",
      "2022-03-26 18:17:19.074809 Epoch 300, Training Loss 0.2464894223243684\n",
      "2022-03-26 18:17:19.098499 Epoch 300, Training Loss 0.24705763793814822\n",
      "2022-03-26 18:17:19.125567 Epoch 300, Training Loss 0.24747059599060536\n",
      "2022-03-26 18:17:19.149249 Epoch 300, Training Loss 0.24816364549157566\n",
      "2022-03-26 18:17:19.172759 Epoch 300, Training Loss 0.24877322040250538\n",
      "2022-03-26 18:17:19.197059 Epoch 300, Training Loss 0.24951079434446058\n",
      "2022-03-26 18:17:19.229955 Epoch 300, Training Loss 0.2501961117052971\n",
      "2022-03-26 18:17:19.253947 Epoch 300, Training Loss 0.25110756832620373\n",
      "2022-03-26 18:17:19.277224 Epoch 300, Training Loss 0.2518747586118596\n",
      "2022-03-26 18:17:19.300512 Epoch 300, Training Loss 0.2526757694266336\n",
      "2022-03-26 18:17:19.323635 Epoch 300, Training Loss 0.25345083263219165\n",
      "2022-03-26 18:17:19.351024 Epoch 300, Training Loss 0.2540712418687313\n",
      "2022-03-26 18:17:19.375720 Epoch 300, Training Loss 0.2544381601731186\n",
      "2022-03-26 18:17:19.399610 Epoch 300, Training Loss 0.25503744470798756\n",
      "2022-03-26 18:17:19.443801 Epoch 300, Training Loss 0.2557169137251042\n",
      "2022-03-26 18:17:19.470393 Epoch 300, Training Loss 0.25625937403467913\n",
      "2022-03-26 18:17:19.494538 Epoch 300, Training Loss 0.25690238642723057\n",
      "2022-03-26 18:17:19.518284 Epoch 300, Training Loss 0.2574824586015223\n",
      "2022-03-26 18:17:19.542811 Epoch 300, Training Loss 0.25828059833220507\n",
      "2022-03-26 18:17:19.566524 Epoch 300, Training Loss 0.2589635667212479\n",
      "2022-03-26 18:17:19.589693 Epoch 300, Training Loss 0.25956965617054256\n",
      "2022-03-26 18:17:19.613200 Epoch 300, Training Loss 0.26003852185538356\n",
      "2022-03-26 18:17:19.636711 Epoch 300, Training Loss 0.2607019129387863\n",
      "2022-03-26 18:17:19.671498 Epoch 300, Training Loss 0.2613801246180254\n",
      "2022-03-26 18:17:19.700262 Epoch 300, Training Loss 0.2620284207870283\n",
      "2022-03-26 18:17:19.727159 Epoch 300, Training Loss 0.2626451746277187\n",
      "2022-03-26 18:17:19.751561 Epoch 300, Training Loss 0.2631816214993787\n",
      "2022-03-26 18:17:19.775019 Epoch 300, Training Loss 0.26379291305456626\n",
      "2022-03-26 18:17:19.798355 Epoch 300, Training Loss 0.26424136384368857\n",
      "2022-03-26 18:17:19.822305 Epoch 300, Training Loss 0.26468146540929594\n",
      "2022-03-26 18:17:19.846396 Epoch 300, Training Loss 0.2656134023233448\n",
      "2022-03-26 18:17:19.870382 Epoch 300, Training Loss 0.2661853262301906\n",
      "2022-03-26 18:17:19.908032 Epoch 300, Training Loss 0.2672589459001561\n",
      "2022-03-26 18:17:19.932347 Epoch 300, Training Loss 0.2679123115890166\n",
      "2022-03-26 18:17:19.963626 Epoch 300, Training Loss 0.2686568464693206\n",
      "2022-03-26 18:17:19.987734 Epoch 300, Training Loss 0.2691891626704989\n",
      "2022-03-26 18:17:20.011263 Epoch 300, Training Loss 0.2699608848908978\n",
      "2022-03-26 18:17:20.034312 Epoch 300, Training Loss 0.2707085197462755\n",
      "2022-03-26 18:17:20.057641 Epoch 300, Training Loss 0.2716215764698775\n",
      "2022-03-26 18:17:20.080632 Epoch 300, Training Loss 0.27223902677788453\n",
      "2022-03-26 18:17:20.103853 Epoch 300, Training Loss 0.27338079696573564\n",
      "2022-03-26 18:17:20.137427 Epoch 300, Training Loss 0.274253832524085\n",
      "2022-03-26 18:17:20.163412 Epoch 300, Training Loss 0.27478018490707173\n",
      "2022-03-26 18:17:20.186533 Epoch 300, Training Loss 0.2753215862433319\n",
      "2022-03-26 18:17:20.214794 Epoch 300, Training Loss 0.2759027540531305\n",
      "2022-03-26 18:17:20.238974 Epoch 300, Training Loss 0.2764444511259913\n",
      "2022-03-26 18:17:20.263127 Epoch 300, Training Loss 0.27730516148040363\n",
      "2022-03-26 18:17:20.286404 Epoch 300, Training Loss 0.27801932855640227\n",
      "2022-03-26 18:17:20.309659 Epoch 300, Training Loss 0.2787236639148439\n",
      "2022-03-26 18:17:20.335996 Epoch 300, Training Loss 0.27956071960956547\n",
      "2022-03-26 18:17:20.368115 Epoch 300, Training Loss 0.28034251288074974\n",
      "2022-03-26 18:17:20.391237 Epoch 300, Training Loss 0.28104510285970197\n",
      "2022-03-26 18:17:20.414097 Epoch 300, Training Loss 0.2817075440798269\n",
      "2022-03-26 18:17:20.441602 Epoch 300, Training Loss 0.2824079008663402\n",
      "2022-03-26 18:17:20.473524 Epoch 300, Training Loss 0.28332795610513223\n",
      "2022-03-26 18:17:20.498402 Epoch 300, Training Loss 0.28442087159742174\n",
      "2022-03-26 18:17:20.522428 Epoch 300, Training Loss 0.2850181182936939\n",
      "2022-03-26 18:17:20.546007 Epoch 300, Training Loss 0.2858228519597017\n",
      "2022-03-26 18:17:20.572593 Epoch 300, Training Loss 0.28650773364259763\n",
      "2022-03-26 18:17:20.601049 Epoch 300, Training Loss 0.28727558925938423\n",
      "2022-03-26 18:17:20.624199 Epoch 300, Training Loss 0.28787008175612105\n",
      "2022-03-26 18:17:20.647578 Epoch 300, Training Loss 0.2886069376221703\n",
      "2022-03-26 18:17:20.670771 Epoch 300, Training Loss 0.28933116965129246\n",
      "2022-03-26 18:17:20.694302 Epoch 300, Training Loss 0.29024560376049\n",
      "2022-03-26 18:17:20.720783 Epoch 300, Training Loss 0.2910846732842648\n",
      "2022-03-26 18:17:20.743648 Epoch 300, Training Loss 0.29173496895281553\n",
      "2022-03-26 18:17:20.766938 Epoch 300, Training Loss 0.2926374279400882\n",
      "2022-03-26 18:17:20.798836 Epoch 300, Training Loss 0.29311021426906975\n",
      "2022-03-26 18:17:20.822744 Epoch 300, Training Loss 0.2935400823955341\n",
      "2022-03-26 18:17:20.845451 Epoch 300, Training Loss 0.29431664753143133\n",
      "2022-03-26 18:17:20.876004 Epoch 300, Training Loss 0.2950454407640735\n",
      "2022-03-26 18:17:20.903161 Epoch 300, Training Loss 0.2958120500949947\n",
      "2022-03-26 18:17:20.928255 Epoch 300, Training Loss 0.2967120001230703\n",
      "2022-03-26 18:17:20.955702 Epoch 300, Training Loss 0.29745862505320086\n",
      "2022-03-26 18:17:20.979102 Epoch 300, Training Loss 0.29834799121712785\n",
      "2022-03-26 18:17:21.003369 Epoch 300, Training Loss 0.2990342653011117\n",
      "2022-03-26 18:17:21.039026 Epoch 300, Training Loss 0.29954931159001175\n",
      "2022-03-26 18:17:21.061820 Epoch 300, Training Loss 0.3001050135821028\n",
      "2022-03-26 18:17:21.085378 Epoch 300, Training Loss 0.30079888359969836\n",
      "2022-03-26 18:17:21.108334 Epoch 300, Training Loss 0.301308577132347\n",
      "2022-03-26 18:17:21.130998 Epoch 300, Training Loss 0.30212144530795115\n",
      "2022-03-26 18:17:21.153652 Epoch 300, Training Loss 0.30268313009720627\n",
      "2022-03-26 18:17:21.180484 Epoch 300, Training Loss 0.30341305444612526\n",
      "2022-03-26 18:17:21.203580 Epoch 300, Training Loss 0.3043738992317863\n",
      "2022-03-26 18:17:21.233457 Epoch 300, Training Loss 0.30488957720034565\n",
      "2022-03-26 18:17:21.260834 Epoch 300, Training Loss 0.30584334275301767\n",
      "2022-03-26 18:17:21.284760 Epoch 300, Training Loss 0.30629469172271623\n",
      "2022-03-26 18:17:21.307670 Epoch 300, Training Loss 0.3068798403148456\n",
      "2022-03-26 18:17:21.333398 Epoch 300, Training Loss 0.30764310469712747\n",
      "2022-03-26 18:17:21.356427 Epoch 300, Training Loss 0.3083119456420469\n",
      "2022-03-26 18:17:21.380493 Epoch 300, Training Loss 0.3088701157771108\n",
      "2022-03-26 18:17:21.404613 Epoch 300, Training Loss 0.30956925043974387\n",
      "2022-03-26 18:17:21.431081 Epoch 300, Training Loss 0.310427761977286\n",
      "2022-03-26 18:17:21.464668 Epoch 300, Training Loss 0.31108892063045746\n",
      "2022-03-26 18:17:21.495148 Epoch 300, Training Loss 0.311716233754097\n",
      "2022-03-26 18:17:21.522514 Epoch 300, Training Loss 0.3125057779157253\n",
      "2022-03-26 18:17:21.546462 Epoch 300, Training Loss 0.31349844914263164\n",
      "2022-03-26 18:17:21.569747 Epoch 300, Training Loss 0.31436746763756207\n",
      "2022-03-26 18:17:21.593704 Epoch 300, Training Loss 0.3151433140115665\n",
      "2022-03-26 18:17:21.618003 Epoch 300, Training Loss 0.3157650773101451\n",
      "2022-03-26 18:17:21.642179 Epoch 300, Training Loss 0.31651960953574654\n",
      "2022-03-26 18:17:21.667069 Epoch 300, Training Loss 0.3174778697512034\n",
      "2022-03-26 18:17:21.697973 Epoch 300, Training Loss 0.31811973906081653\n",
      "2022-03-26 18:17:21.722691 Epoch 300, Training Loss 0.3187192229511183\n",
      "2022-03-26 18:17:21.746578 Epoch 300, Training Loss 0.31964506334660914\n",
      "2022-03-26 18:17:21.770350 Epoch 300, Training Loss 0.3201877759850543\n",
      "2022-03-26 18:17:21.798172 Epoch 300, Training Loss 0.3210217642509724\n",
      "2022-03-26 18:17:21.820240 Epoch 300, Training Loss 0.32159690288326626\n",
      "2022-03-26 18:17:21.842807 Epoch 300, Training Loss 0.3225442057527849\n",
      "2022-03-26 18:17:21.865306 Epoch 300, Training Loss 0.32316173174801993\n",
      "2022-03-26 18:17:21.893977 Epoch 300, Training Loss 0.3239960220006421\n",
      "2022-03-26 18:17:21.917696 Epoch 300, Training Loss 0.3246671851638638\n",
      "2022-03-26 18:17:21.940699 Epoch 300, Training Loss 0.3254112198834529\n",
      "2022-03-26 18:17:21.963609 Epoch 300, Training Loss 0.32607464168382727\n",
      "2022-03-26 18:17:21.985815 Epoch 300, Training Loss 0.32670359241078273\n",
      "2022-03-26 18:17:22.015567 Epoch 300, Training Loss 0.3273392840648246\n",
      "2022-03-26 18:17:22.037819 Epoch 300, Training Loss 0.3281144727297756\n",
      "2022-03-26 18:17:22.066065 Epoch 300, Training Loss 0.32873106494431603\n",
      "2022-03-26 18:17:22.090461 Epoch 300, Training Loss 0.32959676646363095\n",
      "2022-03-26 18:17:22.121208 Epoch 300, Training Loss 0.3304459969787037\n",
      "2022-03-26 18:17:22.144299 Epoch 300, Training Loss 0.3310751694699992\n",
      "2022-03-26 18:17:22.166386 Epoch 300, Training Loss 0.3318479173171246\n",
      "2022-03-26 18:17:22.188991 Epoch 300, Training Loss 0.3324981072674627\n",
      "2022-03-26 18:17:22.211351 Epoch 300, Training Loss 0.3330755535217807\n",
      "2022-03-26 18:17:22.234215 Epoch 300, Training Loss 0.33368111209338885\n",
      "2022-03-26 18:17:22.256566 Epoch 300, Training Loss 0.33437732231738926\n",
      "2022-03-26 18:17:22.286096 Epoch 300, Training Loss 0.33506554323236654\n",
      "2022-03-26 18:17:22.310013 Epoch 300, Training Loss 0.33574780368286633\n",
      "2022-03-26 18:17:22.344565 Epoch 300, Training Loss 0.336364367047844\n",
      "2022-03-26 18:17:22.368513 Epoch 300, Training Loss 0.3373667036023591\n",
      "2022-03-26 18:17:22.392885 Epoch 300, Training Loss 0.3379859529873904\n",
      "2022-03-26 18:17:22.415358 Epoch 300, Training Loss 0.3388454503644153\n",
      "2022-03-26 18:17:22.438671 Epoch 300, Training Loss 0.33944020265965813\n",
      "2022-03-26 18:17:22.461386 Epoch 300, Training Loss 0.34010795722989473\n",
      "2022-03-26 18:17:22.483786 Epoch 300, Training Loss 0.3408286840180912\n",
      "2022-03-26 18:17:22.511570 Epoch 300, Training Loss 0.3414904653187603\n",
      "2022-03-26 18:17:22.543323 Epoch 300, Training Loss 0.3421431503561147\n",
      "2022-03-26 18:17:22.576616 Epoch 300, Training Loss 0.34296485244313163\n",
      "2022-03-26 18:17:22.600152 Epoch 300, Training Loss 0.3435436829810252\n",
      "2022-03-26 18:17:22.623351 Epoch 300, Training Loss 0.3441496316886619\n",
      "2022-03-26 18:17:22.646956 Epoch 300, Training Loss 0.34497196617943554\n",
      "2022-03-26 18:17:22.670297 Epoch 300, Training Loss 0.3456733248880147\n",
      "2022-03-26 18:17:22.693169 Epoch 300, Training Loss 0.34641581941443633\n",
      "2022-03-26 18:17:22.717548 Epoch 300, Training Loss 0.34701636814705245\n",
      "2022-03-26 18:17:22.740274 Epoch 300, Training Loss 0.34783159291652765\n",
      "2022-03-26 18:17:22.763368 Epoch 300, Training Loss 0.3485228628148813\n",
      "2022-03-26 18:17:22.792845 Epoch 300, Training Loss 0.3491242644579514\n",
      "2022-03-26 18:17:22.819714 Epoch 300, Training Loss 0.3497318457383329\n",
      "2022-03-26 18:17:22.842937 Epoch 300, Training Loss 0.3500859333807245\n",
      "2022-03-26 18:17:22.865297 Epoch 300, Training Loss 0.3507244295019018\n",
      "2022-03-26 18:17:22.887876 Epoch 300, Training Loss 0.3512978853319612\n",
      "2022-03-26 18:17:22.910506 Epoch 300, Training Loss 0.3521616846094351\n",
      "2022-03-26 18:17:22.933266 Epoch 300, Training Loss 0.35293187014282207\n",
      "2022-03-26 18:17:22.956030 Epoch 300, Training Loss 0.3538012378051153\n",
      "2022-03-26 18:17:22.984909 Epoch 300, Training Loss 0.354679173444543\n",
      "2022-03-26 18:17:23.019697 Epoch 300, Training Loss 0.3554175531162935\n",
      "2022-03-26 18:17:23.043357 Epoch 300, Training Loss 0.35605359591943836\n",
      "2022-03-26 18:17:23.065920 Epoch 300, Training Loss 0.35662141629039784\n",
      "2022-03-26 18:17:23.088546 Epoch 300, Training Loss 0.35732306113176027\n",
      "2022-03-26 18:17:23.113250 Epoch 300, Training Loss 0.35830198304579997\n",
      "2022-03-26 18:17:23.139241 Epoch 300, Training Loss 0.35878356532824923\n",
      "2022-03-26 18:17:23.162008 Epoch 300, Training Loss 0.3594770396838103\n",
      "2022-03-26 18:17:23.184633 Epoch 300, Training Loss 0.3603297081368659\n",
      "2022-03-26 18:17:23.210686 Epoch 300, Training Loss 0.36117523127352186\n",
      "2022-03-26 18:17:23.241351 Epoch 300, Training Loss 0.36210959772472184\n",
      "2022-03-26 18:17:23.265331 Epoch 300, Training Loss 0.3628362524311256\n",
      "2022-03-26 18:17:23.288419 Epoch 300, Training Loss 0.3638805511128872\n",
      "2022-03-26 18:17:23.313153 Epoch 300, Training Loss 0.36443767416507694\n",
      "2022-03-26 18:17:23.340713 Epoch 300, Training Loss 0.3652632390446675\n",
      "2022-03-26 18:17:23.364689 Epoch 300, Training Loss 0.3660511153433329\n",
      "2022-03-26 18:17:23.388152 Epoch 300, Training Loss 0.366571089572004\n",
      "2022-03-26 18:17:23.411397 Epoch 300, Training Loss 0.3672308685529567\n",
      "2022-03-26 18:17:23.435666 Epoch 300, Training Loss 0.3679098755197452\n",
      "2022-03-26 18:17:23.469974 Epoch 300, Training Loss 0.36847724256765507\n",
      "2022-03-26 18:17:23.493675 Epoch 300, Training Loss 0.36907697025010044\n",
      "2022-03-26 18:17:23.516543 Epoch 300, Training Loss 0.36975579466813663\n",
      "2022-03-26 18:17:23.543344 Epoch 300, Training Loss 0.3708384176882941\n",
      "2022-03-26 18:17:23.574694 Epoch 300, Training Loss 0.3713863028208618\n",
      "2022-03-26 18:17:23.597357 Epoch 300, Training Loss 0.37209677585707907\n",
      "2022-03-26 18:17:23.621856 Epoch 300, Training Loss 0.3730750380803252\n",
      "2022-03-26 18:17:23.644999 Epoch 300, Training Loss 0.3739562233162048\n",
      "2022-03-26 18:17:23.668786 Epoch 300, Training Loss 0.37458063067530123\n",
      "2022-03-26 18:17:23.701336 Epoch 300, Training Loss 0.37530836611605056\n",
      "2022-03-26 18:17:23.726505 Epoch 300, Training Loss 0.3759295994897023\n",
      "2022-03-26 18:17:23.749349 Epoch 300, Training Loss 0.37665368929086135\n",
      "2022-03-26 18:17:23.773147 Epoch 300, Training Loss 0.3774643682534128\n",
      "2022-03-26 18:17:23.795431 Epoch 300, Training Loss 0.3780198788932522\n",
      "2022-03-26 18:17:23.817654 Epoch 300, Training Loss 0.37869849484747325\n",
      "2022-03-26 18:17:23.840768 Epoch 300, Training Loss 0.3794165600062636\n",
      "2022-03-26 18:17:23.868820 Epoch 300, Training Loss 0.3801433808739533\n",
      "2022-03-26 18:17:23.896784 Epoch 300, Training Loss 0.38061069538983544\n",
      "2022-03-26 18:17:23.929086 Epoch 300, Training Loss 0.3811537777752523\n",
      "2022-03-26 18:17:23.953062 Epoch 300, Training Loss 0.3819586269157317\n",
      "2022-03-26 18:17:23.976056 Epoch 300, Training Loss 0.3825775830032271\n",
      "2022-03-26 18:17:23.999157 Epoch 300, Training Loss 0.38340263682253223\n",
      "2022-03-26 18:17:24.022051 Epoch 300, Training Loss 0.3842192011720994\n",
      "2022-03-26 18:17:24.044522 Epoch 300, Training Loss 0.38486124761878987\n",
      "2022-03-26 18:17:24.067129 Epoch 300, Training Loss 0.3853443361380521\n",
      "2022-03-26 18:17:24.089479 Epoch 300, Training Loss 0.38602672738339894\n",
      "2022-03-26 18:17:24.117444 Epoch 300, Training Loss 0.38678263989098544\n",
      "2022-03-26 18:17:24.145277 Epoch 300, Training Loss 0.3875284409507766\n",
      "2022-03-26 18:17:24.171434 Epoch 300, Training Loss 0.3882977141215063\n",
      "2022-03-26 18:17:24.194206 Epoch 300, Training Loss 0.38904950239926656\n",
      "2022-03-26 18:17:24.216668 Epoch 300, Training Loss 0.3898046761751175\n",
      "2022-03-26 18:17:24.239077 Epoch 300, Training Loss 0.390322642253183\n",
      "2022-03-26 18:17:24.261631 Epoch 300, Training Loss 0.39088629033711864\n",
      "2022-03-26 18:17:24.284398 Epoch 300, Training Loss 0.3916175791141017\n",
      "2022-03-26 18:17:24.307190 Epoch 300, Training Loss 0.39259677969128887\n",
      "2022-03-26 18:17:24.332968 Epoch 300, Training Loss 0.39355935171589523\n",
      "2022-03-26 18:17:24.364909 Epoch 300, Training Loss 0.39413741936006813\n",
      "2022-03-26 18:17:24.391804 Epoch 300, Training Loss 0.394903583180569\n",
      "2022-03-26 18:17:24.414025 Epoch 300, Training Loss 0.3953661583649838\n",
      "2022-03-26 18:17:24.436597 Epoch 300, Training Loss 0.39625598436883647\n",
      "2022-03-26 18:17:24.459018 Epoch 300, Training Loss 0.3968033499043921\n",
      "2022-03-26 18:17:24.482521 Epoch 300, Training Loss 0.3974650639783391\n",
      "2022-03-26 18:17:24.505330 Epoch 300, Training Loss 0.39810844878558915\n",
      "2022-03-26 18:17:24.527543 Epoch 300, Training Loss 0.39872513029276563\n",
      "2022-03-26 18:17:24.549875 Epoch 300, Training Loss 0.39914533869384805\n",
      "2022-03-26 18:17:24.583896 Epoch 300, Training Loss 0.39964987566251586\n",
      "2022-03-26 18:17:24.614525 Epoch 300, Training Loss 0.4003770440207113\n",
      "2022-03-26 18:17:24.637315 Epoch 300, Training Loss 0.4011111919151243\n",
      "2022-03-26 18:17:24.659824 Epoch 300, Training Loss 0.401949447553481\n",
      "2022-03-26 18:17:24.682942 Epoch 300, Training Loss 0.40247286959072515\n",
      "2022-03-26 18:17:24.705489 Epoch 300, Training Loss 0.4031083503799975\n",
      "2022-03-26 18:17:24.730494 Epoch 300, Training Loss 0.4036504435722175\n",
      "2022-03-26 18:17:24.752785 Epoch 300, Training Loss 0.4044210258347299\n",
      "2022-03-26 18:17:24.776335 Epoch 300, Training Loss 0.4051359577099686\n",
      "2022-03-26 18:17:24.806058 Epoch 300, Training Loss 0.40585414428845085\n",
      "2022-03-26 18:17:24.829332 Epoch 300, Training Loss 0.406305209373879\n",
      "2022-03-26 18:17:24.855013 Epoch 300, Training Loss 0.4070493496592392\n",
      "2022-03-26 18:17:24.877713 Epoch 300, Training Loss 0.40783631344280585\n",
      "2022-03-26 18:17:24.899871 Epoch 300, Training Loss 0.408665361078194\n",
      "2022-03-26 18:17:24.922288 Epoch 300, Training Loss 0.40935160772269946\n",
      "2022-03-26 18:17:24.945270 Epoch 300, Training Loss 0.40996213676527027\n",
      "2022-03-26 18:17:24.968506 Epoch 300, Training Loss 0.4105840565069862\n",
      "2022-03-26 18:17:24.991123 Epoch 300, Training Loss 0.4114232449351674\n",
      "2022-03-26 18:17:25.027228 Epoch 300, Training Loss 0.4121787160482553\n",
      "2022-03-26 18:17:25.054382 Epoch 300, Training Loss 0.41304200853380707\n",
      "2022-03-26 18:17:25.082394 Epoch 300, Training Loss 0.4136525303735148\n",
      "2022-03-26 18:17:25.105535 Epoch 300, Training Loss 0.4140993639483781\n",
      "2022-03-26 18:17:25.129237 Epoch 300, Training Loss 0.4146380340275557\n",
      "2022-03-26 18:17:25.152258 Epoch 300, Training Loss 0.415304682863033\n",
      "2022-03-26 18:17:25.177116 Epoch 300, Training Loss 0.4158010142843437\n",
      "2022-03-26 18:17:25.199616 Epoch 300, Training Loss 0.4166447295404761\n",
      "2022-03-26 18:17:25.222029 Epoch 300, Training Loss 0.41716982226085175\n",
      "2022-03-26 18:17:25.252714 Epoch 300, Training Loss 0.4177557835950876\n",
      "2022-03-26 18:17:25.277381 Epoch 300, Training Loss 0.4181082686957191\n",
      "2022-03-26 18:17:25.300203 Epoch 300, Training Loss 0.41906451935048605\n",
      "2022-03-26 18:17:25.323161 Epoch 300, Training Loss 0.41994815912393046\n",
      "2022-03-26 18:17:25.349160 Epoch 300, Training Loss 0.42077797415006496\n",
      "2022-03-26 18:17:25.373820 Epoch 300, Training Loss 0.4214088350077114\n",
      "2022-03-26 18:17:25.398761 Epoch 300, Training Loss 0.42214819030536105\n",
      "2022-03-26 18:17:25.423474 Epoch 300, Training Loss 0.4224600481903157\n",
      "2022-03-26 18:17:25.447278 Epoch 300, Training Loss 0.42319014327376697\n",
      "2022-03-26 18:17:25.481894 Epoch 300, Training Loss 0.4238336070457383\n",
      "2022-03-26 18:17:25.506343 Epoch 300, Training Loss 0.4242391891377356\n",
      "2022-03-26 18:17:25.530163 Epoch 300, Training Loss 0.42516476222697425\n",
      "2022-03-26 18:17:25.553577 Epoch 300, Training Loss 0.4259040605877062\n",
      "2022-03-26 18:17:25.576825 Epoch 300, Training Loss 0.42687600487104765\n",
      "2022-03-26 18:17:25.600008 Epoch 300, Training Loss 0.427695360653998\n",
      "2022-03-26 18:17:25.634377 Epoch 300, Training Loss 0.4286548643351516\n",
      "2022-03-26 18:17:25.657546 Epoch 300, Training Loss 0.42953309197636214\n",
      "2022-03-26 18:17:25.681254 Epoch 300, Training Loss 0.43029927652891337\n",
      "2022-03-26 18:17:25.714864 Epoch 300, Training Loss 0.43101274046827764\n",
      "2022-03-26 18:17:25.739347 Epoch 300, Training Loss 0.4317840397396051\n",
      "2022-03-26 18:17:25.762717 Epoch 300, Training Loss 0.4325064718151641\n",
      "2022-03-26 18:17:25.785608 Epoch 300, Training Loss 0.4329878671661667\n",
      "2022-03-26 18:17:25.808782 Epoch 300, Training Loss 0.43390148197827133\n",
      "2022-03-26 18:17:25.836547 Epoch 300, Training Loss 0.43455342127157903\n",
      "2022-03-26 18:17:25.859804 Epoch 300, Training Loss 0.4352459044522032\n",
      "2022-03-26 18:17:25.884119 Epoch 300, Training Loss 0.4359649558315801\n",
      "2022-03-26 18:17:25.907539 Epoch 300, Training Loss 0.43647720649495453\n",
      "2022-03-26 18:17:25.939580 Epoch 300, Training Loss 0.43696827247090964\n",
      "2022-03-26 18:17:25.963473 Epoch 300, Training Loss 0.4376471728048361\n",
      "2022-03-26 18:17:25.994360 Epoch 300, Training Loss 0.4383296994182765\n",
      "2022-03-26 18:17:26.018450 Epoch 300, Training Loss 0.43894391019097373\n",
      "2022-03-26 18:17:26.042061 Epoch 300, Training Loss 0.44001159523530387\n",
      "2022-03-26 18:17:26.067096 Epoch 300, Training Loss 0.44084436236821173\n",
      "2022-03-26 18:17:26.092962 Epoch 300, Training Loss 0.4417405254433832\n",
      "2022-03-26 18:17:26.118513 Epoch 300, Training Loss 0.4423925609273069\n",
      "2022-03-26 18:17:26.144144 Epoch 300, Training Loss 0.4430943716250722\n",
      "2022-03-26 18:17:26.173215 Epoch 300, Training Loss 0.44380712251910165\n",
      "2022-03-26 18:17:26.196852 Epoch 300, Training Loss 0.4443401615599842\n",
      "2022-03-26 18:17:26.219993 Epoch 300, Training Loss 0.445180470433534\n",
      "2022-03-26 18:17:26.243207 Epoch 300, Training Loss 0.44590926641012396\n",
      "2022-03-26 18:17:26.266639 Epoch 300, Training Loss 0.44647338207039383\n",
      "2022-03-26 18:17:26.290333 Epoch 300, Training Loss 0.44706864329174045\n",
      "2022-03-26 18:17:26.313542 Epoch 300, Training Loss 0.4476908080832428\n",
      "2022-03-26 18:17:26.343807 Epoch 300, Training Loss 0.44848257674814185\n",
      "2022-03-26 18:17:26.375168 Epoch 300, Training Loss 0.44905602026854635\n",
      "2022-03-26 18:17:26.399963 Epoch 300, Training Loss 0.44961844622860175\n",
      "2022-03-26 18:17:26.423817 Epoch 300, Training Loss 0.45049228881249953\n",
      "2022-03-26 18:17:26.447699 Epoch 300, Training Loss 0.45119262989753345\n",
      "2022-03-26 18:17:26.471221 Epoch 300, Training Loss 0.45188973144725764\n",
      "2022-03-26 18:17:26.494419 Epoch 300, Training Loss 0.45275120172278044\n",
      "2022-03-26 18:17:26.517865 Epoch 300, Training Loss 0.4534946435971943\n",
      "2022-03-26 18:17:26.541587 Epoch 300, Training Loss 0.4541306434880437\n",
      "2022-03-26 18:17:26.564897 Epoch 300, Training Loss 0.45499601405676066\n",
      "2022-03-26 18:17:26.598819 Epoch 300, Training Loss 0.4558027853327029\n",
      "2022-03-26 18:17:26.627354 Epoch 300, Training Loss 0.4563514609128008\n",
      "2022-03-26 18:17:26.660501 Epoch 300, Training Loss 0.45701927867005854\n",
      "2022-03-26 18:17:26.683808 Epoch 300, Training Loss 0.45779730937898616\n",
      "2022-03-26 18:17:26.707570 Epoch 300, Training Loss 0.458639130320238\n",
      "2022-03-26 18:17:26.732300 Epoch 300, Training Loss 0.4593092869881474\n",
      "2022-03-26 18:17:26.756363 Epoch 300, Training Loss 0.45991191380392865\n",
      "2022-03-26 18:17:26.780594 Epoch 300, Training Loss 0.46057048666736355\n",
      "2022-03-26 18:17:26.804069 Epoch 300, Training Loss 0.4611514976910313\n",
      "2022-03-26 18:17:26.833971 Epoch 300, Training Loss 0.4617837134491452\n",
      "2022-03-26 18:17:26.862115 Epoch 300, Training Loss 0.4623756987016524\n",
      "2022-03-26 18:17:26.885642 Epoch 300, Training Loss 0.46312241898397044\n",
      "2022-03-26 18:17:26.913567 Epoch 300, Training Loss 0.46386395770189404\n",
      "2022-03-26 18:17:26.939908 Epoch 300, Training Loss 0.46451314420575074\n",
      "2022-03-26 18:17:26.963792 Epoch 300, Training Loss 0.4651531014410431\n",
      "2022-03-26 18:17:26.987143 Epoch 300, Training Loss 0.4659625715421289\n",
      "2022-03-26 18:17:27.019049 Epoch 300, Training Loss 0.46650826818574115\n",
      "2022-03-26 18:17:27.043381 Epoch 300, Training Loss 0.46715737138029256\n",
      "2022-03-26 18:17:27.067072 Epoch 300, Training Loss 0.4676618495446337\n",
      "2022-03-26 18:17:27.093387 Epoch 300, Training Loss 0.4683867853201564\n",
      "2022-03-26 18:17:27.123048 Epoch 300, Training Loss 0.46901377370519104\n",
      "2022-03-26 18:17:27.146335 Epoch 300, Training Loss 0.4696630486251448\n",
      "2022-03-26 18:17:27.170169 Epoch 300, Training Loss 0.47013538685219974\n",
      "2022-03-26 18:17:27.193845 Epoch 300, Training Loss 0.4706986006682791\n",
      "2022-03-26 18:17:27.216562 Epoch 300, Training Loss 0.4713683550429466\n",
      "2022-03-26 18:17:27.246604 Epoch 300, Training Loss 0.4719415675762974\n",
      "2022-03-26 18:17:27.269626 Epoch 300, Training Loss 0.4724200575057503\n",
      "2022-03-26 18:17:27.292384 Epoch 300, Training Loss 0.4737492586150194\n",
      "2022-03-26 18:17:27.315122 Epoch 300, Training Loss 0.4744864131140587\n",
      "2022-03-26 18:17:27.347556 Epoch 300, Training Loss 0.4750302562971249\n",
      "2022-03-26 18:17:27.371282 Epoch 300, Training Loss 0.4758354965263925\n",
      "2022-03-26 18:17:27.395493 Epoch 300, Training Loss 0.47638220080862875\n",
      "2022-03-26 18:17:27.419154 Epoch 300, Training Loss 0.47703130857642656\n",
      "2022-03-26 18:17:27.444073 Epoch 300, Training Loss 0.4778492419273042\n",
      "2022-03-26 18:17:27.476603 Epoch 300, Training Loss 0.4786965158551245\n",
      "2022-03-26 18:17:27.499660 Epoch 300, Training Loss 0.4794450793081842\n",
      "2022-03-26 18:17:27.521850 Epoch 300, Training Loss 0.48038579891328614\n",
      "2022-03-26 18:17:27.544216 Epoch 300, Training Loss 0.4811760209824728\n",
      "2022-03-26 18:17:27.566619 Epoch 300, Training Loss 0.4818771962276505\n",
      "2022-03-26 18:17:27.588923 Epoch 300, Training Loss 0.48236052541400465\n",
      "2022-03-26 18:17:27.611438 Epoch 300, Training Loss 0.4829476589094038\n",
      "2022-03-26 18:17:27.640854 Epoch 300, Training Loss 0.48367512658657624\n",
      "2022-03-26 18:17:27.670423 Epoch 300, Training Loss 0.4842302692706323\n",
      "2022-03-26 18:17:27.703905 Epoch 300, Training Loss 0.4849735104557498\n",
      "2022-03-26 18:17:27.728218 Epoch 300, Training Loss 0.4854736422829311\n",
      "2022-03-26 18:17:27.750992 Epoch 300, Training Loss 0.4863565241932259\n",
      "2022-03-26 18:17:27.773226 Epoch 300, Training Loss 0.4869837000051423\n",
      "2022-03-26 18:17:27.796728 Epoch 300, Training Loss 0.48766780217818895\n",
      "2022-03-26 18:17:27.820075 Epoch 300, Training Loss 0.4883192912353884\n",
      "2022-03-26 18:17:27.843099 Epoch 300, Training Loss 0.4887230189903008\n",
      "2022-03-26 18:17:27.867424 Epoch 300, Training Loss 0.48983868844140216\n",
      "2022-03-26 18:17:27.890854 Epoch 300, Training Loss 0.4909296751670215\n",
      "2022-03-26 18:17:27.918153 Epoch 300, Training Loss 0.49156643985711096\n",
      "2022-03-26 18:17:27.945092 Epoch 300, Training Loss 0.49224783314387205\n",
      "2022-03-26 18:17:27.967513 Epoch 300, Training Loss 0.4927994724544113\n",
      "2022-03-26 18:17:27.989747 Epoch 300, Training Loss 0.4932473733868745\n",
      "2022-03-26 18:17:28.012153 Epoch 300, Training Loss 0.4938653377658876\n",
      "2022-03-26 18:17:28.034516 Epoch 300, Training Loss 0.4942451310165398\n",
      "2022-03-26 18:17:28.058050 Epoch 300, Training Loss 0.49500418054249584\n",
      "2022-03-26 18:17:28.084645 Epoch 300, Training Loss 0.4957260804446152\n",
      "2022-03-26 18:17:28.114282 Epoch 300, Training Loss 0.4962220726072636\n",
      "2022-03-26 18:17:28.148616 Epoch 300, Training Loss 0.49692314525928033\n",
      "2022-03-26 18:17:28.171608 Epoch 300, Training Loss 0.49793114349284134\n",
      "2022-03-26 18:17:28.194369 Epoch 300, Training Loss 0.49880852264440273\n",
      "2022-03-26 18:17:28.217294 Epoch 300, Training Loss 0.4996348453871429\n",
      "2022-03-26 18:17:28.240270 Epoch 300, Training Loss 0.5004485410154628\n",
      "2022-03-26 18:17:28.262766 Epoch 300, Training Loss 0.5010922868805163\n",
      "2022-03-26 18:17:28.285807 Epoch 300, Training Loss 0.5016422446845742\n",
      "2022-03-26 18:17:28.309403 Epoch 300, Training Loss 0.5024494271715889\n",
      "2022-03-26 18:17:28.341014 Epoch 300, Training Loss 0.5029095530776722\n",
      "2022-03-26 18:17:28.372393 Epoch 300, Training Loss 0.5035725147332377\n",
      "2022-03-26 18:17:28.398639 Epoch 300, Training Loss 0.5041129245515674\n",
      "2022-03-26 18:17:28.421778 Epoch 300, Training Loss 0.5047338518225933\n",
      "2022-03-26 18:17:28.444807 Epoch 300, Training Loss 0.5055171739300499\n",
      "2022-03-26 18:17:28.467695 Epoch 300, Training Loss 0.5062206873998922\n",
      "2022-03-26 18:17:28.490945 Epoch 300, Training Loss 0.5072079139482945\n",
      "2022-03-26 18:17:28.514068 Epoch 300, Training Loss 0.5080025979815541\n",
      "2022-03-26 18:17:28.536366 Epoch 300, Training Loss 0.508415706608149\n",
      "2022-03-26 18:17:28.559049 Epoch 300, Training Loss 0.5089693369577303\n",
      "2022-03-26 18:17:28.591014 Epoch 300, Training Loss 0.509560767780332\n",
      "2022-03-26 18:17:28.615359 Epoch 300, Training Loss 0.5104395942119382\n",
      "2022-03-26 18:17:28.638110 Epoch 300, Training Loss 0.5109871668774454\n",
      "2022-03-26 18:17:28.660772 Epoch 300, Training Loss 0.511619751505992\n",
      "2022-03-26 18:17:28.687901 Epoch 300, Training Loss 0.5121703395034041\n",
      "2022-03-26 18:17:28.715574 Epoch 300, Training Loss 0.512896668289781\n",
      "2022-03-26 18:17:28.739843 Epoch 300, Training Loss 0.513458811165884\n",
      "2022-03-26 18:17:28.762531 Epoch 300, Training Loss 0.514522202084284\n",
      "2022-03-26 18:17:28.786899 Epoch 300, Training Loss 0.5153478427273234\n",
      "2022-03-26 18:17:28.818846 Epoch 300, Training Loss 0.5159094611282848\n",
      "2022-03-26 18:17:28.842625 Epoch 300, Training Loss 0.5166731416759893\n",
      "2022-03-26 18:17:28.864863 Epoch 300, Training Loss 0.5173637950245071\n",
      "2022-03-26 18:17:28.887405 Epoch 300, Training Loss 0.5180088546308105\n",
      "2022-03-26 18:17:28.910239 Epoch 300, Training Loss 0.5186123608818749\n",
      "2022-03-26 18:17:28.932662 Epoch 300, Training Loss 0.5193771630944803\n",
      "2022-03-26 18:17:28.961889 Epoch 300, Training Loss 0.5200518032969417\n",
      "2022-03-26 18:17:28.989980 Epoch 300, Training Loss 0.520698225654452\n",
      "2022-03-26 18:17:29.019158 Epoch 300, Training Loss 0.5214904912787935\n",
      "2022-03-26 18:17:29.050594 Epoch 300, Training Loss 0.5220995946880191\n",
      "2022-03-26 18:17:29.073771 Epoch 300, Training Loss 0.5227733041586169\n",
      "2022-03-26 18:17:29.097406 Epoch 300, Training Loss 0.5237694860381239\n",
      "2022-03-26 18:17:29.127067 Epoch 300, Training Loss 0.5242607776275681\n",
      "2022-03-26 18:17:29.150774 Epoch 300, Training Loss 0.5246861381718265\n",
      "2022-03-26 18:17:29.176259 Epoch 300, Training Loss 0.5254150628090819\n",
      "2022-03-26 18:17:29.199086 Epoch 300, Training Loss 0.5261311146723645\n",
      "2022-03-26 18:17:29.225801 Epoch 300, Training Loss 0.5266736007826712\n",
      "2022-03-26 18:17:29.248420 Epoch 300, Training Loss 0.5272872609365017\n",
      "2022-03-26 18:17:29.281186 Epoch 300, Training Loss 0.52817244448549\n",
      "2022-03-26 18:17:29.304585 Epoch 300, Training Loss 0.5288997645992453\n",
      "2022-03-26 18:17:29.327295 Epoch 300, Training Loss 0.5295849380552616\n",
      "2022-03-26 18:17:29.350601 Epoch 300, Training Loss 0.530399412431223\n",
      "2022-03-26 18:17:29.374574 Epoch 300, Training Loss 0.5312418016745611\n",
      "2022-03-26 18:17:29.399409 Epoch 300, Training Loss 0.5315658492619729\n",
      "2022-03-26 18:17:29.424496 Epoch 300, Training Loss 0.5322270735412302\n",
      "2022-03-26 18:17:29.448065 Epoch 300, Training Loss 0.5330484771476988\n",
      "2022-03-26 18:17:29.470969 Epoch 300, Training Loss 0.5338976929521622\n",
      "2022-03-26 18:17:29.502809 Epoch 300, Training Loss 0.534226196787089\n",
      "2022-03-26 18:17:29.515021 Epoch 300, Training Loss 0.5346684341730974\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1648320380445,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "WEidwK5byhQO"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "      correct = 0\n",
    "      total = 0\n",
    "    \n",
    "      with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "          outputs = model(imgs.to('cuda:0'))\n",
    "          _, predicted = torch.max(outputs.to('cuda:0'), dim=1)\n",
    "          total += labels.shape[0]\n",
    "          correct += int((predicted.to('cuda:0') == labels.to('cuda:0')).sum())\n",
    "    \n",
    "      print(\"Accuracy {}: {:.2f}\".format(name, correct/total))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18551,
     "status": "ok",
     "timestamp": 1648320409676,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "D1I65dPmyhG8",
    "outputId": "c61b0b8d-547b-4429-a6b4-825855fe7fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.83\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp2JBQqKjaVD"
   },
   "source": [
    "Problem 1 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1648320943730,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "orDmL5_1jdYl"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8,4, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Linear(4*4*4, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1648320947491,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "zW_NEQ6eygtY",
    "outputId": "548ca5c6-e16a-4d52-fd8c-ee21ee82f022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5408395,
     "status": "ok",
     "timestamp": 1648326358958,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "wJ5UdniykWL-",
    "outputId": "1a9d3358-a15a-4e70-a1f7-957e4221b30f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "2022-03-26 18:56:01.101308 Epoch 1, Training Loss 1.2934900006979628\n",
      "2022-03-26 18:56:01.130706 Epoch 1, Training Loss 1.2960777843699736\n",
      "2022-03-26 18:56:01.154474 Epoch 1, Training Loss 1.2986304610586532\n",
      "2022-03-26 18:56:01.177788 Epoch 1, Training Loss 1.3013318987453686\n",
      "2022-03-26 18:56:01.205197 Epoch 1, Training Loss 1.303989147896047\n",
      "2022-03-26 18:56:01.241344 Epoch 1, Training Loss 1.3064948196911141\n",
      "2022-03-26 18:56:01.265036 Epoch 1, Training Loss 1.3090027355781906\n",
      "2022-03-26 18:56:01.289266 Epoch 1, Training Loss 1.3115314899198234\n",
      "2022-03-26 18:56:01.312878 Epoch 1, Training Loss 1.3140480091504734\n",
      "2022-03-26 18:56:01.344243 Epoch 1, Training Loss 1.316501781458745\n",
      "2022-03-26 18:56:01.368998 Epoch 1, Training Loss 1.3191832560102652\n",
      "2022-03-26 18:56:01.393291 Epoch 1, Training Loss 1.3217977102455276\n",
      "2022-03-26 18:56:01.427263 Epoch 1, Training Loss 1.3242343548313735\n",
      "2022-03-26 18:56:01.457348 Epoch 1, Training Loss 1.3267566019007007\n",
      "2022-03-26 18:56:01.481389 Epoch 1, Training Loss 1.3292132199877669\n",
      "2022-03-26 18:56:01.504935 Epoch 1, Training Loss 1.3316464367729928\n",
      "2022-03-26 18:56:01.528614 Epoch 1, Training Loss 1.3341054843209894\n",
      "2022-03-26 18:56:01.556473 Epoch 1, Training Loss 1.336664671483247\n",
      "2022-03-26 18:56:01.581253 Epoch 1, Training Loss 1.3392554692295202\n",
      "2022-03-26 18:56:01.604867 Epoch 1, Training Loss 1.3418121447648539\n",
      "2022-03-26 18:56:01.628850 Epoch 1, Training Loss 1.3443007463079584\n",
      "2022-03-26 18:56:01.652682 Epoch 1, Training Loss 1.34671125814433\n",
      "2022-03-26 18:56:01.676579 Epoch 1, Training Loss 1.3492843543781954\n",
      "2022-03-26 18:56:01.700836 Epoch 1, Training Loss 1.351758600653285\n",
      "2022-03-26 18:56:01.727855 Epoch 1, Training Loss 1.3542667738616925\n",
      "2022-03-26 18:56:01.752224 Epoch 1, Training Loss 1.356837972503184\n",
      "2022-03-26 18:56:01.780997 Epoch 1, Training Loss 1.359257556440885\n",
      "2022-03-26 18:56:01.804589 Epoch 1, Training Loss 1.3617328406904665\n",
      "2022-03-26 18:56:01.827695 Epoch 1, Training Loss 1.3643589118862396\n",
      "2022-03-26 18:56:01.852024 Epoch 1, Training Loss 1.3667919524490375\n",
      "2022-03-26 18:56:01.877720 Epoch 1, Training Loss 1.3694289299228308\n",
      "2022-03-26 18:56:01.900679 Epoch 1, Training Loss 1.3720298772272856\n",
      "2022-03-26 18:56:01.923852 Epoch 1, Training Loss 1.3745367341029369\n",
      "2022-03-26 18:56:01.947260 Epoch 1, Training Loss 1.3770380099411206\n",
      "2022-03-26 18:56:01.970506 Epoch 1, Training Loss 1.379634337656943\n",
      "2022-03-26 18:56:02.004593 Epoch 1, Training Loss 1.3820709000767954\n",
      "2022-03-26 18:56:02.028716 Epoch 1, Training Loss 1.3847745359706147\n",
      "2022-03-26 18:56:02.052603 Epoch 1, Training Loss 1.3871764231215962\n",
      "2022-03-26 18:56:02.075690 Epoch 1, Training Loss 1.3896187077397886\n",
      "2022-03-26 18:56:02.098804 Epoch 1, Training Loss 1.3921961214231409\n",
      "2022-03-26 18:56:02.122388 Epoch 1, Training Loss 1.3946672516405736\n",
      "2022-03-26 18:56:02.149260 Epoch 1, Training Loss 1.3971254305766367\n",
      "2022-03-26 18:56:02.172786 Epoch 1, Training Loss 1.3995828143775921\n",
      "2022-03-26 18:56:02.206942 Epoch 1, Training Loss 1.401941439532258\n",
      "2022-03-26 18:56:02.241189 Epoch 1, Training Loss 1.4043676812020713\n",
      "2022-03-26 18:56:02.268378 Epoch 1, Training Loss 1.4068610768793794\n",
      "2022-03-26 18:56:02.291717 Epoch 1, Training Loss 1.4093776787333476\n",
      "2022-03-26 18:56:02.314997 Epoch 1, Training Loss 1.4120775371256387\n",
      "2022-03-26 18:56:02.345610 Epoch 1, Training Loss 1.414636389831143\n",
      "2022-03-26 18:56:02.369285 Epoch 1, Training Loss 1.4171010019529202\n",
      "2022-03-26 18:56:02.392610 Epoch 1, Training Loss 1.4196071467741067\n",
      "2022-03-26 18:56:02.415579 Epoch 1, Training Loss 1.4222784985971573\n",
      "2022-03-26 18:56:02.444802 Epoch 1, Training Loss 1.424690542318632\n",
      "2022-03-26 18:56:02.468755 Epoch 1, Training Loss 1.427179885947186\n",
      "2022-03-26 18:56:02.491761 Epoch 1, Training Loss 1.4297030062016929\n",
      "2022-03-26 18:56:02.520107 Epoch 1, Training Loss 1.4320816966273902\n",
      "2022-03-26 18:56:02.543080 Epoch 1, Training Loss 1.4345195508369095\n",
      "2022-03-26 18:56:02.566181 Epoch 1, Training Loss 1.4369780197167945\n",
      "2022-03-26 18:56:02.589025 Epoch 1, Training Loss 1.4395203969972519\n",
      "2022-03-26 18:56:02.611759 Epoch 1, Training Loss 1.4420663468977983\n",
      "2022-03-26 18:56:02.641282 Epoch 1, Training Loss 1.444633361018832\n",
      "2022-03-26 18:56:02.665053 Epoch 1, Training Loss 1.4470921420990048\n",
      "2022-03-26 18:56:02.688208 Epoch 1, Training Loss 1.4495654336326873\n",
      "2022-03-26 18:56:02.711674 Epoch 1, Training Loss 1.4520722842582352\n",
      "2022-03-26 18:56:02.737490 Epoch 1, Training Loss 1.4544714518520228\n",
      "2022-03-26 18:56:02.760527 Epoch 1, Training Loss 1.4568640766546244\n",
      "2022-03-26 18:56:02.783734 Epoch 1, Training Loss 1.4593643062864727\n",
      "2022-03-26 18:56:02.806578 Epoch 1, Training Loss 1.4618370426280418\n",
      "2022-03-26 18:56:02.829359 Epoch 1, Training Loss 1.4644058174489405\n",
      "2022-03-26 18:56:02.860351 Epoch 1, Training Loss 1.4669216753881607\n",
      "2022-03-26 18:56:02.885586 Epoch 1, Training Loss 1.4693855987790296\n",
      "2022-03-26 18:56:02.917301 Epoch 1, Training Loss 1.4719787471739532\n",
      "2022-03-26 18:56:02.941068 Epoch 1, Training Loss 1.4745827356872656\n",
      "2022-03-26 18:56:02.963932 Epoch 1, Training Loss 1.4770226534980033\n",
      "2022-03-26 18:56:02.991605 Epoch 1, Training Loss 1.4796236306810013\n",
      "2022-03-26 18:56:03.015188 Epoch 1, Training Loss 1.4822139991518786\n",
      "2022-03-26 18:56:03.038472 Epoch 1, Training Loss 1.484832872667581\n",
      "2022-03-26 18:56:03.062204 Epoch 1, Training Loss 1.4874491479695606\n",
      "2022-03-26 18:56:03.091437 Epoch 1, Training Loss 1.4898760071800798\n",
      "2022-03-26 18:56:03.114077 Epoch 1, Training Loss 1.492348086041258\n",
      "2022-03-26 18:56:03.136854 Epoch 1, Training Loss 1.494808832092968\n",
      "2022-03-26 18:56:03.159668 Epoch 1, Training Loss 1.497356053508456\n",
      "2022-03-26 18:56:03.182564 Epoch 1, Training Loss 1.5000070598729127\n",
      "2022-03-26 18:56:03.205751 Epoch 1, Training Loss 1.502406721987078\n",
      "2022-03-26 18:56:03.228057 Epoch 1, Training Loss 1.5048411761403389\n",
      "2022-03-26 18:56:03.264674 Epoch 1, Training Loss 1.5073664330155647\n",
      "2022-03-26 18:56:03.302984 Epoch 1, Training Loss 1.5100489155105923\n",
      "2022-03-26 18:56:03.327196 Epoch 1, Training Loss 1.5124107893470609\n",
      "2022-03-26 18:56:03.353095 Epoch 1, Training Loss 1.5149870103277514\n",
      "2022-03-26 18:56:03.377658 Epoch 1, Training Loss 1.5174356206603672\n",
      "2022-03-26 18:56:03.402291 Epoch 1, Training Loss 1.519894137102015\n",
      "2022-03-26 18:56:03.426120 Epoch 1, Training Loss 1.5223704943876437\n",
      "2022-03-26 18:56:03.452603 Epoch 1, Training Loss 1.5248339013065524\n",
      "2022-03-26 18:56:03.477654 Epoch 1, Training Loss 1.5272658519122913\n",
      "2022-03-26 18:56:03.501615 Epoch 1, Training Loss 1.5297689802201508\n",
      "2022-03-26 18:56:03.531190 Epoch 1, Training Loss 1.532159661545473\n",
      "2022-03-26 18:56:03.555441 Epoch 1, Training Loss 1.534614235848722\n",
      "2022-03-26 18:56:03.579054 Epoch 1, Training Loss 1.5372652385545813\n",
      "2022-03-26 18:56:03.602589 Epoch 1, Training Loss 1.5396836746074354\n",
      "2022-03-26 18:56:03.626126 Epoch 1, Training Loss 1.542126467313303\n",
      "2022-03-26 18:56:03.650657 Epoch 1, Training Loss 1.544619164808327\n",
      "2022-03-26 18:56:03.677743 Epoch 1, Training Loss 1.5471334375079027\n",
      "2022-03-26 18:56:03.702027 Epoch 1, Training Loss 1.5496025667775928\n",
      "2022-03-26 18:56:03.726266 Epoch 1, Training Loss 1.552005783828628\n",
      "2022-03-26 18:56:03.756007 Epoch 1, Training Loss 1.554507101588237\n",
      "2022-03-26 18:56:03.779849 Epoch 1, Training Loss 1.5570826176792154\n",
      "2022-03-26 18:56:03.803579 Epoch 1, Training Loss 1.559510076137455\n",
      "2022-03-26 18:56:03.826742 Epoch 1, Training Loss 1.5620502782294818\n",
      "2022-03-26 18:56:03.851940 Epoch 1, Training Loss 1.5644940488478716\n",
      "2022-03-26 18:56:03.877739 Epoch 1, Training Loss 1.5669109257285858\n",
      "2022-03-26 18:56:03.901075 Epoch 1, Training Loss 1.5693463997157944\n",
      "2022-03-26 18:56:03.924475 Epoch 1, Training Loss 1.571756756061788\n",
      "2022-03-26 18:56:03.948118 Epoch 1, Training Loss 1.5742125578243713\n",
      "2022-03-26 18:56:03.978066 Epoch 1, Training Loss 1.5766811658964133\n",
      "2022-03-26 18:56:04.002409 Epoch 1, Training Loss 1.57926136621124\n",
      "2022-03-26 18:56:04.025918 Epoch 1, Training Loss 1.5816603889855583\n",
      "2022-03-26 18:56:04.050164 Epoch 1, Training Loss 1.584137035910126\n",
      "2022-03-26 18:56:04.077658 Epoch 1, Training Loss 1.5867153648525247\n",
      "2022-03-26 18:56:04.105623 Epoch 1, Training Loss 1.5892677381825264\n",
      "2022-03-26 18:56:04.129097 Epoch 1, Training Loss 1.591920388781506\n",
      "2022-03-26 18:56:04.155769 Epoch 1, Training Loss 1.5943110960218914\n",
      "2022-03-26 18:56:04.179863 Epoch 1, Training Loss 1.5969210309750588\n",
      "2022-03-26 18:56:04.208997 Epoch 1, Training Loss 1.5995321007031005\n",
      "2022-03-26 18:56:04.232838 Epoch 1, Training Loss 1.6018678331009262\n",
      "2022-03-26 18:56:04.256760 Epoch 1, Training Loss 1.6042987423784592\n",
      "2022-03-26 18:56:04.281742 Epoch 1, Training Loss 1.6068615908817867\n",
      "2022-03-26 18:56:04.315717 Epoch 1, Training Loss 1.6093674157281666\n",
      "2022-03-26 18:56:04.348413 Epoch 1, Training Loss 1.6120024550601344\n",
      "2022-03-26 18:56:04.372636 Epoch 1, Training Loss 1.614377856102136\n",
      "2022-03-26 18:56:04.400607 Epoch 1, Training Loss 1.6167685994711678\n",
      "2022-03-26 18:56:04.426100 Epoch 1, Training Loss 1.6192554860468715\n",
      "2022-03-26 18:56:04.449563 Epoch 1, Training Loss 1.6219103561947719\n",
      "2022-03-26 18:56:04.473460 Epoch 1, Training Loss 1.6244601314634923\n",
      "2022-03-26 18:56:04.496425 Epoch 1, Training Loss 1.6268585277030536\n",
      "2022-03-26 18:56:04.519041 Epoch 1, Training Loss 1.6291913980108392\n",
      "2022-03-26 18:56:04.541750 Epoch 1, Training Loss 1.6316162704506798\n",
      "2022-03-26 18:56:04.564440 Epoch 1, Training Loss 1.6341866507859486\n",
      "2022-03-26 18:56:04.590638 Epoch 1, Training Loss 1.63669707540356\n",
      "2022-03-26 18:56:04.619685 Epoch 1, Training Loss 1.639124934173301\n",
      "2022-03-26 18:56:04.643065 Epoch 1, Training Loss 1.6417030314045489\n",
      "2022-03-26 18:56:04.665972 Epoch 1, Training Loss 1.6443423918446007\n",
      "2022-03-26 18:56:04.689389 Epoch 1, Training Loss 1.6466549017545207\n",
      "2022-03-26 18:56:04.716527 Epoch 1, Training Loss 1.6490666630018094\n",
      "2022-03-26 18:56:04.739752 Epoch 1, Training Loss 1.6514587682836197\n",
      "2022-03-26 18:56:04.763076 Epoch 1, Training Loss 1.65391811583658\n",
      "2022-03-26 18:56:04.786095 Epoch 1, Training Loss 1.6564142554617294\n",
      "2022-03-26 18:56:04.808552 Epoch 1, Training Loss 1.6588657012071146\n",
      "2022-03-26 18:56:04.836623 Epoch 1, Training Loss 1.6612588741895182\n",
      "2022-03-26 18:56:04.860420 Epoch 1, Training Loss 1.6639212533031278\n",
      "2022-03-26 18:56:04.883357 Epoch 1, Training Loss 1.6663815484327429\n",
      "2022-03-26 18:56:04.905948 Epoch 1, Training Loss 1.6689456143342625\n",
      "2022-03-26 18:56:04.928336 Epoch 1, Training Loss 1.671502605423598\n",
      "2022-03-26 18:56:04.950837 Epoch 1, Training Loss 1.6740909317875152\n",
      "2022-03-26 18:56:04.973389 Epoch 1, Training Loss 1.6766793264452453\n",
      "2022-03-26 18:56:04.999719 Epoch 1, Training Loss 1.6791974501231748\n",
      "2022-03-26 18:56:05.028623 Epoch 1, Training Loss 1.681643132968327\n",
      "2022-03-26 18:56:05.057403 Epoch 1, Training Loss 1.6842073229572656\n",
      "2022-03-26 18:56:05.081133 Epoch 1, Training Loss 1.6867188375319362\n",
      "2022-03-26 18:56:05.105938 Epoch 1, Training Loss 1.689106566064498\n",
      "2022-03-26 18:56:05.133240 Epoch 1, Training Loss 1.6916343289263107\n",
      "2022-03-26 18:56:05.156350 Epoch 1, Training Loss 1.6941737514322677\n",
      "2022-03-26 18:56:05.178955 Epoch 1, Training Loss 1.6967101607786115\n",
      "2022-03-26 18:56:05.201568 Epoch 1, Training Loss 1.699010079626537\n",
      "2022-03-26 18:56:05.224460 Epoch 1, Training Loss 1.7014139093401488\n",
      "2022-03-26 18:56:05.247272 Epoch 1, Training Loss 1.7038872443196718\n",
      "2022-03-26 18:56:05.276518 Epoch 1, Training Loss 1.7063749635311038\n",
      "2022-03-26 18:56:05.300500 Epoch 1, Training Loss 1.7088839224232433\n",
      "2022-03-26 18:56:05.323561 Epoch 1, Training Loss 1.711221363690808\n",
      "2022-03-26 18:56:05.357307 Epoch 1, Training Loss 1.7136843831033048\n",
      "2022-03-26 18:56:05.380989 Epoch 1, Training Loss 1.716149898898571\n",
      "2022-03-26 18:56:05.410596 Epoch 1, Training Loss 1.7185829386991613\n",
      "2022-03-26 18:56:05.433962 Epoch 1, Training Loss 1.7210780488865456\n",
      "2022-03-26 18:56:05.458029 Epoch 1, Training Loss 1.723395221068731\n",
      "2022-03-26 18:56:05.482011 Epoch 1, Training Loss 1.726094397132659\n",
      "2022-03-26 18:56:05.509135 Epoch 1, Training Loss 1.728641610773628\n",
      "2022-03-26 18:56:05.531512 Epoch 1, Training Loss 1.7311160587288839\n",
      "2022-03-26 18:56:05.553953 Epoch 1, Training Loss 1.7334750046205643\n",
      "2022-03-26 18:56:05.579151 Epoch 1, Training Loss 1.7359647008464159\n",
      "2022-03-26 18:56:05.601673 Epoch 1, Training Loss 1.7385810224906257\n",
      "2022-03-26 18:56:05.631425 Epoch 1, Training Loss 1.7410737533703484\n",
      "2022-03-26 18:56:05.654885 Epoch 1, Training Loss 1.7435666598627328\n",
      "2022-03-26 18:56:05.678221 Epoch 1, Training Loss 1.7458395206409951\n",
      "2022-03-26 18:56:05.708073 Epoch 1, Training Loss 1.7481795916776828\n",
      "2022-03-26 18:56:05.731437 Epoch 1, Training Loss 1.7504887175377069\n",
      "2022-03-26 18:56:05.753852 Epoch 1, Training Loss 1.7528818033235458\n",
      "2022-03-26 18:56:05.777160 Epoch 1, Training Loss 1.755497027388619\n",
      "2022-03-26 18:56:05.800766 Epoch 1, Training Loss 1.7579546183576364\n",
      "2022-03-26 18:56:05.828099 Epoch 1, Training Loss 1.7606210932707238\n",
      "2022-03-26 18:56:05.852005 Epoch 1, Training Loss 1.7631968843662524\n",
      "2022-03-26 18:56:05.875445 Epoch 1, Training Loss 1.7658999556165826\n",
      "2022-03-26 18:56:05.898836 Epoch 1, Training Loss 1.7683151617379445\n",
      "2022-03-26 18:56:05.935472 Epoch 1, Training Loss 1.7708509168051698\n",
      "2022-03-26 18:56:05.959857 Epoch 1, Training Loss 1.7733280271520395\n",
      "2022-03-26 18:56:05.983161 Epoch 1, Training Loss 1.7758576383676066\n",
      "2022-03-26 18:56:06.007801 Epoch 1, Training Loss 1.778344002068805\n",
      "2022-03-26 18:56:06.031480 Epoch 1, Training Loss 1.7808404830105775\n",
      "2022-03-26 18:56:06.061728 Epoch 1, Training Loss 1.7833466336245427\n",
      "2022-03-26 18:56:06.085638 Epoch 1, Training Loss 1.7858821778651088\n",
      "2022-03-26 18:56:06.109551 Epoch 1, Training Loss 1.788344542693604\n",
      "2022-03-26 18:56:06.134142 Epoch 1, Training Loss 1.7907337480798706\n",
      "2022-03-26 18:56:06.164500 Epoch 1, Training Loss 1.7933081635428816\n",
      "2022-03-26 18:56:06.188675 Epoch 1, Training Loss 1.7957794894952603\n",
      "2022-03-26 18:56:06.212698 Epoch 1, Training Loss 1.7981651952809385\n",
      "2022-03-26 18:56:06.236314 Epoch 1, Training Loss 1.8006974347412128\n",
      "2022-03-26 18:56:06.260576 Epoch 1, Training Loss 1.8030668620563224\n",
      "2022-03-26 18:56:06.283567 Epoch 1, Training Loss 1.8054656188201417\n",
      "2022-03-26 18:56:06.306708 Epoch 1, Training Loss 1.8079956323289506\n",
      "2022-03-26 18:56:06.336482 Epoch 1, Training Loss 1.8103855898618089\n",
      "2022-03-26 18:56:06.360572 Epoch 1, Training Loss 1.81281339146597\n",
      "2022-03-26 18:56:06.400114 Epoch 1, Training Loss 1.8152518063554983\n",
      "2022-03-26 18:56:06.424386 Epoch 1, Training Loss 1.8175574437431667\n",
      "2022-03-26 18:56:06.450082 Epoch 1, Training Loss 1.8200053254051891\n",
      "2022-03-26 18:56:06.476339 Epoch 1, Training Loss 1.8225167337281014\n",
      "2022-03-26 18:56:06.499885 Epoch 1, Training Loss 1.8249984394253977\n",
      "2022-03-26 18:56:06.523997 Epoch 1, Training Loss 1.8274902026061817\n",
      "2022-03-26 18:56:06.548236 Epoch 1, Training Loss 1.8300233079344415\n",
      "2022-03-26 18:56:06.572494 Epoch 1, Training Loss 1.832273172905378\n",
      "2022-03-26 18:56:06.596060 Epoch 1, Training Loss 1.8346046838918915\n",
      "2022-03-26 18:56:06.625645 Epoch 1, Training Loss 1.8369538878540859\n",
      "2022-03-26 18:56:06.650322 Epoch 1, Training Loss 1.8394220890596396\n",
      "2022-03-26 18:56:06.674354 Epoch 1, Training Loss 1.8417538733738463\n",
      "2022-03-26 18:56:06.698223 Epoch 1, Training Loss 1.8440229369856207\n",
      "2022-03-26 18:56:06.721715 Epoch 1, Training Loss 1.8464925741905447\n",
      "2022-03-26 18:56:06.745502 Epoch 1, Training Loss 1.848936059438359\n",
      "2022-03-26 18:56:06.772270 Epoch 1, Training Loss 1.8512513430222222\n",
      "2022-03-26 18:56:06.796460 Epoch 1, Training Loss 1.8536351771305895\n",
      "2022-03-26 18:56:06.821302 Epoch 1, Training Loss 1.8559776636035852\n",
      "2022-03-26 18:56:06.851410 Epoch 1, Training Loss 1.858450632723396\n",
      "2022-03-26 18:56:06.875670 Epoch 1, Training Loss 1.86084327155062\n",
      "2022-03-26 18:56:06.898937 Epoch 1, Training Loss 1.8633852492817833\n",
      "2022-03-26 18:56:06.922795 Epoch 1, Training Loss 1.8657828436788086\n",
      "2022-03-26 18:56:06.949778 Epoch 1, Training Loss 1.8681610011688583\n",
      "2022-03-26 18:56:06.973145 Epoch 1, Training Loss 1.8704800689616776\n",
      "2022-03-26 18:56:06.996591 Epoch 1, Training Loss 1.873035858049417\n",
      "2022-03-26 18:56:07.020065 Epoch 1, Training Loss 1.8755149344349151\n",
      "2022-03-26 18:56:07.043878 Epoch 1, Training Loss 1.8781488310650487\n",
      "2022-03-26 18:56:07.073682 Epoch 1, Training Loss 1.8807665981600046\n",
      "2022-03-26 18:56:07.098020 Epoch 1, Training Loss 1.8833199772993316\n",
      "2022-03-26 18:56:07.128638 Epoch 1, Training Loss 1.8856598050393107\n",
      "2022-03-26 18:56:07.153326 Epoch 1, Training Loss 1.8880613469101888\n",
      "2022-03-26 18:56:07.177322 Epoch 1, Training Loss 1.8902334183683176\n",
      "2022-03-26 18:56:07.200408 Epoch 1, Training Loss 1.892623436115587\n",
      "2022-03-26 18:56:07.223830 Epoch 1, Training Loss 1.8949638579202734\n",
      "2022-03-26 18:56:07.248409 Epoch 1, Training Loss 1.8974565302624422\n",
      "2022-03-26 18:56:07.277863 Epoch 1, Training Loss 1.8998346725083373\n",
      "2022-03-26 18:56:07.305201 Epoch 1, Training Loss 1.9022620648069455\n",
      "2022-03-26 18:56:07.330480 Epoch 1, Training Loss 1.9046549690349022\n",
      "2022-03-26 18:56:07.353744 Epoch 1, Training Loss 1.9071011255159402\n",
      "2022-03-26 18:56:07.381019 Epoch 1, Training Loss 1.909651635552916\n",
      "2022-03-26 18:56:07.408446 Epoch 1, Training Loss 1.9122635043795457\n",
      "2022-03-26 18:56:07.438095 Epoch 1, Training Loss 1.914691887090883\n",
      "2022-03-26 18:56:07.461770 Epoch 1, Training Loss 1.91711822373178\n",
      "2022-03-26 18:56:07.492058 Epoch 1, Training Loss 1.9195023058625438\n",
      "2022-03-26 18:56:07.516125 Epoch 1, Training Loss 1.9217268630976567\n",
      "2022-03-26 18:56:07.538767 Epoch 1, Training Loss 1.9242364630065\n",
      "2022-03-26 18:56:07.561220 Epoch 1, Training Loss 1.9268164874037819\n",
      "2022-03-26 18:56:07.583743 Epoch 1, Training Loss 1.9291968909675812\n",
      "2022-03-26 18:56:07.606508 Epoch 1, Training Loss 1.93145870811799\n",
      "2022-03-26 18:56:07.628978 Epoch 1, Training Loss 1.9338795614364508\n",
      "2022-03-26 18:56:07.654493 Epoch 1, Training Loss 1.9365413682844939\n",
      "2022-03-26 18:56:07.677507 Epoch 1, Training Loss 1.9389796148785545\n",
      "2022-03-26 18:56:07.704112 Epoch 1, Training Loss 1.9413285037440717\n",
      "2022-03-26 18:56:07.728468 Epoch 1, Training Loss 1.9439592573343945\n",
      "2022-03-26 18:56:07.751085 Epoch 1, Training Loss 1.9463251841342664\n",
      "2022-03-26 18:56:07.774077 Epoch 1, Training Loss 1.9487655417388663\n",
      "2022-03-26 18:56:07.797275 Epoch 1, Training Loss 1.951147095474136\n",
      "2022-03-26 18:56:07.821860 Epoch 1, Training Loss 1.9535833822796718\n",
      "2022-03-26 18:56:07.845460 Epoch 1, Training Loss 1.9561533464495178\n",
      "2022-03-26 18:56:07.869817 Epoch 1, Training Loss 1.9584586198067726\n",
      "2022-03-26 18:56:07.892808 Epoch 1, Training Loss 1.9607207700419609\n",
      "2022-03-26 18:56:07.920952 Epoch 1, Training Loss 1.9630401822002344\n",
      "2022-03-26 18:56:07.944652 Epoch 1, Training Loss 1.965435546682314\n",
      "2022-03-26 18:56:07.967811 Epoch 1, Training Loss 1.96792359745411\n",
      "2022-03-26 18:56:07.990291 Epoch 1, Training Loss 1.9704079303290227\n",
      "2022-03-26 18:56:08.013450 Epoch 1, Training Loss 1.9728507111444498\n",
      "2022-03-26 18:56:08.040172 Epoch 1, Training Loss 1.9753610801208965\n",
      "2022-03-26 18:56:08.068084 Epoch 1, Training Loss 1.9779202444169222\n",
      "2022-03-26 18:56:08.091402 Epoch 1, Training Loss 1.980349565710863\n",
      "2022-03-26 18:56:08.114685 Epoch 1, Training Loss 1.9828379634396194\n",
      "2022-03-26 18:56:08.147380 Epoch 1, Training Loss 1.9854033909490347\n",
      "2022-03-26 18:56:08.174793 Epoch 1, Training Loss 1.9877421223294094\n",
      "2022-03-26 18:56:08.199097 Epoch 1, Training Loss 1.9901995453078423\n",
      "2022-03-26 18:56:08.224706 Epoch 1, Training Loss 1.9923947162335487\n",
      "2022-03-26 18:56:08.247917 Epoch 1, Training Loss 1.994919614413815\n",
      "2022-03-26 18:56:08.271713 Epoch 1, Training Loss 1.9972896726844866\n",
      "2022-03-26 18:56:08.294845 Epoch 1, Training Loss 1.999850632741933\n",
      "2022-03-26 18:56:08.317838 Epoch 1, Training Loss 2.0022623415493292\n",
      "2022-03-26 18:56:08.344634 Epoch 1, Training Loss 2.0046256187626774\n",
      "2022-03-26 18:56:08.375094 Epoch 1, Training Loss 2.007217436342898\n",
      "2022-03-26 18:56:08.398494 Epoch 1, Training Loss 2.0095314612169095\n",
      "2022-03-26 18:56:08.420755 Epoch 1, Training Loss 2.011927899954569\n",
      "2022-03-26 18:56:08.455178 Epoch 1, Training Loss 2.014599356352521\n",
      "2022-03-26 18:56:08.482905 Epoch 1, Training Loss 2.0173421974682135\n",
      "2022-03-26 18:56:08.506284 Epoch 1, Training Loss 2.019925073741952\n",
      "2022-03-26 18:56:08.529916 Epoch 1, Training Loss 2.0222446360551487\n",
      "2022-03-26 18:56:08.553033 Epoch 1, Training Loss 2.024619925814821\n",
      "2022-03-26 18:56:08.576892 Epoch 1, Training Loss 2.027083478010524\n",
      "2022-03-26 18:56:08.605106 Epoch 1, Training Loss 2.0295401723183635\n",
      "2022-03-26 18:56:08.628866 Epoch 1, Training Loss 2.0318799281059325\n",
      "2022-03-26 18:56:08.651789 Epoch 1, Training Loss 2.0344037367864645\n",
      "2022-03-26 18:56:08.675349 Epoch 1, Training Loss 2.036862360851844\n",
      "2022-03-26 18:56:08.699298 Epoch 1, Training Loss 2.0394669685827194\n",
      "2022-03-26 18:56:08.724568 Epoch 1, Training Loss 2.0419200783800284\n",
      "2022-03-26 18:56:08.747537 Epoch 1, Training Loss 2.0444357818959618\n",
      "2022-03-26 18:56:08.770639 Epoch 1, Training Loss 2.047068105939099\n",
      "2022-03-26 18:56:08.798820 Epoch 1, Training Loss 2.0493670563258783\n",
      "2022-03-26 18:56:08.823309 Epoch 1, Training Loss 2.0518773571609534\n",
      "2022-03-26 18:56:08.832402 Epoch 1, Training Loss 2.0541127529900396\n",
      "2022-03-26 19:10:31.368929 Epoch 50, Training Loss 0.0011174742065732132\n",
      "2022-03-26 19:10:31.394766 Epoch 50, Training Loss 0.001965924373368168\n",
      "2022-03-26 19:10:31.419036 Epoch 50, Training Loss 0.0028662471972463075\n",
      "2022-03-26 19:10:31.443120 Epoch 50, Training Loss 0.004129528389562426\n",
      "2022-03-26 19:10:31.466593 Epoch 50, Training Loss 0.005327617039765848\n",
      "2022-03-26 19:10:31.489586 Epoch 50, Training Loss 0.006272673987976426\n",
      "2022-03-26 19:10:31.513428 Epoch 50, Training Loss 0.007333754883397875\n",
      "2022-03-26 19:10:31.537178 Epoch 50, Training Loss 0.008224960330807034\n",
      "2022-03-26 19:10:31.561306 Epoch 50, Training Loss 0.009328205948290617\n",
      "2022-03-26 19:10:31.591373 Epoch 50, Training Loss 0.010557622022336096\n",
      "2022-03-26 19:10:31.616374 Epoch 50, Training Loss 0.011405010662420327\n",
      "2022-03-26 19:10:31.640477 Epoch 50, Training Loss 0.012405694217023337\n",
      "2022-03-26 19:10:31.667843 Epoch 50, Training Loss 0.013289148392884628\n",
      "2022-03-26 19:10:31.692085 Epoch 50, Training Loss 0.014143765734894502\n",
      "2022-03-26 19:10:31.716179 Epoch 50, Training Loss 0.01509931134750776\n",
      "2022-03-26 19:10:31.740321 Epoch 50, Training Loss 0.016110322435798546\n",
      "2022-03-26 19:10:31.764176 Epoch 50, Training Loss 0.017339789196658316\n",
      "2022-03-26 19:10:31.788062 Epoch 50, Training Loss 0.018248136848439952\n",
      "2022-03-26 19:10:31.827572 Epoch 50, Training Loss 0.019317524359964044\n",
      "2022-03-26 19:10:31.852380 Epoch 50, Training Loss 0.02029719659129677\n",
      "2022-03-26 19:10:31.878113 Epoch 50, Training Loss 0.021182254376008992\n",
      "2022-03-26 19:10:31.901953 Epoch 50, Training Loss 0.02239747486455971\n",
      "2022-03-26 19:10:31.926198 Epoch 50, Training Loss 0.02363291680050628\n",
      "2022-03-26 19:10:31.950205 Epoch 50, Training Loss 0.024690990633976735\n",
      "2022-03-26 19:10:31.974386 Epoch 50, Training Loss 0.02561828097723939\n",
      "2022-03-26 19:10:31.998809 Epoch 50, Training Loss 0.0267796341872886\n",
      "2022-03-26 19:10:32.023172 Epoch 50, Training Loss 0.027876711486245664\n",
      "2022-03-26 19:10:32.052821 Epoch 50, Training Loss 0.029096928627594658\n",
      "2022-03-26 19:10:32.076680 Epoch 50, Training Loss 0.030049553002847736\n",
      "2022-03-26 19:10:32.100509 Epoch 50, Training Loss 0.030975654225825044\n",
      "2022-03-26 19:10:32.134901 Epoch 50, Training Loss 0.032009474914092235\n",
      "2022-03-26 19:10:32.158438 Epoch 50, Training Loss 0.03301847209710904\n",
      "2022-03-26 19:10:32.184584 Epoch 50, Training Loss 0.0341746451147377\n",
      "2022-03-26 19:10:32.212654 Epoch 50, Training Loss 0.034986940033905345\n",
      "2022-03-26 19:10:32.236826 Epoch 50, Training Loss 0.03608216067104388\n",
      "2022-03-26 19:10:32.266130 Epoch 50, Training Loss 0.03712822752230612\n",
      "2022-03-26 19:10:32.291283 Epoch 50, Training Loss 0.03811684098390057\n",
      "2022-03-26 19:10:32.315512 Epoch 50, Training Loss 0.03910178891228288\n",
      "2022-03-26 19:10:32.341877 Epoch 50, Training Loss 0.04012550051559877\n",
      "2022-03-26 19:10:32.364748 Epoch 50, Training Loss 0.04096476341147557\n",
      "2022-03-26 19:10:32.390096 Epoch 50, Training Loss 0.04225330165280101\n",
      "2022-03-26 19:10:32.412875 Epoch 50, Training Loss 0.043080620372386844\n",
      "2022-03-26 19:10:32.436123 Epoch 50, Training Loss 0.044003319176261685\n",
      "2022-03-26 19:10:32.459486 Epoch 50, Training Loss 0.04520529798229637\n",
      "2022-03-26 19:10:32.489262 Epoch 50, Training Loss 0.04645113795614608\n",
      "2022-03-26 19:10:32.513340 Epoch 50, Training Loss 0.04757840958092829\n",
      "2022-03-26 19:10:32.537103 Epoch 50, Training Loss 0.04847804314035284\n",
      "2022-03-26 19:10:32.564475 Epoch 50, Training Loss 0.04951924802092335\n",
      "2022-03-26 19:10:32.588338 Epoch 50, Training Loss 0.05043066706498871\n",
      "2022-03-26 19:10:32.612993 Epoch 50, Training Loss 0.05151864421337157\n",
      "2022-03-26 19:10:32.636941 Epoch 50, Training Loss 0.05253650434791585\n",
      "2022-03-26 19:10:32.661282 Epoch 50, Training Loss 0.05367210050068243\n",
      "2022-03-26 19:10:32.685192 Epoch 50, Training Loss 0.054705112715206484\n",
      "2022-03-26 19:10:32.715103 Epoch 50, Training Loss 0.055998882293091405\n",
      "2022-03-26 19:10:32.739708 Epoch 50, Training Loss 0.05698257090185609\n",
      "2022-03-26 19:10:32.764023 Epoch 50, Training Loss 0.058204436622312304\n",
      "2022-03-26 19:10:32.790884 Epoch 50, Training Loss 0.05920966331611204\n",
      "2022-03-26 19:10:32.814566 Epoch 50, Training Loss 0.06024934065616344\n",
      "2022-03-26 19:10:32.842594 Epoch 50, Training Loss 0.06126602500905771\n",
      "2022-03-26 19:10:32.872549 Epoch 50, Training Loss 0.06241450750309488\n",
      "2022-03-26 19:10:32.895866 Epoch 50, Training Loss 0.06334232293126528\n",
      "2022-03-26 19:10:32.925136 Epoch 50, Training Loss 0.06412738363456238\n",
      "2022-03-26 19:10:32.950345 Epoch 50, Training Loss 0.06524256702579195\n",
      "2022-03-26 19:10:32.974049 Epoch 50, Training Loss 0.06625430594624766\n",
      "2022-03-26 19:10:33.001951 Epoch 50, Training Loss 0.06716016194094783\n",
      "2022-03-26 19:10:33.033588 Epoch 50, Training Loss 0.06823053643526629\n",
      "2022-03-26 19:10:33.057223 Epoch 50, Training Loss 0.06956436231618038\n",
      "2022-03-26 19:10:33.080339 Epoch 50, Training Loss 0.07047699288943844\n",
      "2022-03-26 19:10:33.107638 Epoch 50, Training Loss 0.07154489036106393\n",
      "2022-03-26 19:10:33.135127 Epoch 50, Training Loss 0.07247739374790045\n",
      "2022-03-26 19:10:33.162380 Epoch 50, Training Loss 0.07371000309124627\n",
      "2022-03-26 19:10:33.185827 Epoch 50, Training Loss 0.0749217548486217\n",
      "2022-03-26 19:10:33.214778 Epoch 50, Training Loss 0.07585960237876228\n",
      "2022-03-26 19:10:33.238900 Epoch 50, Training Loss 0.07731184043237925\n",
      "2022-03-26 19:10:33.262902 Epoch 50, Training Loss 0.07841585504124536\n",
      "2022-03-26 19:10:33.286768 Epoch 50, Training Loss 0.07941928399188439\n",
      "2022-03-26 19:10:33.310144 Epoch 50, Training Loss 0.08043103106796284\n",
      "2022-03-26 19:10:33.337616 Epoch 50, Training Loss 0.08155657835018909\n",
      "2022-03-26 19:10:33.367859 Epoch 50, Training Loss 0.08262393800803768\n",
      "2022-03-26 19:10:33.393252 Epoch 50, Training Loss 0.08365276905581774\n",
      "2022-03-26 19:10:33.417028 Epoch 50, Training Loss 0.08482035163723295\n",
      "2022-03-26 19:10:33.444441 Epoch 50, Training Loss 0.0858939753468994\n",
      "2022-03-26 19:10:33.468056 Epoch 50, Training Loss 0.08675587184898689\n",
      "2022-03-26 19:10:33.491081 Epoch 50, Training Loss 0.08787316122018468\n",
      "2022-03-26 19:10:33.513506 Epoch 50, Training Loss 0.08893492490129398\n",
      "2022-03-26 19:10:33.536296 Epoch 50, Training Loss 0.08985617634890329\n",
      "2022-03-26 19:10:33.559268 Epoch 50, Training Loss 0.0907528087153764\n",
      "2022-03-26 19:10:33.587470 Epoch 50, Training Loss 0.09174356779174121\n",
      "2022-03-26 19:10:33.611436 Epoch 50, Training Loss 0.09291037375969655\n",
      "2022-03-26 19:10:33.635395 Epoch 50, Training Loss 0.09421785057658125\n",
      "2022-03-26 19:10:33.662140 Epoch 50, Training Loss 0.09506350870022688\n",
      "2022-03-26 19:10:33.685604 Epoch 50, Training Loss 0.09602166868536674\n",
      "2022-03-26 19:10:33.708588 Epoch 50, Training Loss 0.09716842409290011\n",
      "2022-03-26 19:10:33.731367 Epoch 50, Training Loss 0.09811061102411021\n",
      "2022-03-26 19:10:33.754446 Epoch 50, Training Loss 0.0990056480135759\n",
      "2022-03-26 19:10:33.777311 Epoch 50, Training Loss 0.10006880722082484\n",
      "2022-03-26 19:10:33.807067 Epoch 50, Training Loss 0.10128221586537178\n",
      "2022-03-26 19:10:33.831073 Epoch 50, Training Loss 0.10209225297279065\n",
      "2022-03-26 19:10:33.854273 Epoch 50, Training Loss 0.10285842022322633\n",
      "2022-03-26 19:10:33.890771 Epoch 50, Training Loss 0.10399200087008269\n",
      "2022-03-26 19:10:33.913700 Epoch 50, Training Loss 0.1054001974937556\n",
      "2022-03-26 19:10:33.944149 Epoch 50, Training Loss 0.10642495446497827\n",
      "2022-03-26 19:10:33.967351 Epoch 50, Training Loss 0.1074080913877853\n",
      "2022-03-26 19:10:33.990312 Epoch 50, Training Loss 0.1085219958706585\n",
      "2022-03-26 19:10:34.023546 Epoch 50, Training Loss 0.1095982927952886\n",
      "2022-03-26 19:10:34.047512 Epoch 50, Training Loss 0.11058265801585848\n",
      "2022-03-26 19:10:34.070376 Epoch 50, Training Loss 0.11143946388493413\n",
      "2022-03-26 19:10:34.097396 Epoch 50, Training Loss 0.11232961405573598\n",
      "2022-03-26 19:10:34.120335 Epoch 50, Training Loss 0.11306106952755042\n",
      "2022-03-26 19:10:34.143164 Epoch 50, Training Loss 0.11404788760882814\n",
      "2022-03-26 19:10:34.166038 Epoch 50, Training Loss 0.11491042550872355\n",
      "2022-03-26 19:10:34.188688 Epoch 50, Training Loss 0.11593925175459488\n",
      "2022-03-26 19:10:34.211122 Epoch 50, Training Loss 0.11700006968834821\n",
      "2022-03-26 19:10:34.240657 Epoch 50, Training Loss 0.11810353756560694\n",
      "2022-03-26 19:10:34.265161 Epoch 50, Training Loss 0.11922008210740735\n",
      "2022-03-26 19:10:34.287836 Epoch 50, Training Loss 0.12029479943273012\n",
      "2022-03-26 19:10:34.311311 Epoch 50, Training Loss 0.12162304702012436\n",
      "2022-03-26 19:10:34.341960 Epoch 50, Training Loss 0.12271096921332962\n",
      "2022-03-26 19:10:34.365084 Epoch 50, Training Loss 0.12345452397070883\n",
      "2022-03-26 19:10:34.389305 Epoch 50, Training Loss 0.12446443473591524\n",
      "2022-03-26 19:10:34.412046 Epoch 50, Training Loss 0.12560836684978222\n",
      "2022-03-26 19:10:34.435176 Epoch 50, Training Loss 0.12669551387772232\n",
      "2022-03-26 19:10:34.463813 Epoch 50, Training Loss 0.12763868077941562\n",
      "2022-03-26 19:10:34.487096 Epoch 50, Training Loss 0.12870184234950854\n",
      "2022-03-26 19:10:34.509447 Epoch 50, Training Loss 0.1295306067485029\n",
      "2022-03-26 19:10:34.535220 Epoch 50, Training Loss 0.13052877684688324\n",
      "2022-03-26 19:10:34.557658 Epoch 50, Training Loss 0.13167129407453415\n",
      "2022-03-26 19:10:34.580290 Epoch 50, Training Loss 0.13263313255041762\n",
      "2022-03-26 19:10:34.602969 Epoch 50, Training Loss 0.13376771024121042\n",
      "2022-03-26 19:10:34.626450 Epoch 50, Training Loss 0.13494285217026616\n",
      "2022-03-26 19:10:34.649252 Epoch 50, Training Loss 0.1361926518895132\n",
      "2022-03-26 19:10:34.678138 Epoch 50, Training Loss 0.13689989468935507\n",
      "2022-03-26 19:10:34.702153 Epoch 50, Training Loss 0.13784280785209382\n",
      "2022-03-26 19:10:34.725127 Epoch 50, Training Loss 0.13914654832666792\n",
      "2022-03-26 19:10:34.747774 Epoch 50, Training Loss 0.14007781693697585\n",
      "2022-03-26 19:10:34.774287 Epoch 50, Training Loss 0.1409959210001904\n",
      "2022-03-26 19:10:34.796659 Epoch 50, Training Loss 0.14206221288122484\n",
      "2022-03-26 19:10:34.819719 Epoch 50, Training Loss 0.1432009412504523\n",
      "2022-03-26 19:10:34.843399 Epoch 50, Training Loss 0.14412462086323888\n",
      "2022-03-26 19:10:34.865875 Epoch 50, Training Loss 0.144865267798114\n",
      "2022-03-26 19:10:34.895212 Epoch 50, Training Loss 0.14574426161053847\n",
      "2022-03-26 19:10:34.926370 Epoch 50, Training Loss 0.14682093796217838\n",
      "2022-03-26 19:10:34.949086 Epoch 50, Training Loss 0.14787299904372075\n",
      "2022-03-26 19:10:34.975314 Epoch 50, Training Loss 0.14895760509973902\n",
      "2022-03-26 19:10:34.998207 Epoch 50, Training Loss 0.15019900925324092\n",
      "2022-03-26 19:10:35.021240 Epoch 50, Training Loss 0.15127223737709358\n",
      "2022-03-26 19:10:35.044553 Epoch 50, Training Loss 0.15229483242230038\n",
      "2022-03-26 19:10:35.067407 Epoch 50, Training Loss 0.15357927501658955\n",
      "2022-03-26 19:10:35.089943 Epoch 50, Training Loss 0.15476903860526317\n",
      "2022-03-26 19:10:35.127385 Epoch 50, Training Loss 0.15558301899439234\n",
      "2022-03-26 19:10:35.154639 Epoch 50, Training Loss 0.15654077531431643\n",
      "2022-03-26 19:10:35.179696 Epoch 50, Training Loss 0.15765366682311152\n",
      "2022-03-26 19:10:35.207092 Epoch 50, Training Loss 0.1584968963242553\n",
      "2022-03-26 19:10:35.230224 Epoch 50, Training Loss 0.15939394470370943\n",
      "2022-03-26 19:10:35.253019 Epoch 50, Training Loss 0.16060123221038858\n",
      "2022-03-26 19:10:35.275682 Epoch 50, Training Loss 0.16156286237489842\n",
      "2022-03-26 19:10:35.298208 Epoch 50, Training Loss 0.16267549785811578\n",
      "2022-03-26 19:10:35.321405 Epoch 50, Training Loss 0.16388624632145132\n",
      "2022-03-26 19:10:35.353287 Epoch 50, Training Loss 0.16492229425693716\n",
      "2022-03-26 19:10:35.377802 Epoch 50, Training Loss 0.16604356235250486\n",
      "2022-03-26 19:10:35.404134 Epoch 50, Training Loss 0.16701983612821536\n",
      "2022-03-26 19:10:35.429565 Epoch 50, Training Loss 0.1678420331167138\n",
      "2022-03-26 19:10:35.452662 Epoch 50, Training Loss 0.16885976116065785\n",
      "2022-03-26 19:10:35.476462 Epoch 50, Training Loss 0.1700210830439692\n",
      "2022-03-26 19:10:35.498855 Epoch 50, Training Loss 0.17099012438293612\n",
      "2022-03-26 19:10:35.521300 Epoch 50, Training Loss 0.1717887526887762\n",
      "2022-03-26 19:10:35.544133 Epoch 50, Training Loss 0.17317256217112625\n",
      "2022-03-26 19:10:35.572092 Epoch 50, Training Loss 0.173991061537467\n",
      "2022-03-26 19:10:35.595342 Epoch 50, Training Loss 0.17477945911000148\n",
      "2022-03-26 19:10:35.618398 Epoch 50, Training Loss 0.17599171567755892\n",
      "2022-03-26 19:10:35.641057 Epoch 50, Training Loss 0.1768971022285159\n",
      "2022-03-26 19:10:35.663756 Epoch 50, Training Loss 0.1777690078139\n",
      "2022-03-26 19:10:35.690202 Epoch 50, Training Loss 0.17891443103475643\n",
      "2022-03-26 19:10:35.713062 Epoch 50, Training Loss 0.18024006318253324\n",
      "2022-03-26 19:10:35.737027 Epoch 50, Training Loss 0.1810147487141592\n",
      "2022-03-26 19:10:35.759611 Epoch 50, Training Loss 0.1821390768451154\n",
      "2022-03-26 19:10:35.786793 Epoch 50, Training Loss 0.18294116397343024\n",
      "2022-03-26 19:10:35.810490 Epoch 50, Training Loss 0.1841121329675855\n",
      "2022-03-26 19:10:35.834069 Epoch 50, Training Loss 0.18514465896979623\n",
      "2022-03-26 19:10:35.860337 Epoch 50, Training Loss 0.18633936989642774\n",
      "2022-03-26 19:10:35.883234 Epoch 50, Training Loss 0.18724432686710601\n",
      "2022-03-26 19:10:35.905377 Epoch 50, Training Loss 0.1881027366499157\n",
      "2022-03-26 19:10:35.935363 Epoch 50, Training Loss 0.18913672692940364\n",
      "2022-03-26 19:10:35.959448 Epoch 50, Training Loss 0.19006561752780318\n",
      "2022-03-26 19:10:35.981827 Epoch 50, Training Loss 0.19121082534875405\n",
      "2022-03-26 19:10:36.010957 Epoch 50, Training Loss 0.1921531312605914\n",
      "2022-03-26 19:10:36.040217 Epoch 50, Training Loss 0.19334148026793205\n",
      "2022-03-26 19:10:36.064190 Epoch 50, Training Loss 0.19442226972116533\n",
      "2022-03-26 19:10:36.093079 Epoch 50, Training Loss 0.19545792329036976\n",
      "2022-03-26 19:10:36.116346 Epoch 50, Training Loss 0.19640468522105986\n",
      "2022-03-26 19:10:36.140357 Epoch 50, Training Loss 0.19738409189921816\n",
      "2022-03-26 19:10:36.165089 Epoch 50, Training Loss 0.19832352040064\n",
      "2022-03-26 19:10:36.187676 Epoch 50, Training Loss 0.19935973365898327\n",
      "2022-03-26 19:10:36.210595 Epoch 50, Training Loss 0.20054060418892394\n",
      "2022-03-26 19:10:36.240748 Epoch 50, Training Loss 0.20156600621655163\n",
      "2022-03-26 19:10:36.264247 Epoch 50, Training Loss 0.20250187139681844\n",
      "2022-03-26 19:10:36.288543 Epoch 50, Training Loss 0.2033309960914085\n",
      "2022-03-26 19:10:36.312152 Epoch 50, Training Loss 0.204303856853329\n",
      "2022-03-26 19:10:36.337895 Epoch 50, Training Loss 0.20523462377850663\n",
      "2022-03-26 19:10:36.360617 Epoch 50, Training Loss 0.20614404804871211\n",
      "2022-03-26 19:10:36.383953 Epoch 50, Training Loss 0.20715820400611215\n",
      "2022-03-26 19:10:36.407115 Epoch 50, Training Loss 0.20811407309968757\n",
      "2022-03-26 19:10:36.430510 Epoch 50, Training Loss 0.20912534211907546\n",
      "2022-03-26 19:10:36.459510 Epoch 50, Training Loss 0.2102677169663217\n",
      "2022-03-26 19:10:36.483151 Epoch 50, Training Loss 0.21103382438345028\n",
      "2022-03-26 19:10:36.506011 Epoch 50, Training Loss 0.21215385381522997\n",
      "2022-03-26 19:10:36.530387 Epoch 50, Training Loss 0.2129895908905722\n",
      "2022-03-26 19:10:36.553342 Epoch 50, Training Loss 0.21402654935941673\n",
      "2022-03-26 19:10:36.576246 Epoch 50, Training Loss 0.21506465243561493\n",
      "2022-03-26 19:10:36.598425 Epoch 50, Training Loss 0.21608153328566296\n",
      "2022-03-26 19:10:36.621200 Epoch 50, Training Loss 0.21708691546984035\n",
      "2022-03-26 19:10:36.644584 Epoch 50, Training Loss 0.21832279880028552\n",
      "2022-03-26 19:10:36.672399 Epoch 50, Training Loss 0.21927545838953588\n",
      "2022-03-26 19:10:36.696175 Epoch 50, Training Loss 0.2201568803671376\n",
      "2022-03-26 19:10:36.719131 Epoch 50, Training Loss 0.22153613199968167\n",
      "2022-03-26 19:10:36.745320 Epoch 50, Training Loss 0.2224483276572069\n",
      "2022-03-26 19:10:36.769056 Epoch 50, Training Loss 0.22363919362692578\n",
      "2022-03-26 19:10:36.792039 Epoch 50, Training Loss 0.2249274670772845\n",
      "2022-03-26 19:10:36.814830 Epoch 50, Training Loss 0.22588556494249407\n",
      "2022-03-26 19:10:36.837891 Epoch 50, Training Loss 0.22691418584960196\n",
      "2022-03-26 19:10:36.861177 Epoch 50, Training Loss 0.2280113917330037\n",
      "2022-03-26 19:10:36.889971 Epoch 50, Training Loss 0.22909054213472643\n",
      "2022-03-26 19:10:36.913872 Epoch 50, Training Loss 0.23036934286737076\n",
      "2022-03-26 19:10:36.941372 Epoch 50, Training Loss 0.231367274547172\n",
      "2022-03-26 19:10:36.977667 Epoch 50, Training Loss 0.2322344656490609\n",
      "2022-03-26 19:10:37.000636 Epoch 50, Training Loss 0.2331369163282692\n",
      "2022-03-26 19:10:37.029034 Epoch 50, Training Loss 0.2340052642328355\n",
      "2022-03-26 19:10:37.053289 Epoch 50, Training Loss 0.23508146793945975\n",
      "2022-03-26 19:10:37.076666 Epoch 50, Training Loss 0.23623660984246628\n",
      "2022-03-26 19:10:37.105851 Epoch 50, Training Loss 0.23770719271181795\n",
      "2022-03-26 19:10:37.130730 Epoch 50, Training Loss 0.238780727090738\n",
      "2022-03-26 19:10:37.154468 Epoch 50, Training Loss 0.2398094412920725\n",
      "2022-03-26 19:10:37.177674 Epoch 50, Training Loss 0.24083696660178397\n",
      "2022-03-26 19:10:37.205077 Epoch 50, Training Loss 0.24183851289932076\n",
      "2022-03-26 19:10:37.228497 Epoch 50, Training Loss 0.24303286939935612\n",
      "2022-03-26 19:10:37.252688 Epoch 50, Training Loss 0.24415975328906417\n",
      "2022-03-26 19:10:37.276312 Epoch 50, Training Loss 0.24507715718825454\n",
      "2022-03-26 19:10:37.299483 Epoch 50, Training Loss 0.2461257968717219\n",
      "2022-03-26 19:10:37.330586 Epoch 50, Training Loss 0.24714521061428976\n",
      "2022-03-26 19:10:37.355107 Epoch 50, Training Loss 0.24833585988834997\n",
      "2022-03-26 19:10:37.378550 Epoch 50, Training Loss 0.24914983113098632\n",
      "2022-03-26 19:10:37.406911 Epoch 50, Training Loss 0.2502816481053677\n",
      "2022-03-26 19:10:37.430591 Epoch 50, Training Loss 0.25112759983143235\n",
      "2022-03-26 19:10:37.454991 Epoch 50, Training Loss 0.25239077484821115\n",
      "2022-03-26 19:10:37.478646 Epoch 50, Training Loss 0.25329593982537996\n",
      "2022-03-26 19:10:37.501569 Epoch 50, Training Loss 0.25423333711941226\n",
      "2022-03-26 19:10:37.524781 Epoch 50, Training Loss 0.255286864764855\n",
      "2022-03-26 19:10:37.553912 Epoch 50, Training Loss 0.25609248274427543\n",
      "2022-03-26 19:10:37.579485 Epoch 50, Training Loss 0.2570584156476628\n",
      "2022-03-26 19:10:37.604983 Epoch 50, Training Loss 0.2579435682510171\n",
      "2022-03-26 19:10:37.629339 Epoch 50, Training Loss 0.2589956713302056\n",
      "2022-03-26 19:10:37.659020 Epoch 50, Training Loss 0.26020237529064383\n",
      "2022-03-26 19:10:37.682301 Epoch 50, Training Loss 0.2613073176587634\n",
      "2022-03-26 19:10:37.706177 Epoch 50, Training Loss 0.26248062777397274\n",
      "2022-03-26 19:10:37.730546 Epoch 50, Training Loss 0.26348726470452133\n",
      "2022-03-26 19:10:37.755542 Epoch 50, Training Loss 0.26463374404041357\n",
      "2022-03-26 19:10:37.788111 Epoch 50, Training Loss 0.2657644015443904\n",
      "2022-03-26 19:10:37.812266 Epoch 50, Training Loss 0.26662426462868594\n",
      "2022-03-26 19:10:37.836485 Epoch 50, Training Loss 0.26753871558267445\n",
      "2022-03-26 19:10:37.871156 Epoch 50, Training Loss 0.26858414210321957\n",
      "2022-03-26 19:10:37.895349 Epoch 50, Training Loss 0.26974723314690163\n",
      "2022-03-26 19:10:37.919374 Epoch 50, Training Loss 0.2708179244147542\n",
      "2022-03-26 19:10:37.944727 Epoch 50, Training Loss 0.2718204471003979\n",
      "2022-03-26 19:10:37.972932 Epoch 50, Training Loss 0.2727792500077611\n",
      "2022-03-26 19:10:38.006425 Epoch 50, Training Loss 0.27376876820993545\n",
      "2022-03-26 19:10:38.029513 Epoch 50, Training Loss 0.27467140730689554\n",
      "2022-03-26 19:10:38.054673 Epoch 50, Training Loss 0.2757261744545549\n",
      "2022-03-26 19:10:38.078807 Epoch 50, Training Loss 0.27652889147134085\n",
      "2022-03-26 19:10:38.102671 Epoch 50, Training Loss 0.2773980457154686\n",
      "2022-03-26 19:10:38.127332 Epoch 50, Training Loss 0.278279344367859\n",
      "2022-03-26 19:10:38.151772 Epoch 50, Training Loss 0.2792199507088917\n",
      "2022-03-26 19:10:38.175621 Epoch 50, Training Loss 0.2800685190941062\n",
      "2022-03-26 19:10:38.205371 Epoch 50, Training Loss 0.28101301940200885\n",
      "2022-03-26 19:10:38.229324 Epoch 50, Training Loss 0.2819641256881187\n",
      "2022-03-26 19:10:38.253636 Epoch 50, Training Loss 0.2832170668465402\n",
      "2022-03-26 19:10:38.279484 Epoch 50, Training Loss 0.28425352629798145\n",
      "2022-03-26 19:10:38.303255 Epoch 50, Training Loss 0.2855906783010039\n",
      "2022-03-26 19:10:38.326785 Epoch 50, Training Loss 0.28647107175548975\n",
      "2022-03-26 19:10:38.350754 Epoch 50, Training Loss 0.28783522877851714\n",
      "2022-03-26 19:10:38.373339 Epoch 50, Training Loss 0.2890745498945036\n",
      "2022-03-26 19:10:38.396676 Epoch 50, Training Loss 0.29020566624753613\n",
      "2022-03-26 19:10:38.425430 Epoch 50, Training Loss 0.2912405451850208\n",
      "2022-03-26 19:10:38.449969 Epoch 50, Training Loss 0.29227729945841346\n",
      "2022-03-26 19:10:38.473702 Epoch 50, Training Loss 0.29360879100192233\n",
      "2022-03-26 19:10:38.500777 Epoch 50, Training Loss 0.2947226713228104\n",
      "2022-03-26 19:10:38.523236 Epoch 50, Training Loss 0.29575948131358837\n",
      "2022-03-26 19:10:38.546461 Epoch 50, Training Loss 0.29664929512211735\n",
      "2022-03-26 19:10:38.569365 Epoch 50, Training Loss 0.2972219874105795\n",
      "2022-03-26 19:10:38.591694 Epoch 50, Training Loss 0.29834526072225304\n",
      "2022-03-26 19:10:38.614443 Epoch 50, Training Loss 0.2993905575150419\n",
      "2022-03-26 19:10:38.643671 Epoch 50, Training Loss 0.30031524979702345\n",
      "2022-03-26 19:10:38.667423 Epoch 50, Training Loss 0.30137424033773524\n",
      "2022-03-26 19:10:38.689679 Epoch 50, Training Loss 0.302412005222362\n",
      "2022-03-26 19:10:38.714741 Epoch 50, Training Loss 0.30367696593941934\n",
      "2022-03-26 19:10:38.737802 Epoch 50, Training Loss 0.3044568974419933\n",
      "2022-03-26 19:10:38.760891 Epoch 50, Training Loss 0.30567226831412986\n",
      "2022-03-26 19:10:38.783868 Epoch 50, Training Loss 0.30675544805081606\n",
      "2022-03-26 19:10:38.806820 Epoch 50, Training Loss 0.3077868276925953\n",
      "2022-03-26 19:10:38.830499 Epoch 50, Training Loss 0.3092762854169397\n",
      "2022-03-26 19:10:38.860162 Epoch 50, Training Loss 0.31043219311005626\n",
      "2022-03-26 19:10:38.884951 Epoch 50, Training Loss 0.31135256245465537\n",
      "2022-03-26 19:10:38.908312 Epoch 50, Training Loss 0.3124874446474378\n",
      "2022-03-26 19:10:38.935292 Epoch 50, Training Loss 0.3137468241745859\n",
      "2022-03-26 19:10:38.958074 Epoch 50, Training Loss 0.314579158373501\n",
      "2022-03-26 19:10:38.980751 Epoch 50, Training Loss 0.3156780332631772\n",
      "2022-03-26 19:10:39.006534 Epoch 50, Training Loss 0.3164544015208169\n",
      "2022-03-26 19:10:39.035308 Epoch 50, Training Loss 0.3173527888705968\n",
      "2022-03-26 19:10:39.069967 Epoch 50, Training Loss 0.31809473232082697\n",
      "2022-03-26 19:10:39.094217 Epoch 50, Training Loss 0.3190036670250051\n",
      "2022-03-26 19:10:39.118432 Epoch 50, Training Loss 0.31992181003703485\n",
      "2022-03-26 19:10:39.141723 Epoch 50, Training Loss 0.3208540618190985\n",
      "2022-03-26 19:10:39.169843 Epoch 50, Training Loss 0.3217737699484886\n",
      "2022-03-26 19:10:39.193169 Epoch 50, Training Loss 0.32298966674396146\n",
      "2022-03-26 19:10:39.215882 Epoch 50, Training Loss 0.32420218962690106\n",
      "2022-03-26 19:10:39.238363 Epoch 50, Training Loss 0.3252176286848\n",
      "2022-03-26 19:10:39.261029 Epoch 50, Training Loss 0.3261348114675268\n",
      "2022-03-26 19:10:39.289354 Epoch 50, Training Loss 0.3274535979227642\n",
      "2022-03-26 19:10:39.313082 Epoch 50, Training Loss 0.328509630487703\n",
      "2022-03-26 19:10:39.338614 Epoch 50, Training Loss 0.329343294205568\n",
      "2022-03-26 19:10:39.365379 Epoch 50, Training Loss 0.3303457466918794\n",
      "2022-03-26 19:10:39.388213 Epoch 50, Training Loss 0.33117163711039305\n",
      "2022-03-26 19:10:39.412068 Epoch 50, Training Loss 0.3323976589971796\n",
      "2022-03-26 19:10:39.435545 Epoch 50, Training Loss 0.33357705526492176\n",
      "2022-03-26 19:10:39.458856 Epoch 50, Training Loss 0.3342936495152276\n",
      "2022-03-26 19:10:39.482349 Epoch 50, Training Loss 0.33521630571169014\n",
      "2022-03-26 19:10:39.512791 Epoch 50, Training Loss 0.33628069817105216\n",
      "2022-03-26 19:10:39.537227 Epoch 50, Training Loss 0.33722247930286486\n",
      "2022-03-26 19:10:39.560481 Epoch 50, Training Loss 0.3382092160565774\n",
      "2022-03-26 19:10:39.587497 Epoch 50, Training Loss 0.33928523111678754\n",
      "2022-03-26 19:10:39.611182 Epoch 50, Training Loss 0.3403734023232594\n",
      "2022-03-26 19:10:39.634509 Epoch 50, Training Loss 0.3412991959954162\n",
      "2022-03-26 19:10:39.658053 Epoch 50, Training Loss 0.34234733986275273\n",
      "2022-03-26 19:10:39.681564 Epoch 50, Training Loss 0.34335039990484867\n",
      "2022-03-26 19:10:39.704494 Epoch 50, Training Loss 0.34440319910836037\n",
      "2022-03-26 19:10:39.733684 Epoch 50, Training Loss 0.34551460301632164\n",
      "2022-03-26 19:10:39.757628 Epoch 50, Training Loss 0.34651324027182195\n",
      "2022-03-26 19:10:39.780667 Epoch 50, Training Loss 0.34752049588638806\n",
      "2022-03-26 19:10:39.809730 Epoch 50, Training Loss 0.34836020124385425\n",
      "2022-03-26 19:10:39.834272 Epoch 50, Training Loss 0.3494258053848506\n",
      "2022-03-26 19:10:39.858792 Epoch 50, Training Loss 0.35074293053211153\n",
      "2022-03-26 19:10:39.882966 Epoch 50, Training Loss 0.3518507110569483\n",
      "2022-03-26 19:10:39.906437 Epoch 50, Training Loss 0.35288465270758285\n",
      "2022-03-26 19:10:39.930147 Epoch 50, Training Loss 0.3537150511275167\n",
      "2022-03-26 19:10:39.965504 Epoch 50, Training Loss 0.35471586570562913\n",
      "2022-03-26 19:10:39.989551 Epoch 50, Training Loss 0.3558323749572115\n",
      "2022-03-26 19:10:40.016855 Epoch 50, Training Loss 0.3570623965290806\n",
      "2022-03-26 19:10:40.051703 Epoch 50, Training Loss 0.3582034805775299\n",
      "2022-03-26 19:10:40.075405 Epoch 50, Training Loss 0.3593627073042228\n",
      "2022-03-26 19:10:40.098664 Epoch 50, Training Loss 0.3602671535957195\n",
      "2022-03-26 19:10:40.121639 Epoch 50, Training Loss 0.3614021489001296\n",
      "2022-03-26 19:10:40.144960 Epoch 50, Training Loss 0.3622962381604992\n",
      "2022-03-26 19:10:40.172056 Epoch 50, Training Loss 0.36327841371069175\n",
      "2022-03-26 19:10:40.197563 Epoch 50, Training Loss 0.3645338320442478\n",
      "2022-03-26 19:10:40.221047 Epoch 50, Training Loss 0.36591672824929133\n",
      "2022-03-26 19:10:40.243843 Epoch 50, Training Loss 0.36687780832847977\n",
      "2022-03-26 19:10:40.270847 Epoch 50, Training Loss 0.3676642347555941\n",
      "2022-03-26 19:10:40.293941 Epoch 50, Training Loss 0.36872588692571195\n",
      "2022-03-26 19:10:40.317083 Epoch 50, Training Loss 0.3695813048907253\n",
      "2022-03-26 19:10:40.343325 Epoch 50, Training Loss 0.3705801225607962\n",
      "2022-03-26 19:10:40.366321 Epoch 50, Training Loss 0.3716913489505763\n",
      "2022-03-26 19:10:40.396395 Epoch 50, Training Loss 0.3726823597079348\n",
      "2022-03-26 19:10:40.420727 Epoch 50, Training Loss 0.3740507966798285\n",
      "2022-03-26 19:10:40.443825 Epoch 50, Training Loss 0.37507853411195224\n",
      "2022-03-26 19:10:40.469419 Epoch 50, Training Loss 0.37623005758618455\n",
      "2022-03-26 19:10:40.491952 Epoch 50, Training Loss 0.3771716450410121\n",
      "2022-03-26 19:10:40.514268 Epoch 50, Training Loss 0.3781307431514306\n",
      "2022-03-26 19:10:40.537464 Epoch 50, Training Loss 0.3788961526149374\n",
      "2022-03-26 19:10:40.560701 Epoch 50, Training Loss 0.37987734682267277\n",
      "2022-03-26 19:10:40.583434 Epoch 50, Training Loss 0.3809025493805366\n",
      "2022-03-26 19:10:40.609588 Epoch 50, Training Loss 0.38242265319122987\n",
      "2022-03-26 19:10:40.633489 Epoch 50, Training Loss 0.3832959533118836\n",
      "2022-03-26 19:10:40.656114 Epoch 50, Training Loss 0.38415103270422163\n",
      "2022-03-26 19:10:40.678698 Epoch 50, Training Loss 0.3850068434348802\n",
      "2022-03-26 19:10:40.704942 Epoch 50, Training Loss 0.3859427942872962\n",
      "2022-03-26 19:10:40.727612 Epoch 50, Training Loss 0.38680898887879406\n",
      "2022-03-26 19:10:40.750145 Epoch 50, Training Loss 0.3878153895249452\n",
      "2022-03-26 19:10:40.772829 Epoch 50, Training Loss 0.38861427926803793\n",
      "2022-03-26 19:10:40.795556 Epoch 50, Training Loss 0.3901462893732978\n",
      "2022-03-26 19:10:40.823080 Epoch 50, Training Loss 0.391398409329107\n",
      "2022-03-26 19:10:40.847721 Epoch 50, Training Loss 0.3926157645328575\n",
      "2022-03-26 19:10:40.874969 Epoch 50, Training Loss 0.39393959893747366\n",
      "2022-03-26 19:10:40.904334 Epoch 50, Training Loss 0.3949558281761301\n",
      "2022-03-26 19:10:40.927918 Epoch 50, Training Loss 0.39590732577969046\n",
      "2022-03-26 19:10:40.950658 Epoch 50, Training Loss 0.3968267226996629\n",
      "2022-03-26 19:10:40.973338 Epoch 50, Training Loss 0.3980386527374272\n",
      "2022-03-26 19:10:40.997640 Epoch 50, Training Loss 0.39918541546215486\n",
      "2022-03-26 19:10:41.020714 Epoch 50, Training Loss 0.4003661644961828\n",
      "2022-03-26 19:10:41.051786 Epoch 50, Training Loss 0.40150441450383656\n",
      "2022-03-26 19:10:41.081437 Epoch 50, Training Loss 0.4023453485401695\n",
      "2022-03-26 19:10:41.107389 Epoch 50, Training Loss 0.40344843051165263\n",
      "2022-03-26 19:10:41.130155 Epoch 50, Training Loss 0.40438225251786847\n",
      "2022-03-26 19:10:41.152634 Epoch 50, Training Loss 0.4053612392195655\n",
      "2022-03-26 19:10:41.175562 Epoch 50, Training Loss 0.4063721198178923\n",
      "2022-03-26 19:10:41.197800 Epoch 50, Training Loss 0.40739342071058804\n",
      "2022-03-26 19:10:41.221062 Epoch 50, Training Loss 0.4082590433414025\n",
      "2022-03-26 19:10:41.243478 Epoch 50, Training Loss 0.40922198381722735\n",
      "2022-03-26 19:10:41.272270 Epoch 50, Training Loss 0.4101398357421236\n",
      "2022-03-26 19:10:41.295169 Epoch 50, Training Loss 0.41096064482655975\n",
      "2022-03-26 19:10:41.317282 Epoch 50, Training Loss 0.4122841539209151\n",
      "2022-03-26 19:10:41.346109 Epoch 50, Training Loss 0.4130770656687524\n",
      "2022-03-26 19:10:41.368459 Epoch 50, Training Loss 0.41407129606779886\n",
      "2022-03-26 19:10:41.390782 Epoch 50, Training Loss 0.41501067083357546\n",
      "2022-03-26 19:10:41.414912 Epoch 50, Training Loss 0.415916727822455\n",
      "2022-03-26 19:10:41.437627 Epoch 50, Training Loss 0.4170154848748156\n",
      "2022-03-26 19:10:41.460672 Epoch 50, Training Loss 0.417845509126973\n",
      "2022-03-26 19:10:41.490623 Epoch 50, Training Loss 0.41895948876352873\n",
      "2022-03-26 19:10:41.514922 Epoch 50, Training Loss 0.42014234545438184\n",
      "2022-03-26 19:10:41.538496 Epoch 50, Training Loss 0.42111772554152455\n",
      "2022-03-26 19:10:41.566705 Epoch 50, Training Loss 0.4221353808708508\n",
      "2022-03-26 19:10:41.590099 Epoch 50, Training Loss 0.42312586311336675\n",
      "2022-03-26 19:10:41.613268 Epoch 50, Training Loss 0.42424111876188947\n",
      "2022-03-26 19:10:41.637832 Epoch 50, Training Loss 0.4251962891396354\n",
      "2022-03-26 19:10:41.660998 Epoch 50, Training Loss 0.4261151980942168\n",
      "2022-03-26 19:10:41.683668 Epoch 50, Training Loss 0.4274738190500328\n",
      "2022-03-26 19:10:41.713234 Epoch 50, Training Loss 0.4282339119621555\n",
      "2022-03-26 19:10:41.736667 Epoch 50, Training Loss 0.4292996538340893\n",
      "2022-03-26 19:10:41.760136 Epoch 50, Training Loss 0.4301146590115164\n",
      "2022-03-26 19:10:41.783516 Epoch 50, Training Loss 0.4312630195523162\n",
      "2022-03-26 19:10:41.809905 Epoch 50, Training Loss 0.43229821556821807\n",
      "2022-03-26 19:10:41.833395 Epoch 50, Training Loss 0.4333873715470819\n",
      "2022-03-26 19:10:41.857729 Epoch 50, Training Loss 0.43426625255276174\n",
      "2022-03-26 19:10:41.886234 Epoch 50, Training Loss 0.4351269848587568\n",
      "2022-03-26 19:10:41.909719 Epoch 50, Training Loss 0.4364541315133005\n",
      "2022-03-26 19:10:41.939092 Epoch 50, Training Loss 0.4378813154938276\n",
      "2022-03-26 19:10:41.962566 Epoch 50, Training Loss 0.43867180967117514\n",
      "2022-03-26 19:10:41.988487 Epoch 50, Training Loss 0.4397332910305399\n",
      "2022-03-26 19:10:42.013403 Epoch 50, Training Loss 0.44069883810437244\n",
      "2022-03-26 19:10:42.036367 Epoch 50, Training Loss 0.4419431315587305\n",
      "2022-03-26 19:10:42.059807 Epoch 50, Training Loss 0.44289103474306024\n",
      "2022-03-26 19:10:42.094845 Epoch 50, Training Loss 0.4439114806673411\n",
      "2022-03-26 19:10:42.119657 Epoch 50, Training Loss 0.4451263620496711\n",
      "2022-03-26 19:10:42.146598 Epoch 50, Training Loss 0.4461897706512905\n",
      "2022-03-26 19:10:42.171996 Epoch 50, Training Loss 0.4470000317715623\n",
      "2022-03-26 19:10:42.194481 Epoch 50, Training Loss 0.4483563539088535\n",
      "2022-03-26 19:10:42.221314 Epoch 50, Training Loss 0.4492848065426892\n",
      "2022-03-26 19:10:42.244686 Epoch 50, Training Loss 0.45031362265119773\n",
      "2022-03-26 19:10:42.267512 Epoch 50, Training Loss 0.4513557130647132\n",
      "2022-03-26 19:10:42.290096 Epoch 50, Training Loss 0.4523035532144634\n",
      "2022-03-26 19:10:42.312619 Epoch 50, Training Loss 0.4532656596825861\n",
      "2022-03-26 19:10:42.337361 Epoch 50, Training Loss 0.45425092484182716\n",
      "2022-03-26 19:10:42.366462 Epoch 50, Training Loss 0.4554372157358452\n",
      "2022-03-26 19:10:42.391668 Epoch 50, Training Loss 0.456464756747036\n",
      "2022-03-26 19:10:42.416468 Epoch 50, Training Loss 0.45760012004533995\n",
      "2022-03-26 19:10:42.443178 Epoch 50, Training Loss 0.45882736969634397\n",
      "2022-03-26 19:10:42.465805 Epoch 50, Training Loss 0.45985419171697955\n",
      "2022-03-26 19:10:42.488350 Epoch 50, Training Loss 0.46104182806008914\n",
      "2022-03-26 19:10:42.510658 Epoch 50, Training Loss 0.4620767193453391\n",
      "2022-03-26 19:10:42.532816 Epoch 50, Training Loss 0.46301692263092226\n",
      "2022-03-26 19:10:42.555219 Epoch 50, Training Loss 0.4645606121977272\n",
      "2022-03-26 19:10:42.583952 Epoch 50, Training Loss 0.46550920342697816\n",
      "2022-03-26 19:10:42.608623 Epoch 50, Training Loss 0.4665194145782524\n",
      "2022-03-26 19:10:42.631336 Epoch 50, Training Loss 0.4677981635569916\n",
      "2022-03-26 19:10:42.657648 Epoch 50, Training Loss 0.4689107179413061\n",
      "2022-03-26 19:10:42.680638 Epoch 50, Training Loss 0.46998840604749176\n",
      "2022-03-26 19:10:42.703055 Epoch 50, Training Loss 0.47107699105654227\n",
      "2022-03-26 19:10:42.725513 Epoch 50, Training Loss 0.4717980761585943\n",
      "2022-03-26 19:10:42.748036 Epoch 50, Training Loss 0.47292387535047653\n",
      "2022-03-26 19:10:42.770421 Epoch 50, Training Loss 0.4740331552141463\n",
      "2022-03-26 19:10:42.796561 Epoch 50, Training Loss 0.4750993166052167\n",
      "2022-03-26 19:10:42.820520 Epoch 50, Training Loss 0.4760249596651253\n",
      "2022-03-26 19:10:42.843628 Epoch 50, Training Loss 0.4770002970686349\n",
      "2022-03-26 19:10:42.870810 Epoch 50, Training Loss 0.4778122340550508\n",
      "2022-03-26 19:10:42.893679 Epoch 50, Training Loss 0.4787806432951442\n",
      "2022-03-26 19:10:42.916792 Epoch 50, Training Loss 0.47973946037957127\n",
      "2022-03-26 19:10:42.939896 Epoch 50, Training Loss 0.4806223284176853\n",
      "2022-03-26 19:10:42.962844 Epoch 50, Training Loss 0.48177283426837236\n",
      "2022-03-26 19:10:42.991172 Epoch 50, Training Loss 0.48282997843707004\n",
      "2022-03-26 19:10:43.021659 Epoch 50, Training Loss 0.4838760361418395\n",
      "2022-03-26 19:10:43.045163 Epoch 50, Training Loss 0.4849633748650246\n",
      "2022-03-26 19:10:43.068266 Epoch 50, Training Loss 0.4859210119756591\n",
      "2022-03-26 19:10:43.094533 Epoch 50, Training Loss 0.4869878716252344\n",
      "2022-03-26 19:10:43.127340 Epoch 50, Training Loss 0.4878513769191854\n",
      "2022-03-26 19:10:43.149661 Epoch 50, Training Loss 0.4887388852017615\n",
      "2022-03-26 19:10:43.172320 Epoch 50, Training Loss 0.4899863995173398\n",
      "2022-03-26 19:10:43.194972 Epoch 50, Training Loss 0.49100464685341283\n",
      "2022-03-26 19:10:43.217527 Epoch 50, Training Loss 0.4919872009921867\n",
      "2022-03-26 19:10:43.246995 Epoch 50, Training Loss 0.4928140915034677\n",
      "2022-03-26 19:10:43.271306 Epoch 50, Training Loss 0.49374025012068734\n",
      "2022-03-26 19:10:43.297550 Epoch 50, Training Loss 0.4946490712559132\n",
      "2022-03-26 19:10:43.320206 Epoch 50, Training Loss 0.49546109528645227\n",
      "2022-03-26 19:10:43.345514 Epoch 50, Training Loss 0.4964135296433173\n",
      "2022-03-26 19:10:43.368589 Epoch 50, Training Loss 0.4972385748496751\n",
      "2022-03-26 19:10:43.391288 Epoch 50, Training Loss 0.49823836692611273\n",
      "2022-03-26 19:10:43.415280 Epoch 50, Training Loss 0.4992635889202737\n",
      "2022-03-26 19:10:43.438132 Epoch 50, Training Loss 0.5004335393762345\n",
      "2022-03-26 19:10:43.467904 Epoch 50, Training Loss 0.5019506722536233\n",
      "2022-03-26 19:10:43.491128 Epoch 50, Training Loss 0.5029336047523162\n",
      "2022-03-26 19:10:43.513621 Epoch 50, Training Loss 0.5041164182259908\n",
      "2022-03-26 19:10:43.538854 Epoch 50, Training Loss 0.5050455319988149\n",
      "2022-03-26 19:10:43.561183 Epoch 50, Training Loss 0.5059692571916239\n",
      "2022-03-26 19:10:43.584259 Epoch 50, Training Loss 0.5070042955067456\n",
      "2022-03-26 19:10:43.607139 Epoch 50, Training Loss 0.5083428399108559\n",
      "2022-03-26 19:10:43.630054 Epoch 50, Training Loss 0.5093739344107221\n",
      "2022-03-26 19:10:43.652459 Epoch 50, Training Loss 0.5106526870480583\n",
      "2022-03-26 19:10:43.680535 Epoch 50, Training Loss 0.511977636867472\n",
      "2022-03-26 19:10:43.704029 Epoch 50, Training Loss 0.5131385341248549\n",
      "2022-03-26 19:10:43.726575 Epoch 50, Training Loss 0.5140069927019841\n",
      "2022-03-26 19:10:43.749781 Epoch 50, Training Loss 0.5149351035618721\n",
      "2022-03-26 19:10:43.776338 Epoch 50, Training Loss 0.5158122987164866\n",
      "2022-03-26 19:10:43.799131 Epoch 50, Training Loss 0.5165237687966403\n",
      "2022-03-26 19:10:43.822196 Epoch 50, Training Loss 0.517557008453952\n",
      "2022-03-26 19:10:43.845189 Epoch 50, Training Loss 0.5187790129724366\n",
      "2022-03-26 19:10:43.871970 Epoch 50, Training Loss 0.5199101208649632\n",
      "2022-03-26 19:10:43.901254 Epoch 50, Training Loss 0.5210119661543985\n",
      "2022-03-26 19:10:43.925299 Epoch 50, Training Loss 0.5221560451456958\n",
      "2022-03-26 19:10:43.951185 Epoch 50, Training Loss 0.5232664704551477\n",
      "2022-03-26 19:10:43.979515 Epoch 50, Training Loss 0.5242777399318602\n",
      "2022-03-26 19:10:44.003202 Epoch 50, Training Loss 0.5251979384657062\n",
      "2022-03-26 19:10:44.027342 Epoch 50, Training Loss 0.526022516819827\n",
      "2022-03-26 19:10:44.050962 Epoch 50, Training Loss 0.527265953705134\n",
      "2022-03-26 19:10:44.075169 Epoch 50, Training Loss 0.528285896114986\n",
      "2022-03-26 19:10:44.098641 Epoch 50, Training Loss 0.5292749153759778\n",
      "2022-03-26 19:10:44.132336 Epoch 50, Training Loss 0.5307758034723798\n",
      "2022-03-26 19:10:44.160663 Epoch 50, Training Loss 0.5317151802198966\n",
      "2022-03-26 19:10:44.188631 Epoch 50, Training Loss 0.532489736481091\n",
      "2022-03-26 19:10:44.212204 Epoch 50, Training Loss 0.533684447560164\n",
      "2022-03-26 19:10:44.236132 Epoch 50, Training Loss 0.5349061283690241\n",
      "2022-03-26 19:10:44.259564 Epoch 50, Training Loss 0.5361063207125725\n",
      "2022-03-26 19:10:44.284802 Epoch 50, Training Loss 0.5374134833474293\n",
      "2022-03-26 19:10:44.308545 Epoch 50, Training Loss 0.5381922819044279\n",
      "2022-03-26 19:10:44.335012 Epoch 50, Training Loss 0.5393470127488036\n",
      "2022-03-26 19:10:44.367543 Epoch 50, Training Loss 0.5403411422315461\n",
      "2022-03-26 19:10:44.391354 Epoch 50, Training Loss 0.5413309474049322\n",
      "2022-03-26 19:10:44.416811 Epoch 50, Training Loss 0.5429306779904743\n",
      "2022-03-26 19:10:44.442273 Epoch 50, Training Loss 0.5443567121044144\n",
      "2022-03-26 19:10:44.466181 Epoch 50, Training Loss 0.5452547388537156\n",
      "2022-03-26 19:10:44.490568 Epoch 50, Training Loss 0.5462152163314697\n",
      "2022-03-26 19:10:44.513247 Epoch 50, Training Loss 0.5472554324761681\n",
      "2022-03-26 19:10:44.536346 Epoch 50, Training Loss 0.5481420387621121\n",
      "2022-03-26 19:10:44.565304 Epoch 50, Training Loss 0.5493159559757813\n",
      "2022-03-26 19:10:44.589220 Epoch 50, Training Loss 0.5503063850924182\n",
      "2022-03-26 19:10:44.612433 Epoch 50, Training Loss 0.5515411888318293\n",
      "2022-03-26 19:10:44.640312 Epoch 50, Training Loss 0.5524796433842091\n",
      "2022-03-26 19:10:44.663554 Epoch 50, Training Loss 0.5536914828716947\n",
      "2022-03-26 19:10:44.686214 Epoch 50, Training Loss 0.5545106843837997\n",
      "2022-03-26 19:10:44.708671 Epoch 50, Training Loss 0.5557665032575198\n",
      "2022-03-26 19:10:44.730950 Epoch 50, Training Loss 0.5569708849234349\n",
      "2022-03-26 19:10:44.753527 Epoch 50, Training Loss 0.5579332252368903\n",
      "2022-03-26 19:10:44.781405 Epoch 50, Training Loss 0.5594419357188217\n",
      "2022-03-26 19:10:44.805070 Epoch 50, Training Loss 0.560595902602386\n",
      "2022-03-26 19:10:44.828018 Epoch 50, Training Loss 0.561479905415374\n",
      "2022-03-26 19:10:44.851202 Epoch 50, Training Loss 0.5625576910079287\n",
      "2022-03-26 19:10:44.876354 Epoch 50, Training Loss 0.5634201381288831\n",
      "2022-03-26 19:10:44.899193 Epoch 50, Training Loss 0.5643374184741998\n",
      "2022-03-26 19:10:44.921757 Epoch 50, Training Loss 0.5653412257847579\n",
      "2022-03-26 19:10:44.944543 Epoch 50, Training Loss 0.5666152090596421\n",
      "2022-03-26 19:10:44.968935 Epoch 50, Training Loss 0.5677445031264249\n",
      "2022-03-26 19:10:44.997785 Epoch 50, Training Loss 0.5685044485513512\n",
      "2022-03-26 19:10:45.021560 Epoch 50, Training Loss 0.5694855520563662\n",
      "2022-03-26 19:10:45.044001 Epoch 50, Training Loss 0.5702686708830201\n",
      "2022-03-26 19:10:45.073059 Epoch 50, Training Loss 0.5711973468437219\n",
      "2022-03-26 19:10:45.098004 Epoch 50, Training Loss 0.5722163403811662\n",
      "2022-03-26 19:10:45.121051 Epoch 50, Training Loss 0.5733301338865934\n",
      "2022-03-26 19:10:45.149257 Epoch 50, Training Loss 0.574352807362976\n",
      "2022-03-26 19:10:45.180958 Epoch 50, Training Loss 0.575194650103369\n",
      "2022-03-26 19:10:45.206735 Epoch 50, Training Loss 0.5761748625113227\n",
      "2022-03-26 19:10:45.231687 Epoch 50, Training Loss 0.5770850538673913\n",
      "2022-03-26 19:10:45.254486 Epoch 50, Training Loss 0.5780655408225706\n",
      "2022-03-26 19:10:45.280400 Epoch 50, Training Loss 0.579255900762575\n",
      "2022-03-26 19:10:45.303381 Epoch 50, Training Loss 0.5803286526590357\n",
      "2022-03-26 19:10:45.326898 Epoch 50, Training Loss 0.5811235837238219\n",
      "2022-03-26 19:10:45.350568 Epoch 50, Training Loss 0.5821645674117081\n",
      "2022-03-26 19:10:45.374863 Epoch 50, Training Loss 0.5830114904190878\n",
      "2022-03-26 19:10:45.398292 Epoch 50, Training Loss 0.5839595535908209\n",
      "2022-03-26 19:10:45.430663 Epoch 50, Training Loss 0.5851300844679708\n",
      "2022-03-26 19:10:45.454858 Epoch 50, Training Loss 0.5865059390930873\n",
      "2022-03-26 19:10:45.478894 Epoch 50, Training Loss 0.5875848356796347\n",
      "2022-03-26 19:10:45.502794 Epoch 50, Training Loss 0.5885794836160777\n",
      "2022-03-26 19:10:45.525298 Epoch 50, Training Loss 0.5894851372446246\n",
      "2022-03-26 19:10:45.547838 Epoch 50, Training Loss 0.590340486915825\n",
      "2022-03-26 19:10:45.570419 Epoch 50, Training Loss 0.5913153208811265\n",
      "2022-03-26 19:10:45.593122 Epoch 50, Training Loss 0.5924264004696971\n",
      "2022-03-26 19:10:45.615702 Epoch 50, Training Loss 0.5932346631575118\n",
      "2022-03-26 19:10:45.643477 Epoch 50, Training Loss 0.5941793759689307\n",
      "2022-03-26 19:10:45.667684 Epoch 50, Training Loss 0.5948307443686458\n",
      "2022-03-26 19:10:45.690757 Epoch 50, Training Loss 0.5960494336645926\n",
      "2022-03-26 19:10:45.713444 Epoch 50, Training Loss 0.5969577214068464\n",
      "2022-03-26 19:10:45.739482 Epoch 50, Training Loss 0.5979576329212359\n",
      "2022-03-26 19:10:45.762024 Epoch 50, Training Loss 0.5988131527934233\n",
      "2022-03-26 19:10:45.786046 Epoch 50, Training Loss 0.5996911295920687\n",
      "2022-03-26 19:10:45.809249 Epoch 50, Training Loss 0.6006136931040708\n",
      "2022-03-26 19:10:45.833335 Epoch 50, Training Loss 0.6015745574784706\n",
      "2022-03-26 19:10:45.862040 Epoch 50, Training Loss 0.6027365385190301\n",
      "2022-03-26 19:10:45.886136 Epoch 50, Training Loss 0.6037542749472591\n",
      "2022-03-26 19:10:45.909682 Epoch 50, Training Loss 0.6047583205239547\n",
      "2022-03-26 19:10:45.936559 Epoch 50, Training Loss 0.6057892502345088\n",
      "2022-03-26 19:10:45.959783 Epoch 50, Training Loss 0.6067854218242114\n",
      "2022-03-26 19:10:45.992537 Epoch 50, Training Loss 0.6080762120845068\n",
      "2022-03-26 19:10:46.016725 Epoch 50, Training Loss 0.6089847082905757\n",
      "2022-03-26 19:10:46.040474 Epoch 50, Training Loss 0.6100453238581758\n",
      "2022-03-26 19:10:46.072230 Epoch 50, Training Loss 0.6111956529528894\n",
      "2022-03-26 19:10:46.096297 Epoch 50, Training Loss 0.6125012702877869\n",
      "2022-03-26 19:10:46.119178 Epoch 50, Training Loss 0.6135523231971599\n",
      "2022-03-26 19:10:46.142328 Epoch 50, Training Loss 0.6143962030901629\n",
      "2022-03-26 19:10:46.168600 Epoch 50, Training Loss 0.6155498855940217\n",
      "2022-03-26 19:10:46.204361 Epoch 50, Training Loss 0.6167402722112968\n",
      "2022-03-26 19:10:46.228316 Epoch 50, Training Loss 0.617707959107121\n",
      "2022-03-26 19:10:46.252002 Epoch 50, Training Loss 0.6186294362444402\n",
      "2022-03-26 19:10:46.281669 Epoch 50, Training Loss 0.6196947076817607\n",
      "2022-03-26 19:10:46.307254 Epoch 50, Training Loss 0.6205745206388367\n",
      "2022-03-26 19:10:46.332920 Epoch 50, Training Loss 0.6214905950190771\n",
      "2022-03-26 19:10:46.360390 Epoch 50, Training Loss 0.6223847335180663\n",
      "2022-03-26 19:10:46.383962 Epoch 50, Training Loss 0.6230865399474683\n",
      "2022-03-26 19:10:46.406955 Epoch 50, Training Loss 0.6243061493805913\n",
      "2022-03-26 19:10:46.431875 Epoch 50, Training Loss 0.6256195029715443\n",
      "2022-03-26 19:10:46.455106 Epoch 50, Training Loss 0.6266844442966953\n",
      "2022-03-26 19:10:46.479030 Epoch 50, Training Loss 0.6276970004562832\n",
      "2022-03-26 19:10:46.508985 Epoch 50, Training Loss 0.6287437811913088\n",
      "2022-03-26 19:10:46.532205 Epoch 50, Training Loss 0.629752716330616\n",
      "2022-03-26 19:10:46.555935 Epoch 50, Training Loss 0.6309223553484968\n",
      "2022-03-26 19:10:46.580977 Epoch 50, Training Loss 0.6322706802498044\n",
      "2022-03-26 19:10:46.603941 Epoch 50, Training Loss 0.6333484668332292\n",
      "2022-03-26 19:10:46.627551 Epoch 50, Training Loss 0.6345152485629787\n",
      "2022-03-26 19:10:46.650448 Epoch 50, Training Loss 0.6355309316035732\n",
      "2022-03-26 19:10:46.674056 Epoch 50, Training Loss 0.636865024257194\n",
      "2022-03-26 19:10:46.697612 Epoch 50, Training Loss 0.6376460967847454\n",
      "2022-03-26 19:10:46.726475 Epoch 50, Training Loss 0.6385320622064269\n",
      "2022-03-26 19:10:46.750330 Epoch 50, Training Loss 0.6393436256348325\n",
      "2022-03-26 19:10:46.773295 Epoch 50, Training Loss 0.6402934504973005\n",
      "2022-03-26 19:10:46.799486 Epoch 50, Training Loss 0.6412564306841482\n",
      "2022-03-26 19:10:46.822300 Epoch 50, Training Loss 0.6425836049306118\n",
      "2022-03-26 19:10:46.845419 Epoch 50, Training Loss 0.6440746230466287\n",
      "2022-03-26 19:10:46.868310 Epoch 50, Training Loss 0.6452536180882198\n",
      "2022-03-26 19:10:46.895062 Epoch 50, Training Loss 0.6462049099719128\n",
      "2022-03-26 19:10:46.917781 Epoch 50, Training Loss 0.6472867431924166\n",
      "2022-03-26 19:10:46.949508 Epoch 50, Training Loss 0.6481695817330914\n",
      "2022-03-26 19:10:46.973517 Epoch 50, Training Loss 0.6489528531537336\n",
      "2022-03-26 19:10:46.996823 Epoch 50, Training Loss 0.6500442341694137\n",
      "2022-03-26 19:10:47.028551 Epoch 50, Training Loss 0.6511323514877988\n",
      "2022-03-26 19:10:47.051793 Epoch 50, Training Loss 0.6521465741383755\n",
      "2022-03-26 19:10:47.074401 Epoch 50, Training Loss 0.6531506913625981\n",
      "2022-03-26 19:10:47.097484 Epoch 50, Training Loss 0.6539296396171955\n",
      "2022-03-26 19:10:47.119986 Epoch 50, Training Loss 0.654996368014599\n",
      "2022-03-26 19:10:47.142479 Epoch 50, Training Loss 0.6559932046305493\n",
      "2022-03-26 19:10:47.170905 Epoch 50, Training Loss 0.6569246951195286\n",
      "2022-03-26 19:10:47.194758 Epoch 50, Training Loss 0.6579869419641202\n",
      "2022-03-26 19:10:47.227476 Epoch 50, Training Loss 0.6590522071513374\n",
      "2022-03-26 19:10:47.253837 Epoch 50, Training Loss 0.6598184486408063\n",
      "2022-03-26 19:10:47.276730 Epoch 50, Training Loss 0.6609878907804294\n",
      "2022-03-26 19:10:47.299637 Epoch 50, Training Loss 0.6622270525187788\n",
      "2022-03-26 19:10:47.322214 Epoch 50, Training Loss 0.6633551528920298\n",
      "2022-03-26 19:10:47.347651 Epoch 50, Training Loss 0.6642518265701621\n",
      "2022-03-26 19:10:47.371481 Epoch 50, Training Loss 0.665265739764399\n",
      "2022-03-26 19:10:47.402794 Epoch 50, Training Loss 0.6663566801477881\n",
      "2022-03-26 19:10:47.427089 Epoch 50, Training Loss 0.6673770848366306\n",
      "2022-03-26 19:10:47.449843 Epoch 50, Training Loss 0.6682495918801373\n",
      "2022-03-26 19:10:47.475864 Epoch 50, Training Loss 0.6691614902004257\n",
      "2022-03-26 19:10:47.499171 Epoch 50, Training Loss 0.670354449268802\n",
      "2022-03-26 19:10:47.521801 Epoch 50, Training Loss 0.6714964200697287\n",
      "2022-03-26 19:10:47.544606 Epoch 50, Training Loss 0.6726839198252125\n",
      "2022-03-26 19:10:47.567185 Epoch 50, Training Loss 0.6737686726824402\n",
      "2022-03-26 19:10:47.595141 Epoch 50, Training Loss 0.6748380336691352\n",
      "2022-03-26 19:10:47.619205 Epoch 50, Training Loss 0.6759715624858657\n",
      "2022-03-26 19:10:47.642223 Epoch 50, Training Loss 0.6770082602034444\n",
      "2022-03-26 19:10:47.665595 Epoch 50, Training Loss 0.6780852717740457\n",
      "2022-03-26 19:10:47.692393 Epoch 50, Training Loss 0.679174429277325\n",
      "2022-03-26 19:10:47.716297 Epoch 50, Training Loss 0.6801936669499064\n",
      "2022-03-26 19:10:47.739139 Epoch 50, Training Loss 0.6813751598986824\n",
      "2022-03-26 19:10:47.761725 Epoch 50, Training Loss 0.6823945495554858\n",
      "2022-03-26 19:10:47.784591 Epoch 50, Training Loss 0.6837115890305975\n",
      "2022-03-26 19:10:47.813877 Epoch 50, Training Loss 0.6848414142799499\n",
      "2022-03-26 19:10:47.838490 Epoch 50, Training Loss 0.6859332548306726\n",
      "2022-03-26 19:10:47.861500 Epoch 50, Training Loss 0.6869432238285499\n",
      "2022-03-26 19:10:47.887860 Epoch 50, Training Loss 0.6877161415717791\n",
      "2022-03-26 19:10:47.910553 Epoch 50, Training Loss 0.6888935756881523\n",
      "2022-03-26 19:10:47.933135 Epoch 50, Training Loss 0.6899589440782966\n",
      "2022-03-26 19:10:47.956208 Epoch 50, Training Loss 0.690846523246192\n",
      "2022-03-26 19:10:47.979083 Epoch 50, Training Loss 0.6923213177705969\n",
      "2022-03-26 19:10:48.001836 Epoch 50, Training Loss 0.69345727330431\n",
      "2022-03-26 19:10:48.029827 Epoch 50, Training Loss 0.6945478033150554\n",
      "2022-03-26 19:10:48.054332 Epoch 50, Training Loss 0.6958649924877659\n",
      "2022-03-26 19:10:48.077127 Epoch 50, Training Loss 0.6966342696219759\n",
      "2022-03-26 19:10:48.107336 Epoch 50, Training Loss 0.6976222342923474\n",
      "2022-03-26 19:10:48.130585 Epoch 50, Training Loss 0.6986897430380287\n",
      "2022-03-26 19:10:48.156751 Epoch 50, Training Loss 0.6995743479189056\n",
      "2022-03-26 19:10:48.183687 Epoch 50, Training Loss 0.7005232550070414\n",
      "2022-03-26 19:10:48.206839 Epoch 50, Training Loss 0.7017813322260557\n",
      "2022-03-26 19:10:48.234649 Epoch 50, Training Loss 0.7029520384872051\n",
      "2022-03-26 19:10:48.268467 Epoch 50, Training Loss 0.7039884916885429\n",
      "2022-03-26 19:10:48.291790 Epoch 50, Training Loss 0.7050143533274341\n",
      "2022-03-26 19:10:48.318463 Epoch 50, Training Loss 0.7060254221148503\n",
      "2022-03-26 19:10:48.344135 Epoch 50, Training Loss 0.7069029974586823\n",
      "2022-03-26 19:10:48.366709 Epoch 50, Training Loss 0.7078288228386809\n",
      "2022-03-26 19:10:48.390099 Epoch 50, Training Loss 0.7089864057119545\n",
      "2022-03-26 19:10:48.412639 Epoch 50, Training Loss 0.7097846290568257\n",
      "2022-03-26 19:10:48.436647 Epoch 50, Training Loss 0.7107333987951279\n",
      "2022-03-26 19:10:48.466811 Epoch 50, Training Loss 0.7116911179955353\n",
      "2022-03-26 19:10:48.489390 Epoch 50, Training Loss 0.7129059923655542\n",
      "2022-03-26 19:10:48.512195 Epoch 50, Training Loss 0.7140454119047546\n",
      "2022-03-26 19:10:48.538405 Epoch 50, Training Loss 0.7152557243090456\n",
      "2022-03-26 19:10:48.560597 Epoch 50, Training Loss 0.7160585288272794\n",
      "2022-03-26 19:10:48.585028 Epoch 50, Training Loss 0.7171411470455282\n",
      "2022-03-26 19:10:48.610091 Epoch 50, Training Loss 0.7181996099098259\n",
      "2022-03-26 19:10:48.633452 Epoch 50, Training Loss 0.7192818509114672\n",
      "2022-03-26 19:10:48.662957 Epoch 50, Training Loss 0.7202245835834147\n",
      "2022-03-26 19:10:48.686890 Epoch 50, Training Loss 0.7211395167100155\n",
      "2022-03-26 19:10:48.709924 Epoch 50, Training Loss 0.722200836710003\n",
      "2022-03-26 19:10:48.733398 Epoch 50, Training Loss 0.7231105812598982\n",
      "2022-03-26 19:10:48.761587 Epoch 50, Training Loss 0.7242162971926467\n",
      "2022-03-26 19:10:48.784099 Epoch 50, Training Loss 0.7252140225428144\n",
      "2022-03-26 19:10:48.806723 Epoch 50, Training Loss 0.7262139466336316\n",
      "2022-03-26 19:10:48.829608 Epoch 50, Training Loss 0.727588535384144\n",
      "2022-03-26 19:10:48.852324 Epoch 50, Training Loss 0.7288159284064227\n",
      "2022-03-26 19:10:48.880584 Epoch 50, Training Loss 0.7298748371622447\n",
      "2022-03-26 19:10:48.904589 Epoch 50, Training Loss 0.7307471976332043\n",
      "2022-03-26 19:10:48.927093 Epoch 50, Training Loss 0.7316746347776765\n",
      "2022-03-26 19:10:48.950339 Epoch 50, Training Loss 0.7327563213494123\n",
      "2022-03-26 19:10:48.976768 Epoch 50, Training Loss 0.7337970748505629\n",
      "2022-03-26 19:10:49.006474 Epoch 50, Training Loss 0.735123782473452\n",
      "2022-03-26 19:10:49.029808 Epoch 50, Training Loss 0.7362814732753408\n",
      "2022-03-26 19:10:49.052624 Epoch 50, Training Loss 0.7373716001544157\n",
      "2022-03-26 19:10:49.079082 Epoch 50, Training Loss 0.7386737061507257\n",
      "2022-03-26 19:10:49.108669 Epoch 50, Training Loss 0.7394712063891199\n",
      "2022-03-26 19:10:49.131413 Epoch 50, Training Loss 0.7405788695339657\n",
      "2022-03-26 19:10:49.154151 Epoch 50, Training Loss 0.7415392512784285\n",
      "2022-03-26 19:10:49.179879 Epoch 50, Training Loss 0.7428311842405583\n",
      "2022-03-26 19:10:49.202582 Epoch 50, Training Loss 0.7437380401755843\n",
      "2022-03-26 19:10:49.224888 Epoch 50, Training Loss 0.7448941133439998\n",
      "2022-03-26 19:10:49.247057 Epoch 50, Training Loss 0.7456944236898666\n",
      "2022-03-26 19:10:49.274188 Epoch 50, Training Loss 0.7465129136810522\n",
      "2022-03-26 19:10:49.302307 Epoch 50, Training Loss 0.7474355557766716\n",
      "2022-03-26 19:10:49.335802 Epoch 50, Training Loss 0.7483784584209437\n",
      "2022-03-26 19:10:49.359861 Epoch 50, Training Loss 0.7496233882425386\n",
      "2022-03-26 19:10:49.382804 Epoch 50, Training Loss 0.7506203437628953\n",
      "2022-03-26 19:10:49.409322 Epoch 50, Training Loss 0.7516050027383258\n",
      "2022-03-26 19:10:49.434826 Epoch 50, Training Loss 0.7527533680048135\n",
      "2022-03-26 19:10:49.457764 Epoch 50, Training Loss 0.7537766441588512\n",
      "2022-03-26 19:10:49.481113 Epoch 50, Training Loss 0.7548139766430306\n",
      "2022-03-26 19:10:49.504815 Epoch 50, Training Loss 0.7559776532909145\n",
      "2022-03-26 19:10:49.528222 Epoch 50, Training Loss 0.7569152908328244\n",
      "2022-03-26 19:10:49.557462 Epoch 50, Training Loss 0.7582286096671048\n",
      "2022-03-26 19:10:49.581259 Epoch 50, Training Loss 0.7594960342968822\n",
      "2022-03-26 19:10:49.605620 Epoch 50, Training Loss 0.7606307723561821\n",
      "2022-03-26 19:10:49.634622 Epoch 50, Training Loss 0.761541491007561\n",
      "2022-03-26 19:10:49.658419 Epoch 50, Training Loss 0.7624340561573463\n",
      "2022-03-26 19:10:49.681920 Epoch 50, Training Loss 0.7633910207720973\n",
      "2022-03-26 19:10:49.705767 Epoch 50, Training Loss 0.7644373878569859\n",
      "2022-03-26 19:10:49.729800 Epoch 50, Training Loss 0.7655491638747628\n",
      "2022-03-26 19:10:49.753881 Epoch 50, Training Loss 0.7664774591889223\n",
      "2022-03-26 19:10:49.784027 Epoch 50, Training Loss 0.7676700102474988\n",
      "2022-03-26 19:10:49.808419 Epoch 50, Training Loss 0.7685157368173989\n",
      "2022-03-26 19:10:49.832446 Epoch 50, Training Loss 0.7693880177138711\n",
      "2022-03-26 19:10:49.857658 Epoch 50, Training Loss 0.7705286865877679\n",
      "2022-03-26 19:10:49.881852 Epoch 50, Training Loss 0.7715564905606267\n",
      "2022-03-26 19:10:49.913027 Epoch 50, Training Loss 0.7724442778112334\n",
      "2022-03-26 19:10:49.937631 Epoch 50, Training Loss 0.7735104320375511\n",
      "2022-03-26 19:10:49.961743 Epoch 50, Training Loss 0.7742872308663395\n",
      "2022-03-26 19:10:49.985459 Epoch 50, Training Loss 0.7754742105674866\n",
      "2022-03-26 19:10:50.018723 Epoch 50, Training Loss 0.7767641776434296\n",
      "2022-03-26 19:10:50.042674 Epoch 50, Training Loss 0.7780435401231737\n",
      "2022-03-26 19:10:50.069711 Epoch 50, Training Loss 0.7792844380182988\n",
      "2022-03-26 19:10:50.093987 Epoch 50, Training Loss 0.7801025779274724\n",
      "2022-03-26 19:10:50.118906 Epoch 50, Training Loss 0.7809884237587604\n",
      "2022-03-26 19:10:50.142593 Epoch 50, Training Loss 0.7820102197053792\n",
      "2022-03-26 19:10:50.166449 Epoch 50, Training Loss 0.7831752107042791\n",
      "2022-03-26 19:10:50.190457 Epoch 50, Training Loss 0.7842548142384995\n",
      "2022-03-26 19:10:50.220832 Epoch 50, Training Loss 0.7852919131822294\n",
      "2022-03-26 19:10:50.245116 Epoch 50, Training Loss 0.7861744449342913\n",
      "2022-03-26 19:10:50.268667 Epoch 50, Training Loss 0.7871207830012606\n",
      "2022-03-26 19:10:50.298500 Epoch 50, Training Loss 0.7882396743044524\n",
      "2022-03-26 19:10:50.329308 Epoch 50, Training Loss 0.7893372336235802\n",
      "2022-03-26 19:10:50.353224 Epoch 50, Training Loss 0.7902051151332343\n",
      "2022-03-26 19:10:50.377708 Epoch 50, Training Loss 0.7911074311303361\n",
      "2022-03-26 19:10:50.402145 Epoch 50, Training Loss 0.7922867012526983\n",
      "2022-03-26 19:10:50.431467 Epoch 50, Training Loss 0.7930169398598659\n",
      "2022-03-26 19:10:50.459731 Epoch 50, Training Loss 0.7941430361603227\n",
      "2022-03-26 19:10:50.482955 Epoch 50, Training Loss 0.7950177556642181\n",
      "2022-03-26 19:10:50.506385 Epoch 50, Training Loss 0.7961702686365303\n",
      "2022-03-26 19:10:50.531635 Epoch 50, Training Loss 0.7972550297256016\n",
      "2022-03-26 19:10:50.555000 Epoch 50, Training Loss 0.7985198899074588\n",
      "2022-03-26 19:10:50.578111 Epoch 50, Training Loss 0.7996320126535338\n",
      "2022-03-26 19:10:50.600729 Epoch 50, Training Loss 0.8006865745386504\n",
      "2022-03-26 19:10:50.623540 Epoch 50, Training Loss 0.8017145747037799\n",
      "2022-03-26 19:10:50.652684 Epoch 50, Training Loss 0.8025790635505905\n",
      "2022-03-26 19:10:50.676327 Epoch 50, Training Loss 0.803827558217756\n",
      "2022-03-26 19:10:50.698861 Epoch 50, Training Loss 0.8048584922728941\n",
      "2022-03-26 19:10:50.726829 Epoch 50, Training Loss 0.8059921688054834\n",
      "2022-03-26 19:10:50.750123 Epoch 50, Training Loss 0.806974484220795\n",
      "2022-03-26 19:10:50.773845 Epoch 50, Training Loss 0.8079227954149246\n",
      "2022-03-26 19:10:50.797829 Epoch 50, Training Loss 0.8090486031435334\n",
      "2022-03-26 19:10:50.821270 Epoch 50, Training Loss 0.8098601994230924\n",
      "2022-03-26 19:10:50.830172 Epoch 50, Training Loss 0.8111832877787788\n",
      "2022-03-26 19:25:35.028000 Epoch 100, Training Loss 0.0009933883119422151\n",
      "2022-03-26 19:25:35.051044 Epoch 100, Training Loss 0.0015356457599288668\n",
      "2022-03-26 19:25:35.073412 Epoch 100, Training Loss 0.00239536730224824\n",
      "2022-03-26 19:25:35.100083 Epoch 100, Training Loss 0.0035122108581425895\n",
      "2022-03-26 19:25:35.122990 Epoch 100, Training Loss 0.004389278266740882\n",
      "2022-03-26 19:25:35.149432 Epoch 100, Training Loss 0.005335369683287638\n",
      "2022-03-26 19:25:35.172351 Epoch 100, Training Loss 0.006032069168432289\n",
      "2022-03-26 19:25:35.197395 Epoch 100, Training Loss 0.006988961602110997\n",
      "2022-03-26 19:25:35.220657 Epoch 100, Training Loss 0.007884611863919231\n",
      "2022-03-26 19:25:35.251486 Epoch 100, Training Loss 0.008806509130141315\n",
      "2022-03-26 19:25:35.275278 Epoch 100, Training Loss 0.009481552540493743\n",
      "2022-03-26 19:25:35.297956 Epoch 100, Training Loss 0.010249264176239442\n",
      "2022-03-26 19:25:35.334891 Epoch 100, Training Loss 0.01122837283117387\n",
      "2022-03-26 19:25:35.357771 Epoch 100, Training Loss 0.012183586959643743\n",
      "2022-03-26 19:25:35.382004 Epoch 100, Training Loss 0.013225228432804118\n",
      "2022-03-26 19:25:35.406406 Epoch 100, Training Loss 0.01385721320386433\n",
      "2022-03-26 19:25:35.429731 Epoch 100, Training Loss 0.015080164155691786\n",
      "2022-03-26 19:25:35.453836 Epoch 100, Training Loss 0.01584500516467082\n",
      "2022-03-26 19:25:35.483054 Epoch 100, Training Loss 0.016715730623820858\n",
      "2022-03-26 19:25:35.506432 Epoch 100, Training Loss 0.01761277740263878\n",
      "2022-03-26 19:25:35.530439 Epoch 100, Training Loss 0.01851620919564191\n",
      "2022-03-26 19:25:35.558359 Epoch 100, Training Loss 0.019309795344882\n",
      "2022-03-26 19:25:35.582090 Epoch 100, Training Loss 0.020327815619271123\n",
      "2022-03-26 19:25:35.605000 Epoch 100, Training Loss 0.021224615823887193\n",
      "2022-03-26 19:25:35.628113 Epoch 100, Training Loss 0.02224837445542026\n",
      "2022-03-26 19:25:35.650378 Epoch 100, Training Loss 0.023197571258715657\n",
      "2022-03-26 19:25:35.678796 Epoch 100, Training Loss 0.023961658063142197\n",
      "2022-03-26 19:25:35.703255 Epoch 100, Training Loss 0.024848130276745848\n",
      "2022-03-26 19:25:35.725640 Epoch 100, Training Loss 0.025750212809618783\n",
      "2022-03-26 19:25:35.749838 Epoch 100, Training Loss 0.02662738235405339\n",
      "2022-03-26 19:25:35.773583 Epoch 100, Training Loss 0.0273122128928104\n",
      "2022-03-26 19:25:35.796277 Epoch 100, Training Loss 0.02794288700956213\n",
      "2022-03-26 19:25:35.820343 Epoch 100, Training Loss 0.028772621927663798\n",
      "2022-03-26 19:25:35.843885 Epoch 100, Training Loss 0.029640732480741828\n",
      "2022-03-26 19:25:35.866603 Epoch 100, Training Loss 0.03027448206758865\n",
      "2022-03-26 19:25:35.895132 Epoch 100, Training Loss 0.03125637247586799\n",
      "2022-03-26 19:25:35.919281 Epoch 100, Training Loss 0.032063726993168104\n",
      "2022-03-26 19:25:35.942486 Epoch 100, Training Loss 0.03289338942531429\n",
      "2022-03-26 19:25:35.968876 Epoch 100, Training Loss 0.033843673670383365\n",
      "2022-03-26 19:25:35.998010 Epoch 100, Training Loss 0.0345322817106686\n",
      "2022-03-26 19:25:36.021119 Epoch 100, Training Loss 0.0353040559898557\n",
      "2022-03-26 19:25:36.043894 Epoch 100, Training Loss 0.036079263633779245\n",
      "2022-03-26 19:25:36.070519 Epoch 100, Training Loss 0.0371540705947315\n",
      "2022-03-26 19:25:36.093276 Epoch 100, Training Loss 0.03776366440841304\n",
      "2022-03-26 19:25:36.122141 Epoch 100, Training Loss 0.038713170744269096\n",
      "2022-03-26 19:25:36.145861 Epoch 100, Training Loss 0.0397330812938378\n",
      "2022-03-26 19:25:36.169084 Epoch 100, Training Loss 0.040811992743436026\n",
      "2022-03-26 19:25:36.197009 Epoch 100, Training Loss 0.04141083458805328\n",
      "2022-03-26 19:25:36.220237 Epoch 100, Training Loss 0.04227368202050934\n",
      "2022-03-26 19:25:36.243144 Epoch 100, Training Loss 0.04317287769159088\n",
      "2022-03-26 19:25:36.266248 Epoch 100, Training Loss 0.04405371040639365\n",
      "2022-03-26 19:25:36.289098 Epoch 100, Training Loss 0.0448765110634172\n",
      "2022-03-26 19:25:36.311886 Epoch 100, Training Loss 0.045930815017436774\n",
      "2022-03-26 19:25:36.345663 Epoch 100, Training Loss 0.046760943920715996\n",
      "2022-03-26 19:25:36.374422 Epoch 100, Training Loss 0.04752769220210707\n",
      "2022-03-26 19:25:36.399243 Epoch 100, Training Loss 0.04847501382193602\n",
      "2022-03-26 19:25:36.423458 Epoch 100, Training Loss 0.04932264522518343\n",
      "2022-03-26 19:25:36.448363 Epoch 100, Training Loss 0.05018305679416413\n",
      "2022-03-26 19:25:36.470600 Epoch 100, Training Loss 0.05091798678993264\n",
      "2022-03-26 19:25:36.492991 Epoch 100, Training Loss 0.05171562826541989\n",
      "2022-03-26 19:25:36.515207 Epoch 100, Training Loss 0.052600612649527353\n",
      "2022-03-26 19:25:36.538141 Epoch 100, Training Loss 0.05350374131251479\n",
      "2022-03-26 19:25:36.570479 Epoch 100, Training Loss 0.05438388544885094\n",
      "2022-03-26 19:25:36.594415 Epoch 100, Training Loss 0.05521965773819048\n",
      "2022-03-26 19:25:36.617259 Epoch 100, Training Loss 0.05612288899433887\n",
      "2022-03-26 19:25:36.643774 Epoch 100, Training Loss 0.05696484720920358\n",
      "2022-03-26 19:25:36.665855 Epoch 100, Training Loss 0.05802522230026362\n",
      "2022-03-26 19:25:36.687836 Epoch 100, Training Loss 0.05907458722438959\n",
      "2022-03-26 19:25:36.710457 Epoch 100, Training Loss 0.05992578423541525\n",
      "2022-03-26 19:25:36.732552 Epoch 100, Training Loss 0.06091591875876307\n",
      "2022-03-26 19:25:36.755536 Epoch 100, Training Loss 0.06197556899026837\n",
      "2022-03-26 19:25:36.782964 Epoch 100, Training Loss 0.06291432301406666\n",
      "2022-03-26 19:25:36.807066 Epoch 100, Training Loss 0.06377259064513399\n",
      "2022-03-26 19:25:36.829970 Epoch 100, Training Loss 0.06466799722912976\n",
      "2022-03-26 19:25:36.853951 Epoch 100, Training Loss 0.06552250793827769\n",
      "2022-03-26 19:25:36.879378 Epoch 100, Training Loss 0.06637633570929623\n",
      "2022-03-26 19:25:36.909547 Epoch 100, Training Loss 0.06699408971897476\n",
      "2022-03-26 19:25:36.932611 Epoch 100, Training Loss 0.06777959273141973\n",
      "2022-03-26 19:25:36.955634 Epoch 100, Training Loss 0.06828588956152387\n",
      "2022-03-26 19:25:36.981450 Epoch 100, Training Loss 0.06898363418591297\n",
      "2022-03-26 19:25:37.011348 Epoch 100, Training Loss 0.06972583541479867\n",
      "2022-03-26 19:25:37.034476 Epoch 100, Training Loss 0.07031684023949801\n",
      "2022-03-26 19:25:37.056999 Epoch 100, Training Loss 0.07123632610911299\n",
      "2022-03-26 19:25:37.083563 Epoch 100, Training Loss 0.07196768112194812\n",
      "2022-03-26 19:25:37.106082 Epoch 100, Training Loss 0.0728554032037935\n",
      "2022-03-26 19:25:37.128639 Epoch 100, Training Loss 0.07366090799536547\n",
      "2022-03-26 19:25:37.160088 Epoch 100, Training Loss 0.07457004064489203\n",
      "2022-03-26 19:25:37.182438 Epoch 100, Training Loss 0.07536635648868882\n",
      "2022-03-26 19:25:37.206412 Epoch 100, Training Loss 0.07608721292842074\n",
      "2022-03-26 19:25:37.239168 Epoch 100, Training Loss 0.07679790799575084\n",
      "2022-03-26 19:25:37.262983 Epoch 100, Training Loss 0.07770525768894673\n",
      "2022-03-26 19:25:37.285566 Epoch 100, Training Loss 0.07851357312153673\n",
      "2022-03-26 19:25:37.309435 Epoch 100, Training Loss 0.07931639212171746\n",
      "2022-03-26 19:25:37.334789 Epoch 100, Training Loss 0.08002818934143047\n",
      "2022-03-26 19:25:37.357508 Epoch 100, Training Loss 0.08082601595717623\n",
      "2022-03-26 19:25:37.390389 Epoch 100, Training Loss 0.08172986269606959\n",
      "2022-03-26 19:25:37.413152 Epoch 100, Training Loss 0.0825932978669091\n",
      "2022-03-26 19:25:37.436229 Epoch 100, Training Loss 0.0836230246612178\n",
      "2022-03-26 19:25:37.465491 Epoch 100, Training Loss 0.0845167645255623\n",
      "2022-03-26 19:25:37.489280 Epoch 100, Training Loss 0.08551001144796991\n",
      "2022-03-26 19:25:37.513313 Epoch 100, Training Loss 0.08620949169558942\n",
      "2022-03-26 19:25:37.541532 Epoch 100, Training Loss 0.08691028042522538\n",
      "2022-03-26 19:25:37.565363 Epoch 100, Training Loss 0.08779015123386823\n",
      "2022-03-26 19:25:37.589244 Epoch 100, Training Loss 0.08885617306470261\n",
      "2022-03-26 19:25:37.613836 Epoch 100, Training Loss 0.08968212186832868\n",
      "2022-03-26 19:25:37.637629 Epoch 100, Training Loss 0.09050515591336028\n",
      "2022-03-26 19:25:37.661399 Epoch 100, Training Loss 0.09160066839984006\n",
      "2022-03-26 19:25:37.691216 Epoch 100, Training Loss 0.09262931727997177\n",
      "2022-03-26 19:25:37.716335 Epoch 100, Training Loss 0.09374597089369889\n",
      "2022-03-26 19:25:37.741552 Epoch 100, Training Loss 0.09472657614351844\n",
      "2022-03-26 19:25:37.768834 Epoch 100, Training Loss 0.09556506166372762\n",
      "2022-03-26 19:25:37.792423 Epoch 100, Training Loss 0.09640395321199656\n",
      "2022-03-26 19:25:37.816387 Epoch 100, Training Loss 0.09708737930678346\n",
      "2022-03-26 19:25:37.840519 Epoch 100, Training Loss 0.09776870513816013\n",
      "2022-03-26 19:25:37.864564 Epoch 100, Training Loss 0.0986600576916619\n",
      "2022-03-26 19:25:37.888648 Epoch 100, Training Loss 0.09937453712038982\n",
      "2022-03-26 19:25:37.922373 Epoch 100, Training Loss 0.10026219144196766\n",
      "2022-03-26 19:25:37.947404 Epoch 100, Training Loss 0.10114129790869515\n",
      "2022-03-26 19:25:37.971256 Epoch 100, Training Loss 0.10201728900375269\n",
      "2022-03-26 19:25:38.001523 Epoch 100, Training Loss 0.10308176484864082\n",
      "2022-03-26 19:25:38.025564 Epoch 100, Training Loss 0.10385717020925049\n",
      "2022-03-26 19:25:38.049705 Epoch 100, Training Loss 0.1048940971989156\n",
      "2022-03-26 19:25:38.073368 Epoch 100, Training Loss 0.10588921976211431\n",
      "2022-03-26 19:25:38.102735 Epoch 100, Training Loss 0.10671172475875795\n",
      "2022-03-26 19:25:38.131479 Epoch 100, Training Loss 0.10737032414702198\n",
      "2022-03-26 19:25:38.156254 Epoch 100, Training Loss 0.10806357913919727\n",
      "2022-03-26 19:25:38.182108 Epoch 100, Training Loss 0.10914716627591711\n",
      "2022-03-26 19:25:38.211337 Epoch 100, Training Loss 0.10987550706204856\n",
      "2022-03-26 19:25:38.235035 Epoch 100, Training Loss 0.11069016124281432\n",
      "2022-03-26 19:25:38.259239 Epoch 100, Training Loss 0.11181927680054589\n",
      "2022-03-26 19:25:38.283720 Epoch 100, Training Loss 0.11278713709862946\n",
      "2022-03-26 19:25:38.307924 Epoch 100, Training Loss 0.11358158393284244\n",
      "2022-03-26 19:25:38.333857 Epoch 100, Training Loss 0.11439802800602925\n",
      "2022-03-26 19:25:38.364003 Epoch 100, Training Loss 0.11531409026716677\n",
      "2022-03-26 19:25:38.389902 Epoch 100, Training Loss 0.11602979563081356\n",
      "2022-03-26 19:25:38.425250 Epoch 100, Training Loss 0.11700217353413477\n",
      "2022-03-26 19:25:38.449248 Epoch 100, Training Loss 0.11817014240242942\n",
      "2022-03-26 19:25:38.471415 Epoch 100, Training Loss 0.11915803672102711\n",
      "2022-03-26 19:25:38.493873 Epoch 100, Training Loss 0.12010179212331162\n",
      "2022-03-26 19:25:38.516437 Epoch 100, Training Loss 0.1209249702256049\n",
      "2022-03-26 19:25:38.539518 Epoch 100, Training Loss 0.12156734053436143\n",
      "2022-03-26 19:25:38.569850 Epoch 100, Training Loss 0.12259097667911169\n",
      "2022-03-26 19:25:38.593575 Epoch 100, Training Loss 0.12354669485555585\n",
      "2022-03-26 19:25:38.616473 Epoch 100, Training Loss 0.12448422896587635\n",
      "2022-03-26 19:25:38.641563 Epoch 100, Training Loss 0.12522898442909847\n",
      "2022-03-26 19:25:38.665104 Epoch 100, Training Loss 0.12604503665128938\n",
      "2022-03-26 19:25:38.687773 Epoch 100, Training Loss 0.12691611554616553\n",
      "2022-03-26 19:25:38.710353 Epoch 100, Training Loss 0.12779644489898095\n",
      "2022-03-26 19:25:38.732781 Epoch 100, Training Loss 0.12903103003721408\n",
      "2022-03-26 19:25:38.755420 Epoch 100, Training Loss 0.129870004666126\n",
      "2022-03-26 19:25:38.782147 Epoch 100, Training Loss 0.13058170813428777\n",
      "2022-03-26 19:25:38.806359 Epoch 100, Training Loss 0.1313217769345969\n",
      "2022-03-26 19:25:38.829056 Epoch 100, Training Loss 0.13243473826162042\n",
      "2022-03-26 19:25:38.852040 Epoch 100, Training Loss 0.13323098207678635\n",
      "2022-03-26 19:25:38.878120 Epoch 100, Training Loss 0.13397000345122784\n",
      "2022-03-26 19:25:38.900745 Epoch 100, Training Loss 0.13452368216288974\n",
      "2022-03-26 19:25:38.923333 Epoch 100, Training Loss 0.13555203698327778\n",
      "2022-03-26 19:25:38.946300 Epoch 100, Training Loss 0.13650368771437185\n",
      "2022-03-26 19:25:38.969439 Epoch 100, Training Loss 0.1373802663191505\n",
      "2022-03-26 19:25:38.998539 Epoch 100, Training Loss 0.13829353066813915\n",
      "2022-03-26 19:25:39.025697 Epoch 100, Training Loss 0.13933575675463128\n",
      "2022-03-26 19:25:39.048527 Epoch 100, Training Loss 0.13998134983012744\n",
      "2022-03-26 19:25:39.075514 Epoch 100, Training Loss 0.14067645881639418\n",
      "2022-03-26 19:25:39.101356 Epoch 100, Training Loss 0.14149075948521303\n",
      "2022-03-26 19:25:39.124877 Epoch 100, Training Loss 0.1424153848453556\n",
      "2022-03-26 19:25:39.147602 Epoch 100, Training Loss 0.1431450729860979\n",
      "2022-03-26 19:25:39.170807 Epoch 100, Training Loss 0.14391933198627607\n",
      "2022-03-26 19:25:39.193276 Epoch 100, Training Loss 0.14472761571102435\n",
      "2022-03-26 19:25:39.224355 Epoch 100, Training Loss 0.14536101853146272\n",
      "2022-03-26 19:25:39.248173 Epoch 100, Training Loss 0.1460849880562414\n",
      "2022-03-26 19:25:39.270984 Epoch 100, Training Loss 0.14705240528296937\n",
      "2022-03-26 19:25:39.297476 Epoch 100, Training Loss 0.14791090919843417\n",
      "2022-03-26 19:25:39.321395 Epoch 100, Training Loss 0.14889808262095733\n",
      "2022-03-26 19:25:39.347279 Epoch 100, Training Loss 0.14958904374895804\n",
      "2022-03-26 19:25:39.370357 Epoch 100, Training Loss 0.1501473014998009\n",
      "2022-03-26 19:25:39.393150 Epoch 100, Training Loss 0.15106538810845835\n",
      "2022-03-26 19:25:39.418678 Epoch 100, Training Loss 0.15198987741452044\n",
      "2022-03-26 19:25:39.456789 Epoch 100, Training Loss 0.1529652616938057\n",
      "2022-03-26 19:25:39.481136 Epoch 100, Training Loss 0.15396129227507754\n",
      "2022-03-26 19:25:39.508221 Epoch 100, Training Loss 0.15465782628492322\n",
      "2022-03-26 19:25:39.532681 Epoch 100, Training Loss 0.15554431206582453\n",
      "2022-03-26 19:25:39.557270 Epoch 100, Training Loss 0.15632984953005905\n",
      "2022-03-26 19:25:39.581451 Epoch 100, Training Loss 0.15723329176530815\n",
      "2022-03-26 19:25:39.605629 Epoch 100, Training Loss 0.15816692954591474\n",
      "2022-03-26 19:25:39.630191 Epoch 100, Training Loss 0.15926719698912042\n",
      "2022-03-26 19:25:39.654822 Epoch 100, Training Loss 0.16022034637306048\n",
      "2022-03-26 19:25:39.686600 Epoch 100, Training Loss 0.1610465980780399\n",
      "2022-03-26 19:25:39.710973 Epoch 100, Training Loss 0.16200620069375735\n",
      "2022-03-26 19:25:39.734913 Epoch 100, Training Loss 0.16299042589676654\n",
      "2022-03-26 19:25:39.762366 Epoch 100, Training Loss 0.16371269943311695\n",
      "2022-03-26 19:25:39.786165 Epoch 100, Training Loss 0.1646300534077008\n",
      "2022-03-26 19:25:39.809949 Epoch 100, Training Loss 0.16538196081852974\n",
      "2022-03-26 19:25:39.833795 Epoch 100, Training Loss 0.16630944537232295\n",
      "2022-03-26 19:25:39.857267 Epoch 100, Training Loss 0.16692589322471862\n",
      "2022-03-26 19:25:39.880692 Epoch 100, Training Loss 0.16790396603934296\n",
      "2022-03-26 19:25:39.917979 Epoch 100, Training Loss 0.16854866054814185\n",
      "2022-03-26 19:25:39.942667 Epoch 100, Training Loss 0.1693798352004317\n",
      "2022-03-26 19:25:39.968197 Epoch 100, Training Loss 0.17033481243473794\n",
      "2022-03-26 19:25:39.992680 Epoch 100, Training Loss 0.17103986937524107\n",
      "2022-03-26 19:25:40.025653 Epoch 100, Training Loss 0.17194588528112378\n",
      "2022-03-26 19:25:40.049647 Epoch 100, Training Loss 0.17294849066630655\n",
      "2022-03-26 19:25:40.073108 Epoch 100, Training Loss 0.17379466259418547\n",
      "2022-03-26 19:25:40.097017 Epoch 100, Training Loss 0.1748801848238996\n",
      "2022-03-26 19:25:40.125353 Epoch 100, Training Loss 0.17608227460738032\n",
      "2022-03-26 19:25:40.152336 Epoch 100, Training Loss 0.17720080172771688\n",
      "2022-03-26 19:25:40.175955 Epoch 100, Training Loss 0.17818671369644076\n",
      "2022-03-26 19:25:40.199898 Epoch 100, Training Loss 0.17908866909306373\n",
      "2022-03-26 19:25:40.227384 Epoch 100, Training Loss 0.1802346502499812\n",
      "2022-03-26 19:25:40.251869 Epoch 100, Training Loss 0.18120569459465155\n",
      "2022-03-26 19:25:40.275563 Epoch 100, Training Loss 0.18195583795190162\n",
      "2022-03-26 19:25:40.298984 Epoch 100, Training Loss 0.1825799863128101\n",
      "2022-03-26 19:25:40.323167 Epoch 100, Training Loss 0.1833713545137659\n",
      "2022-03-26 19:25:40.356922 Epoch 100, Training Loss 0.1842728188961668\n",
      "2022-03-26 19:25:40.380576 Epoch 100, Training Loss 0.18520411639414786\n",
      "2022-03-26 19:25:40.407363 Epoch 100, Training Loss 0.18600169963696422\n",
      "2022-03-26 19:25:40.430127 Epoch 100, Training Loss 0.18691229138075544\n",
      "2022-03-26 19:25:40.454447 Epoch 100, Training Loss 0.1877646627252364\n",
      "2022-03-26 19:25:40.484399 Epoch 100, Training Loss 0.18867008715791775\n",
      "2022-03-26 19:25:40.506738 Epoch 100, Training Loss 0.18947323596538485\n",
      "2022-03-26 19:25:40.529316 Epoch 100, Training Loss 0.19033931447264482\n",
      "2022-03-26 19:25:40.552035 Epoch 100, Training Loss 0.1911085079926664\n",
      "2022-03-26 19:25:40.581444 Epoch 100, Training Loss 0.19219374508046738\n",
      "2022-03-26 19:25:40.604681 Epoch 100, Training Loss 0.19297687930371754\n",
      "2022-03-26 19:25:40.628273 Epoch 100, Training Loss 0.1938954055537958\n",
      "2022-03-26 19:25:40.653598 Epoch 100, Training Loss 0.19466547904264592\n",
      "2022-03-26 19:25:40.677438 Epoch 100, Training Loss 0.1955856117979645\n",
      "2022-03-26 19:25:40.700446 Epoch 100, Training Loss 0.19644467928982756\n",
      "2022-03-26 19:25:40.723249 Epoch 100, Training Loss 0.19747129673390743\n",
      "2022-03-26 19:25:40.746128 Epoch 100, Training Loss 0.19839235115081758\n",
      "2022-03-26 19:25:40.769477 Epoch 100, Training Loss 0.1990878677657803\n",
      "2022-03-26 19:25:40.799414 Epoch 100, Training Loss 0.20003280428516895\n",
      "2022-03-26 19:25:40.824392 Epoch 100, Training Loss 0.20078254062348924\n",
      "2022-03-26 19:25:40.847646 Epoch 100, Training Loss 0.20164346996018345\n",
      "2022-03-26 19:25:40.871247 Epoch 100, Training Loss 0.20244048962660152\n",
      "2022-03-26 19:25:40.896653 Epoch 100, Training Loss 0.20327496471459908\n",
      "2022-03-26 19:25:40.919642 Epoch 100, Training Loss 0.2042408573734181\n",
      "2022-03-26 19:25:40.943173 Epoch 100, Training Loss 0.2051810284176141\n",
      "2022-03-26 19:25:40.966652 Epoch 100, Training Loss 0.20614434870155265\n",
      "2022-03-26 19:25:40.989870 Epoch 100, Training Loss 0.207122967028252\n",
      "2022-03-26 19:25:41.020698 Epoch 100, Training Loss 0.20812138438682118\n",
      "2022-03-26 19:25:41.044776 Epoch 100, Training Loss 0.20903143420091372\n",
      "2022-03-26 19:25:41.067349 Epoch 100, Training Loss 0.2097977293498071\n",
      "2022-03-26 19:25:41.093627 Epoch 100, Training Loss 0.21071265637874603\n",
      "2022-03-26 19:25:41.120490 Epoch 100, Training Loss 0.211719950027478\n",
      "2022-03-26 19:25:41.143492 Epoch 100, Training Loss 0.21255425339007317\n",
      "2022-03-26 19:25:41.169292 Epoch 100, Training Loss 0.21330138816095678\n",
      "2022-03-26 19:25:41.192757 Epoch 100, Training Loss 0.21394784131165964\n",
      "2022-03-26 19:25:41.216474 Epoch 100, Training Loss 0.21486081388752784\n",
      "2022-03-26 19:25:41.249484 Epoch 100, Training Loss 0.21557715653306078\n",
      "2022-03-26 19:25:41.273596 Epoch 100, Training Loss 0.21631962804080884\n",
      "2022-03-26 19:25:41.296626 Epoch 100, Training Loss 0.21717075309942446\n",
      "2022-03-26 19:25:41.323629 Epoch 100, Training Loss 0.21812646281536277\n",
      "2022-03-26 19:25:41.349426 Epoch 100, Training Loss 0.2192483708605437\n",
      "2022-03-26 19:25:41.373370 Epoch 100, Training Loss 0.22005977879857164\n",
      "2022-03-26 19:25:41.397185 Epoch 100, Training Loss 0.22116961888492565\n",
      "2022-03-26 19:25:41.422422 Epoch 100, Training Loss 0.22228025029534879\n",
      "2022-03-26 19:25:41.445999 Epoch 100, Training Loss 0.22318036648471032\n",
      "2022-03-26 19:25:41.477625 Epoch 100, Training Loss 0.22417629607345746\n",
      "2022-03-26 19:25:41.509625 Epoch 100, Training Loss 0.225014293475834\n",
      "2022-03-26 19:25:41.534532 Epoch 100, Training Loss 0.22582205402119385\n",
      "2022-03-26 19:25:41.557800 Epoch 100, Training Loss 0.22676820298442452\n",
      "2022-03-26 19:25:41.580064 Epoch 100, Training Loss 0.22771522749567885\n",
      "2022-03-26 19:25:41.602513 Epoch 100, Training Loss 0.22852187049205955\n",
      "2022-03-26 19:25:41.625345 Epoch 100, Training Loss 0.22931324730596275\n",
      "2022-03-26 19:25:41.648482 Epoch 100, Training Loss 0.23050033303020556\n",
      "2022-03-26 19:25:41.670982 Epoch 100, Training Loss 0.2313325289646378\n",
      "2022-03-26 19:25:41.701093 Epoch 100, Training Loss 0.232022017507297\n",
      "2022-03-26 19:25:41.725275 Epoch 100, Training Loss 0.23257990688314217\n",
      "2022-03-26 19:25:41.748712 Epoch 100, Training Loss 0.23342574603112456\n",
      "2022-03-26 19:25:41.774670 Epoch 100, Training Loss 0.234395403691265\n",
      "2022-03-26 19:25:41.797151 Epoch 100, Training Loss 0.23509618578969366\n",
      "2022-03-26 19:25:41.820032 Epoch 100, Training Loss 0.23610526056545775\n",
      "2022-03-26 19:25:41.843751 Epoch 100, Training Loss 0.23716633505833423\n",
      "2022-03-26 19:25:41.871098 Epoch 100, Training Loss 0.23792476590027284\n",
      "2022-03-26 19:25:41.894797 Epoch 100, Training Loss 0.238629324738022\n",
      "2022-03-26 19:25:41.925239 Epoch 100, Training Loss 0.23926337364384587\n",
      "2022-03-26 19:25:41.948849 Epoch 100, Training Loss 0.2401549217036313\n",
      "2022-03-26 19:25:41.971595 Epoch 100, Training Loss 0.2410162468548016\n",
      "2022-03-26 19:25:41.998446 Epoch 100, Training Loss 0.2417291055250046\n",
      "2022-03-26 19:25:42.027225 Epoch 100, Training Loss 0.24279686320773172\n",
      "2022-03-26 19:25:42.051038 Epoch 100, Training Loss 0.24393291348386603\n",
      "2022-03-26 19:25:42.073969 Epoch 100, Training Loss 0.24479112417801566\n",
      "2022-03-26 19:25:42.096513 Epoch 100, Training Loss 0.2456332374259334\n",
      "2022-03-26 19:25:42.123172 Epoch 100, Training Loss 0.24675805428448847\n",
      "2022-03-26 19:25:42.153925 Epoch 100, Training Loss 0.2474827262599145\n",
      "2022-03-26 19:25:42.176777 Epoch 100, Training Loss 0.24824019893051108\n",
      "2022-03-26 19:25:42.199452 Epoch 100, Training Loss 0.24894242526015356\n",
      "2022-03-26 19:25:42.226121 Epoch 100, Training Loss 0.25000495549358065\n",
      "2022-03-26 19:25:42.250535 Epoch 100, Training Loss 0.2510112053750421\n",
      "2022-03-26 19:25:42.273194 Epoch 100, Training Loss 0.25208846748332536\n",
      "2022-03-26 19:25:42.295728 Epoch 100, Training Loss 0.2527108478271748\n",
      "2022-03-26 19:25:42.318623 Epoch 100, Training Loss 0.2535237362012839\n",
      "2022-03-26 19:25:42.344321 Epoch 100, Training Loss 0.2543907034427614\n",
      "2022-03-26 19:25:42.375058 Epoch 100, Training Loss 0.25516947097790516\n",
      "2022-03-26 19:25:42.398800 Epoch 100, Training Loss 0.25609436638824773\n",
      "2022-03-26 19:25:42.423414 Epoch 100, Training Loss 0.25729250526794084\n",
      "2022-03-26 19:25:42.448393 Epoch 100, Training Loss 0.25843202938204224\n",
      "2022-03-26 19:25:42.470998 Epoch 100, Training Loss 0.25943578310939663\n",
      "2022-03-26 19:25:42.493343 Epoch 100, Training Loss 0.2601945979515915\n",
      "2022-03-26 19:25:42.525171 Epoch 100, Training Loss 0.2609746668802198\n",
      "2022-03-26 19:25:42.548453 Epoch 100, Training Loss 0.26180989670631527\n",
      "2022-03-26 19:25:42.571053 Epoch 100, Training Loss 0.26266667277306854\n",
      "2022-03-26 19:25:42.600584 Epoch 100, Training Loss 0.26351192646929067\n",
      "2022-03-26 19:25:42.624368 Epoch 100, Training Loss 0.2641260503884167\n",
      "2022-03-26 19:25:42.648541 Epoch 100, Training Loss 0.2652198984418684\n",
      "2022-03-26 19:25:42.675283 Epoch 100, Training Loss 0.26596377241184643\n",
      "2022-03-26 19:25:42.698627 Epoch 100, Training Loss 0.26684468492979896\n",
      "2022-03-26 19:25:42.722214 Epoch 100, Training Loss 0.26749620554239856\n",
      "2022-03-26 19:25:42.746739 Epoch 100, Training Loss 0.26844749795963696\n",
      "2022-03-26 19:25:42.770998 Epoch 100, Training Loss 0.2693067384726556\n",
      "2022-03-26 19:25:42.794269 Epoch 100, Training Loss 0.2700978803741353\n",
      "2022-03-26 19:25:42.824221 Epoch 100, Training Loss 0.27088456145485346\n",
      "2022-03-26 19:25:42.849342 Epoch 100, Training Loss 0.27182374208632026\n",
      "2022-03-26 19:25:42.875595 Epoch 100, Training Loss 0.2728456557940339\n",
      "2022-03-26 19:25:42.902901 Epoch 100, Training Loss 0.27371053290946407\n",
      "2022-03-26 19:25:42.932961 Epoch 100, Training Loss 0.2746454427385574\n",
      "2022-03-26 19:25:42.957562 Epoch 100, Training Loss 0.27560533956646005\n",
      "2022-03-26 19:25:42.980817 Epoch 100, Training Loss 0.2765484244546012\n",
      "2022-03-26 19:25:43.008014 Epoch 100, Training Loss 0.27755426891776913\n",
      "2022-03-26 19:25:43.035887 Epoch 100, Training Loss 0.2782812063270213\n",
      "2022-03-26 19:25:43.062247 Epoch 100, Training Loss 0.2793981352501818\n",
      "2022-03-26 19:25:43.086386 Epoch 100, Training Loss 0.28021714948784665\n",
      "2022-03-26 19:25:43.113008 Epoch 100, Training Loss 0.2812896514563914\n",
      "2022-03-26 19:25:43.138540 Epoch 100, Training Loss 0.28203612737491\n",
      "2022-03-26 19:25:43.162120 Epoch 100, Training Loss 0.28298633753338737\n",
      "2022-03-26 19:25:43.185686 Epoch 100, Training Loss 0.2838163473798186\n",
      "2022-03-26 19:25:43.209399 Epoch 100, Training Loss 0.28495001354638266\n",
      "2022-03-26 19:25:43.233314 Epoch 100, Training Loss 0.2856565108308402\n",
      "2022-03-26 19:25:43.265721 Epoch 100, Training Loss 0.2865464998709271\n",
      "2022-03-26 19:25:43.289575 Epoch 100, Training Loss 0.28718992263612236\n",
      "2022-03-26 19:25:43.314332 Epoch 100, Training Loss 0.28798711349439743\n",
      "2022-03-26 19:25:43.342365 Epoch 100, Training Loss 0.2891621800411083\n",
      "2022-03-26 19:25:43.366967 Epoch 100, Training Loss 0.2900771028016839\n",
      "2022-03-26 19:25:43.390573 Epoch 100, Training Loss 0.29080784934408527\n",
      "2022-03-26 19:25:43.416095 Epoch 100, Training Loss 0.2915463452525151\n",
      "2022-03-26 19:25:43.439989 Epoch 100, Training Loss 0.2928112815027042\n",
      "2022-03-26 19:25:43.463879 Epoch 100, Training Loss 0.29351316156137325\n",
      "2022-03-26 19:25:43.493708 Epoch 100, Training Loss 0.2943504230521829\n",
      "2022-03-26 19:25:43.517594 Epoch 100, Training Loss 0.29521941540338803\n",
      "2022-03-26 19:25:43.551480 Epoch 100, Training Loss 0.2960969803811949\n",
      "2022-03-26 19:25:43.577753 Epoch 100, Training Loss 0.2967712332296859\n",
      "2022-03-26 19:25:43.600949 Epoch 100, Training Loss 0.29782075768389055\n",
      "2022-03-26 19:25:43.624425 Epoch 100, Training Loss 0.29842620062858555\n",
      "2022-03-26 19:25:43.647959 Epoch 100, Training Loss 0.29926164440639186\n",
      "2022-03-26 19:25:43.671638 Epoch 100, Training Loss 0.29985426873197335\n",
      "2022-03-26 19:25:43.695590 Epoch 100, Training Loss 0.30050750347354527\n",
      "2022-03-26 19:25:43.725331 Epoch 100, Training Loss 0.3014503545925745\n",
      "2022-03-26 19:25:43.749250 Epoch 100, Training Loss 0.30231614643350585\n",
      "2022-03-26 19:25:43.773542 Epoch 100, Training Loss 0.30317146012850127\n",
      "2022-03-26 19:25:43.800564 Epoch 100, Training Loss 0.3039774060096887\n",
      "2022-03-26 19:25:43.824646 Epoch 100, Training Loss 0.3048794720026538\n",
      "2022-03-26 19:25:43.848605 Epoch 100, Training Loss 0.30563916894785886\n",
      "2022-03-26 19:25:43.872879 Epoch 100, Training Loss 0.3064638995148642\n",
      "2022-03-26 19:25:43.896637 Epoch 100, Training Loss 0.3076031482433114\n",
      "2022-03-26 19:25:43.928011 Epoch 100, Training Loss 0.3081729075183039\n",
      "2022-03-26 19:25:43.953675 Epoch 100, Training Loss 0.3089966466054892\n",
      "2022-03-26 19:25:43.978326 Epoch 100, Training Loss 0.3098521068730318\n",
      "2022-03-26 19:25:44.005538 Epoch 100, Training Loss 0.3107177041223287\n",
      "2022-03-26 19:25:44.029646 Epoch 100, Training Loss 0.3117337629313359\n",
      "2022-03-26 19:25:44.053982 Epoch 100, Training Loss 0.312577175286115\n",
      "2022-03-26 19:25:44.077830 Epoch 100, Training Loss 0.31333238831566423\n",
      "2022-03-26 19:25:44.101439 Epoch 100, Training Loss 0.3138900998684451\n",
      "2022-03-26 19:25:44.125702 Epoch 100, Training Loss 0.3146477866431941\n",
      "2022-03-26 19:25:44.158730 Epoch 100, Training Loss 0.3153302153129407\n",
      "2022-03-26 19:25:44.182230 Epoch 100, Training Loss 0.31613408265363835\n",
      "2022-03-26 19:25:44.207266 Epoch 100, Training Loss 0.31693732323091656\n",
      "2022-03-26 19:25:44.236433 Epoch 100, Training Loss 0.3181070720829317\n",
      "2022-03-26 19:25:44.259926 Epoch 100, Training Loss 0.3193737470051822\n",
      "2022-03-26 19:25:44.283152 Epoch 100, Training Loss 0.3202164221526412\n",
      "2022-03-26 19:25:44.306303 Epoch 100, Training Loss 0.3211913609215061\n",
      "2022-03-26 19:25:44.331892 Epoch 100, Training Loss 0.3222984046201267\n",
      "2022-03-26 19:25:44.355057 Epoch 100, Training Loss 0.32311420683818093\n",
      "2022-03-26 19:25:44.385880 Epoch 100, Training Loss 0.32415166973610365\n",
      "2022-03-26 19:25:44.409054 Epoch 100, Training Loss 0.32494551137737604\n",
      "2022-03-26 19:25:44.436825 Epoch 100, Training Loss 0.3262575318670029\n",
      "2022-03-26 19:25:44.460085 Epoch 100, Training Loss 0.3270668419044646\n",
      "2022-03-26 19:25:44.482774 Epoch 100, Training Loss 0.3280401136487951\n",
      "2022-03-26 19:25:44.505236 Epoch 100, Training Loss 0.32881227924543266\n",
      "2022-03-26 19:25:44.528024 Epoch 100, Training Loss 0.3299282958821567\n",
      "2022-03-26 19:25:44.550777 Epoch 100, Training Loss 0.33131612158949725\n",
      "2022-03-26 19:25:44.585248 Epoch 100, Training Loss 0.3321114884084448\n",
      "2022-03-26 19:25:44.618571 Epoch 100, Training Loss 0.33327244991993965\n",
      "2022-03-26 19:25:44.642175 Epoch 100, Training Loss 0.33418080054433147\n",
      "2022-03-26 19:25:44.664931 Epoch 100, Training Loss 0.33488344784130525\n",
      "2022-03-26 19:25:44.690811 Epoch 100, Training Loss 0.3358342662415541\n",
      "2022-03-26 19:25:44.713424 Epoch 100, Training Loss 0.33681054245632935\n",
      "2022-03-26 19:25:44.737002 Epoch 100, Training Loss 0.33749775862907205\n",
      "2022-03-26 19:25:44.759710 Epoch 100, Training Loss 0.33828431386929336\n",
      "2022-03-26 19:25:44.783176 Epoch 100, Training Loss 0.3392566436010858\n",
      "2022-03-26 19:25:44.805978 Epoch 100, Training Loss 0.34013094648223396\n",
      "2022-03-26 19:25:44.834228 Epoch 100, Training Loss 0.34074757798858313\n",
      "2022-03-26 19:25:44.858560 Epoch 100, Training Loss 0.3414928470273762\n",
      "2022-03-26 19:25:44.881468 Epoch 100, Training Loss 0.342481983668359\n",
      "2022-03-26 19:25:44.907877 Epoch 100, Training Loss 0.34339424144581454\n",
      "2022-03-26 19:25:44.930556 Epoch 100, Training Loss 0.34420201449138127\n",
      "2022-03-26 19:25:44.953128 Epoch 100, Training Loss 0.34514214460502196\n",
      "2022-03-26 19:25:44.977243 Epoch 100, Training Loss 0.3458185359035307\n",
      "2022-03-26 19:25:44.999733 Epoch 100, Training Loss 0.3466605521224039\n",
      "2022-03-26 19:25:45.022918 Epoch 100, Training Loss 0.34750437591691763\n",
      "2022-03-26 19:25:45.058472 Epoch 100, Training Loss 0.34824479296994026\n",
      "2022-03-26 19:25:45.082325 Epoch 100, Training Loss 0.3489868069243858\n",
      "2022-03-26 19:25:45.105457 Epoch 100, Training Loss 0.34973516572466895\n",
      "2022-03-26 19:25:45.130557 Epoch 100, Training Loss 0.35058502742396597\n",
      "2022-03-26 19:25:45.157604 Epoch 100, Training Loss 0.3514236461018662\n",
      "2022-03-26 19:25:45.180876 Epoch 100, Training Loss 0.3522609646820351\n",
      "2022-03-26 19:25:45.204481 Epoch 100, Training Loss 0.353136073826524\n",
      "2022-03-26 19:25:45.227816 Epoch 100, Training Loss 0.35411384930391143\n",
      "2022-03-26 19:25:45.250772 Epoch 100, Training Loss 0.3548556714868911\n",
      "2022-03-26 19:25:45.282097 Epoch 100, Training Loss 0.3556005260371186\n",
      "2022-03-26 19:25:45.305606 Epoch 100, Training Loss 0.3565040509719068\n",
      "2022-03-26 19:25:45.330052 Epoch 100, Training Loss 0.3572477968909856\n",
      "2022-03-26 19:25:45.355348 Epoch 100, Training Loss 0.358159993158277\n",
      "2022-03-26 19:25:45.379007 Epoch 100, Training Loss 0.35907082979941307\n",
      "2022-03-26 19:25:45.401998 Epoch 100, Training Loss 0.35992799847936996\n",
      "2022-03-26 19:25:45.426858 Epoch 100, Training Loss 0.36067879756393334\n",
      "2022-03-26 19:25:45.449933 Epoch 100, Training Loss 0.3614289275063273\n",
      "2022-03-26 19:25:45.473894 Epoch 100, Training Loss 0.36211751199439357\n",
      "2022-03-26 19:25:45.503346 Epoch 100, Training Loss 0.3629783732659372\n",
      "2022-03-26 19:25:45.526857 Epoch 100, Training Loss 0.3637161879130946\n",
      "2022-03-26 19:25:45.549604 Epoch 100, Training Loss 0.36460040483023504\n",
      "2022-03-26 19:25:45.573248 Epoch 100, Training Loss 0.3653902118773107\n",
      "2022-03-26 19:25:45.610211 Epoch 100, Training Loss 0.36623868231883133\n",
      "2022-03-26 19:25:45.634435 Epoch 100, Training Loss 0.367146347566029\n",
      "2022-03-26 19:25:45.657658 Epoch 100, Training Loss 0.36806733071651604\n",
      "2022-03-26 19:25:45.680471 Epoch 100, Training Loss 0.36910361325954233\n",
      "2022-03-26 19:25:45.703449 Epoch 100, Training Loss 0.36995596615859616\n",
      "2022-03-26 19:25:45.733788 Epoch 100, Training Loss 0.3706488363883075\n",
      "2022-03-26 19:25:45.756903 Epoch 100, Training Loss 0.37177520357739285\n",
      "2022-03-26 19:25:45.783388 Epoch 100, Training Loss 0.3724557245174027\n",
      "2022-03-26 19:25:45.806589 Epoch 100, Training Loss 0.37349671704689863\n",
      "2022-03-26 19:25:45.829944 Epoch 100, Training Loss 0.37438241866848354\n",
      "2022-03-26 19:25:45.852826 Epoch 100, Training Loss 0.37521148314866265\n",
      "2022-03-26 19:25:45.875740 Epoch 100, Training Loss 0.37623776583110585\n",
      "2022-03-26 19:25:45.898715 Epoch 100, Training Loss 0.37724283863516417\n",
      "2022-03-26 19:25:45.921385 Epoch 100, Training Loss 0.3783548770810637\n",
      "2022-03-26 19:25:45.954467 Epoch 100, Training Loss 0.37952851974750723\n",
      "2022-03-26 19:25:45.980815 Epoch 100, Training Loss 0.3805231398633679\n",
      "2022-03-26 19:25:46.003245 Epoch 100, Training Loss 0.38153406497462633\n",
      "2022-03-26 19:25:46.027559 Epoch 100, Training Loss 0.3822998787131151\n",
      "2022-03-26 19:25:46.055065 Epoch 100, Training Loss 0.38293085077686995\n",
      "2022-03-26 19:25:46.078018 Epoch 100, Training Loss 0.383672588431012\n",
      "2022-03-26 19:25:46.100590 Epoch 100, Training Loss 0.38484244193414896\n",
      "2022-03-26 19:25:46.123703 Epoch 100, Training Loss 0.38566407877618397\n",
      "2022-03-26 19:25:46.147080 Epoch 100, Training Loss 0.3866576164046212\n",
      "2022-03-26 19:25:46.176850 Epoch 100, Training Loss 0.38723340512389115\n",
      "2022-03-26 19:25:46.200532 Epoch 100, Training Loss 0.3883541796899513\n",
      "2022-03-26 19:25:46.223509 Epoch 100, Training Loss 0.3893107752437177\n",
      "2022-03-26 19:25:46.251717 Epoch 100, Training Loss 0.3903427032939613\n",
      "2022-03-26 19:25:46.274639 Epoch 100, Training Loss 0.39134387374686463\n",
      "2022-03-26 19:25:46.297281 Epoch 100, Training Loss 0.3919957053402196\n",
      "2022-03-26 19:25:46.321006 Epoch 100, Training Loss 0.39302964889637343\n",
      "2022-03-26 19:25:46.346117 Epoch 100, Training Loss 0.3941107015399372\n",
      "2022-03-26 19:25:46.368985 Epoch 100, Training Loss 0.3950636263774789\n",
      "2022-03-26 19:25:46.399340 Epoch 100, Training Loss 0.3957561125307132\n",
      "2022-03-26 19:25:46.424380 Epoch 100, Training Loss 0.39696381551682797\n",
      "2022-03-26 19:25:46.447974 Epoch 100, Training Loss 0.39782300686744776\n",
      "2022-03-26 19:25:46.474412 Epoch 100, Training Loss 0.3988488203348101\n",
      "2022-03-26 19:25:46.497835 Epoch 100, Training Loss 0.3997224219086225\n",
      "2022-03-26 19:25:46.520544 Epoch 100, Training Loss 0.400615205278482\n",
      "2022-03-26 19:25:46.543350 Epoch 100, Training Loss 0.4017357196268218\n",
      "2022-03-26 19:25:46.567444 Epoch 100, Training Loss 0.40276271878453473\n",
      "2022-03-26 19:25:46.593917 Epoch 100, Training Loss 0.40373340580621947\n",
      "2022-03-26 19:25:46.627330 Epoch 100, Training Loss 0.40476046914182356\n",
      "2022-03-26 19:25:46.656556 Epoch 100, Training Loss 0.4055884842143949\n",
      "2022-03-26 19:25:46.680040 Epoch 100, Training Loss 0.40641762704952905\n",
      "2022-03-26 19:25:46.708080 Epoch 100, Training Loss 0.4075079482915761\n",
      "2022-03-26 19:25:46.730558 Epoch 100, Training Loss 0.4083707703425146\n",
      "2022-03-26 19:25:46.753356 Epoch 100, Training Loss 0.4091518653551941\n",
      "2022-03-26 19:25:46.776817 Epoch 100, Training Loss 0.4101115676676831\n",
      "2022-03-26 19:25:46.799575 Epoch 100, Training Loss 0.41089780659169495\n",
      "2022-03-26 19:25:46.822430 Epoch 100, Training Loss 0.41184514048306836\n",
      "2022-03-26 19:25:46.858862 Epoch 100, Training Loss 0.4124881883182794\n",
      "2022-03-26 19:25:46.882857 Epoch 100, Training Loss 0.4134127230519224\n",
      "2022-03-26 19:25:46.905603 Epoch 100, Training Loss 0.41435076357306116\n",
      "2022-03-26 19:25:46.929904 Epoch 100, Training Loss 0.41513651430301957\n",
      "2022-03-26 19:25:46.956917 Epoch 100, Training Loss 0.4160128448854017\n",
      "2022-03-26 19:25:46.980159 Epoch 100, Training Loss 0.41693689348295215\n",
      "2022-03-26 19:25:47.003055 Epoch 100, Training Loss 0.4177076560838143\n",
      "2022-03-26 19:25:47.026089 Epoch 100, Training Loss 0.41856514245195464\n",
      "2022-03-26 19:25:47.048469 Epoch 100, Training Loss 0.4193159590672959\n",
      "2022-03-26 19:25:47.077766 Epoch 100, Training Loss 0.42026764097268626\n",
      "2022-03-26 19:25:47.101174 Epoch 100, Training Loss 0.42121213647868017\n",
      "2022-03-26 19:25:47.124225 Epoch 100, Training Loss 0.4220280478448819\n",
      "2022-03-26 19:25:47.150403 Epoch 100, Training Loss 0.422758049176782\n",
      "2022-03-26 19:25:47.173877 Epoch 100, Training Loss 0.42371926934975185\n",
      "2022-03-26 19:25:47.198200 Epoch 100, Training Loss 0.42454302650126047\n",
      "2022-03-26 19:25:47.221172 Epoch 100, Training Loss 0.4254664005449666\n",
      "2022-03-26 19:25:47.243421 Epoch 100, Training Loss 0.4263585263582142\n",
      "2022-03-26 19:25:47.266063 Epoch 100, Training Loss 0.4272070393309264\n",
      "2022-03-26 19:25:47.293446 Epoch 100, Training Loss 0.4282357577244034\n",
      "2022-03-26 19:25:47.317218 Epoch 100, Training Loss 0.4291846693095649\n",
      "2022-03-26 19:25:47.343602 Epoch 100, Training Loss 0.4300646847090148\n",
      "2022-03-26 19:25:47.372270 Epoch 100, Training Loss 0.4308444412086931\n",
      "2022-03-26 19:25:47.396046 Epoch 100, Training Loss 0.4316131722972826\n",
      "2022-03-26 19:25:47.420637 Epoch 100, Training Loss 0.4324300417586056\n",
      "2022-03-26 19:25:47.444554 Epoch 100, Training Loss 0.4336630970696964\n",
      "2022-03-26 19:25:47.469338 Epoch 100, Training Loss 0.43446401420914\n",
      "2022-03-26 19:25:47.492594 Epoch 100, Training Loss 0.43515879597962664\n",
      "2022-03-26 19:25:47.526950 Epoch 100, Training Loss 0.4362670886318397\n",
      "2022-03-26 19:25:47.550584 Epoch 100, Training Loss 0.4372027007591389\n",
      "2022-03-26 19:25:47.573963 Epoch 100, Training Loss 0.43807347362760996\n",
      "2022-03-26 19:25:47.600332 Epoch 100, Training Loss 0.4389710332960119\n",
      "2022-03-26 19:25:47.623165 Epoch 100, Training Loss 0.4403179607656606\n",
      "2022-03-26 19:25:47.647719 Epoch 100, Training Loss 0.4412392348508396\n",
      "2022-03-26 19:25:47.677684 Epoch 100, Training Loss 0.4421149574963333\n",
      "2022-03-26 19:25:47.700537 Epoch 100, Training Loss 0.44280710408602225\n",
      "2022-03-26 19:25:47.723746 Epoch 100, Training Loss 0.44373144643843326\n",
      "2022-03-26 19:25:47.755484 Epoch 100, Training Loss 0.44493542977458683\n",
      "2022-03-26 19:25:47.778736 Epoch 100, Training Loss 0.44578875090612474\n",
      "2022-03-26 19:25:47.803393 Epoch 100, Training Loss 0.44669202217817916\n",
      "2022-03-26 19:25:47.826907 Epoch 100, Training Loss 0.44750294276058217\n",
      "2022-03-26 19:25:47.850282 Epoch 100, Training Loss 0.4483750983501029\n",
      "2022-03-26 19:25:47.873945 Epoch 100, Training Loss 0.4491265471405385\n",
      "2022-03-26 19:25:47.897281 Epoch 100, Training Loss 0.4498430494685917\n",
      "2022-03-26 19:25:47.920416 Epoch 100, Training Loss 0.4507923702830854\n",
      "2022-03-26 19:25:47.943383 Epoch 100, Training Loss 0.4518123493551293\n",
      "2022-03-26 19:25:47.973908 Epoch 100, Training Loss 0.45255076751837037\n",
      "2022-03-26 19:25:47.997582 Epoch 100, Training Loss 0.45321851957332143\n",
      "2022-03-26 19:25:48.020366 Epoch 100, Training Loss 0.45397220033666363\n",
      "2022-03-26 19:25:48.047185 Epoch 100, Training Loss 0.4546256487250633\n",
      "2022-03-26 19:25:48.078175 Epoch 100, Training Loss 0.45537240467870327\n",
      "2022-03-26 19:25:48.102925 Epoch 100, Training Loss 0.4560731171875659\n",
      "2022-03-26 19:25:48.126524 Epoch 100, Training Loss 0.45687705274585566\n",
      "2022-03-26 19:25:48.149800 Epoch 100, Training Loss 0.45766705179305944\n",
      "2022-03-26 19:25:48.177267 Epoch 100, Training Loss 0.45860847796473053\n",
      "2022-03-26 19:25:48.206440 Epoch 100, Training Loss 0.459599539180241\n",
      "2022-03-26 19:25:48.231044 Epoch 100, Training Loss 0.4604532202262708\n",
      "2022-03-26 19:25:48.255350 Epoch 100, Training Loss 0.46147698343105026\n",
      "2022-03-26 19:25:48.282923 Epoch 100, Training Loss 0.4621211371534621\n",
      "2022-03-26 19:25:48.306644 Epoch 100, Training Loss 0.46292720399701687\n",
      "2022-03-26 19:25:48.331924 Epoch 100, Training Loss 0.4638574940850363\n",
      "2022-03-26 19:25:48.356171 Epoch 100, Training Loss 0.46470327271372464\n",
      "2022-03-26 19:25:48.380459 Epoch 100, Training Loss 0.4655736477859795\n",
      "2022-03-26 19:25:48.410607 Epoch 100, Training Loss 0.4666598232277214\n",
      "2022-03-26 19:25:48.436240 Epoch 100, Training Loss 0.4675557838605188\n",
      "2022-03-26 19:25:48.459472 Epoch 100, Training Loss 0.4684203134854431\n",
      "2022-03-26 19:25:48.486426 Epoch 100, Training Loss 0.4692640083906291\n",
      "2022-03-26 19:25:48.510063 Epoch 100, Training Loss 0.470410310589444\n",
      "2022-03-26 19:25:48.533516 Epoch 100, Training Loss 0.47143519782196835\n",
      "2022-03-26 19:25:48.556441 Epoch 100, Training Loss 0.4723934273966743\n",
      "2022-03-26 19:25:48.579717 Epoch 100, Training Loss 0.4733810941200427\n",
      "2022-03-26 19:25:48.602838 Epoch 100, Training Loss 0.4742952914875182\n",
      "2022-03-26 19:25:48.633669 Epoch 100, Training Loss 0.4754869612052922\n",
      "2022-03-26 19:25:48.657365 Epoch 100, Training Loss 0.4761609768547365\n",
      "2022-03-26 19:25:48.689676 Epoch 100, Training Loss 0.4771195569687792\n",
      "2022-03-26 19:25:48.715928 Epoch 100, Training Loss 0.4780424393122763\n",
      "2022-03-26 19:25:48.739554 Epoch 100, Training Loss 0.47897230347861414\n",
      "2022-03-26 19:25:48.762467 Epoch 100, Training Loss 0.48008141245531\n",
      "2022-03-26 19:25:48.786142 Epoch 100, Training Loss 0.4807851900301321\n",
      "2022-03-26 19:25:48.809098 Epoch 100, Training Loss 0.48164299759261137\n",
      "2022-03-26 19:25:48.832455 Epoch 100, Training Loss 0.48270009526663726\n",
      "2022-03-26 19:25:48.862408 Epoch 100, Training Loss 0.48390128808405697\n",
      "2022-03-26 19:25:48.886218 Epoch 100, Training Loss 0.4846836250380177\n",
      "2022-03-26 19:25:48.910571 Epoch 100, Training Loss 0.48559060460313813\n",
      "2022-03-26 19:25:48.935788 Epoch 100, Training Loss 0.48669271407377385\n",
      "2022-03-26 19:25:48.960977 Epoch 100, Training Loss 0.4874323705959198\n",
      "2022-03-26 19:25:48.985998 Epoch 100, Training Loss 0.48834327362534946\n",
      "2022-03-26 19:25:49.008644 Epoch 100, Training Loss 0.48945545620473146\n",
      "2022-03-26 19:25:49.031805 Epoch 100, Training Loss 0.4905433906694812\n",
      "2022-03-26 19:25:49.059066 Epoch 100, Training Loss 0.4915377447748428\n",
      "2022-03-26 19:25:49.089163 Epoch 100, Training Loss 0.492527680743076\n",
      "2022-03-26 19:25:49.112351 Epoch 100, Training Loss 0.49354683060932647\n",
      "2022-03-26 19:25:49.135560 Epoch 100, Training Loss 0.4942792047701223\n",
      "2022-03-26 19:25:49.160925 Epoch 100, Training Loss 0.4953550209703348\n",
      "2022-03-26 19:25:49.184730 Epoch 100, Training Loss 0.49621596936222234\n",
      "2022-03-26 19:25:49.207984 Epoch 100, Training Loss 0.49717877965296625\n",
      "2022-03-26 19:25:49.230910 Epoch 100, Training Loss 0.49803196373955366\n",
      "2022-03-26 19:25:49.254006 Epoch 100, Training Loss 0.49906584982524443\n",
      "2022-03-26 19:25:49.278044 Epoch 100, Training Loss 0.5001719893930513\n",
      "2022-03-26 19:25:49.309262 Epoch 100, Training Loss 0.5008854970831396\n",
      "2022-03-26 19:25:49.336313 Epoch 100, Training Loss 0.5015416768429529\n",
      "2022-03-26 19:25:49.358614 Epoch 100, Training Loss 0.5023558326923024\n",
      "2022-03-26 19:25:49.384506 Epoch 100, Training Loss 0.5033383894988033\n",
      "2022-03-26 19:25:49.407175 Epoch 100, Training Loss 0.5048964352482725\n",
      "2022-03-26 19:25:49.430973 Epoch 100, Training Loss 0.5062604084267945\n",
      "2022-03-26 19:25:49.453599 Epoch 100, Training Loss 0.5072815547437619\n",
      "2022-03-26 19:25:49.476732 Epoch 100, Training Loss 0.5083598610766403\n",
      "2022-03-26 19:25:49.499496 Epoch 100, Training Loss 0.509501157552385\n",
      "2022-03-26 19:25:49.528838 Epoch 100, Training Loss 0.5100035054223312\n",
      "2022-03-26 19:25:49.552664 Epoch 100, Training Loss 0.5107821178482012\n",
      "2022-03-26 19:25:49.576264 Epoch 100, Training Loss 0.5116053841379292\n",
      "2022-03-26 19:25:49.602996 Epoch 100, Training Loss 0.5124086388922713\n",
      "2022-03-26 19:25:49.626445 Epoch 100, Training Loss 0.5132410177565596\n",
      "2022-03-26 19:25:49.649370 Epoch 100, Training Loss 0.5141439707306645\n",
      "2022-03-26 19:25:49.672391 Epoch 100, Training Loss 0.5149677022719932\n",
      "2022-03-26 19:25:49.697760 Epoch 100, Training Loss 0.5158277106330827\n",
      "2022-03-26 19:25:49.728366 Epoch 100, Training Loss 0.516633947830066\n",
      "2022-03-26 19:25:49.760413 Epoch 100, Training Loss 0.5174800691473515\n",
      "2022-03-26 19:25:49.785390 Epoch 100, Training Loss 0.5180135504973818\n",
      "2022-03-26 19:25:49.812840 Epoch 100, Training Loss 0.5188139648083836\n",
      "2022-03-26 19:25:49.836130 Epoch 100, Training Loss 0.5201053011142994\n",
      "2022-03-26 19:25:49.859495 Epoch 100, Training Loss 0.520896829150217\n",
      "2022-03-26 19:25:49.888658 Epoch 100, Training Loss 0.521901597985831\n",
      "2022-03-26 19:25:49.911868 Epoch 100, Training Loss 0.5225502814325835\n",
      "2022-03-26 19:25:49.938157 Epoch 100, Training Loss 0.5235386087613947\n",
      "2022-03-26 19:25:49.961794 Epoch 100, Training Loss 0.5245988220357529\n",
      "2022-03-26 19:25:49.992234 Epoch 100, Training Loss 0.5255104302597777\n",
      "2022-03-26 19:25:50.015552 Epoch 100, Training Loss 0.5264913245387699\n",
      "2022-03-26 19:25:50.038651 Epoch 100, Training Loss 0.5277176532141693\n",
      "2022-03-26 19:25:50.065282 Epoch 100, Training Loss 0.528547562677842\n",
      "2022-03-26 19:25:50.088461 Epoch 100, Training Loss 0.5293468374882817\n",
      "2022-03-26 19:25:50.112052 Epoch 100, Training Loss 0.5300940531294059\n",
      "2022-03-26 19:25:50.135195 Epoch 100, Training Loss 0.5311385265854008\n",
      "2022-03-26 19:25:50.158259 Epoch 100, Training Loss 0.5319496601286446\n",
      "2022-03-26 19:25:50.188054 Epoch 100, Training Loss 0.5328812572504859\n",
      "2022-03-26 19:25:50.212803 Epoch 100, Training Loss 0.5337407489109527\n",
      "2022-03-26 19:25:50.235714 Epoch 100, Training Loss 0.5343518547923364\n",
      "2022-03-26 19:25:50.259080 Epoch 100, Training Loss 0.5353221963814763\n",
      "2022-03-26 19:25:50.286519 Epoch 100, Training Loss 0.536109096520697\n",
      "2022-03-26 19:25:50.310206 Epoch 100, Training Loss 0.5371513319061235\n",
      "2022-03-26 19:25:50.337032 Epoch 100, Training Loss 0.5379043277495962\n",
      "2022-03-26 19:25:50.359792 Epoch 100, Training Loss 0.5387294406400007\n",
      "2022-03-26 19:25:50.384802 Epoch 100, Training Loss 0.5394737744499045\n",
      "2022-03-26 19:25:50.415901 Epoch 100, Training Loss 0.5402741682575182\n",
      "2022-03-26 19:25:50.442430 Epoch 100, Training Loss 0.5411063890399226\n",
      "2022-03-26 19:25:50.466354 Epoch 100, Training Loss 0.5419501941603468\n",
      "2022-03-26 19:25:50.493694 Epoch 100, Training Loss 0.542873016289433\n",
      "2022-03-26 19:25:50.517387 Epoch 100, Training Loss 0.5440120876521406\n",
      "2022-03-26 19:25:50.541182 Epoch 100, Training Loss 0.5446303245585288\n",
      "2022-03-26 19:25:50.563939 Epoch 100, Training Loss 0.5455019184009499\n",
      "2022-03-26 19:25:50.587103 Epoch 100, Training Loss 0.5464262732535677\n",
      "2022-03-26 19:25:50.609848 Epoch 100, Training Loss 0.5473166666448573\n",
      "2022-03-26 19:25:50.639917 Epoch 100, Training Loss 0.5481411331831036\n",
      "2022-03-26 19:25:50.663484 Epoch 100, Training Loss 0.5491364825031032\n",
      "2022-03-26 19:25:50.686304 Epoch 100, Training Loss 0.5502568988315285\n",
      "2022-03-26 19:25:50.711140 Epoch 100, Training Loss 0.5510460427578758\n",
      "2022-03-26 19:25:50.744555 Epoch 100, Training Loss 0.5518050409872514\n",
      "2022-03-26 19:25:50.767289 Epoch 100, Training Loss 0.552547324991897\n",
      "2022-03-26 19:25:50.790502 Epoch 100, Training Loss 0.5532791405306448\n",
      "2022-03-26 19:25:50.812956 Epoch 100, Training Loss 0.5541713476714576\n",
      "2022-03-26 19:25:50.836064 Epoch 100, Training Loss 0.5549617242401518\n",
      "2022-03-26 19:25:50.866848 Epoch 100, Training Loss 0.5557723573177976\n",
      "2022-03-26 19:25:50.890406 Epoch 100, Training Loss 0.5568488840480594\n",
      "2022-03-26 19:25:50.913158 Epoch 100, Training Loss 0.5577652773360158\n",
      "2022-03-26 19:25:50.937817 Epoch 100, Training Loss 0.5585037233960598\n",
      "2022-03-26 19:25:50.960387 Epoch 100, Training Loss 0.5594792064574673\n",
      "2022-03-26 19:25:50.984149 Epoch 100, Training Loss 0.5602980812111169\n",
      "2022-03-26 19:25:51.007182 Epoch 100, Training Loss 0.561261001412216\n",
      "2022-03-26 19:25:51.029877 Epoch 100, Training Loss 0.5623707598661218\n",
      "2022-03-26 19:25:51.052501 Epoch 100, Training Loss 0.5635850230979798\n",
      "2022-03-26 19:25:51.087174 Epoch 100, Training Loss 0.564516728148436\n",
      "2022-03-26 19:25:51.113332 Epoch 100, Training Loss 0.565500543436126\n",
      "2022-03-26 19:25:51.136376 Epoch 100, Training Loss 0.566664908814918\n",
      "2022-03-26 19:25:51.160431 Epoch 100, Training Loss 0.567692588014371\n",
      "2022-03-26 19:25:51.187943 Epoch 100, Training Loss 0.5686386367472847\n",
      "2022-03-26 19:25:51.210556 Epoch 100, Training Loss 0.5696201455943725\n",
      "2022-03-26 19:25:51.233493 Epoch 100, Training Loss 0.5703633086531973\n",
      "2022-03-26 19:25:51.256016 Epoch 100, Training Loss 0.5713141601332619\n",
      "2022-03-26 19:25:51.278951 Epoch 100, Training Loss 0.5721028359878398\n",
      "2022-03-26 19:25:51.309752 Epoch 100, Training Loss 0.5729433913593707\n",
      "2022-03-26 19:25:51.337546 Epoch 100, Training Loss 0.5736578278758032\n",
      "2022-03-26 19:25:51.360606 Epoch 100, Training Loss 0.5745053315330344\n",
      "2022-03-26 19:25:51.388873 Epoch 100, Training Loss 0.5753780679248482\n",
      "2022-03-26 19:25:51.412088 Epoch 100, Training Loss 0.5763048908061079\n",
      "2022-03-26 19:25:51.437673 Epoch 100, Training Loss 0.5770913903289439\n",
      "2022-03-26 19:25:51.460364 Epoch 100, Training Loss 0.5778171474595204\n",
      "2022-03-26 19:25:51.484283 Epoch 100, Training Loss 0.5785958457099812\n",
      "2022-03-26 19:25:51.507121 Epoch 100, Training Loss 0.5795097698259841\n",
      "2022-03-26 19:25:51.536673 Epoch 100, Training Loss 0.5803503000446598\n",
      "2022-03-26 19:25:51.560047 Epoch 100, Training Loss 0.581122549179265\n",
      "2022-03-26 19:25:51.583565 Epoch 100, Training Loss 0.5819878301123524\n",
      "2022-03-26 19:25:51.607049 Epoch 100, Training Loss 0.5827679283097577\n",
      "2022-03-26 19:25:51.631855 Epoch 100, Training Loss 0.5836024380782071\n",
      "2022-03-26 19:25:51.654380 Epoch 100, Training Loss 0.5848168843923627\n",
      "2022-03-26 19:25:51.677790 Epoch 100, Training Loss 0.5856424612386147\n",
      "2022-03-26 19:25:51.700267 Epoch 100, Training Loss 0.5866392187374022\n",
      "2022-03-26 19:25:51.722990 Epoch 100, Training Loss 0.5876437722493315\n",
      "2022-03-26 19:25:51.751486 Epoch 100, Training Loss 0.5886906503182848\n",
      "2022-03-26 19:25:51.788556 Epoch 100, Training Loss 0.5894491844393713\n",
      "2022-03-26 19:25:51.811584 Epoch 100, Training Loss 0.5903550611661218\n",
      "2022-03-26 19:25:51.835833 Epoch 100, Training Loss 0.5909988096989024\n",
      "2022-03-26 19:25:51.859618 Epoch 100, Training Loss 0.5919791453558466\n",
      "2022-03-26 19:25:51.882421 Epoch 100, Training Loss 0.5928139311959372\n",
      "2022-03-26 19:25:51.904783 Epoch 100, Training Loss 0.5936816053850876\n",
      "2022-03-26 19:25:51.927367 Epoch 100, Training Loss 0.5947717339410197\n",
      "2022-03-26 19:25:51.950121 Epoch 100, Training Loss 0.5954873522605433\n",
      "2022-03-26 19:25:51.986260 Epoch 100, Training Loss 0.5963883067259703\n",
      "2022-03-26 19:25:52.009899 Epoch 100, Training Loss 0.5975145008176794\n",
      "2022-03-26 19:25:52.035078 Epoch 100, Training Loss 0.5984771050836729\n",
      "2022-03-26 19:25:52.060553 Epoch 100, Training Loss 0.5994233121271328\n",
      "2022-03-26 19:25:52.084019 Epoch 100, Training Loss 0.6004221598281885\n",
      "2022-03-26 19:25:52.107126 Epoch 100, Training Loss 0.6014645109929697\n",
      "2022-03-26 19:25:52.130073 Epoch 100, Training Loss 0.6025058329486481\n",
      "2022-03-26 19:25:52.152558 Epoch 100, Training Loss 0.6033145205672744\n",
      "2022-03-26 19:25:52.174811 Epoch 100, Training Loss 0.6041489588406385\n",
      "2022-03-26 19:25:52.204839 Epoch 100, Training Loss 0.6050286223287777\n",
      "2022-03-26 19:25:52.228649 Epoch 100, Training Loss 0.6059683365056582\n",
      "2022-03-26 19:25:52.251124 Epoch 100, Training Loss 0.6068460030095352\n",
      "2022-03-26 19:25:52.277312 Epoch 100, Training Loss 0.607731672694616\n",
      "2022-03-26 19:25:52.300414 Epoch 100, Training Loss 0.6088008518947665\n",
      "2022-03-26 19:25:52.323429 Epoch 100, Training Loss 0.6095684044577582\n",
      "2022-03-26 19:25:52.348800 Epoch 100, Training Loss 0.6106190159726326\n",
      "2022-03-26 19:25:52.372247 Epoch 100, Training Loss 0.6118814789349466\n",
      "2022-03-26 19:25:52.395628 Epoch 100, Training Loss 0.6127656586563496\n",
      "2022-03-26 19:25:52.425494 Epoch 100, Training Loss 0.6138562326464811\n",
      "2022-03-26 19:25:52.450524 Epoch 100, Training Loss 0.6147670994710435\n",
      "2022-03-26 19:25:52.474095 Epoch 100, Training Loss 0.61554826910386\n",
      "2022-03-26 19:25:52.501431 Epoch 100, Training Loss 0.6166059723519304\n",
      "2022-03-26 19:25:52.524821 Epoch 100, Training Loss 0.61738163297591\n",
      "2022-03-26 19:25:52.548394 Epoch 100, Training Loss 0.61823783906372\n",
      "2022-03-26 19:25:52.571711 Epoch 100, Training Loss 0.6190072794628265\n",
      "2022-03-26 19:25:52.595543 Epoch 100, Training Loss 0.6198759429976154\n",
      "2022-03-26 19:25:52.619304 Epoch 100, Training Loss 0.6207444750515702\n",
      "2022-03-26 19:25:52.650119 Epoch 100, Training Loss 0.6218537531621621\n",
      "2022-03-26 19:25:52.674341 Epoch 100, Training Loss 0.6230855442755058\n",
      "2022-03-26 19:25:52.698781 Epoch 100, Training Loss 0.6238575128414442\n",
      "2022-03-26 19:25:52.726372 Epoch 100, Training Loss 0.6246767421741315\n",
      "2022-03-26 19:25:52.750112 Epoch 100, Training Loss 0.6255866275037951\n",
      "2022-03-26 19:25:52.773637 Epoch 100, Training Loss 0.6265504565613959\n",
      "2022-03-26 19:25:52.804452 Epoch 100, Training Loss 0.6275740952976524\n",
      "2022-03-26 19:25:52.831890 Epoch 100, Training Loss 0.6286947982161856\n",
      "2022-03-26 19:25:52.859500 Epoch 100, Training Loss 0.6295529569277678\n",
      "2022-03-26 19:25:52.894084 Epoch 100, Training Loss 0.6302637248621572\n",
      "2022-03-26 19:25:52.918085 Epoch 100, Training Loss 0.6312440181403514\n",
      "2022-03-26 19:25:52.943162 Epoch 100, Training Loss 0.6321759616856075\n",
      "2022-03-26 19:25:52.971570 Epoch 100, Training Loss 0.6330251261934905\n",
      "2022-03-26 19:25:52.996740 Epoch 100, Training Loss 0.6340345574538117\n",
      "2022-03-26 19:25:53.021004 Epoch 100, Training Loss 0.6349084438646541\n",
      "2022-03-26 19:25:53.045413 Epoch 100, Training Loss 0.6357843153693182\n",
      "2022-03-26 19:25:53.074383 Epoch 100, Training Loss 0.6368163732997597\n",
      "2022-03-26 19:25:53.100223 Epoch 100, Training Loss 0.6379230850569123\n",
      "2022-03-26 19:25:53.123974 Epoch 100, Training Loss 0.6386141637935663\n",
      "2022-03-26 19:25:53.153212 Epoch 100, Training Loss 0.6392972205224854\n",
      "2022-03-26 19:25:53.176805 Epoch 100, Training Loss 0.6400930713433439\n",
      "2022-03-26 19:25:53.202172 Epoch 100, Training Loss 0.640907424375834\n",
      "2022-03-26 19:25:53.225999 Epoch 100, Training Loss 0.6418190243680154\n",
      "2022-03-26 19:25:53.249761 Epoch 100, Training Loss 0.6429637073327208\n",
      "2022-03-26 19:25:53.273338 Epoch 100, Training Loss 0.6437675817619504\n",
      "2022-03-26 19:25:53.304808 Epoch 100, Training Loss 0.6444032422798064\n",
      "2022-03-26 19:25:53.331883 Epoch 100, Training Loss 0.6453409766220985\n",
      "2022-03-26 19:25:53.356242 Epoch 100, Training Loss 0.6459965300758171\n",
      "2022-03-26 19:25:53.383544 Epoch 100, Training Loss 0.6469215287653076\n",
      "2022-03-26 19:25:53.407821 Epoch 100, Training Loss 0.64763832667752\n",
      "2022-03-26 19:25:53.432612 Epoch 100, Training Loss 0.6487235136882729\n",
      "2022-03-26 19:25:53.456566 Epoch 100, Training Loss 0.6496417771291245\n",
      "2022-03-26 19:25:53.479879 Epoch 100, Training Loss 0.6505160192623163\n",
      "2022-03-26 19:25:53.503439 Epoch 100, Training Loss 0.651444698576732\n",
      "2022-03-26 19:25:53.537409 Epoch 100, Training Loss 0.6522489706497363\n",
      "2022-03-26 19:25:53.561489 Epoch 100, Training Loss 0.653002614598445\n",
      "2022-03-26 19:25:53.585754 Epoch 100, Training Loss 0.6539563245861731\n",
      "2022-03-26 19:25:53.609910 Epoch 100, Training Loss 0.6549043424827669\n",
      "2022-03-26 19:25:53.636507 Epoch 100, Training Loss 0.6557769928213275\n",
      "2022-03-26 19:25:53.660152 Epoch 100, Training Loss 0.6567317931472189\n",
      "2022-03-26 19:25:53.683430 Epoch 100, Training Loss 0.6576661266710447\n",
      "2022-03-26 19:25:53.706768 Epoch 100, Training Loss 0.6582526151481491\n",
      "2022-03-26 19:25:53.731078 Epoch 100, Training Loss 0.6590954499019076\n",
      "2022-03-26 19:25:53.762525 Epoch 100, Training Loss 0.660002050802226\n",
      "2022-03-26 19:25:53.787374 Epoch 100, Training Loss 0.660955632357951\n",
      "2022-03-26 19:25:53.810961 Epoch 100, Training Loss 0.6618858484355995\n",
      "2022-03-26 19:25:53.848912 Epoch 100, Training Loss 0.6628188582332543\n",
      "2022-03-26 19:25:53.873243 Epoch 100, Training Loss 0.663812320708009\n",
      "2022-03-26 19:25:53.897146 Epoch 100, Training Loss 0.6648935919527508\n",
      "2022-03-26 19:25:53.920737 Epoch 100, Training Loss 0.6659677385369225\n",
      "2022-03-26 19:25:53.944576 Epoch 100, Training Loss 0.6667260094676786\n",
      "2022-03-26 19:25:53.975152 Epoch 100, Training Loss 0.6678242112517052\n",
      "2022-03-26 19:25:54.000826 Epoch 100, Training Loss 0.668613312646861\n",
      "2022-03-26 19:25:54.025508 Epoch 100, Training Loss 0.6694902824928693\n",
      "2022-03-26 19:25:54.051942 Epoch 100, Training Loss 0.6705954253216229\n",
      "2022-03-26 19:25:54.076325 Epoch 100, Training Loss 0.6717222915281116\n",
      "2022-03-26 19:25:54.107625 Epoch 100, Training Loss 0.6728456515790252\n",
      "2022-03-26 19:25:54.131382 Epoch 100, Training Loss 0.6735430090781063\n",
      "2022-03-26 19:25:54.155945 Epoch 100, Training Loss 0.6744475447765702\n",
      "2022-03-26 19:25:54.186170 Epoch 100, Training Loss 0.6753002097234702\n",
      "2022-03-26 19:25:54.214064 Epoch 100, Training Loss 0.6760550607043458\n",
      "2022-03-26 19:25:54.237990 Epoch 100, Training Loss 0.6769529807445643\n",
      "2022-03-26 19:25:54.263014 Epoch 100, Training Loss 0.677635298741748\n",
      "2022-03-26 19:25:54.288967 Epoch 100, Training Loss 0.6787387653994743\n",
      "2022-03-26 19:25:54.313215 Epoch 100, Training Loss 0.6796335284514805\n",
      "2022-03-26 19:25:54.339690 Epoch 100, Training Loss 0.6806654695354765\n",
      "2022-03-26 19:25:54.363821 Epoch 100, Training Loss 0.6814935602190549\n",
      "2022-03-26 19:25:54.389736 Epoch 100, Training Loss 0.6825129696932595\n",
      "2022-03-26 19:25:54.421205 Epoch 100, Training Loss 0.6830817848977531\n",
      "2022-03-26 19:25:54.446482 Epoch 100, Training Loss 0.6839859387301424\n",
      "2022-03-26 19:25:54.473712 Epoch 100, Training Loss 0.6850382612489373\n",
      "2022-03-26 19:25:54.497671 Epoch 100, Training Loss 0.6856685087580205\n",
      "2022-03-26 19:25:54.520817 Epoch 100, Training Loss 0.6865059829047878\n",
      "2022-03-26 19:25:54.544775 Epoch 100, Training Loss 0.6875521145437075\n",
      "2022-03-26 19:25:54.568390 Epoch 100, Training Loss 0.6882562318345165\n",
      "2022-03-26 19:25:54.592068 Epoch 100, Training Loss 0.6892547905826203\n",
      "2022-03-26 19:25:54.623769 Epoch 100, Training Loss 0.6899970065221153\n",
      "2022-03-26 19:25:54.647800 Epoch 100, Training Loss 0.6907161593513416\n",
      "2022-03-26 19:25:54.656991 Epoch 100, Training Loss 0.6918027305313389\n",
      "2022-03-26 19:40:39.778333 Epoch 150, Training Loss 0.0014255234347584913\n",
      "2022-03-26 19:40:39.801456 Epoch 150, Training Loss 0.002269871094647576\n",
      "2022-03-26 19:40:39.823790 Epoch 150, Training Loss 0.0029849979426244946\n",
      "2022-03-26 19:40:39.846532 Epoch 150, Training Loss 0.0037028885558438116\n",
      "2022-03-26 19:40:39.875516 Epoch 150, Training Loss 0.004375251967583775\n",
      "2022-03-26 19:40:39.898524 Epoch 150, Training Loss 0.004952903179561391\n",
      "2022-03-26 19:40:39.924193 Epoch 150, Training Loss 0.005894349168633561\n",
      "2022-03-26 19:40:39.948255 Epoch 150, Training Loss 0.006604531422600417\n",
      "2022-03-26 19:40:39.971714 Epoch 150, Training Loss 0.007300391717030264\n",
      "2022-03-26 19:40:40.001962 Epoch 150, Training Loss 0.008188026221207036\n",
      "2022-03-26 19:40:40.025331 Epoch 150, Training Loss 0.009061236172685843\n",
      "2022-03-26 19:40:40.047720 Epoch 150, Training Loss 0.009873614858483414\n",
      "2022-03-26 19:40:40.073414 Epoch 150, Training Loss 0.010731753600222984\n",
      "2022-03-26 19:40:40.095619 Epoch 150, Training Loss 0.011576806988252704\n",
      "2022-03-26 19:40:40.118346 Epoch 150, Training Loss 0.012246065623010212\n",
      "2022-03-26 19:40:40.140673 Epoch 150, Training Loss 0.012791655557539762\n",
      "2022-03-26 19:40:40.173204 Epoch 150, Training Loss 0.013494418405206002\n",
      "2022-03-26 19:40:40.196226 Epoch 150, Training Loss 0.014088119654094471\n",
      "2022-03-26 19:40:40.226282 Epoch 150, Training Loss 0.014779437647756103\n",
      "2022-03-26 19:40:40.249550 Epoch 150, Training Loss 0.015880975919916197\n",
      "2022-03-26 19:40:40.272539 Epoch 150, Training Loss 0.016787080089454457\n",
      "2022-03-26 19:40:40.297288 Epoch 150, Training Loss 0.017370855983565834\n",
      "2022-03-26 19:40:40.322077 Epoch 150, Training Loss 0.018215166905042157\n",
      "2022-03-26 19:40:40.347709 Epoch 150, Training Loss 0.018944199699575028\n",
      "2022-03-26 19:40:40.371204 Epoch 150, Training Loss 0.019846724236712736\n",
      "2022-03-26 19:40:40.394128 Epoch 150, Training Loss 0.02049136943067126\n",
      "2022-03-26 19:40:40.416350 Epoch 150, Training Loss 0.02108140735675002\n",
      "2022-03-26 19:40:40.447446 Epoch 150, Training Loss 0.021952919795385103\n",
      "2022-03-26 19:40:40.474551 Epoch 150, Training Loss 0.023059021710129954\n",
      "2022-03-26 19:40:40.496364 Epoch 150, Training Loss 0.023964241642476347\n",
      "2022-03-26 19:40:40.518957 Epoch 150, Training Loss 0.024715809718422268\n",
      "2022-03-26 19:40:40.545383 Epoch 150, Training Loss 0.025435900215602592\n",
      "2022-03-26 19:40:40.567682 Epoch 150, Training Loss 0.026323568714244287\n",
      "2022-03-26 19:40:40.590092 Epoch 150, Training Loss 0.027015692209038895\n",
      "2022-03-26 19:40:40.612579 Epoch 150, Training Loss 0.028096551709162913\n",
      "2022-03-26 19:40:40.635380 Epoch 150, Training Loss 0.02872360960754287\n",
      "2022-03-26 19:40:40.664460 Epoch 150, Training Loss 0.02944331778132397\n",
      "2022-03-26 19:40:40.691642 Epoch 150, Training Loss 0.030168625514220705\n",
      "2022-03-26 19:40:40.713890 Epoch 150, Training Loss 0.030839831742179365\n",
      "2022-03-26 19:40:40.736526 Epoch 150, Training Loss 0.031731303962295315\n",
      "2022-03-26 19:40:40.763209 Epoch 150, Training Loss 0.03252624950902846\n",
      "2022-03-26 19:40:40.785971 Epoch 150, Training Loss 0.03332005067707023\n",
      "2022-03-26 19:40:40.808821 Epoch 150, Training Loss 0.034177933469452824\n",
      "2022-03-26 19:40:40.832227 Epoch 150, Training Loss 0.03466587820473839\n",
      "2022-03-26 19:40:40.855696 Epoch 150, Training Loss 0.03561091815571651\n",
      "2022-03-26 19:40:40.885858 Epoch 150, Training Loss 0.03618713977086879\n",
      "2022-03-26 19:40:40.910507 Epoch 150, Training Loss 0.03713101773615688\n",
      "2022-03-26 19:40:40.934911 Epoch 150, Training Loss 0.03769683742614658\n",
      "2022-03-26 19:40:40.962696 Epoch 150, Training Loss 0.03838720365101114\n",
      "2022-03-26 19:40:40.986824 Epoch 150, Training Loss 0.039312446018314115\n",
      "2022-03-26 19:40:41.011061 Epoch 150, Training Loss 0.04007236994898228\n",
      "2022-03-26 19:40:41.035827 Epoch 150, Training Loss 0.0408883073826885\n",
      "2022-03-26 19:40:41.058785 Epoch 150, Training Loss 0.04167707622661005\n",
      "2022-03-26 19:40:41.081090 Epoch 150, Training Loss 0.042612790642187114\n",
      "2022-03-26 19:40:41.114418 Epoch 150, Training Loss 0.0433963988062061\n",
      "2022-03-26 19:40:41.139189 Epoch 150, Training Loss 0.044144889697089525\n",
      "2022-03-26 19:40:41.162196 Epoch 150, Training Loss 0.04472798322472731\n",
      "2022-03-26 19:40:41.191742 Epoch 150, Training Loss 0.04552489122771241\n",
      "2022-03-26 19:40:41.219205 Epoch 150, Training Loss 0.04625607642066448\n",
      "2022-03-26 19:40:41.243513 Epoch 150, Training Loss 0.04702544783997109\n",
      "2022-03-26 19:40:41.267567 Epoch 150, Training Loss 0.04784601599054263\n",
      "2022-03-26 19:40:41.290534 Epoch 150, Training Loss 0.04856440341076278\n",
      "2022-03-26 19:40:41.313562 Epoch 150, Training Loss 0.049200495795520674\n",
      "2022-03-26 19:40:41.346321 Epoch 150, Training Loss 0.04998100421312825\n",
      "2022-03-26 19:40:41.372072 Epoch 150, Training Loss 0.050578327320725716\n",
      "2022-03-26 19:40:41.396423 Epoch 150, Training Loss 0.05119382984497968\n",
      "2022-03-26 19:40:41.421816 Epoch 150, Training Loss 0.05238652183576618\n",
      "2022-03-26 19:40:41.445772 Epoch 150, Training Loss 0.053061966091165765\n",
      "2022-03-26 19:40:41.468520 Epoch 150, Training Loss 0.05406317671241663\n",
      "2022-03-26 19:40:41.491985 Epoch 150, Training Loss 0.054790437755072516\n",
      "2022-03-26 19:40:41.514567 Epoch 150, Training Loss 0.05558170953674999\n",
      "2022-03-26 19:40:41.537296 Epoch 150, Training Loss 0.05637546748761326\n",
      "2022-03-26 19:40:41.567274 Epoch 150, Training Loss 0.05714915025874477\n",
      "2022-03-26 19:40:41.591366 Epoch 150, Training Loss 0.05779658459946323\n",
      "2022-03-26 19:40:41.614836 Epoch 150, Training Loss 0.05847116778878605\n",
      "2022-03-26 19:40:41.641761 Epoch 150, Training Loss 0.05906910188210285\n",
      "2022-03-26 19:40:41.664981 Epoch 150, Training Loss 0.059874701263654566\n",
      "2022-03-26 19:40:41.687906 Epoch 150, Training Loss 0.06050801879304754\n",
      "2022-03-26 19:40:41.711492 Epoch 150, Training Loss 0.061496270205968484\n",
      "2022-03-26 19:40:41.734499 Epoch 150, Training Loss 0.06233039284910997\n",
      "2022-03-26 19:40:41.757367 Epoch 150, Training Loss 0.0631202992118533\n",
      "2022-03-26 19:40:41.787648 Epoch 150, Training Loss 0.06385072196840935\n",
      "2022-03-26 19:40:41.811157 Epoch 150, Training Loss 0.0646909455509137\n",
      "2022-03-26 19:40:41.834753 Epoch 150, Training Loss 0.06569187476506928\n",
      "2022-03-26 19:40:41.862604 Epoch 150, Training Loss 0.0664314174896006\n",
      "2022-03-26 19:40:41.890302 Epoch 150, Training Loss 0.06707527867668425\n",
      "2022-03-26 19:40:41.913319 Epoch 150, Training Loss 0.06793928451245398\n",
      "2022-03-26 19:40:41.936301 Epoch 150, Training Loss 0.06874354240839439\n",
      "2022-03-26 19:40:41.959787 Epoch 150, Training Loss 0.06938459691794022\n",
      "2022-03-26 19:40:41.986752 Epoch 150, Training Loss 0.07038835148372309\n",
      "2022-03-26 19:40:42.016644 Epoch 150, Training Loss 0.07120912024737014\n",
      "2022-03-26 19:40:42.040255 Epoch 150, Training Loss 0.07199594126942822\n",
      "2022-03-26 19:40:42.063985 Epoch 150, Training Loss 0.07263714608633914\n",
      "2022-03-26 19:40:42.092104 Epoch 150, Training Loss 0.07354429280361556\n",
      "2022-03-26 19:40:42.115303 Epoch 150, Training Loss 0.07465941384625252\n",
      "2022-03-26 19:40:42.139267 Epoch 150, Training Loss 0.07562255424916592\n",
      "2022-03-26 19:40:42.162016 Epoch 150, Training Loss 0.07653832740491004\n",
      "2022-03-26 19:40:42.184494 Epoch 150, Training Loss 0.07735454998052943\n",
      "2022-03-26 19:40:42.211376 Epoch 150, Training Loss 0.07823754614576355\n",
      "2022-03-26 19:40:42.248438 Epoch 150, Training Loss 0.07868692629477557\n",
      "2022-03-26 19:40:42.272250 Epoch 150, Training Loss 0.0794852590164565\n",
      "2022-03-26 19:40:42.294321 Epoch 150, Training Loss 0.08034946508419788\n",
      "2022-03-26 19:40:42.320715 Epoch 150, Training Loss 0.08103638681609307\n",
      "2022-03-26 19:40:42.345815 Epoch 150, Training Loss 0.08224324672423361\n",
      "2022-03-26 19:40:42.369131 Epoch 150, Training Loss 0.08310590017482143\n",
      "2022-03-26 19:40:42.392608 Epoch 150, Training Loss 0.08369100524488922\n",
      "2022-03-26 19:40:42.415797 Epoch 150, Training Loss 0.08452333090707774\n",
      "2022-03-26 19:40:42.440037 Epoch 150, Training Loss 0.08519242978309427\n",
      "2022-03-26 19:40:42.469746 Epoch 150, Training Loss 0.08623316986938877\n",
      "2022-03-26 19:40:42.493237 Epoch 150, Training Loss 0.0872446124434776\n",
      "2022-03-26 19:40:42.516286 Epoch 150, Training Loss 0.08797826574129217\n",
      "2022-03-26 19:40:42.542471 Epoch 150, Training Loss 0.08869291163618913\n",
      "2022-03-26 19:40:42.564713 Epoch 150, Training Loss 0.08919124514855387\n",
      "2022-03-26 19:40:42.587854 Epoch 150, Training Loss 0.09018724859523042\n",
      "2022-03-26 19:40:42.610008 Epoch 150, Training Loss 0.09108834841367229\n",
      "2022-03-26 19:40:42.633184 Epoch 150, Training Loss 0.09175056203856798\n",
      "2022-03-26 19:40:42.655792 Epoch 150, Training Loss 0.09259844535147138\n",
      "2022-03-26 19:40:42.684993 Epoch 150, Training Loss 0.09349741662859612\n",
      "2022-03-26 19:40:42.709183 Epoch 150, Training Loss 0.09430151240295157\n",
      "2022-03-26 19:40:42.732175 Epoch 150, Training Loss 0.09533943902805943\n",
      "2022-03-26 19:40:42.756367 Epoch 150, Training Loss 0.09631452657987395\n",
      "2022-03-26 19:40:42.780147 Epoch 150, Training Loss 0.09705770320599647\n",
      "2022-03-26 19:40:42.803040 Epoch 150, Training Loss 0.09793538411559961\n",
      "2022-03-26 19:40:42.825797 Epoch 150, Training Loss 0.09867141527288101\n",
      "2022-03-26 19:40:42.849470 Epoch 150, Training Loss 0.09946155761513868\n",
      "2022-03-26 19:40:42.878710 Epoch 150, Training Loss 0.10024252572023046\n",
      "2022-03-26 19:40:42.908608 Epoch 150, Training Loss 0.1010701692927524\n",
      "2022-03-26 19:40:42.931251 Epoch 150, Training Loss 0.10190968935751854\n",
      "2022-03-26 19:40:42.954848 Epoch 150, Training Loss 0.10301522213174864\n",
      "2022-03-26 19:40:42.982386 Epoch 150, Training Loss 0.10383071588433307\n",
      "2022-03-26 19:40:43.004834 Epoch 150, Training Loss 0.10449320992545398\n",
      "2022-03-26 19:40:43.028407 Epoch 150, Training Loss 0.10534737215322607\n",
      "2022-03-26 19:40:43.050821 Epoch 150, Training Loss 0.10611169394629691\n",
      "2022-03-26 19:40:43.073561 Epoch 150, Training Loss 0.10672603692392559\n",
      "2022-03-26 19:40:43.096325 Epoch 150, Training Loss 0.10754815624345598\n",
      "2022-03-26 19:40:43.126252 Epoch 150, Training Loss 0.10802381472361972\n",
      "2022-03-26 19:40:43.149706 Epoch 150, Training Loss 0.10866721629943994\n",
      "2022-03-26 19:40:43.171949 Epoch 150, Training Loss 0.10967916048243832\n",
      "2022-03-26 19:40:43.200336 Epoch 150, Training Loss 0.11053057053052556\n",
      "2022-03-26 19:40:43.223775 Epoch 150, Training Loss 0.11119671608023632\n",
      "2022-03-26 19:40:43.248806 Epoch 150, Training Loss 0.11193318596428922\n",
      "2022-03-26 19:40:43.279900 Epoch 150, Training Loss 0.11304582990801243\n",
      "2022-03-26 19:40:43.302290 Epoch 150, Training Loss 0.11365736106320111\n",
      "2022-03-26 19:40:43.324910 Epoch 150, Training Loss 0.11432723418983352\n",
      "2022-03-26 19:40:43.361319 Epoch 150, Training Loss 0.11519303403394607\n",
      "2022-03-26 19:40:43.384780 Epoch 150, Training Loss 0.11568120857486335\n",
      "2022-03-26 19:40:43.407655 Epoch 150, Training Loss 0.1167516944277317\n",
      "2022-03-26 19:40:43.435360 Epoch 150, Training Loss 0.11753008992928068\n",
      "2022-03-26 19:40:43.458966 Epoch 150, Training Loss 0.11820229602134441\n",
      "2022-03-26 19:40:43.483130 Epoch 150, Training Loss 0.11908765663118924\n",
      "2022-03-26 19:40:43.506401 Epoch 150, Training Loss 0.11972451034714193\n",
      "2022-03-26 19:40:43.529627 Epoch 150, Training Loss 0.1205685083061228\n",
      "2022-03-26 19:40:43.562342 Epoch 150, Training Loss 0.12146288834874282\n",
      "2022-03-26 19:40:43.593789 Epoch 150, Training Loss 0.12221516055219314\n",
      "2022-03-26 19:40:43.618581 Epoch 150, Training Loss 0.1232777709698738\n",
      "2022-03-26 19:40:43.641961 Epoch 150, Training Loss 0.1240875664574411\n",
      "2022-03-26 19:40:43.666463 Epoch 150, Training Loss 0.12485040003991188\n",
      "2022-03-26 19:40:43.689487 Epoch 150, Training Loss 0.125614743937007\n",
      "2022-03-26 19:40:43.712506 Epoch 150, Training Loss 0.12675974337036347\n",
      "2022-03-26 19:40:43.735633 Epoch 150, Training Loss 0.12758995680248036\n",
      "2022-03-26 19:40:43.758796 Epoch 150, Training Loss 0.12842797265028405\n",
      "2022-03-26 19:40:43.783080 Epoch 150, Training Loss 0.1291229003835517\n",
      "2022-03-26 19:40:43.813415 Epoch 150, Training Loss 0.12995400346453537\n",
      "2022-03-26 19:40:43.838203 Epoch 150, Training Loss 0.13079831446223247\n",
      "2022-03-26 19:40:43.862092 Epoch 150, Training Loss 0.13170172841957464\n",
      "2022-03-26 19:40:43.889155 Epoch 150, Training Loss 0.13230708820740586\n",
      "2022-03-26 19:40:43.912041 Epoch 150, Training Loss 0.13314014970494048\n",
      "2022-03-26 19:40:43.934204 Epoch 150, Training Loss 0.13379549035025984\n",
      "2022-03-26 19:40:43.956801 Epoch 150, Training Loss 0.13439909656486854\n",
      "2022-03-26 19:40:43.979493 Epoch 150, Training Loss 0.13544403077543848\n",
      "2022-03-26 19:40:44.002030 Epoch 150, Training Loss 0.1359805126324334\n",
      "2022-03-26 19:40:44.030439 Epoch 150, Training Loss 0.1367400524104038\n",
      "2022-03-26 19:40:44.058683 Epoch 150, Training Loss 0.13737524984895116\n",
      "2022-03-26 19:40:44.085576 Epoch 150, Training Loss 0.13793154346668507\n",
      "2022-03-26 19:40:44.109002 Epoch 150, Training Loss 0.13891567575657154\n",
      "2022-03-26 19:40:44.140569 Epoch 150, Training Loss 0.13938774854478325\n",
      "2022-03-26 19:40:44.164030 Epoch 150, Training Loss 0.14004010011625412\n",
      "2022-03-26 19:40:44.187539 Epoch 150, Training Loss 0.14081901948317846\n",
      "2022-03-26 19:40:44.215018 Epoch 150, Training Loss 0.1415897255282268\n",
      "2022-03-26 19:40:44.241557 Epoch 150, Training Loss 0.14231605904029154\n",
      "2022-03-26 19:40:44.267581 Epoch 150, Training Loss 0.1432008488327646\n",
      "2022-03-26 19:40:44.300824 Epoch 150, Training Loss 0.14385397747502\n",
      "2022-03-26 19:40:44.327583 Epoch 150, Training Loss 0.14473987894747264\n",
      "2022-03-26 19:40:44.352874 Epoch 150, Training Loss 0.14543213537129598\n",
      "2022-03-26 19:40:44.377339 Epoch 150, Training Loss 0.14608067887670853\n",
      "2022-03-26 19:40:44.401669 Epoch 150, Training Loss 0.1470381991408975\n",
      "2022-03-26 19:40:44.427964 Epoch 150, Training Loss 0.14759674092845235\n",
      "2022-03-26 19:40:44.459672 Epoch 150, Training Loss 0.14818473141211683\n",
      "2022-03-26 19:40:44.485741 Epoch 150, Training Loss 0.14871481758401828\n",
      "2022-03-26 19:40:44.508792 Epoch 150, Training Loss 0.14956691590568905\n",
      "2022-03-26 19:40:44.536686 Epoch 150, Training Loss 0.15021098022113372\n",
      "2022-03-26 19:40:44.560167 Epoch 150, Training Loss 0.15102399405463576\n",
      "2022-03-26 19:40:44.583567 Epoch 150, Training Loss 0.151677609747633\n",
      "2022-03-26 19:40:44.607766 Epoch 150, Training Loss 0.15251622301385837\n",
      "2022-03-26 19:40:44.632729 Epoch 150, Training Loss 0.15327910240501394\n",
      "2022-03-26 19:40:44.656193 Epoch 150, Training Loss 0.15415350376340128\n",
      "2022-03-26 19:40:44.686909 Epoch 150, Training Loss 0.15495139303262276\n",
      "2022-03-26 19:40:44.710499 Epoch 150, Training Loss 0.1556763821626868\n",
      "2022-03-26 19:40:44.735315 Epoch 150, Training Loss 0.15648532085254063\n",
      "2022-03-26 19:40:44.763673 Epoch 150, Training Loss 0.1572759309616845\n",
      "2022-03-26 19:40:44.787910 Epoch 150, Training Loss 0.157872382141745\n",
      "2022-03-26 19:40:44.811624 Epoch 150, Training Loss 0.15856659492415845\n",
      "2022-03-26 19:40:44.836896 Epoch 150, Training Loss 0.1592425839294253\n",
      "2022-03-26 19:40:44.860966 Epoch 150, Training Loss 0.16003521015424557\n",
      "2022-03-26 19:40:44.884560 Epoch 150, Training Loss 0.16093246288159313\n",
      "2022-03-26 19:40:44.914900 Epoch 150, Training Loss 0.16148186770394024\n",
      "2022-03-26 19:40:44.939421 Epoch 150, Training Loss 0.1624129144355769\n",
      "2022-03-26 19:40:44.963683 Epoch 150, Training Loss 0.163032470113786\n",
      "2022-03-26 19:40:44.997312 Epoch 150, Training Loss 0.1639567380747222\n",
      "2022-03-26 19:40:45.021766 Epoch 150, Training Loss 0.1650042810937023\n",
      "2022-03-26 19:40:45.045602 Epoch 150, Training Loss 0.16590990766387462\n",
      "2022-03-26 19:40:45.074818 Epoch 150, Training Loss 0.16685973374587496\n",
      "2022-03-26 19:40:45.099351 Epoch 150, Training Loss 0.16804554704052713\n",
      "2022-03-26 19:40:45.129771 Epoch 150, Training Loss 0.16901273034570163\n",
      "2022-03-26 19:40:45.155406 Epoch 150, Training Loss 0.16987610110998763\n",
      "2022-03-26 19:40:45.179101 Epoch 150, Training Loss 0.17071620830337106\n",
      "2022-03-26 19:40:45.204241 Epoch 150, Training Loss 0.1713191262443962\n",
      "2022-03-26 19:40:45.229564 Epoch 150, Training Loss 0.17216832489918565\n",
      "2022-03-26 19:40:45.253677 Epoch 150, Training Loss 0.17274786227042108\n",
      "2022-03-26 19:40:45.277372 Epoch 150, Training Loss 0.17364050089703192\n",
      "2022-03-26 19:40:45.303751 Epoch 150, Training Loss 0.17431433148243847\n",
      "2022-03-26 19:40:45.337254 Epoch 150, Training Loss 0.1752897258990866\n",
      "2022-03-26 19:40:45.364655 Epoch 150, Training Loss 0.176034863990591\n",
      "2022-03-26 19:40:45.387229 Epoch 150, Training Loss 0.17698212425269738\n",
      "2022-03-26 19:40:45.410488 Epoch 150, Training Loss 0.1780248552255923\n",
      "2022-03-26 19:40:45.435947 Epoch 150, Training Loss 0.17892607943633634\n",
      "2022-03-26 19:40:45.459706 Epoch 150, Training Loss 0.17966257439702368\n",
      "2022-03-26 19:40:45.482698 Epoch 150, Training Loss 0.180333275586138\n",
      "2022-03-26 19:40:45.505100 Epoch 150, Training Loss 0.18113359424006908\n",
      "2022-03-26 19:40:45.528118 Epoch 150, Training Loss 0.18195840079918543\n",
      "2022-03-26 19:40:45.557422 Epoch 150, Training Loss 0.18268437653093997\n",
      "2022-03-26 19:40:45.582544 Epoch 150, Training Loss 0.18341609526930563\n",
      "2022-03-26 19:40:45.604706 Epoch 150, Training Loss 0.18417863677377286\n",
      "2022-03-26 19:40:45.627039 Epoch 150, Training Loss 0.18531146573136226\n",
      "2022-03-26 19:40:45.652893 Epoch 150, Training Loss 0.18614151490771252\n",
      "2022-03-26 19:40:45.675467 Epoch 150, Training Loss 0.1869861608194878\n",
      "2022-03-26 19:40:45.697477 Epoch 150, Training Loss 0.18783538798084648\n",
      "2022-03-26 19:40:45.719031 Epoch 150, Training Loss 0.1886107008094373\n",
      "2022-03-26 19:40:45.742044 Epoch 150, Training Loss 0.1895526086201753\n",
      "2022-03-26 19:40:45.769140 Epoch 150, Training Loss 0.19040358847821764\n",
      "2022-03-26 19:40:45.794214 Epoch 150, Training Loss 0.19114408205689676\n",
      "2022-03-26 19:40:45.816943 Epoch 150, Training Loss 0.19192961769183273\n",
      "2022-03-26 19:40:45.841117 Epoch 150, Training Loss 0.19274099392201893\n",
      "2022-03-26 19:40:45.866253 Epoch 150, Training Loss 0.1934421695483005\n",
      "2022-03-26 19:40:45.895357 Epoch 150, Training Loss 0.19401628731766626\n",
      "2022-03-26 19:40:45.917991 Epoch 150, Training Loss 0.19473335535629935\n",
      "2022-03-26 19:40:45.940345 Epoch 150, Training Loss 0.19571206888274464\n",
      "2022-03-26 19:40:45.962936 Epoch 150, Training Loss 0.19658506808378506\n",
      "2022-03-26 19:40:45.994768 Epoch 150, Training Loss 0.19727040526202266\n",
      "2022-03-26 19:40:46.017533 Epoch 150, Training Loss 0.1977677439408534\n",
      "2022-03-26 19:40:46.040132 Epoch 150, Training Loss 0.1986573117849467\n",
      "2022-03-26 19:40:46.066681 Epoch 150, Training Loss 0.1993202655897726\n",
      "2022-03-26 19:40:46.088608 Epoch 150, Training Loss 0.200292242190722\n",
      "2022-03-26 19:40:46.111102 Epoch 150, Training Loss 0.2011671600972905\n",
      "2022-03-26 19:40:46.133674 Epoch 150, Training Loss 0.20184170197495413\n",
      "2022-03-26 19:40:46.156197 Epoch 150, Training Loss 0.2024803459263214\n",
      "2022-03-26 19:40:46.179617 Epoch 150, Training Loss 0.20323415862782226\n",
      "2022-03-26 19:40:46.209172 Epoch 150, Training Loss 0.20421392346739464\n",
      "2022-03-26 19:40:46.232965 Epoch 150, Training Loss 0.20478986135071806\n",
      "2022-03-26 19:40:46.256101 Epoch 150, Training Loss 0.20527986393255346\n",
      "2022-03-26 19:40:46.278609 Epoch 150, Training Loss 0.20606643388338405\n",
      "2022-03-26 19:40:46.306354 Epoch 150, Training Loss 0.20681860547541353\n",
      "2022-03-26 19:40:46.337116 Epoch 150, Training Loss 0.2074528046123817\n",
      "2022-03-26 19:40:46.363859 Epoch 150, Training Loss 0.20805261846241133\n",
      "2022-03-26 19:40:46.387433 Epoch 150, Training Loss 0.2086476317756926\n",
      "2022-03-26 19:40:46.411525 Epoch 150, Training Loss 0.20935940742492676\n",
      "2022-03-26 19:40:46.444740 Epoch 150, Training Loss 0.21000996925641813\n",
      "2022-03-26 19:40:46.468023 Epoch 150, Training Loss 0.21084113064629342\n",
      "2022-03-26 19:40:46.491758 Epoch 150, Training Loss 0.21170879271634094\n",
      "2022-03-26 19:40:46.517541 Epoch 150, Training Loss 0.21239381121552509\n",
      "2022-03-26 19:40:46.540473 Epoch 150, Training Loss 0.21318611723687642\n",
      "2022-03-26 19:40:46.563930 Epoch 150, Training Loss 0.21409385809508127\n",
      "2022-03-26 19:40:46.585763 Epoch 150, Training Loss 0.21493336771760146\n",
      "2022-03-26 19:40:46.607848 Epoch 150, Training Loss 0.21560343375901128\n",
      "2022-03-26 19:40:46.637748 Epoch 150, Training Loss 0.21651931301407193\n",
      "2022-03-26 19:40:46.661293 Epoch 150, Training Loss 0.2174391017088195\n",
      "2022-03-26 19:40:46.683484 Epoch 150, Training Loss 0.21830181514515595\n",
      "2022-03-26 19:40:46.709751 Epoch 150, Training Loss 0.21887976845816884\n",
      "2022-03-26 19:40:46.732552 Epoch 150, Training Loss 0.21959682346304968\n",
      "2022-03-26 19:40:46.755562 Epoch 150, Training Loss 0.22084469403452275\n",
      "2022-03-26 19:40:46.778600 Epoch 150, Training Loss 0.22179513910542364\n",
      "2022-03-26 19:40:46.801660 Epoch 150, Training Loss 0.22237567081475806\n",
      "2022-03-26 19:40:46.824333 Epoch 150, Training Loss 0.22316389338439688\n",
      "2022-03-26 19:40:46.854762 Epoch 150, Training Loss 0.22414259768812858\n",
      "2022-03-26 19:40:46.879127 Epoch 150, Training Loss 0.22517142179981828\n",
      "2022-03-26 19:40:46.902201 Epoch 150, Training Loss 0.2260631136882031\n",
      "2022-03-26 19:40:46.926187 Epoch 150, Training Loss 0.2268301786669075\n",
      "2022-03-26 19:40:46.950463 Epoch 150, Training Loss 0.2279441169155833\n",
      "2022-03-26 19:40:46.972815 Epoch 150, Training Loss 0.22862716526021737\n",
      "2022-03-26 19:40:46.995449 Epoch 150, Training Loss 0.22951396240297792\n",
      "2022-03-26 19:40:47.017825 Epoch 150, Training Loss 0.23014570776458895\n",
      "2022-03-26 19:40:47.040166 Epoch 150, Training Loss 0.23079436826888863\n",
      "2022-03-26 19:40:47.066489 Epoch 150, Training Loss 0.23158262574764163\n",
      "2022-03-26 19:40:47.096596 Epoch 150, Training Loss 0.23245738709674163\n",
      "2022-03-26 19:40:47.120031 Epoch 150, Training Loss 0.23325336665448632\n",
      "2022-03-26 19:40:47.143674 Epoch 150, Training Loss 0.23392546489415572\n",
      "2022-03-26 19:40:47.166425 Epoch 150, Training Loss 0.23465274895548516\n",
      "2022-03-26 19:40:47.192786 Epoch 150, Training Loss 0.2354508978326607\n",
      "2022-03-26 19:40:47.215585 Epoch 150, Training Loss 0.23640274727131094\n",
      "2022-03-26 19:40:47.238684 Epoch 150, Training Loss 0.23705811124018697\n",
      "2022-03-26 19:40:47.262436 Epoch 150, Training Loss 0.23753568430995697\n",
      "2022-03-26 19:40:47.294079 Epoch 150, Training Loss 0.238243141290172\n",
      "2022-03-26 19:40:47.317951 Epoch 150, Training Loss 0.23901034750596947\n",
      "2022-03-26 19:40:47.342891 Epoch 150, Training Loss 0.23978756714964766\n",
      "2022-03-26 19:40:47.379349 Epoch 150, Training Loss 0.2406171808004989\n",
      "2022-03-26 19:40:47.406383 Epoch 150, Training Loss 0.2415745458029725\n",
      "2022-03-26 19:40:47.430990 Epoch 150, Training Loss 0.24239883489925843\n",
      "2022-03-26 19:40:47.455026 Epoch 150, Training Loss 0.24334917935873845\n",
      "2022-03-26 19:40:47.477343 Epoch 150, Training Loss 0.24421493979671116\n",
      "2022-03-26 19:40:47.508128 Epoch 150, Training Loss 0.24499152086275008\n",
      "2022-03-26 19:40:47.532899 Epoch 150, Training Loss 0.24582731906715258\n",
      "2022-03-26 19:40:47.558213 Epoch 150, Training Loss 0.24657619273875986\n",
      "2022-03-26 19:40:47.584974 Epoch 150, Training Loss 0.24748581480187223\n",
      "2022-03-26 19:40:47.610455 Epoch 150, Training Loss 0.24824116335195653\n",
      "2022-03-26 19:40:47.635896 Epoch 150, Training Loss 0.24924040976387765\n",
      "2022-03-26 19:40:47.660591 Epoch 150, Training Loss 0.25017535351121517\n",
      "2022-03-26 19:40:47.684140 Epoch 150, Training Loss 0.2510240681640937\n",
      "2022-03-26 19:40:47.707235 Epoch 150, Training Loss 0.2519683711364141\n",
      "2022-03-26 19:40:47.740156 Epoch 150, Training Loss 0.25268214186439125\n",
      "2022-03-26 19:40:47.763544 Epoch 150, Training Loss 0.25360201615506733\n",
      "2022-03-26 19:40:47.786391 Epoch 150, Training Loss 0.25446142778372216\n",
      "2022-03-26 19:40:47.808242 Epoch 150, Training Loss 0.2553046687179819\n",
      "2022-03-26 19:40:47.835217 Epoch 150, Training Loss 0.25601701579435404\n",
      "2022-03-26 19:40:47.858837 Epoch 150, Training Loss 0.25683122057744\n",
      "2022-03-26 19:40:47.880931 Epoch 150, Training Loss 0.2578675447370085\n",
      "2022-03-26 19:40:47.903593 Epoch 150, Training Loss 0.25866654469533956\n",
      "2022-03-26 19:40:47.927118 Epoch 150, Training Loss 0.259171191917356\n",
      "2022-03-26 19:40:47.956210 Epoch 150, Training Loss 0.259996953065438\n",
      "2022-03-26 19:40:47.980980 Epoch 150, Training Loss 0.26094275270886436\n",
      "2022-03-26 19:40:48.010094 Epoch 150, Training Loss 0.2616945864904262\n",
      "2022-03-26 19:40:48.034032 Epoch 150, Training Loss 0.26282188494492065\n",
      "2022-03-26 19:40:48.058178 Epoch 150, Training Loss 0.2637614238902431\n",
      "2022-03-26 19:40:48.085262 Epoch 150, Training Loss 0.2644732188233329\n",
      "2022-03-26 19:40:48.108353 Epoch 150, Training Loss 0.26521915913847705\n",
      "2022-03-26 19:40:48.131710 Epoch 150, Training Loss 0.2660865776831537\n",
      "2022-03-26 19:40:48.154537 Epoch 150, Training Loss 0.2668368537407702\n",
      "2022-03-26 19:40:48.184738 Epoch 150, Training Loss 0.2677738908916483\n",
      "2022-03-26 19:40:48.208055 Epoch 150, Training Loss 0.26857278109206567\n",
      "2022-03-26 19:40:48.231061 Epoch 150, Training Loss 0.26945277225330966\n",
      "2022-03-26 19:40:48.255036 Epoch 150, Training Loss 0.2702215323057931\n",
      "2022-03-26 19:40:48.280926 Epoch 150, Training Loss 0.2708640177079174\n",
      "2022-03-26 19:40:48.304324 Epoch 150, Training Loss 0.2717172478318519\n",
      "2022-03-26 19:40:48.327013 Epoch 150, Training Loss 0.27268642392914616\n",
      "2022-03-26 19:40:48.350815 Epoch 150, Training Loss 0.27333850964255957\n",
      "2022-03-26 19:40:48.372954 Epoch 150, Training Loss 0.27411623211467967\n",
      "2022-03-26 19:40:48.402697 Epoch 150, Training Loss 0.27530186842469606\n",
      "2022-03-26 19:40:48.438048 Epoch 150, Training Loss 0.2759512102664889\n",
      "2022-03-26 19:40:48.463606 Epoch 150, Training Loss 0.27657608508758835\n",
      "2022-03-26 19:40:48.487525 Epoch 150, Training Loss 0.2773503628380768\n",
      "2022-03-26 19:40:48.512559 Epoch 150, Training Loss 0.27806959813817994\n",
      "2022-03-26 19:40:48.537624 Epoch 150, Training Loss 0.2786774985930499\n",
      "2022-03-26 19:40:48.561699 Epoch 150, Training Loss 0.2795553765333522\n",
      "2022-03-26 19:40:48.585813 Epoch 150, Training Loss 0.2801866425425195\n",
      "2022-03-26 19:40:48.613362 Epoch 150, Training Loss 0.2810961046944494\n",
      "2022-03-26 19:40:48.641125 Epoch 150, Training Loss 0.2818747810695482\n",
      "2022-03-26 19:40:48.665255 Epoch 150, Training Loss 0.2825726178448523\n",
      "2022-03-26 19:40:48.688774 Epoch 150, Training Loss 0.28343317049848454\n",
      "2022-03-26 19:40:48.711699 Epoch 150, Training Loss 0.28414022129819827\n",
      "2022-03-26 19:40:48.736278 Epoch 150, Training Loss 0.2850339561319717\n",
      "2022-03-26 19:40:48.762433 Epoch 150, Training Loss 0.28572237011416796\n",
      "2022-03-26 19:40:48.785651 Epoch 150, Training Loss 0.28693797582250724\n",
      "2022-03-26 19:40:48.808709 Epoch 150, Training Loss 0.2875537211480348\n",
      "2022-03-26 19:40:48.840431 Epoch 150, Training Loss 0.28832360332274376\n",
      "2022-03-26 19:40:48.865666 Epoch 150, Training Loss 0.2890351188304784\n",
      "2022-03-26 19:40:48.889860 Epoch 150, Training Loss 0.2898960467189779\n",
      "2022-03-26 19:40:48.922116 Epoch 150, Training Loss 0.2906044912917535\n",
      "2022-03-26 19:40:48.949374 Epoch 150, Training Loss 0.29119887517388826\n",
      "2022-03-26 19:40:48.973480 Epoch 150, Training Loss 0.29205661139372363\n",
      "2022-03-26 19:40:48.996394 Epoch 150, Training Loss 0.2927598239439528\n",
      "2022-03-26 19:40:49.022329 Epoch 150, Training Loss 0.2939299238688501\n",
      "2022-03-26 19:40:49.046948 Epoch 150, Training Loss 0.2948997483000426\n",
      "2022-03-26 19:40:49.078596 Epoch 150, Training Loss 0.29575852020774657\n",
      "2022-03-26 19:40:49.101527 Epoch 150, Training Loss 0.2965293787705624\n",
      "2022-03-26 19:40:49.124388 Epoch 150, Training Loss 0.297156812627907\n",
      "2022-03-26 19:40:49.149380 Epoch 150, Training Loss 0.2980430906309801\n",
      "2022-03-26 19:40:49.174338 Epoch 150, Training Loss 0.29894375736298767\n",
      "2022-03-26 19:40:49.196197 Epoch 150, Training Loss 0.2998115785057892\n",
      "2022-03-26 19:40:49.219385 Epoch 150, Training Loss 0.30037483390030045\n",
      "2022-03-26 19:40:49.242452 Epoch 150, Training Loss 0.30106803851054453\n",
      "2022-03-26 19:40:49.274562 Epoch 150, Training Loss 0.30169689922076665\n",
      "2022-03-26 19:40:49.298134 Epoch 150, Training Loss 0.30260871842389214\n",
      "2022-03-26 19:40:49.321436 Epoch 150, Training Loss 0.30324005272687243\n",
      "2022-03-26 19:40:49.354106 Epoch 150, Training Loss 0.30377053593278236\n",
      "2022-03-26 19:40:49.378613 Epoch 150, Training Loss 0.3045048775041805\n",
      "2022-03-26 19:40:49.401385 Epoch 150, Training Loss 0.3055643258268571\n",
      "2022-03-26 19:40:49.423616 Epoch 150, Training Loss 0.3064274432332924\n",
      "2022-03-26 19:40:49.458354 Epoch 150, Training Loss 0.3073168634377477\n",
      "2022-03-26 19:40:49.484420 Epoch 150, Training Loss 0.30826747558458384\n",
      "2022-03-26 19:40:49.510688 Epoch 150, Training Loss 0.30885402534319006\n",
      "2022-03-26 19:40:49.532347 Epoch 150, Training Loss 0.30972481536133517\n",
      "2022-03-26 19:40:49.554414 Epoch 150, Training Loss 0.31047857425097003\n",
      "2022-03-26 19:40:49.580125 Epoch 150, Training Loss 0.31127965252112855\n",
      "2022-03-26 19:40:49.602899 Epoch 150, Training Loss 0.3122215124652209\n",
      "2022-03-26 19:40:49.625265 Epoch 150, Training Loss 0.31288026345660314\n",
      "2022-03-26 19:40:49.647566 Epoch 150, Training Loss 0.31343539215414723\n",
      "2022-03-26 19:40:49.670843 Epoch 150, Training Loss 0.3142191067985866\n",
      "2022-03-26 19:40:49.702460 Epoch 150, Training Loss 0.3151998054950743\n",
      "2022-03-26 19:40:49.728720 Epoch 150, Training Loss 0.31597696065597825\n",
      "2022-03-26 19:40:49.751908 Epoch 150, Training Loss 0.3166442159039285\n",
      "2022-03-26 19:40:49.774568 Epoch 150, Training Loss 0.3177974142534349\n",
      "2022-03-26 19:40:49.801490 Epoch 150, Training Loss 0.3185927470779175\n",
      "2022-03-26 19:40:49.823580 Epoch 150, Training Loss 0.3191980282059106\n",
      "2022-03-26 19:40:49.846054 Epoch 150, Training Loss 0.32019377395015236\n",
      "2022-03-26 19:40:49.869570 Epoch 150, Training Loss 0.32094122633299865\n",
      "2022-03-26 19:40:49.891459 Epoch 150, Training Loss 0.32191927483319627\n",
      "2022-03-26 19:40:49.919045 Epoch 150, Training Loss 0.3226932621063174\n",
      "2022-03-26 19:40:49.943492 Epoch 150, Training Loss 0.32346439514013814\n",
      "2022-03-26 19:40:49.966583 Epoch 150, Training Loss 0.32422805747107775\n",
      "2022-03-26 19:40:49.989630 Epoch 150, Training Loss 0.3253676664951207\n",
      "2022-03-26 19:40:50.013562 Epoch 150, Training Loss 0.32608939200411063\n",
      "2022-03-26 19:40:50.039491 Epoch 150, Training Loss 0.32701538880462844\n",
      "2022-03-26 19:40:50.062403 Epoch 150, Training Loss 0.3277848871315227\n",
      "2022-03-26 19:40:50.085624 Epoch 150, Training Loss 0.32849151681146355\n",
      "2022-03-26 19:40:50.111618 Epoch 150, Training Loss 0.3291175556762139\n",
      "2022-03-26 19:40:50.142178 Epoch 150, Training Loss 0.32986282456256544\n",
      "2022-03-26 19:40:50.168874 Epoch 150, Training Loss 0.3307888048993962\n",
      "2022-03-26 19:40:50.191568 Epoch 150, Training Loss 0.3314573130644191\n",
      "2022-03-26 19:40:50.214720 Epoch 150, Training Loss 0.33222099544142214\n",
      "2022-03-26 19:40:50.238867 Epoch 150, Training Loss 0.33309037293619514\n",
      "2022-03-26 19:40:50.266887 Epoch 150, Training Loss 0.3338170926589185\n",
      "2022-03-26 19:40:50.289943 Epoch 150, Training Loss 0.3348428061246262\n",
      "2022-03-26 19:40:50.313449 Epoch 150, Training Loss 0.33566971302337356\n",
      "2022-03-26 19:40:50.339201 Epoch 150, Training Loss 0.33679345654099796\n",
      "2022-03-26 19:40:50.373527 Epoch 150, Training Loss 0.33740646622674847\n",
      "2022-03-26 19:40:50.396927 Epoch 150, Training Loss 0.3380236435500557\n",
      "2022-03-26 19:40:50.419749 Epoch 150, Training Loss 0.33894292202294635\n",
      "2022-03-26 19:40:50.445284 Epoch 150, Training Loss 0.33959360199663646\n",
      "2022-03-26 19:40:50.482127 Epoch 150, Training Loss 0.3402815920388912\n",
      "2022-03-26 19:40:50.505533 Epoch 150, Training Loss 0.3411439905309921\n",
      "2022-03-26 19:40:50.528519 Epoch 150, Training Loss 0.3419259229813085\n",
      "2022-03-26 19:40:50.551341 Epoch 150, Training Loss 0.34280323711655025\n",
      "2022-03-26 19:40:50.574047 Epoch 150, Training Loss 0.3436272559339738\n",
      "2022-03-26 19:40:50.605190 Epoch 150, Training Loss 0.3442284941978162\n",
      "2022-03-26 19:40:50.629533 Epoch 150, Training Loss 0.34500450315073017\n",
      "2022-03-26 19:40:50.652398 Epoch 150, Training Loss 0.3457977253457774\n",
      "2022-03-26 19:40:50.676095 Epoch 150, Training Loss 0.346611114337926\n",
      "2022-03-26 19:40:50.701650 Epoch 150, Training Loss 0.3472754590956451\n",
      "2022-03-26 19:40:50.725492 Epoch 150, Training Loss 0.34807747236603054\n",
      "2022-03-26 19:40:50.748457 Epoch 150, Training Loss 0.3489864832909821\n",
      "2022-03-26 19:40:50.771837 Epoch 150, Training Loss 0.34979495726278065\n",
      "2022-03-26 19:40:50.794419 Epoch 150, Training Loss 0.35074380154499923\n",
      "2022-03-26 19:40:50.824555 Epoch 150, Training Loss 0.3515782837215287\n",
      "2022-03-26 19:40:50.851287 Epoch 150, Training Loss 0.35239177972764313\n",
      "2022-03-26 19:40:50.874274 Epoch 150, Training Loss 0.35326785634240837\n",
      "2022-03-26 19:40:50.897540 Epoch 150, Training Loss 0.35402516132730355\n",
      "2022-03-26 19:40:50.923671 Epoch 150, Training Loss 0.35515809348782007\n",
      "2022-03-26 19:40:50.947370 Epoch 150, Training Loss 0.3562294129672868\n",
      "2022-03-26 19:40:50.971096 Epoch 150, Training Loss 0.35712831213955987\n",
      "2022-03-26 19:40:50.994956 Epoch 150, Training Loss 0.3579527824888449\n",
      "2022-03-26 19:40:51.025383 Epoch 150, Training Loss 0.3588594929184145\n",
      "2022-03-26 19:40:51.056487 Epoch 150, Training Loss 0.3599789442156282\n",
      "2022-03-26 19:40:51.079988 Epoch 150, Training Loss 0.36117780582069436\n",
      "2022-03-26 19:40:51.104364 Epoch 150, Training Loss 0.36181122152244344\n",
      "2022-03-26 19:40:51.132278 Epoch 150, Training Loss 0.36258646735297445\n",
      "2022-03-26 19:40:51.155333 Epoch 150, Training Loss 0.36362903612806363\n",
      "2022-03-26 19:40:51.178235 Epoch 150, Training Loss 0.3646198890322005\n",
      "2022-03-26 19:40:51.201637 Epoch 150, Training Loss 0.36522913718467476\n",
      "2022-03-26 19:40:51.225793 Epoch 150, Training Loss 0.366044363631007\n",
      "2022-03-26 19:40:51.248630 Epoch 150, Training Loss 0.3668377720334036\n",
      "2022-03-26 19:40:51.280715 Epoch 150, Training Loss 0.36759486558187343\n",
      "2022-03-26 19:40:51.305338 Epoch 150, Training Loss 0.36850398443544\n",
      "2022-03-26 19:40:51.330773 Epoch 150, Training Loss 0.3695899873133511\n",
      "2022-03-26 19:40:51.358595 Epoch 150, Training Loss 0.37038285294762047\n",
      "2022-03-26 19:40:51.382872 Epoch 150, Training Loss 0.37121339809254306\n",
      "2022-03-26 19:40:51.406340 Epoch 150, Training Loss 0.37175074040584855\n",
      "2022-03-26 19:40:51.429364 Epoch 150, Training Loss 0.37238132103782173\n",
      "2022-03-26 19:40:51.452877 Epoch 150, Training Loss 0.3731711469114284\n",
      "2022-03-26 19:40:51.476905 Epoch 150, Training Loss 0.37381585506374576\n",
      "2022-03-26 19:40:51.516452 Epoch 150, Training Loss 0.37464564455592114\n",
      "2022-03-26 19:40:51.543383 Epoch 150, Training Loss 0.3755715881543391\n",
      "2022-03-26 19:40:51.567426 Epoch 150, Training Loss 0.37624663476596404\n",
      "2022-03-26 19:40:51.591873 Epoch 150, Training Loss 0.37691825616847524\n",
      "2022-03-26 19:40:51.617958 Epoch 150, Training Loss 0.37747056751757324\n",
      "2022-03-26 19:40:51.641960 Epoch 150, Training Loss 0.37830363092062724\n",
      "2022-03-26 19:40:51.665987 Epoch 150, Training Loss 0.3792615298496183\n",
      "2022-03-26 19:40:51.689277 Epoch 150, Training Loss 0.379828875593822\n",
      "2022-03-26 19:40:51.713320 Epoch 150, Training Loss 0.3806450652420673\n",
      "2022-03-26 19:40:51.745811 Epoch 150, Training Loss 0.3814231388632904\n",
      "2022-03-26 19:40:51.770305 Epoch 150, Training Loss 0.3822004748960895\n",
      "2022-03-26 19:40:51.794310 Epoch 150, Training Loss 0.38308570303423023\n",
      "2022-03-26 19:40:51.821186 Epoch 150, Training Loss 0.3842385748539434\n",
      "2022-03-26 19:40:51.845461 Epoch 150, Training Loss 0.38521120459069985\n",
      "2022-03-26 19:40:51.869383 Epoch 150, Training Loss 0.38633030778764155\n",
      "2022-03-26 19:40:51.892853 Epoch 150, Training Loss 0.38714660216322944\n",
      "2022-03-26 19:40:51.922139 Epoch 150, Training Loss 0.3881846313814983\n",
      "2022-03-26 19:40:51.947039 Epoch 150, Training Loss 0.38900769240868366\n",
      "2022-03-26 19:40:51.977838 Epoch 150, Training Loss 0.38966321293502815\n",
      "2022-03-26 19:40:52.002469 Epoch 150, Training Loss 0.3906755848690067\n",
      "2022-03-26 19:40:52.026388 Epoch 150, Training Loss 0.3914870761163399\n",
      "2022-03-26 19:40:52.049561 Epoch 150, Training Loss 0.39238400452429684\n",
      "2022-03-26 19:40:52.079392 Epoch 150, Training Loss 0.3934061298208773\n",
      "2022-03-26 19:40:52.103227 Epoch 150, Training Loss 0.3940695167121375\n",
      "2022-03-26 19:40:52.129503 Epoch 150, Training Loss 0.39506081360227924\n",
      "2022-03-26 19:40:52.152913 Epoch 150, Training Loss 0.39587287780116587\n",
      "2022-03-26 19:40:52.177134 Epoch 150, Training Loss 0.3967992093633203\n",
      "2022-03-26 19:40:52.209463 Epoch 150, Training Loss 0.3975752007854564\n",
      "2022-03-26 19:40:52.233843 Epoch 150, Training Loss 0.39815939555082785\n",
      "2022-03-26 19:40:52.257618 Epoch 150, Training Loss 0.39898435538991944\n",
      "2022-03-26 19:40:52.282326 Epoch 150, Training Loss 0.3997480525537525\n",
      "2022-03-26 19:40:52.310534 Epoch 150, Training Loss 0.4007142475803795\n",
      "2022-03-26 19:40:52.337699 Epoch 150, Training Loss 0.4017722357417007\n",
      "2022-03-26 19:40:52.361812 Epoch 150, Training Loss 0.4024684298068971\n",
      "2022-03-26 19:40:52.385499 Epoch 150, Training Loss 0.4030265508176726\n",
      "2022-03-26 19:40:52.409917 Epoch 150, Training Loss 0.4037856291550809\n",
      "2022-03-26 19:40:52.442942 Epoch 150, Training Loss 0.40451985883438374\n",
      "2022-03-26 19:40:52.469781 Epoch 150, Training Loss 0.4052648525637434\n",
      "2022-03-26 19:40:52.493430 Epoch 150, Training Loss 0.4060922676263868\n",
      "2022-03-26 19:40:52.521871 Epoch 150, Training Loss 0.4068040138544024\n",
      "2022-03-26 19:40:52.557338 Epoch 150, Training Loss 0.4075811382221139\n",
      "2022-03-26 19:40:52.581114 Epoch 150, Training Loss 0.4086165282961048\n",
      "2022-03-26 19:40:52.604403 Epoch 150, Training Loss 0.4092973818635697\n",
      "2022-03-26 19:40:52.628323 Epoch 150, Training Loss 0.41025080991065715\n",
      "2022-03-26 19:40:52.659839 Epoch 150, Training Loss 0.4111583367409304\n",
      "2022-03-26 19:40:52.685377 Epoch 150, Training Loss 0.41205270230160346\n",
      "2022-03-26 19:40:52.709351 Epoch 150, Training Loss 0.4129426140538262\n",
      "2022-03-26 19:40:52.733305 Epoch 150, Training Loss 0.41370352168979546\n",
      "2022-03-26 19:40:52.759749 Epoch 150, Training Loss 0.41463667180989405\n",
      "2022-03-26 19:40:52.783315 Epoch 150, Training Loss 0.41572863759134737\n",
      "2022-03-26 19:40:52.806544 Epoch 150, Training Loss 0.41695566292461533\n",
      "2022-03-26 19:40:52.830577 Epoch 150, Training Loss 0.41810670498844305\n",
      "2022-03-26 19:40:52.855814 Epoch 150, Training Loss 0.41920579291518084\n",
      "2022-03-26 19:40:52.886894 Epoch 150, Training Loss 0.4198454153126158\n",
      "2022-03-26 19:40:52.911625 Epoch 150, Training Loss 0.4205136217958177\n",
      "2022-03-26 19:40:52.935169 Epoch 150, Training Loss 0.42129082528069195\n",
      "2022-03-26 19:40:52.962393 Epoch 150, Training Loss 0.4220689758848961\n",
      "2022-03-26 19:40:52.985880 Epoch 150, Training Loss 0.4226492208897915\n",
      "2022-03-26 19:40:53.010035 Epoch 150, Training Loss 0.423323042404926\n",
      "2022-03-26 19:40:53.034252 Epoch 150, Training Loss 0.42434063767228286\n",
      "2022-03-26 19:40:53.057525 Epoch 150, Training Loss 0.4251320547307544\n",
      "2022-03-26 19:40:53.081368 Epoch 150, Training Loss 0.42568039749284536\n",
      "2022-03-26 19:40:53.113241 Epoch 150, Training Loss 0.42667430593534506\n",
      "2022-03-26 19:40:53.144592 Epoch 150, Training Loss 0.42725732903498825\n",
      "2022-03-26 19:40:53.168784 Epoch 150, Training Loss 0.4279610689567483\n",
      "2022-03-26 19:40:53.196334 Epoch 150, Training Loss 0.42866514409746964\n",
      "2022-03-26 19:40:53.221636 Epoch 150, Training Loss 0.42977722621787234\n",
      "2022-03-26 19:40:53.245411 Epoch 150, Training Loss 0.4305265873975461\n",
      "2022-03-26 19:40:53.270388 Epoch 150, Training Loss 0.43125513824812894\n",
      "2022-03-26 19:40:53.293730 Epoch 150, Training Loss 0.4320724518478984\n",
      "2022-03-26 19:40:53.319855 Epoch 150, Training Loss 0.43274966434901935\n",
      "2022-03-26 19:40:53.350398 Epoch 150, Training Loss 0.4335818465637124\n",
      "2022-03-26 19:40:53.375271 Epoch 150, Training Loss 0.4344097924278215\n",
      "2022-03-26 19:40:53.403354 Epoch 150, Training Loss 0.4352340428039546\n",
      "2022-03-26 19:40:53.427389 Epoch 150, Training Loss 0.4358780154258089\n",
      "2022-03-26 19:40:53.451599 Epoch 150, Training Loss 0.43684841562872345\n",
      "2022-03-26 19:40:53.477359 Epoch 150, Training Loss 0.43734190653047295\n",
      "2022-03-26 19:40:53.501205 Epoch 150, Training Loss 0.4382711157317052\n",
      "2022-03-26 19:40:53.524945 Epoch 150, Training Loss 0.4395100145846072\n",
      "2022-03-26 19:40:53.559336 Epoch 150, Training Loss 0.4401619993054958\n",
      "2022-03-26 19:40:53.588347 Epoch 150, Training Loss 0.440726377355778\n",
      "2022-03-26 19:40:53.616987 Epoch 150, Training Loss 0.44121971512999375\n",
      "2022-03-26 19:40:53.640404 Epoch 150, Training Loss 0.44210884173203\n",
      "2022-03-26 19:40:53.663590 Epoch 150, Training Loss 0.44301900930721744\n",
      "2022-03-26 19:40:53.686841 Epoch 150, Training Loss 0.44380198483881744\n",
      "2022-03-26 19:40:53.709765 Epoch 150, Training Loss 0.4449168766093681\n",
      "2022-03-26 19:40:53.733024 Epoch 150, Training Loss 0.44551312172656776\n",
      "2022-03-26 19:40:53.767556 Epoch 150, Training Loss 0.4463089146196385\n",
      "2022-03-26 19:40:53.791981 Epoch 150, Training Loss 0.4469158502338487\n",
      "2022-03-26 19:40:53.815305 Epoch 150, Training Loss 0.44770553288862225\n",
      "2022-03-26 19:40:53.843064 Epoch 150, Training Loss 0.4484847263454476\n",
      "2022-03-26 19:40:53.866791 Epoch 150, Training Loss 0.44930721442107957\n",
      "2022-03-26 19:40:53.889842 Epoch 150, Training Loss 0.4500803141795156\n",
      "2022-03-26 19:40:53.912781 Epoch 150, Training Loss 0.45106269567823776\n",
      "2022-03-26 19:40:53.935557 Epoch 150, Training Loss 0.45157496253852647\n",
      "2022-03-26 19:40:53.958297 Epoch 150, Training Loss 0.452221266144072\n",
      "2022-03-26 19:40:53.988842 Epoch 150, Training Loss 0.4527705708123229\n",
      "2022-03-26 19:40:54.012970 Epoch 150, Training Loss 0.45365654759089963\n",
      "2022-03-26 19:40:54.039894 Epoch 150, Training Loss 0.45468436787500405\n",
      "2022-03-26 19:40:54.068224 Epoch 150, Training Loss 0.455402002005321\n",
      "2022-03-26 19:40:54.092222 Epoch 150, Training Loss 0.45615377069434243\n",
      "2022-03-26 19:40:54.115567 Epoch 150, Training Loss 0.45705481083191873\n",
      "2022-03-26 19:40:54.142371 Epoch 150, Training Loss 0.45769910602008596\n",
      "2022-03-26 19:40:54.165940 Epoch 150, Training Loss 0.45843797876401937\n",
      "2022-03-26 19:40:54.188954 Epoch 150, Training Loss 0.45925490744888325\n",
      "2022-03-26 19:40:54.221564 Epoch 150, Training Loss 0.45988419865403335\n",
      "2022-03-26 19:40:54.245595 Epoch 150, Training Loss 0.46051293367620016\n",
      "2022-03-26 19:40:54.269268 Epoch 150, Training Loss 0.4613501165833924\n",
      "2022-03-26 19:40:54.294445 Epoch 150, Training Loss 0.46206743218709745\n",
      "2022-03-26 19:40:54.318643 Epoch 150, Training Loss 0.4628445086881633\n",
      "2022-03-26 19:40:54.344503 Epoch 150, Training Loss 0.46360281719576063\n",
      "2022-03-26 19:40:54.367694 Epoch 150, Training Loss 0.46437962578080805\n",
      "2022-03-26 19:40:54.391583 Epoch 150, Training Loss 0.46513487928358793\n",
      "2022-03-26 19:40:54.415262 Epoch 150, Training Loss 0.4658590403511701\n",
      "2022-03-26 19:40:54.446227 Epoch 150, Training Loss 0.46667981002946646\n",
      "2022-03-26 19:40:54.472107 Epoch 150, Training Loss 0.46765672032485534\n",
      "2022-03-26 19:40:54.496284 Epoch 150, Training Loss 0.4685101611992282\n",
      "2022-03-26 19:40:54.523619 Epoch 150, Training Loss 0.4693335837415417\n",
      "2022-03-26 19:40:54.547328 Epoch 150, Training Loss 0.47005043302655525\n",
      "2022-03-26 19:40:54.581755 Epoch 150, Training Loss 0.47098553394112747\n",
      "2022-03-26 19:40:54.614022 Epoch 150, Training Loss 0.4720524488507634\n",
      "2022-03-26 19:40:54.640484 Epoch 150, Training Loss 0.4729520716630589\n",
      "2022-03-26 19:40:54.675311 Epoch 150, Training Loss 0.4736403803843671\n",
      "2022-03-26 19:40:54.699469 Epoch 150, Training Loss 0.47448126632539206\n",
      "2022-03-26 19:40:54.722909 Epoch 150, Training Loss 0.4755841332780736\n",
      "2022-03-26 19:40:54.750187 Epoch 150, Training Loss 0.47628605914542743\n",
      "2022-03-26 19:40:54.774317 Epoch 150, Training Loss 0.4768253535489597\n",
      "2022-03-26 19:40:54.798156 Epoch 150, Training Loss 0.4776137472342347\n",
      "2022-03-26 19:40:54.821941 Epoch 150, Training Loss 0.478473920155974\n",
      "2022-03-26 19:40:54.846739 Epoch 150, Training Loss 0.4792878641877943\n",
      "2022-03-26 19:40:54.871326 Epoch 150, Training Loss 0.48016449172630943\n",
      "2022-03-26 19:40:54.904995 Epoch 150, Training Loss 0.48105837496192866\n",
      "2022-03-26 19:40:54.932896 Epoch 150, Training Loss 0.4820196801972816\n",
      "2022-03-26 19:40:54.957471 Epoch 150, Training Loss 0.48282531585992144\n",
      "2022-03-26 19:40:54.982939 Epoch 150, Training Loss 0.48356692523450195\n",
      "2022-03-26 19:40:55.011386 Epoch 150, Training Loss 0.48445524907935306\n",
      "2022-03-26 19:40:55.035859 Epoch 150, Training Loss 0.48529477790950815\n",
      "2022-03-26 19:40:55.059072 Epoch 150, Training Loss 0.485847042230389\n",
      "2022-03-26 19:40:55.083394 Epoch 150, Training Loss 0.4867136797408009\n",
      "2022-03-26 19:40:55.107356 Epoch 150, Training Loss 0.48771414446556355\n",
      "2022-03-26 19:40:55.139806 Epoch 150, Training Loss 0.48823423233940777\n",
      "2022-03-26 19:40:55.163889 Epoch 150, Training Loss 0.48908416484780326\n",
      "2022-03-26 19:40:55.187592 Epoch 150, Training Loss 0.48974190126447115\n",
      "2022-03-26 19:40:55.216489 Epoch 150, Training Loss 0.49032431989527114\n",
      "2022-03-26 19:40:55.240695 Epoch 150, Training Loss 0.4909216979961566\n",
      "2022-03-26 19:40:55.264374 Epoch 150, Training Loss 0.49195238037030103\n",
      "2022-03-26 19:40:55.288791 Epoch 150, Training Loss 0.4928349410862569\n",
      "2022-03-26 19:40:55.312459 Epoch 150, Training Loss 0.4935693564393636\n",
      "2022-03-26 19:40:55.348221 Epoch 150, Training Loss 0.4943517198419327\n",
      "2022-03-26 19:40:55.372353 Epoch 150, Training Loss 0.4951218481335189\n",
      "2022-03-26 19:40:55.396817 Epoch 150, Training Loss 0.49576455823448307\n",
      "2022-03-26 19:40:55.423959 Epoch 150, Training Loss 0.4967071774899197\n",
      "2022-03-26 19:40:55.447306 Epoch 150, Training Loss 0.49739275758376206\n",
      "2022-03-26 19:40:55.472486 Epoch 150, Training Loss 0.49814007391252785\n",
      "2022-03-26 19:40:55.496194 Epoch 150, Training Loss 0.49889690998722525\n",
      "2022-03-26 19:40:55.519200 Epoch 150, Training Loss 0.49969941259497574\n",
      "2022-03-26 19:40:55.543467 Epoch 150, Training Loss 0.5005901452449276\n",
      "2022-03-26 19:40:55.575328 Epoch 150, Training Loss 0.501414451033563\n",
      "2022-03-26 19:40:55.598459 Epoch 150, Training Loss 0.5020760290534295\n",
      "2022-03-26 19:40:55.634283 Epoch 150, Training Loss 0.5029736487075801\n",
      "2022-03-26 19:40:55.659425 Epoch 150, Training Loss 0.5037728109399376\n",
      "2022-03-26 19:40:55.682333 Epoch 150, Training Loss 0.5044432616675906\n",
      "2022-03-26 19:40:55.705236 Epoch 150, Training Loss 0.5052761736961887\n",
      "2022-03-26 19:40:55.728025 Epoch 150, Training Loss 0.5059643432383647\n",
      "2022-03-26 19:40:55.751068 Epoch 150, Training Loss 0.5067070011059036\n",
      "2022-03-26 19:40:55.774304 Epoch 150, Training Loss 0.507350339113599\n",
      "2022-03-26 19:40:55.804980 Epoch 150, Training Loss 0.5079748516192522\n",
      "2022-03-26 19:40:55.827851 Epoch 150, Training Loss 0.5086802466751059\n",
      "2022-03-26 19:40:55.851333 Epoch 150, Training Loss 0.5095547677001075\n",
      "2022-03-26 19:40:55.878959 Epoch 150, Training Loss 0.5103732072331412\n",
      "2022-03-26 19:40:55.901882 Epoch 150, Training Loss 0.5111529593120145\n",
      "2022-03-26 19:40:55.925027 Epoch 150, Training Loss 0.5117331191020853\n",
      "2022-03-26 19:40:55.948328 Epoch 150, Training Loss 0.5126313487510852\n",
      "2022-03-26 19:40:55.970734 Epoch 150, Training Loss 0.5135402628756545\n",
      "2022-03-26 19:40:55.994040 Epoch 150, Training Loss 0.5143538421910742\n",
      "2022-03-26 19:40:56.024621 Epoch 150, Training Loss 0.5152628482378963\n",
      "2022-03-26 19:40:56.051356 Epoch 150, Training Loss 0.5158839101529182\n",
      "2022-03-26 19:40:56.075554 Epoch 150, Training Loss 0.5165585558432753\n",
      "2022-03-26 19:40:56.100364 Epoch 150, Training Loss 0.5175368603690506\n",
      "2022-03-26 19:40:56.125359 Epoch 150, Training Loss 0.5185697363008319\n",
      "2022-03-26 19:40:56.155977 Epoch 150, Training Loss 0.5193834051756603\n",
      "2022-03-26 19:40:56.179215 Epoch 150, Training Loss 0.5201402113718145\n",
      "2022-03-26 19:40:56.206060 Epoch 150, Training Loss 0.5210520869783123\n",
      "2022-03-26 19:40:56.233746 Epoch 150, Training Loss 0.5221263515522413\n",
      "2022-03-26 19:40:56.259128 Epoch 150, Training Loss 0.5228697271145823\n",
      "2022-03-26 19:40:56.282617 Epoch 150, Training Loss 0.5240323279824708\n",
      "2022-03-26 19:40:56.307269 Epoch 150, Training Loss 0.5248016006196551\n",
      "2022-03-26 19:40:56.335736 Epoch 150, Training Loss 0.5259094877011331\n",
      "2022-03-26 19:40:56.358872 Epoch 150, Training Loss 0.5265892535219412\n",
      "2022-03-26 19:40:56.383163 Epoch 150, Training Loss 0.5272449694783486\n",
      "2022-03-26 19:40:56.406873 Epoch 150, Training Loss 0.5279522990936514\n",
      "2022-03-26 19:40:56.430321 Epoch 150, Training Loss 0.5290342672249241\n",
      "2022-03-26 19:40:56.466741 Epoch 150, Training Loss 0.529834817285123\n",
      "2022-03-26 19:40:56.491310 Epoch 150, Training Loss 0.5306668775465787\n",
      "2022-03-26 19:40:56.514758 Epoch 150, Training Loss 0.5315966548974557\n",
      "2022-03-26 19:40:56.542625 Epoch 150, Training Loss 0.5322437239882282\n",
      "2022-03-26 19:40:56.566725 Epoch 150, Training Loss 0.5333943158159475\n",
      "2022-03-26 19:40:56.591018 Epoch 150, Training Loss 0.5339894156398066\n",
      "2022-03-26 19:40:56.614136 Epoch 150, Training Loss 0.5347804818540582\n",
      "2022-03-26 19:40:56.643816 Epoch 150, Training Loss 0.5355691975721008\n",
      "2022-03-26 19:40:56.670766 Epoch 150, Training Loss 0.5363488969824198\n",
      "2022-03-26 19:40:56.700166 Epoch 150, Training Loss 0.5370836713353692\n",
      "2022-03-26 19:40:56.723417 Epoch 150, Training Loss 0.5377373507489329\n",
      "2022-03-26 19:40:56.747383 Epoch 150, Training Loss 0.5387161963659785\n",
      "2022-03-26 19:40:56.775219 Epoch 150, Training Loss 0.5392056663932703\n",
      "2022-03-26 19:40:56.798545 Epoch 150, Training Loss 0.5397921507163426\n",
      "2022-03-26 19:40:56.822359 Epoch 150, Training Loss 0.5404426662818246\n",
      "2022-03-26 19:40:56.846849 Epoch 150, Training Loss 0.5414906897203392\n",
      "2022-03-26 19:40:56.870635 Epoch 150, Training Loss 0.5422911459527662\n",
      "2022-03-26 19:40:56.904760 Epoch 150, Training Loss 0.5431146704022537\n",
      "2022-03-26 19:40:56.929764 Epoch 150, Training Loss 0.543866137104571\n",
      "2022-03-26 19:40:56.953298 Epoch 150, Training Loss 0.5445976256562011\n",
      "2022-03-26 19:40:56.981288 Epoch 150, Training Loss 0.5454404334278058\n",
      "2022-03-26 19:40:57.005093 Epoch 150, Training Loss 0.5461052950385892\n",
      "2022-03-26 19:40:57.029622 Epoch 150, Training Loss 0.5468616433003369\n",
      "2022-03-26 19:40:57.058864 Epoch 150, Training Loss 0.5473971520848286\n",
      "2022-03-26 19:40:57.083213 Epoch 150, Training Loss 0.5483802823002076\n",
      "2022-03-26 19:40:57.107129 Epoch 150, Training Loss 0.5491364136376344\n",
      "2022-03-26 19:40:57.139300 Epoch 150, Training Loss 0.550318627482485\n",
      "2022-03-26 19:40:57.163244 Epoch 150, Training Loss 0.5510863664052675\n",
      "2022-03-26 19:40:57.187501 Epoch 150, Training Loss 0.551869542931047\n",
      "2022-03-26 19:40:57.215152 Epoch 150, Training Loss 0.5525384050653414\n",
      "2022-03-26 19:40:57.238917 Epoch 150, Training Loss 0.5532660718311739\n",
      "2022-03-26 19:40:57.263392 Epoch 150, Training Loss 0.5539541824546921\n",
      "2022-03-26 19:40:57.287034 Epoch 150, Training Loss 0.5547537217512155\n",
      "2022-03-26 19:40:57.310291 Epoch 150, Training Loss 0.5554382291901142\n",
      "2022-03-26 19:40:57.342554 Epoch 150, Training Loss 0.5562690704527413\n",
      "2022-03-26 19:40:57.366176 Epoch 150, Training Loss 0.5570166410540071\n",
      "2022-03-26 19:40:57.389385 Epoch 150, Training Loss 0.5581079155892668\n",
      "2022-03-26 19:40:57.418579 Epoch 150, Training Loss 0.5587933946905843\n",
      "2022-03-26 19:40:57.441526 Epoch 150, Training Loss 0.5596531478645247\n",
      "2022-03-26 19:40:57.465316 Epoch 150, Training Loss 0.560308572581357\n",
      "2022-03-26 19:40:57.488524 Epoch 150, Training Loss 0.5610800877861355\n",
      "2022-03-26 19:40:57.511565 Epoch 150, Training Loss 0.5619880095162355\n",
      "2022-03-26 19:40:57.534356 Epoch 150, Training Loss 0.562715196014975\n",
      "2022-03-26 19:40:57.569069 Epoch 150, Training Loss 0.5634898778117831\n",
      "2022-03-26 19:40:57.595862 Epoch 150, Training Loss 0.5643254793665903\n",
      "2022-03-26 19:40:57.618857 Epoch 150, Training Loss 0.5651353564103851\n",
      "2022-03-26 19:40:57.643534 Epoch 150, Training Loss 0.5662417008596308\n",
      "2022-03-26 19:40:57.676340 Epoch 150, Training Loss 0.5671715668552672\n",
      "2022-03-26 19:40:57.702512 Epoch 150, Training Loss 0.5679342833626301\n",
      "2022-03-26 19:40:57.726803 Epoch 150, Training Loss 0.5688695999057701\n",
      "2022-03-26 19:40:57.752221 Epoch 150, Training Loss 0.569722116908149\n",
      "2022-03-26 19:40:57.784109 Epoch 150, Training Loss 0.5705277272654922\n",
      "2022-03-26 19:40:57.810994 Epoch 150, Training Loss 0.5712320291630143\n",
      "2022-03-26 19:40:57.837548 Epoch 150, Training Loss 0.571902065630764\n",
      "2022-03-26 19:40:57.861808 Epoch 150, Training Loss 0.5725725681885429\n",
      "2022-03-26 19:40:57.889854 Epoch 150, Training Loss 0.5732863058367044\n",
      "2022-03-26 19:40:57.913352 Epoch 150, Training Loss 0.5740906973476605\n",
      "2022-03-26 19:40:57.937662 Epoch 150, Training Loss 0.5749031521017899\n",
      "2022-03-26 19:40:57.965829 Epoch 150, Training Loss 0.5756540625449031\n",
      "2022-03-26 19:40:57.997096 Epoch 150, Training Loss 0.5765758922795201\n",
      "2022-03-26 19:40:58.022862 Epoch 150, Training Loss 0.5773817036310425\n",
      "2022-03-26 19:40:58.046102 Epoch 150, Training Loss 0.578294658981016\n",
      "2022-03-26 19:40:58.072350 Epoch 150, Training Loss 0.5790793088543446\n",
      "2022-03-26 19:40:58.095624 Epoch 150, Training Loss 0.5797943933997922\n",
      "2022-03-26 19:40:58.119436 Epoch 150, Training Loss 0.5810360703474421\n",
      "2022-03-26 19:40:58.143109 Epoch 150, Training Loss 0.5818938929254137\n",
      "2022-03-26 19:40:58.166840 Epoch 150, Training Loss 0.5828344741135912\n",
      "2022-03-26 19:40:58.190534 Epoch 150, Training Loss 0.5839944931552233\n",
      "2022-03-26 19:40:58.222139 Epoch 150, Training Loss 0.5846841760608547\n",
      "2022-03-26 19:40:58.246930 Epoch 150, Training Loss 0.585588550476162\n",
      "2022-03-26 19:40:58.270945 Epoch 150, Training Loss 0.5864541295849149\n",
      "2022-03-26 19:40:58.300783 Epoch 150, Training Loss 0.5873062963528402\n",
      "2022-03-26 19:40:58.324198 Epoch 150, Training Loss 0.5881890034126809\n",
      "2022-03-26 19:40:58.350308 Epoch 150, Training Loss 0.5889591092953597\n",
      "2022-03-26 19:40:58.374250 Epoch 150, Training Loss 0.5895646103965048\n",
      "2022-03-26 19:40:58.398859 Epoch 150, Training Loss 0.5905324848716521\n",
      "2022-03-26 19:40:58.422386 Epoch 150, Training Loss 0.591682988435716\n",
      "2022-03-26 19:40:58.456716 Epoch 150, Training Loss 0.592520730727164\n",
      "2022-03-26 19:40:58.482652 Epoch 150, Training Loss 0.5930663377351468\n",
      "2022-03-26 19:40:58.506913 Epoch 150, Training Loss 0.5938765782758099\n",
      "2022-03-26 19:40:58.531551 Epoch 150, Training Loss 0.5944427381010007\n",
      "2022-03-26 19:40:58.554828 Epoch 150, Training Loss 0.5954239592908899\n",
      "2022-03-26 19:40:58.578086 Epoch 150, Training Loss 0.5961776687894635\n",
      "2022-03-26 19:40:58.601046 Epoch 150, Training Loss 0.5970292525446933\n",
      "2022-03-26 19:40:58.623996 Epoch 150, Training Loss 0.597711527172257\n",
      "2022-03-26 19:40:58.647202 Epoch 150, Training Loss 0.5990062167348764\n",
      "2022-03-26 19:40:58.675988 Epoch 150, Training Loss 0.5998132911408344\n",
      "2022-03-26 19:40:58.709571 Epoch 150, Training Loss 0.6005600391675139\n",
      "2022-03-26 19:40:58.732107 Epoch 150, Training Loss 0.6015293478889538\n",
      "2022-03-26 19:40:58.757630 Epoch 150, Training Loss 0.6022880221800426\n",
      "2022-03-26 19:40:58.780371 Epoch 150, Training Loss 0.6030786612530803\n",
      "2022-03-26 19:40:58.802993 Epoch 150, Training Loss 0.6040048463189084\n",
      "2022-03-26 19:40:58.825048 Epoch 150, Training Loss 0.6045939167747107\n",
      "2022-03-26 19:40:58.851205 Epoch 150, Training Loss 0.6052806878943577\n",
      "2022-03-26 19:40:58.876321 Epoch 150, Training Loss 0.6063230232052181\n",
      "2022-03-26 19:40:58.909233 Epoch 150, Training Loss 0.6069546727192067\n",
      "2022-03-26 19:40:58.933232 Epoch 150, Training Loss 0.607792031848827\n",
      "2022-03-26 19:40:58.956510 Epoch 150, Training Loss 0.6086263828494055\n",
      "2022-03-26 19:40:58.984604 Epoch 150, Training Loss 0.60957573232291\n",
      "2022-03-26 19:40:59.007793 Epoch 150, Training Loss 0.6104022082694046\n",
      "2022-03-26 19:40:59.030342 Epoch 150, Training Loss 0.611236201108569\n",
      "2022-03-26 19:40:59.053166 Epoch 150, Training Loss 0.6118926287383375\n",
      "2022-03-26 19:40:59.075732 Epoch 150, Training Loss 0.6127615720795854\n",
      "2022-03-26 19:40:59.098398 Epoch 150, Training Loss 0.613406526958546\n",
      "2022-03-26 19:40:59.129039 Epoch 150, Training Loss 0.6141993505951694\n",
      "2022-03-26 19:40:59.153242 Epoch 150, Training Loss 0.6147001910849911\n",
      "2022-03-26 19:40:59.175712 Epoch 150, Training Loss 0.6154285265356684\n",
      "2022-03-26 19:40:59.201431 Epoch 150, Training Loss 0.6160969212841805\n",
      "2022-03-26 19:40:59.225656 Epoch 150, Training Loss 0.6170687180041047\n",
      "2022-03-26 19:40:59.248286 Epoch 150, Training Loss 0.6178669014092907\n",
      "2022-03-26 19:40:59.271221 Epoch 150, Training Loss 0.6185458325364096\n",
      "2022-03-26 19:40:59.294083 Epoch 150, Training Loss 0.6192907985976285\n",
      "2022-03-26 19:40:59.317903 Epoch 150, Training Loss 0.620320289031319\n",
      "2022-03-26 19:40:59.353362 Epoch 150, Training Loss 0.6210614701975947\n",
      "2022-03-26 19:40:59.377167 Epoch 150, Training Loss 0.6219226865817213\n",
      "2022-03-26 19:40:59.400457 Epoch 150, Training Loss 0.6226263382398259\n",
      "2022-03-26 19:40:59.427225 Epoch 150, Training Loss 0.6236284763916679\n",
      "2022-03-26 19:40:59.449419 Epoch 150, Training Loss 0.6245418046136646\n",
      "2022-03-26 19:40:59.473914 Epoch 150, Training Loss 0.6254094155395732\n",
      "2022-03-26 19:40:59.497127 Epoch 150, Training Loss 0.6261675747306755\n",
      "2022-03-26 19:40:59.520457 Epoch 150, Training Loss 0.6270648414826454\n",
      "2022-03-26 19:40:59.529801 Epoch 150, Training Loss 0.6285233925980376\n",
      "2022-03-26 19:55:40.099110 Epoch 200, Training Loss 0.0010470103882157895\n",
      "2022-03-26 19:55:40.125777 Epoch 200, Training Loss 0.0019444955127013615\n",
      "2022-03-26 19:55:40.147675 Epoch 200, Training Loss 0.002727292763912464\n",
      "2022-03-26 19:55:40.170050 Epoch 200, Training Loss 0.0034852114784748047\n",
      "2022-03-26 19:55:40.193090 Epoch 200, Training Loss 0.0041449930509337984\n",
      "2022-03-26 19:55:40.216306 Epoch 200, Training Loss 0.004840711574725178\n",
      "2022-03-26 19:55:40.239131 Epoch 200, Training Loss 0.005647734455440355\n",
      "2022-03-26 19:55:40.261901 Epoch 200, Training Loss 0.006307384394623739\n",
      "2022-03-26 19:55:40.285197 Epoch 200, Training Loss 0.007153926526798922\n",
      "2022-03-26 19:55:40.315284 Epoch 200, Training Loss 0.007820626582635944\n",
      "2022-03-26 19:55:40.346078 Epoch 200, Training Loss 0.008782836025023399\n",
      "2022-03-26 19:55:40.368939 Epoch 200, Training Loss 0.009478259117097196\n",
      "2022-03-26 19:55:40.391791 Epoch 200, Training Loss 0.010099552857601429\n",
      "2022-03-26 19:55:40.418305 Epoch 200, Training Loss 0.010921377888725846\n",
      "2022-03-26 19:55:40.441286 Epoch 200, Training Loss 0.011784724338585153\n",
      "2022-03-26 19:55:40.463281 Epoch 200, Training Loss 0.012577243778101928\n",
      "2022-03-26 19:55:40.486164 Epoch 200, Training Loss 0.013194068542221929\n",
      "2022-03-26 19:55:40.509049 Epoch 200, Training Loss 0.014012631705349974\n",
      "2022-03-26 19:55:40.539182 Epoch 200, Training Loss 0.015011015831661955\n",
      "2022-03-26 19:55:40.562400 Epoch 200, Training Loss 0.015431914716730338\n",
      "2022-03-26 19:55:40.586048 Epoch 200, Training Loss 0.01640333894573514\n",
      "2022-03-26 19:55:40.612639 Epoch 200, Training Loss 0.01702027342965841\n",
      "2022-03-26 19:55:40.638241 Epoch 200, Training Loss 0.01757056252730777\n",
      "2022-03-26 19:55:40.662238 Epoch 200, Training Loss 0.018031454109169943\n",
      "2022-03-26 19:55:40.685797 Epoch 200, Training Loss 0.01885693308794895\n",
      "2022-03-26 19:55:40.708050 Epoch 200, Training Loss 0.019745848710884525\n",
      "2022-03-26 19:55:40.738146 Epoch 200, Training Loss 0.02071853279305236\n",
      "2022-03-26 19:55:40.768003 Epoch 200, Training Loss 0.021428135159375417\n",
      "2022-03-26 19:55:40.791168 Epoch 200, Training Loss 0.022036201165765143\n",
      "2022-03-26 19:55:40.815738 Epoch 200, Training Loss 0.02290800796902698\n",
      "2022-03-26 19:55:40.848614 Epoch 200, Training Loss 0.0236390436168217\n",
      "2022-03-26 19:55:40.871053 Epoch 200, Training Loss 0.024388630714867732\n",
      "2022-03-26 19:55:40.894172 Epoch 200, Training Loss 0.02491901269959062\n",
      "2022-03-26 19:55:40.917346 Epoch 200, Training Loss 0.025747230595639905\n",
      "2022-03-26 19:55:40.940368 Epoch 200, Training Loss 0.026609043178655912\n",
      "2022-03-26 19:55:40.967537 Epoch 200, Training Loss 0.027416969427977073\n",
      "2022-03-26 19:55:41.003344 Epoch 200, Training Loss 0.027978818434888444\n",
      "2022-03-26 19:55:41.027885 Epoch 200, Training Loss 0.0287282383045577\n",
      "2022-03-26 19:55:41.050588 Epoch 200, Training Loss 0.02958512169015987\n",
      "2022-03-26 19:55:41.073479 Epoch 200, Training Loss 0.030395862620200036\n",
      "2022-03-26 19:55:41.101769 Epoch 200, Training Loss 0.031264510529730324\n",
      "2022-03-26 19:55:41.124384 Epoch 200, Training Loss 0.032240229570652215\n",
      "2022-03-26 19:55:41.146956 Epoch 200, Training Loss 0.03297859849527364\n",
      "2022-03-26 19:55:41.170059 Epoch 200, Training Loss 0.033666388839102154\n",
      "2022-03-26 19:55:41.192591 Epoch 200, Training Loss 0.03422452052078589\n",
      "2022-03-26 19:55:41.221374 Epoch 200, Training Loss 0.03489027719211091\n",
      "2022-03-26 19:55:41.245442 Epoch 200, Training Loss 0.03556184036195126\n",
      "2022-03-26 19:55:41.268862 Epoch 200, Training Loss 0.03646527992947327\n",
      "2022-03-26 19:55:41.291777 Epoch 200, Training Loss 0.0371758200399711\n",
      "2022-03-26 19:55:41.317802 Epoch 200, Training Loss 0.03791997404507054\n",
      "2022-03-26 19:55:41.344393 Epoch 200, Training Loss 0.038621395559567014\n",
      "2022-03-26 19:55:41.368188 Epoch 200, Training Loss 0.039462593548438125\n",
      "2022-03-26 19:55:41.391918 Epoch 200, Training Loss 0.04064238425868247\n",
      "2022-03-26 19:55:41.414314 Epoch 200, Training Loss 0.04128078506578265\n",
      "2022-03-26 19:55:41.445325 Epoch 200, Training Loss 0.04197955760352142\n",
      "2022-03-26 19:55:41.469595 Epoch 200, Training Loss 0.04252806728910607\n",
      "2022-03-26 19:55:41.493023 Epoch 200, Training Loss 0.043588281401892755\n",
      "2022-03-26 19:55:41.519899 Epoch 200, Training Loss 0.04425249688918023\n",
      "2022-03-26 19:55:41.543096 Epoch 200, Training Loss 0.0449797519866158\n",
      "2022-03-26 19:55:41.566585 Epoch 200, Training Loss 0.045797177326038975\n",
      "2022-03-26 19:55:41.590028 Epoch 200, Training Loss 0.04681194731798928\n",
      "2022-03-26 19:55:41.613419 Epoch 200, Training Loss 0.04741737741948394\n",
      "2022-03-26 19:55:41.637176 Epoch 200, Training Loss 0.04806487105996408\n",
      "2022-03-26 19:55:41.669593 Epoch 200, Training Loss 0.04863131324500989\n",
      "2022-03-26 19:55:41.694553 Epoch 200, Training Loss 0.049428409055980574\n",
      "2022-03-26 19:55:41.717884 Epoch 200, Training Loss 0.05021990850910811\n",
      "2022-03-26 19:55:41.744883 Epoch 200, Training Loss 0.05093587401425442\n",
      "2022-03-26 19:55:41.768730 Epoch 200, Training Loss 0.05164985328226748\n",
      "2022-03-26 19:55:41.792168 Epoch 200, Training Loss 0.0525463925831763\n",
      "2022-03-26 19:55:41.816041 Epoch 200, Training Loss 0.05325985210173575\n",
      "2022-03-26 19:55:41.840007 Epoch 200, Training Loss 0.05402132049393471\n",
      "2022-03-26 19:55:41.890024 Epoch 200, Training Loss 0.054792190718528866\n",
      "2022-03-26 19:55:41.920941 Epoch 200, Training Loss 0.05535178309511345\n",
      "2022-03-26 19:55:41.945694 Epoch 200, Training Loss 0.05590129397866671\n",
      "2022-03-26 19:55:41.969583 Epoch 200, Training Loss 0.05667738616466522\n",
      "2022-03-26 19:55:41.998330 Epoch 200, Training Loss 0.057327506830320334\n",
      "2022-03-26 19:55:42.022170 Epoch 200, Training Loss 0.05809877065898817\n",
      "2022-03-26 19:55:42.046137 Epoch 200, Training Loss 0.058811647241072885\n",
      "2022-03-26 19:55:42.069622 Epoch 200, Training Loss 0.05975614038422285\n",
      "2022-03-26 19:55:42.096054 Epoch 200, Training Loss 0.060524068868068784\n",
      "2022-03-26 19:55:42.123538 Epoch 200, Training Loss 0.061119570337293094\n",
      "2022-03-26 19:55:42.146860 Epoch 200, Training Loss 0.06195159241214128\n",
      "2022-03-26 19:55:42.171621 Epoch 200, Training Loss 0.06297403364382742\n",
      "2022-03-26 19:55:42.198079 Epoch 200, Training Loss 0.06350197172378336\n",
      "2022-03-26 19:55:42.221669 Epoch 200, Training Loss 0.06429243183044521\n",
      "2022-03-26 19:55:42.244759 Epoch 200, Training Loss 0.06517893064509878\n",
      "2022-03-26 19:55:42.268187 Epoch 200, Training Loss 0.06590295100913328\n",
      "2022-03-26 19:55:42.292422 Epoch 200, Training Loss 0.06679711534696467\n",
      "2022-03-26 19:55:42.324693 Epoch 200, Training Loss 0.06775485566052634\n",
      "2022-03-26 19:55:42.351186 Epoch 200, Training Loss 0.06870463517163415\n",
      "2022-03-26 19:55:42.374555 Epoch 200, Training Loss 0.06959527163097011\n",
      "2022-03-26 19:55:42.397229 Epoch 200, Training Loss 0.07020617312634997\n",
      "2022-03-26 19:55:42.425061 Epoch 200, Training Loss 0.07095660132063014\n",
      "2022-03-26 19:55:42.447322 Epoch 200, Training Loss 0.07164425591526129\n",
      "2022-03-26 19:55:42.470595 Epoch 200, Training Loss 0.07230815417169\n",
      "2022-03-26 19:55:42.494781 Epoch 200, Training Loss 0.0730030924234244\n",
      "2022-03-26 19:55:42.517565 Epoch 200, Training Loss 0.07380805665727162\n",
      "2022-03-26 19:55:42.548079 Epoch 200, Training Loss 0.07459586976891588\n",
      "2022-03-26 19:55:42.571256 Epoch 200, Training Loss 0.07527256739871277\n",
      "2022-03-26 19:55:42.595568 Epoch 200, Training Loss 0.07614563108252748\n",
      "2022-03-26 19:55:42.624056 Epoch 200, Training Loss 0.07681116839046673\n",
      "2022-03-26 19:55:42.647573 Epoch 200, Training Loss 0.07737947844178475\n",
      "2022-03-26 19:55:42.671514 Epoch 200, Training Loss 0.07795493655345019\n",
      "2022-03-26 19:55:42.695430 Epoch 200, Training Loss 0.07855075559652674\n",
      "2022-03-26 19:55:42.719676 Epoch 200, Training Loss 0.07918728640317307\n",
      "2022-03-26 19:55:42.743302 Epoch 200, Training Loss 0.08003040073472825\n",
      "2022-03-26 19:55:42.776146 Epoch 200, Training Loss 0.0807414614331082\n",
      "2022-03-26 19:55:42.800881 Epoch 200, Training Loss 0.08155894912112399\n",
      "2022-03-26 19:55:42.824215 Epoch 200, Training Loss 0.08221781162349769\n",
      "2022-03-26 19:55:42.853557 Epoch 200, Training Loss 0.08301448387563076\n",
      "2022-03-26 19:55:42.877726 Epoch 200, Training Loss 0.08389698391985101\n",
      "2022-03-26 19:55:42.900783 Epoch 200, Training Loss 0.08472706495648455\n",
      "2022-03-26 19:55:42.934106 Epoch 200, Training Loss 0.08541123504224031\n",
      "2022-03-26 19:55:42.957096 Epoch 200, Training Loss 0.086036439556295\n",
      "2022-03-26 19:55:42.982594 Epoch 200, Training Loss 0.08671610453701994\n",
      "2022-03-26 19:55:43.011797 Epoch 200, Training Loss 0.08740349068208729\n",
      "2022-03-26 19:55:43.034783 Epoch 200, Training Loss 0.08800136555186318\n",
      "2022-03-26 19:55:43.058235 Epoch 200, Training Loss 0.08847697307843991\n",
      "2022-03-26 19:55:43.088807 Epoch 200, Training Loss 0.08912522846932912\n",
      "2022-03-26 19:55:43.111215 Epoch 200, Training Loss 0.08970582824381416\n",
      "2022-03-26 19:55:43.136870 Epoch 200, Training Loss 0.09011839421661309\n",
      "2022-03-26 19:55:43.164485 Epoch 200, Training Loss 0.09086402804802751\n",
      "2022-03-26 19:55:43.187079 Epoch 200, Training Loss 0.09148911746871441\n",
      "2022-03-26 19:55:43.216545 Epoch 200, Training Loss 0.09191285584436353\n",
      "2022-03-26 19:55:43.239759 Epoch 200, Training Loss 0.09273648349677815\n",
      "2022-03-26 19:55:43.262809 Epoch 200, Training Loss 0.09325494695350033\n",
      "2022-03-26 19:55:43.289389 Epoch 200, Training Loss 0.094322796939584\n",
      "2022-03-26 19:55:43.312176 Epoch 200, Training Loss 0.09480647853268381\n",
      "2022-03-26 19:55:43.337640 Epoch 200, Training Loss 0.09528345818562275\n",
      "2022-03-26 19:55:43.360489 Epoch 200, Training Loss 0.09605259232966186\n",
      "2022-03-26 19:55:43.385460 Epoch 200, Training Loss 0.09684355213971393\n",
      "2022-03-26 19:55:43.410665 Epoch 200, Training Loss 0.09743251375225194\n",
      "2022-03-26 19:55:43.445499 Epoch 200, Training Loss 0.09851953882695463\n",
      "2022-03-26 19:55:43.469065 Epoch 200, Training Loss 0.09912687913536111\n",
      "2022-03-26 19:55:43.492053 Epoch 200, Training Loss 0.09993591988483048\n",
      "2022-03-26 19:55:43.518053 Epoch 200, Training Loss 0.10072946487485296\n",
      "2022-03-26 19:55:43.540781 Epoch 200, Training Loss 0.10153934161376466\n",
      "2022-03-26 19:55:43.563501 Epoch 200, Training Loss 0.10223154887518919\n",
      "2022-03-26 19:55:43.586297 Epoch 200, Training Loss 0.10301272292881061\n",
      "2022-03-26 19:55:43.609394 Epoch 200, Training Loss 0.10377211605801302\n",
      "2022-03-26 19:55:43.632719 Epoch 200, Training Loss 0.10459158251352628\n",
      "2022-03-26 19:55:43.663282 Epoch 200, Training Loss 0.10530460963163839\n",
      "2022-03-26 19:55:43.687762 Epoch 200, Training Loss 0.10614969701413303\n",
      "2022-03-26 19:55:43.710399 Epoch 200, Training Loss 0.10683366054159296\n",
      "2022-03-26 19:55:43.733384 Epoch 200, Training Loss 0.10748374500238073\n",
      "2022-03-26 19:55:43.759194 Epoch 200, Training Loss 0.10820517614674385\n",
      "2022-03-26 19:55:43.781806 Epoch 200, Training Loss 0.10924453999075438\n",
      "2022-03-26 19:55:43.804278 Epoch 200, Training Loss 0.10991896555551788\n",
      "2022-03-26 19:55:43.827008 Epoch 200, Training Loss 0.11107091952467818\n",
      "2022-03-26 19:55:43.849383 Epoch 200, Training Loss 0.11192718841840545\n",
      "2022-03-26 19:55:43.880324 Epoch 200, Training Loss 0.11289088157436732\n",
      "2022-03-26 19:55:43.907996 Epoch 200, Training Loss 0.1140825226331306\n",
      "2022-03-26 19:55:43.931038 Epoch 200, Training Loss 0.11495821661961353\n",
      "2022-03-26 19:55:43.963298 Epoch 200, Training Loss 0.11546111335534878\n",
      "2022-03-26 19:55:43.994464 Epoch 200, Training Loss 0.11630290060701882\n",
      "2022-03-26 19:55:44.016498 Epoch 200, Training Loss 0.11700034659841786\n",
      "2022-03-26 19:55:44.039640 Epoch 200, Training Loss 0.11794957571932117\n",
      "2022-03-26 19:55:44.065806 Epoch 200, Training Loss 0.1186897017614311\n",
      "2022-03-26 19:55:44.096912 Epoch 200, Training Loss 0.11923884876701228\n",
      "2022-03-26 19:55:44.120594 Epoch 200, Training Loss 0.11981760495154144\n",
      "2022-03-26 19:55:44.143111 Epoch 200, Training Loss 0.12068599302445532\n",
      "2022-03-26 19:55:44.165651 Epoch 200, Training Loss 0.1212122957496082\n",
      "2022-03-26 19:55:44.192198 Epoch 200, Training Loss 0.12195040239854847\n",
      "2022-03-26 19:55:44.215595 Epoch 200, Training Loss 0.12266259550895837\n",
      "2022-03-26 19:55:44.239796 Epoch 200, Training Loss 0.1233475997548579\n",
      "2022-03-26 19:55:44.263376 Epoch 200, Training Loss 0.12390938755648825\n",
      "2022-03-26 19:55:44.287637 Epoch 200, Training Loss 0.12479917567861659\n",
      "2022-03-26 19:55:44.320816 Epoch 200, Training Loss 0.1255975271506078\n",
      "2022-03-26 19:55:44.347904 Epoch 200, Training Loss 0.12623444298649078\n",
      "2022-03-26 19:55:44.371620 Epoch 200, Training Loss 0.12677029324004718\n",
      "2022-03-26 19:55:44.399039 Epoch 200, Training Loss 0.12751415143232517\n",
      "2022-03-26 19:55:44.422951 Epoch 200, Training Loss 0.12811443266813713\n",
      "2022-03-26 19:55:44.447070 Epoch 200, Training Loss 0.12883534177642345\n",
      "2022-03-26 19:55:44.470603 Epoch 200, Training Loss 0.12960146390416127\n",
      "2022-03-26 19:55:44.494587 Epoch 200, Training Loss 0.13031294778027497\n",
      "2022-03-26 19:55:44.518507 Epoch 200, Training Loss 0.13091229904643106\n",
      "2022-03-26 19:55:44.552534 Epoch 200, Training Loss 0.13165134351576685\n",
      "2022-03-26 19:55:44.577563 Epoch 200, Training Loss 0.13208218082747497\n",
      "2022-03-26 19:55:44.602238 Epoch 200, Training Loss 0.1325662250027937\n",
      "2022-03-26 19:55:44.627371 Epoch 200, Training Loss 0.1335727469161953\n",
      "2022-03-26 19:55:44.651444 Epoch 200, Training Loss 0.134425294254442\n",
      "2022-03-26 19:55:44.675293 Epoch 200, Training Loss 0.13529255189706602\n",
      "2022-03-26 19:55:44.700391 Epoch 200, Training Loss 0.13607770642813513\n",
      "2022-03-26 19:55:44.724571 Epoch 200, Training Loss 0.13674513919426656\n",
      "2022-03-26 19:55:44.748555 Epoch 200, Training Loss 0.13772999154179907\n",
      "2022-03-26 19:55:44.780937 Epoch 200, Training Loss 0.13841959037591733\n",
      "2022-03-26 19:55:44.805896 Epoch 200, Training Loss 0.1393343176302093\n",
      "2022-03-26 19:55:44.829902 Epoch 200, Training Loss 0.14002439010021328\n",
      "2022-03-26 19:55:44.857043 Epoch 200, Training Loss 0.14058066835946134\n",
      "2022-03-26 19:55:44.884863 Epoch 200, Training Loss 0.14139142575318855\n",
      "2022-03-26 19:55:44.908638 Epoch 200, Training Loss 0.1420917783475593\n",
      "2022-03-26 19:55:44.935336 Epoch 200, Training Loss 0.1428540251825167\n",
      "2022-03-26 19:55:44.962138 Epoch 200, Training Loss 0.14368163552278143\n",
      "2022-03-26 19:55:44.999040 Epoch 200, Training Loss 0.1444073215393764\n",
      "2022-03-26 19:55:45.025330 Epoch 200, Training Loss 0.14519316289583437\n",
      "2022-03-26 19:55:45.049068 Epoch 200, Training Loss 0.1458754302062037\n",
      "2022-03-26 19:55:45.076059 Epoch 200, Training Loss 0.14693591741802137\n",
      "2022-03-26 19:55:45.099734 Epoch 200, Training Loss 0.14750662327879835\n",
      "2022-03-26 19:55:45.123551 Epoch 200, Training Loss 0.1480450467837741\n",
      "2022-03-26 19:55:45.147010 Epoch 200, Training Loss 0.1486626068496948\n",
      "2022-03-26 19:55:45.170225 Epoch 200, Training Loss 0.1494543170532607\n",
      "2022-03-26 19:55:45.194428 Epoch 200, Training Loss 0.15035959819088812\n",
      "2022-03-26 19:55:45.225610 Epoch 200, Training Loss 0.15120150747201633\n",
      "2022-03-26 19:55:45.249284 Epoch 200, Training Loss 0.15199320105945363\n",
      "2022-03-26 19:55:45.273858 Epoch 200, Training Loss 0.15256395032796102\n",
      "2022-03-26 19:55:45.300765 Epoch 200, Training Loss 0.15301437028076337\n",
      "2022-03-26 19:55:45.323433 Epoch 200, Training Loss 0.15361996608621933\n",
      "2022-03-26 19:55:45.347712 Epoch 200, Training Loss 0.1546541889915076\n",
      "2022-03-26 19:55:45.371826 Epoch 200, Training Loss 0.15544841585256863\n",
      "2022-03-26 19:55:45.395399 Epoch 200, Training Loss 0.15630400561920518\n",
      "2022-03-26 19:55:45.419640 Epoch 200, Training Loss 0.15690514552013954\n",
      "2022-03-26 19:55:45.451384 Epoch 200, Training Loss 0.15759821948797806\n",
      "2022-03-26 19:55:45.475751 Epoch 200, Training Loss 0.15803025232251647\n",
      "2022-03-26 19:55:45.499532 Epoch 200, Training Loss 0.15854312315621338\n",
      "2022-03-26 19:55:45.529673 Epoch 200, Training Loss 0.159357086243227\n",
      "2022-03-26 19:55:45.560356 Epoch 200, Training Loss 0.15991781316602321\n",
      "2022-03-26 19:55:45.584589 Epoch 200, Training Loss 0.1604731949927557\n",
      "2022-03-26 19:55:45.608257 Epoch 200, Training Loss 0.16108695533879272\n",
      "2022-03-26 19:55:45.633383 Epoch 200, Training Loss 0.16150941236702074\n",
      "2022-03-26 19:55:45.661909 Epoch 200, Training Loss 0.16226641902380892\n",
      "2022-03-26 19:55:45.688359 Epoch 200, Training Loss 0.16340368090535673\n",
      "2022-03-26 19:55:45.711706 Epoch 200, Training Loss 0.164153629137427\n",
      "2022-03-26 19:55:45.735539 Epoch 200, Training Loss 0.1648069142609301\n",
      "2022-03-26 19:55:45.761448 Epoch 200, Training Loss 0.1656423600966973\n",
      "2022-03-26 19:55:45.785734 Epoch 200, Training Loss 0.16670800646400208\n",
      "2022-03-26 19:55:45.809865 Epoch 200, Training Loss 0.16752206208303455\n",
      "2022-03-26 19:55:45.834083 Epoch 200, Training Loss 0.16832180115420495\n",
      "2022-03-26 19:55:45.857535 Epoch 200, Training Loss 0.16917207193039263\n",
      "2022-03-26 19:55:45.890077 Epoch 200, Training Loss 0.16965370124105908\n",
      "2022-03-26 19:55:45.914352 Epoch 200, Training Loss 0.17029321349947654\n",
      "2022-03-26 19:55:45.936477 Epoch 200, Training Loss 0.1710806161622562\n",
      "2022-03-26 19:55:45.961774 Epoch 200, Training Loss 0.1718852229968971\n",
      "2022-03-26 19:55:45.986160 Epoch 200, Training Loss 0.17286500704410435\n",
      "2022-03-26 19:55:46.015992 Epoch 200, Training Loss 0.17361835098784903\n",
      "2022-03-26 19:55:46.038532 Epoch 200, Training Loss 0.1743202144303895\n",
      "2022-03-26 19:55:46.060835 Epoch 200, Training Loss 0.1751749368046251\n",
      "2022-03-26 19:55:46.089921 Epoch 200, Training Loss 0.17596486024082164\n",
      "2022-03-26 19:55:46.123750 Epoch 200, Training Loss 0.1768023366742122\n",
      "2022-03-26 19:55:46.147375 Epoch 200, Training Loss 0.17765905435585305\n",
      "2022-03-26 19:55:46.171170 Epoch 200, Training Loss 0.17821107564679803\n",
      "2022-03-26 19:55:46.197216 Epoch 200, Training Loss 0.17875386382002964\n",
      "2022-03-26 19:55:46.220070 Epoch 200, Training Loss 0.1793682492526291\n",
      "2022-03-26 19:55:46.242585 Epoch 200, Training Loss 0.1803267279930432\n",
      "2022-03-26 19:55:46.264801 Epoch 200, Training Loss 0.18138601571855034\n",
      "2022-03-26 19:55:46.287760 Epoch 200, Training Loss 0.18221416227195575\n",
      "2022-03-26 19:55:46.311154 Epoch 200, Training Loss 0.18276351785568326\n",
      "2022-03-26 19:55:46.341443 Epoch 200, Training Loss 0.1834195148380821\n",
      "2022-03-26 19:55:46.367094 Epoch 200, Training Loss 0.1841283119319345\n",
      "2022-03-26 19:55:46.389801 Epoch 200, Training Loss 0.18490952051356627\n",
      "2022-03-26 19:55:46.416700 Epoch 200, Training Loss 0.1856815825261728\n",
      "2022-03-26 19:55:46.442875 Epoch 200, Training Loss 0.18660627069223262\n",
      "2022-03-26 19:55:46.465624 Epoch 200, Training Loss 0.18723172524853435\n",
      "2022-03-26 19:55:46.489106 Epoch 200, Training Loss 0.18796319969932138\n",
      "2022-03-26 19:55:46.511791 Epoch 200, Training Loss 0.18880803814477018\n",
      "2022-03-26 19:55:46.534550 Epoch 200, Training Loss 0.18951803670667322\n",
      "2022-03-26 19:55:46.565044 Epoch 200, Training Loss 0.19009261980386036\n",
      "2022-03-26 19:55:46.588570 Epoch 200, Training Loss 0.1909448719390518\n",
      "2022-03-26 19:55:46.611260 Epoch 200, Training Loss 0.19159808076556076\n",
      "2022-03-26 19:55:46.638353 Epoch 200, Training Loss 0.19235646785677546\n",
      "2022-03-26 19:55:46.661354 Epoch 200, Training Loss 0.1929506662937686\n",
      "2022-03-26 19:55:46.684770 Epoch 200, Training Loss 0.19376711486398107\n",
      "2022-03-26 19:55:46.707772 Epoch 200, Training Loss 0.19448515044911133\n",
      "2022-03-26 19:55:46.730534 Epoch 200, Training Loss 0.19505321160149391\n",
      "2022-03-26 19:55:46.753536 Epoch 200, Training Loss 0.1956397758039367\n",
      "2022-03-26 19:55:46.782896 Epoch 200, Training Loss 0.19654222892220977\n",
      "2022-03-26 19:55:46.806911 Epoch 200, Training Loss 0.1974115762335565\n",
      "2022-03-26 19:55:46.830396 Epoch 200, Training Loss 0.1979946900740304\n",
      "2022-03-26 19:55:46.857470 Epoch 200, Training Loss 0.19897991003435286\n",
      "2022-03-26 19:55:46.880684 Epoch 200, Training Loss 0.19987166800614817\n",
      "2022-03-26 19:55:46.903355 Epoch 200, Training Loss 0.20076114842501444\n",
      "2022-03-26 19:55:46.926462 Epoch 200, Training Loss 0.20157996921435647\n",
      "2022-03-26 19:55:46.949267 Epoch 200, Training Loss 0.2025098551036147\n",
      "2022-03-26 19:55:46.971196 Epoch 200, Training Loss 0.2032338585466375\n",
      "2022-03-26 19:55:47.006269 Epoch 200, Training Loss 0.20397538819428904\n",
      "2022-03-26 19:55:47.039377 Epoch 200, Training Loss 0.20462697039327354\n",
      "2022-03-26 19:55:47.062133 Epoch 200, Training Loss 0.20521820849164976\n",
      "2022-03-26 19:55:47.089163 Epoch 200, Training Loss 0.2059170653295639\n",
      "2022-03-26 19:55:47.112242 Epoch 200, Training Loss 0.20641384096554174\n",
      "2022-03-26 19:55:47.136421 Epoch 200, Training Loss 0.20713882590346325\n",
      "2022-03-26 19:55:47.158728 Epoch 200, Training Loss 0.20789592059524467\n",
      "2022-03-26 19:55:47.181538 Epoch 200, Training Loss 0.20840240450923705\n",
      "2022-03-26 19:55:47.204887 Epoch 200, Training Loss 0.20910521956813305\n",
      "2022-03-26 19:55:47.235168 Epoch 200, Training Loss 0.2096969860289103\n",
      "2022-03-26 19:55:47.258348 Epoch 200, Training Loss 0.2104111555439737\n",
      "2022-03-26 19:55:47.280838 Epoch 200, Training Loss 0.2110350699833287\n",
      "2022-03-26 19:55:47.306976 Epoch 200, Training Loss 0.21178286314925268\n",
      "2022-03-26 19:55:47.331510 Epoch 200, Training Loss 0.21240640814651918\n",
      "2022-03-26 19:55:47.353783 Epoch 200, Training Loss 0.21323629222867435\n",
      "2022-03-26 19:55:47.376062 Epoch 200, Training Loss 0.21410453113753472\n",
      "2022-03-26 19:55:47.399073 Epoch 200, Training Loss 0.21488504603390804\n",
      "2022-03-26 19:55:47.422377 Epoch 200, Training Loss 0.21556828607378714\n",
      "2022-03-26 19:55:47.453244 Epoch 200, Training Loss 0.2162954727249682\n",
      "2022-03-26 19:55:47.478416 Epoch 200, Training Loss 0.2172930041694885\n",
      "2022-03-26 19:55:47.501906 Epoch 200, Training Loss 0.2177828484407776\n",
      "2022-03-26 19:55:47.529686 Epoch 200, Training Loss 0.21841496675063277\n",
      "2022-03-26 19:55:47.553469 Epoch 200, Training Loss 0.21915292438796108\n",
      "2022-03-26 19:55:47.576607 Epoch 200, Training Loss 0.21983315881408388\n",
      "2022-03-26 19:55:47.600096 Epoch 200, Training Loss 0.22068730191044186\n",
      "2022-03-26 19:55:47.624877 Epoch 200, Training Loss 0.2211768787230372\n",
      "2022-03-26 19:55:47.647699 Epoch 200, Training Loss 0.22189650023379898\n",
      "2022-03-26 19:55:47.679765 Epoch 200, Training Loss 0.2226969401549805\n",
      "2022-03-26 19:55:47.706749 Epoch 200, Training Loss 0.22379388940303832\n",
      "2022-03-26 19:55:47.729850 Epoch 200, Training Loss 0.22456404498166135\n",
      "2022-03-26 19:55:47.756460 Epoch 200, Training Loss 0.22549660568651947\n",
      "2022-03-26 19:55:47.779828 Epoch 200, Training Loss 0.226153309387929\n",
      "2022-03-26 19:55:47.803491 Epoch 200, Training Loss 0.22693183286415647\n",
      "2022-03-26 19:55:47.827326 Epoch 200, Training Loss 0.2275221648499789\n",
      "2022-03-26 19:55:47.851501 Epoch 200, Training Loss 0.22823530889075735\n",
      "2022-03-26 19:55:47.875290 Epoch 200, Training Loss 0.22924722220434252\n",
      "2022-03-26 19:55:47.913184 Epoch 200, Training Loss 0.23006448938566096\n",
      "2022-03-26 19:55:47.937487 Epoch 200, Training Loss 0.2307140663685396\n",
      "2022-03-26 19:55:47.961612 Epoch 200, Training Loss 0.23136212495739197\n",
      "2022-03-26 19:55:47.986260 Epoch 200, Training Loss 0.23234047635894298\n",
      "2022-03-26 19:55:48.009998 Epoch 200, Training Loss 0.23299266710458205\n",
      "2022-03-26 19:55:48.036096 Epoch 200, Training Loss 0.23344893631575359\n",
      "2022-03-26 19:55:48.067207 Epoch 200, Training Loss 0.23402691264744\n",
      "2022-03-26 19:55:48.090824 Epoch 200, Training Loss 0.23472098975687686\n",
      "2022-03-26 19:55:48.117824 Epoch 200, Training Loss 0.23572493304529457\n",
      "2022-03-26 19:55:48.146348 Epoch 200, Training Loss 0.23657539970886982\n",
      "2022-03-26 19:55:48.169730 Epoch 200, Training Loss 0.23769096153623917\n",
      "2022-03-26 19:55:48.193813 Epoch 200, Training Loss 0.2386221503433974\n",
      "2022-03-26 19:55:48.222080 Epoch 200, Training Loss 0.23956924985589273\n",
      "2022-03-26 19:55:48.246070 Epoch 200, Training Loss 0.2403189759806294\n",
      "2022-03-26 19:55:48.269589 Epoch 200, Training Loss 0.24102684089442347\n",
      "2022-03-26 19:55:48.292978 Epoch 200, Training Loss 0.2419425333323686\n",
      "2022-03-26 19:55:48.317063 Epoch 200, Training Loss 0.24248464134952907\n",
      "2022-03-26 19:55:48.352896 Epoch 200, Training Loss 0.24329689922540085\n",
      "2022-03-26 19:55:48.377029 Epoch 200, Training Loss 0.24395234345475122\n",
      "2022-03-26 19:55:48.398749 Epoch 200, Training Loss 0.24481510414796717\n",
      "2022-03-26 19:55:48.423553 Epoch 200, Training Loss 0.24546020087378714\n",
      "2022-03-26 19:55:48.447646 Epoch 200, Training Loss 0.24619873870364237\n",
      "2022-03-26 19:55:48.470137 Epoch 200, Training Loss 0.24685579545967415\n",
      "2022-03-26 19:55:48.493218 Epoch 200, Training Loss 0.24762758925137923\n",
      "2022-03-26 19:55:48.516760 Epoch 200, Training Loss 0.24813631146460238\n",
      "2022-03-26 19:55:48.539218 Epoch 200, Training Loss 0.24902057800146624\n",
      "2022-03-26 19:55:48.565605 Epoch 200, Training Loss 0.2496123484638341\n",
      "2022-03-26 19:55:48.590492 Epoch 200, Training Loss 0.2502916248710564\n",
      "2022-03-26 19:55:48.613389 Epoch 200, Training Loss 0.2507247836007487\n",
      "2022-03-26 19:55:48.639526 Epoch 200, Training Loss 0.2512778879126624\n",
      "2022-03-26 19:55:48.662186 Epoch 200, Training Loss 0.25204983742340753\n",
      "2022-03-26 19:55:48.684096 Epoch 200, Training Loss 0.25281548583903884\n",
      "2022-03-26 19:55:48.706249 Epoch 200, Training Loss 0.2535587766438799\n",
      "2022-03-26 19:55:48.728743 Epoch 200, Training Loss 0.25420313578127596\n",
      "2022-03-26 19:55:48.751240 Epoch 200, Training Loss 0.25490602751826996\n",
      "2022-03-26 19:55:48.777159 Epoch 200, Training Loss 0.2555958370266058\n",
      "2022-03-26 19:55:48.802830 Epoch 200, Training Loss 0.25622016393467595\n",
      "2022-03-26 19:55:48.825753 Epoch 200, Training Loss 0.2570158290817305\n",
      "2022-03-26 19:55:48.851407 Epoch 200, Training Loss 0.2575219790344043\n",
      "2022-03-26 19:55:48.875507 Epoch 200, Training Loss 0.25830636495519477\n",
      "2022-03-26 19:55:48.898542 Epoch 200, Training Loss 0.2591922953153205\n",
      "2022-03-26 19:55:48.921976 Epoch 200, Training Loss 0.2600007269083691\n",
      "2022-03-26 19:55:48.944549 Epoch 200, Training Loss 0.26072159249459387\n",
      "2022-03-26 19:55:48.967385 Epoch 200, Training Loss 0.26156010461585294\n",
      "2022-03-26 19:55:48.996566 Epoch 200, Training Loss 0.26222174955755856\n",
      "2022-03-26 19:55:49.020336 Epoch 200, Training Loss 0.2628674018565956\n",
      "2022-03-26 19:55:49.042910 Epoch 200, Training Loss 0.2634911002100581\n",
      "2022-03-26 19:55:49.071175 Epoch 200, Training Loss 0.264168322162555\n",
      "2022-03-26 19:55:49.099231 Epoch 200, Training Loss 0.2649777842604596\n",
      "2022-03-26 19:55:49.125635 Epoch 200, Training Loss 0.2654171916834839\n",
      "2022-03-26 19:55:49.148441 Epoch 200, Training Loss 0.26612334963305834\n",
      "2022-03-26 19:55:49.173461 Epoch 200, Training Loss 0.26675166414521845\n",
      "2022-03-26 19:55:49.196329 Epoch 200, Training Loss 0.26749866354800855\n",
      "2022-03-26 19:55:49.230437 Epoch 200, Training Loss 0.26817238216509903\n",
      "2022-03-26 19:55:49.254379 Epoch 200, Training Loss 0.2687730947342675\n",
      "2022-03-26 19:55:49.276927 Epoch 200, Training Loss 0.2693919401491999\n",
      "2022-03-26 19:55:49.303559 Epoch 200, Training Loss 0.27021156262863627\n",
      "2022-03-26 19:55:49.326354 Epoch 200, Training Loss 0.2710558156985456\n",
      "2022-03-26 19:55:49.350225 Epoch 200, Training Loss 0.27194424571893405\n",
      "2022-03-26 19:55:49.374320 Epoch 200, Training Loss 0.2725111019352208\n",
      "2022-03-26 19:55:49.397877 Epoch 200, Training Loss 0.2733212622154094\n",
      "2022-03-26 19:55:49.421359 Epoch 200, Training Loss 0.27398241118854266\n",
      "2022-03-26 19:55:49.453145 Epoch 200, Training Loss 0.2746583742787466\n",
      "2022-03-26 19:55:49.477625 Epoch 200, Training Loss 0.27531491829763594\n",
      "2022-03-26 19:55:49.501926 Epoch 200, Training Loss 0.275925009185091\n",
      "2022-03-26 19:55:49.529449 Epoch 200, Training Loss 0.27691628168458526\n",
      "2022-03-26 19:55:49.552935 Epoch 200, Training Loss 0.27766987814775207\n",
      "2022-03-26 19:55:49.576892 Epoch 200, Training Loss 0.2781733231013998\n",
      "2022-03-26 19:55:49.601065 Epoch 200, Training Loss 0.2788181365146052\n",
      "2022-03-26 19:55:49.624803 Epoch 200, Training Loss 0.27960054648806676\n",
      "2022-03-26 19:55:49.648766 Epoch 200, Training Loss 0.2803383956632346\n",
      "2022-03-26 19:55:49.681643 Epoch 200, Training Loss 0.2809308807715735\n",
      "2022-03-26 19:55:49.706120 Epoch 200, Training Loss 0.28167070604651173\n",
      "2022-03-26 19:55:49.730854 Epoch 200, Training Loss 0.2825298386309153\n",
      "2022-03-26 19:55:49.761326 Epoch 200, Training Loss 0.2832684420106356\n",
      "2022-03-26 19:55:49.784663 Epoch 200, Training Loss 0.28399746451536406\n",
      "2022-03-26 19:55:49.808179 Epoch 200, Training Loss 0.2846804220810571\n",
      "2022-03-26 19:55:49.831351 Epoch 200, Training Loss 0.2853914484038682\n",
      "2022-03-26 19:55:49.854867 Epoch 200, Training Loss 0.28647098044300323\n",
      "2022-03-26 19:55:49.879001 Epoch 200, Training Loss 0.28716359449469525\n",
      "2022-03-26 19:55:49.913184 Epoch 200, Training Loss 0.28785212265561\n",
      "2022-03-26 19:55:49.937827 Epoch 200, Training Loss 0.28879475303927954\n",
      "2022-03-26 19:55:49.961494 Epoch 200, Training Loss 0.28947368226087916\n",
      "2022-03-26 19:55:49.989106 Epoch 200, Training Loss 0.290000064217526\n",
      "2022-03-26 19:55:50.018594 Epoch 200, Training Loss 0.2909784470982564\n",
      "2022-03-26 19:55:50.043588 Epoch 200, Training Loss 0.29178316445301866\n",
      "2022-03-26 19:55:50.067537 Epoch 200, Training Loss 0.29253467391518984\n",
      "2022-03-26 19:55:50.104017 Epoch 200, Training Loss 0.293208790176055\n",
      "2022-03-26 19:55:50.136257 Epoch 200, Training Loss 0.29409924522995035\n",
      "2022-03-26 19:55:50.160558 Epoch 200, Training Loss 0.2948343599848735\n",
      "2022-03-26 19:55:50.183674 Epoch 200, Training Loss 0.29563702296113115\n",
      "2022-03-26 19:55:50.206765 Epoch 200, Training Loss 0.2965309704508623\n",
      "2022-03-26 19:55:50.230073 Epoch 200, Training Loss 0.29731502587837944\n",
      "2022-03-26 19:55:50.254184 Epoch 200, Training Loss 0.2980881505610083\n",
      "2022-03-26 19:55:50.282208 Epoch 200, Training Loss 0.29924243345589896\n",
      "2022-03-26 19:55:50.305622 Epoch 200, Training Loss 0.3000644292977765\n",
      "2022-03-26 19:55:50.331023 Epoch 200, Training Loss 0.30059777093512935\n",
      "2022-03-26 19:55:50.363478 Epoch 200, Training Loss 0.3012607619356926\n",
      "2022-03-26 19:55:50.386884 Epoch 200, Training Loss 0.30216340027044497\n",
      "2022-03-26 19:55:50.410269 Epoch 200, Training Loss 0.30280497384345745\n",
      "2022-03-26 19:55:50.438366 Epoch 200, Training Loss 0.3036907833936574\n",
      "2022-03-26 19:55:50.461254 Epoch 200, Training Loss 0.304182236647362\n",
      "2022-03-26 19:55:50.485206 Epoch 200, Training Loss 0.3048713685530226\n",
      "2022-03-26 19:55:50.508121 Epoch 200, Training Loss 0.305613990291915\n",
      "2022-03-26 19:55:50.531138 Epoch 200, Training Loss 0.30646875504490056\n",
      "2022-03-26 19:55:50.553783 Epoch 200, Training Loss 0.307072613245386\n",
      "2022-03-26 19:55:50.582775 Epoch 200, Training Loss 0.3076493535810115\n",
      "2022-03-26 19:55:50.607262 Epoch 200, Training Loss 0.30846752733220834\n",
      "2022-03-26 19:55:50.631561 Epoch 200, Training Loss 0.3091936308099791\n",
      "2022-03-26 19:55:50.656048 Epoch 200, Training Loss 0.310059855966007\n",
      "2022-03-26 19:55:50.682411 Epoch 200, Training Loss 0.3105459645428621\n",
      "2022-03-26 19:55:50.706248 Epoch 200, Training Loss 0.31145482912392874\n",
      "2022-03-26 19:55:50.729904 Epoch 200, Training Loss 0.3120488694790379\n",
      "2022-03-26 19:55:50.753834 Epoch 200, Training Loss 0.31266735170198523\n",
      "2022-03-26 19:55:50.776998 Epoch 200, Training Loss 0.3134791315973872\n",
      "2022-03-26 19:55:50.807785 Epoch 200, Training Loss 0.3143919356491255\n",
      "2022-03-26 19:55:50.832407 Epoch 200, Training Loss 0.31521759443270886\n",
      "2022-03-26 19:55:50.856397 Epoch 200, Training Loss 0.31586526200899384\n",
      "2022-03-26 19:55:50.885745 Epoch 200, Training Loss 0.3167478260786637\n",
      "2022-03-26 19:55:50.909524 Epoch 200, Training Loss 0.3176475610879376\n",
      "2022-03-26 19:55:50.940187 Epoch 200, Training Loss 0.31848525673227235\n",
      "2022-03-26 19:55:50.964807 Epoch 200, Training Loss 0.31935408780032104\n",
      "2022-03-26 19:55:50.988604 Epoch 200, Training Loss 0.32005764616419896\n",
      "2022-03-26 19:55:51.019191 Epoch 200, Training Loss 0.32052951982564026\n",
      "2022-03-26 19:55:51.048357 Epoch 200, Training Loss 0.3211865440354018\n",
      "2022-03-26 19:55:51.072037 Epoch 200, Training Loss 0.3219337666888371\n",
      "2022-03-26 19:55:51.096059 Epoch 200, Training Loss 0.322441721015879\n",
      "2022-03-26 19:55:51.132051 Epoch 200, Training Loss 0.32298850472016105\n",
      "2022-03-26 19:55:51.156901 Epoch 200, Training Loss 0.323646009044574\n",
      "2022-03-26 19:55:51.180251 Epoch 200, Training Loss 0.32441008342501454\n",
      "2022-03-26 19:55:51.203811 Epoch 200, Training Loss 0.32508215735025725\n",
      "2022-03-26 19:55:51.232636 Epoch 200, Training Loss 0.32581842852675397\n",
      "2022-03-26 19:55:51.258278 Epoch 200, Training Loss 0.326634497517515\n",
      "2022-03-26 19:55:51.280806 Epoch 200, Training Loss 0.3275758698010993\n",
      "2022-03-26 19:55:51.305030 Epoch 200, Training Loss 0.32817759747852754\n",
      "2022-03-26 19:55:51.335544 Epoch 200, Training Loss 0.32855073215863895\n",
      "2022-03-26 19:55:51.359610 Epoch 200, Training Loss 0.3291849389176844\n",
      "2022-03-26 19:55:51.382688 Epoch 200, Training Loss 0.3298032681655396\n",
      "2022-03-26 19:55:51.406645 Epoch 200, Training Loss 0.33056712394480203\n",
      "2022-03-26 19:55:51.430864 Epoch 200, Training Loss 0.33154786784020834\n",
      "2022-03-26 19:55:51.465510 Epoch 200, Training Loss 0.33261434417551433\n",
      "2022-03-26 19:55:51.489674 Epoch 200, Training Loss 0.33342916649930615\n",
      "2022-03-26 19:55:51.513840 Epoch 200, Training Loss 0.3347529501408872\n",
      "2022-03-26 19:55:51.539262 Epoch 200, Training Loss 0.33587837691807076\n",
      "2022-03-26 19:55:51.563414 Epoch 200, Training Loss 0.33660456949792555\n",
      "2022-03-26 19:55:51.587222 Epoch 200, Training Loss 0.3372501278929698\n",
      "2022-03-26 19:55:51.611576 Epoch 200, Training Loss 0.3381432759030091\n",
      "2022-03-26 19:55:51.636415 Epoch 200, Training Loss 0.3387810469740797\n",
      "2022-03-26 19:55:51.660661 Epoch 200, Training Loss 0.3394460336631521\n",
      "2022-03-26 19:55:51.692404 Epoch 200, Training Loss 0.34017676343698333\n",
      "2022-03-26 19:55:51.717126 Epoch 200, Training Loss 0.34087204300534085\n",
      "2022-03-26 19:55:51.741217 Epoch 200, Training Loss 0.3415635490356504\n",
      "2022-03-26 19:55:51.768261 Epoch 200, Training Loss 0.342152223715087\n",
      "2022-03-26 19:55:51.792030 Epoch 200, Training Loss 0.34307444209942733\n",
      "2022-03-26 19:55:51.815878 Epoch 200, Training Loss 0.34372357791646974\n",
      "2022-03-26 19:55:51.840446 Epoch 200, Training Loss 0.3445319217031874\n",
      "2022-03-26 19:55:51.864535 Epoch 200, Training Loss 0.34538642379938794\n",
      "2022-03-26 19:55:51.888184 Epoch 200, Training Loss 0.34621153676601324\n",
      "2022-03-26 19:55:51.921341 Epoch 200, Training Loss 0.3467455531858727\n",
      "2022-03-26 19:55:51.946350 Epoch 200, Training Loss 0.34735611149722045\n",
      "2022-03-26 19:55:51.970282 Epoch 200, Training Loss 0.3480664641808366\n",
      "2022-03-26 19:55:51.999175 Epoch 200, Training Loss 0.3487937826177348\n",
      "2022-03-26 19:55:52.023349 Epoch 200, Training Loss 0.34948529070600526\n",
      "2022-03-26 19:55:52.046973 Epoch 200, Training Loss 0.3503271179735813\n",
      "2022-03-26 19:55:52.070468 Epoch 200, Training Loss 0.3510043105048597\n",
      "2022-03-26 19:55:52.094019 Epoch 200, Training Loss 0.3516175261010294\n",
      "2022-03-26 19:55:52.117753 Epoch 200, Training Loss 0.3523756278978894\n",
      "2022-03-26 19:55:52.163884 Epoch 200, Training Loss 0.35309692451258756\n",
      "2022-03-26 19:55:52.188225 Epoch 200, Training Loss 0.35392532320431125\n",
      "2022-03-26 19:55:52.213117 Epoch 200, Training Loss 0.3547112565592427\n",
      "2022-03-26 19:55:52.242077 Epoch 200, Training Loss 0.35540595014229454\n",
      "2022-03-26 19:55:52.265804 Epoch 200, Training Loss 0.3560289585072061\n",
      "2022-03-26 19:55:52.289571 Epoch 200, Training Loss 0.35687934201391763\n",
      "2022-03-26 19:55:52.313056 Epoch 200, Training Loss 0.35752948806109025\n",
      "2022-03-26 19:55:52.340633 Epoch 200, Training Loss 0.35813115651497757\n",
      "2022-03-26 19:55:52.364525 Epoch 200, Training Loss 0.35866635900629146\n",
      "2022-03-26 19:55:52.396762 Epoch 200, Training Loss 0.359385729369605\n",
      "2022-03-26 19:55:52.421495 Epoch 200, Training Loss 0.3601461170274583\n",
      "2022-03-26 19:55:52.446937 Epoch 200, Training Loss 0.3610260344069937\n",
      "2022-03-26 19:55:52.475438 Epoch 200, Training Loss 0.3615647528863624\n",
      "2022-03-26 19:55:52.499089 Epoch 200, Training Loss 0.3623280576656542\n",
      "2022-03-26 19:55:52.522891 Epoch 200, Training Loss 0.363209277124661\n",
      "2022-03-26 19:55:52.546994 Epoch 200, Training Loss 0.3637812934110842\n",
      "2022-03-26 19:55:52.570615 Epoch 200, Training Loss 0.3647263009682336\n",
      "2022-03-26 19:55:52.595115 Epoch 200, Training Loss 0.36567302585562783\n",
      "2022-03-26 19:55:52.628391 Epoch 200, Training Loss 0.3662052118717252\n",
      "2022-03-26 19:55:52.652590 Epoch 200, Training Loss 0.366983282215455\n",
      "2022-03-26 19:55:52.676790 Epoch 200, Training Loss 0.3676763488660993\n",
      "2022-03-26 19:55:52.705284 Epoch 200, Training Loss 0.3683619390210837\n",
      "2022-03-26 19:55:52.730205 Epoch 200, Training Loss 0.36922147664267696\n",
      "2022-03-26 19:55:52.753776 Epoch 200, Training Loss 0.3698488758957904\n",
      "2022-03-26 19:55:52.777577 Epoch 200, Training Loss 0.3705652893504218\n",
      "2022-03-26 19:55:52.801064 Epoch 200, Training Loss 0.3711506770471173\n",
      "2022-03-26 19:55:52.825728 Epoch 200, Training Loss 0.37183848156800964\n",
      "2022-03-26 19:55:52.859421 Epoch 200, Training Loss 0.37248126156342304\n",
      "2022-03-26 19:55:52.884562 Epoch 200, Training Loss 0.37335475719036043\n",
      "2022-03-26 19:55:52.908597 Epoch 200, Training Loss 0.37395073042806154\n",
      "2022-03-26 19:55:52.937090 Epoch 200, Training Loss 0.3745884579389601\n",
      "2022-03-26 19:55:52.960551 Epoch 200, Training Loss 0.37566287007630633\n",
      "2022-03-26 19:55:52.983706 Epoch 200, Training Loss 0.3766407361801933\n",
      "2022-03-26 19:55:53.007582 Epoch 200, Training Loss 0.3772296262214251\n",
      "2022-03-26 19:55:53.035433 Epoch 200, Training Loss 0.3780397269731897\n",
      "2022-03-26 19:55:53.059152 Epoch 200, Training Loss 0.3786608139267358\n",
      "2022-03-26 19:55:53.092583 Epoch 200, Training Loss 0.3793976762715508\n",
      "2022-03-26 19:55:53.116882 Epoch 200, Training Loss 0.3800246049757199\n",
      "2022-03-26 19:55:53.140279 Epoch 200, Training Loss 0.3806777160871974\n",
      "2022-03-26 19:55:53.168394 Epoch 200, Training Loss 0.3815382884820099\n",
      "2022-03-26 19:55:53.202711 Epoch 200, Training Loss 0.3822378729234266\n",
      "2022-03-26 19:55:53.226530 Epoch 200, Training Loss 0.3828388489878086\n",
      "2022-03-26 19:55:53.250035 Epoch 200, Training Loss 0.3834922904401179\n",
      "2022-03-26 19:55:53.273850 Epoch 200, Training Loss 0.38425628021549996\n",
      "2022-03-26 19:55:53.300674 Epoch 200, Training Loss 0.38480564155389585\n",
      "2022-03-26 19:55:53.329618 Epoch 200, Training Loss 0.3855709916414202\n",
      "2022-03-26 19:55:53.353599 Epoch 200, Training Loss 0.38626165390776857\n",
      "2022-03-26 19:55:53.380805 Epoch 200, Training Loss 0.38698091840042786\n",
      "2022-03-26 19:55:53.405038 Epoch 200, Training Loss 0.38772002216952534\n",
      "2022-03-26 19:55:53.428483 Epoch 200, Training Loss 0.38822741452080517\n",
      "2022-03-26 19:55:53.452745 Epoch 200, Training Loss 0.3890171674511317\n",
      "2022-03-26 19:55:53.475959 Epoch 200, Training Loss 0.38981912393704093\n",
      "2022-03-26 19:55:53.499364 Epoch 200, Training Loss 0.390695924039387\n",
      "2022-03-26 19:55:53.530073 Epoch 200, Training Loss 0.391119330854672\n",
      "2022-03-26 19:55:53.553813 Epoch 200, Training Loss 0.3917417606658033\n",
      "2022-03-26 19:55:53.577012 Epoch 200, Training Loss 0.39237071573734283\n",
      "2022-03-26 19:55:53.600967 Epoch 200, Training Loss 0.39300893563443745\n",
      "2022-03-26 19:55:53.626812 Epoch 200, Training Loss 0.39389931218093616\n",
      "2022-03-26 19:55:53.650561 Epoch 200, Training Loss 0.39477417055908065\n",
      "2022-03-26 19:55:53.673846 Epoch 200, Training Loss 0.39520505059253225\n",
      "2022-03-26 19:55:53.697390 Epoch 200, Training Loss 0.3960848838243338\n",
      "2022-03-26 19:55:53.721697 Epoch 200, Training Loss 0.3970234511834581\n",
      "2022-03-26 19:55:53.753670 Epoch 200, Training Loss 0.39762930869293944\n",
      "2022-03-26 19:55:53.777779 Epoch 200, Training Loss 0.3984097479020848\n",
      "2022-03-26 19:55:53.801119 Epoch 200, Training Loss 0.3992055407951555\n",
      "2022-03-26 19:55:53.828555 Epoch 200, Training Loss 0.4001451313419415\n",
      "2022-03-26 19:55:53.852696 Epoch 200, Training Loss 0.401040773448127\n",
      "2022-03-26 19:55:53.875788 Epoch 200, Training Loss 0.40154495385601696\n",
      "2022-03-26 19:55:53.899199 Epoch 200, Training Loss 0.40217946915675307\n",
      "2022-03-26 19:55:53.922939 Epoch 200, Training Loss 0.40311265388108275\n",
      "2022-03-26 19:55:53.953778 Epoch 200, Training Loss 0.40406444333398434\n",
      "2022-03-26 19:55:53.986407 Epoch 200, Training Loss 0.40463285010946376\n",
      "2022-03-26 19:55:54.012665 Epoch 200, Training Loss 0.4054398913593853\n",
      "2022-03-26 19:55:54.036628 Epoch 200, Training Loss 0.40624967526139505\n",
      "2022-03-26 19:55:54.068798 Epoch 200, Training Loss 0.4071289272335789\n",
      "2022-03-26 19:55:54.092183 Epoch 200, Training Loss 0.40769302414353853\n",
      "2022-03-26 19:55:54.116073 Epoch 200, Training Loss 0.40846021359076584\n",
      "2022-03-26 19:55:54.140425 Epoch 200, Training Loss 0.40904347709072825\n",
      "2022-03-26 19:55:54.164115 Epoch 200, Training Loss 0.4100434106329213\n",
      "2022-03-26 19:55:54.188367 Epoch 200, Training Loss 0.4107920516787283\n",
      "2022-03-26 19:55:54.231266 Epoch 200, Training Loss 0.41143407663116066\n",
      "2022-03-26 19:55:54.255507 Epoch 200, Training Loss 0.41226067414978884\n",
      "2022-03-26 19:55:54.279386 Epoch 200, Training Loss 0.41299462760500893\n",
      "2022-03-26 19:55:54.303920 Epoch 200, Training Loss 0.4137481822229712\n",
      "2022-03-26 19:55:54.329469 Epoch 200, Training Loss 0.4144061534758419\n",
      "2022-03-26 19:55:54.353587 Epoch 200, Training Loss 0.4150193010830818\n",
      "2022-03-26 19:55:54.377481 Epoch 200, Training Loss 0.415485346294425\n",
      "2022-03-26 19:55:54.406775 Epoch 200, Training Loss 0.4163364918564287\n",
      "2022-03-26 19:55:54.431812 Epoch 200, Training Loss 0.4170634032743971\n",
      "2022-03-26 19:55:54.456459 Epoch 200, Training Loss 0.4179971126263099\n",
      "2022-03-26 19:55:54.480481 Epoch 200, Training Loss 0.41885654971270303\n",
      "2022-03-26 19:55:54.505661 Epoch 200, Training Loss 0.4195204606980009\n",
      "2022-03-26 19:55:54.528809 Epoch 200, Training Loss 0.42020612097609683\n",
      "2022-03-26 19:55:54.552564 Epoch 200, Training Loss 0.42088945225224167\n",
      "2022-03-26 19:55:54.575786 Epoch 200, Training Loss 0.4216823169718618\n",
      "2022-03-26 19:55:54.599427 Epoch 200, Training Loss 0.42229614500194557\n",
      "2022-03-26 19:55:54.633078 Epoch 200, Training Loss 0.4231388534578826\n",
      "2022-03-26 19:55:54.657070 Epoch 200, Training Loss 0.4237865163847004\n",
      "2022-03-26 19:55:54.680407 Epoch 200, Training Loss 0.4245476422407438\n",
      "2022-03-26 19:55:54.706923 Epoch 200, Training Loss 0.4255055391879948\n",
      "2022-03-26 19:55:54.730099 Epoch 200, Training Loss 0.4265424356893505\n",
      "2022-03-26 19:55:54.753577 Epoch 200, Training Loss 0.4274815389567324\n",
      "2022-03-26 19:55:54.776022 Epoch 200, Training Loss 0.4285238231996746\n",
      "2022-03-26 19:55:54.798395 Epoch 200, Training Loss 0.4293176821430626\n",
      "2022-03-26 19:55:54.821148 Epoch 200, Training Loss 0.4303502379475957\n",
      "2022-03-26 19:55:54.861807 Epoch 200, Training Loss 0.4309631559397558\n",
      "2022-03-26 19:55:54.886915 Epoch 200, Training Loss 0.43202451206838993\n",
      "2022-03-26 19:55:54.910639 Epoch 200, Training Loss 0.432804395094552\n",
      "2022-03-26 19:55:54.934812 Epoch 200, Training Loss 0.4335370485282615\n",
      "2022-03-26 19:55:54.964493 Epoch 200, Training Loss 0.4343658668153426\n",
      "2022-03-26 19:55:54.987395 Epoch 200, Training Loss 0.43505105764969537\n",
      "2022-03-26 19:55:55.010649 Epoch 200, Training Loss 0.4359966084322966\n",
      "2022-03-26 19:55:55.033420 Epoch 200, Training Loss 0.43645636089470075\n",
      "2022-03-26 19:55:55.056834 Epoch 200, Training Loss 0.4372184095556474\n",
      "2022-03-26 19:55:55.090022 Epoch 200, Training Loss 0.43804479586651257\n",
      "2022-03-26 19:55:55.113606 Epoch 200, Training Loss 0.438674524502681\n",
      "2022-03-26 19:55:55.136412 Epoch 200, Training Loss 0.4395133610957724\n",
      "2022-03-26 19:55:55.162513 Epoch 200, Training Loss 0.44013125630443356\n",
      "2022-03-26 19:55:55.184904 Epoch 200, Training Loss 0.44070533737349693\n",
      "2022-03-26 19:55:55.207849 Epoch 200, Training Loss 0.4416623525988415\n",
      "2022-03-26 19:55:55.230020 Epoch 200, Training Loss 0.4424062776367378\n",
      "2022-03-26 19:55:55.262621 Epoch 200, Training Loss 0.44334399307627814\n",
      "2022-03-26 19:55:55.285545 Epoch 200, Training Loss 0.4440938989295984\n",
      "2022-03-26 19:55:55.315356 Epoch 200, Training Loss 0.4448581399286495\n",
      "2022-03-26 19:55:55.342018 Epoch 200, Training Loss 0.445416074579634\n",
      "2022-03-26 19:55:55.366743 Epoch 200, Training Loss 0.44636572550629716\n",
      "2022-03-26 19:55:55.391357 Epoch 200, Training Loss 0.44741991230898803\n",
      "2022-03-26 19:55:55.415301 Epoch 200, Training Loss 0.44805659338488907\n",
      "2022-03-26 19:55:55.439592 Epoch 200, Training Loss 0.4487037245955918\n",
      "2022-03-26 19:55:55.464157 Epoch 200, Training Loss 0.44960705882600505\n",
      "2022-03-26 19:55:55.486681 Epoch 200, Training Loss 0.4503387090418955\n",
      "2022-03-26 19:55:55.510507 Epoch 200, Training Loss 0.4507746787937096\n",
      "2022-03-26 19:55:55.542459 Epoch 200, Training Loss 0.4516112695417136\n",
      "2022-03-26 19:55:55.567154 Epoch 200, Training Loss 0.45218650527927273\n",
      "2022-03-26 19:55:55.591374 Epoch 200, Training Loss 0.45302502913853093\n",
      "2022-03-26 19:55:55.619209 Epoch 200, Training Loss 0.4537865698642438\n",
      "2022-03-26 19:55:55.643202 Epoch 200, Training Loss 0.45459963118328767\n",
      "2022-03-26 19:55:55.667270 Epoch 200, Training Loss 0.45515836005473076\n",
      "2022-03-26 19:55:55.691189 Epoch 200, Training Loss 0.45583620450228374\n",
      "2022-03-26 19:55:55.714945 Epoch 200, Training Loss 0.4563923168670186\n",
      "2022-03-26 19:55:55.740166 Epoch 200, Training Loss 0.4569998471557027\n",
      "2022-03-26 19:55:55.775243 Epoch 200, Training Loss 0.45762524473697636\n",
      "2022-03-26 19:55:55.800189 Epoch 200, Training Loss 0.45861715672875913\n",
      "2022-03-26 19:55:55.824474 Epoch 200, Training Loss 0.45944777192057246\n",
      "2022-03-26 19:55:55.850609 Epoch 200, Training Loss 0.4602539062195117\n",
      "2022-03-26 19:55:55.876524 Epoch 200, Training Loss 0.46098257041038454\n",
      "2022-03-26 19:55:55.899998 Epoch 200, Training Loss 0.46173571473192376\n",
      "2022-03-26 19:55:55.923537 Epoch 200, Training Loss 0.46238663937429636\n",
      "2022-03-26 19:55:55.947338 Epoch 200, Training Loss 0.46296652404548566\n",
      "2022-03-26 19:55:55.971027 Epoch 200, Training Loss 0.4636991011059802\n",
      "2022-03-26 19:55:56.003072 Epoch 200, Training Loss 0.4647207777670887\n",
      "2022-03-26 19:55:56.028074 Epoch 200, Training Loss 0.4657973055644413\n",
      "2022-03-26 19:55:56.060038 Epoch 200, Training Loss 0.46663238576915866\n",
      "2022-03-26 19:55:56.083746 Epoch 200, Training Loss 0.46766484927033525\n",
      "2022-03-26 19:55:56.109882 Epoch 200, Training Loss 0.4684646710410447\n",
      "2022-03-26 19:55:56.136527 Epoch 200, Training Loss 0.4695594108013241\n",
      "2022-03-26 19:55:56.160555 Epoch 200, Training Loss 0.47045317254103053\n",
      "2022-03-26 19:55:56.183945 Epoch 200, Training Loss 0.4710908229165065\n",
      "2022-03-26 19:55:56.210154 Epoch 200, Training Loss 0.4716462287146722\n",
      "2022-03-26 19:55:56.238404 Epoch 200, Training Loss 0.4724245925083795\n",
      "2022-03-26 19:55:56.266225 Epoch 200, Training Loss 0.473023053363461\n",
      "2022-03-26 19:55:56.297962 Epoch 200, Training Loss 0.47354554257277026\n",
      "2022-03-26 19:55:56.321556 Epoch 200, Training Loss 0.47416309722701605\n",
      "2022-03-26 19:55:56.348088 Epoch 200, Training Loss 0.4748660096960604\n",
      "2022-03-26 19:55:56.372216 Epoch 200, Training Loss 0.4754005763155725\n",
      "2022-03-26 19:55:56.396135 Epoch 200, Training Loss 0.4760124626595651\n",
      "2022-03-26 19:55:56.425493 Epoch 200, Training Loss 0.47652119703000156\n",
      "2022-03-26 19:55:56.452871 Epoch 200, Training Loss 0.47728178903574836\n",
      "2022-03-26 19:55:56.476200 Epoch 200, Training Loss 0.4779011387273174\n",
      "2022-03-26 19:55:56.502224 Epoch 200, Training Loss 0.4786739740758906\n",
      "2022-03-26 19:55:56.526073 Epoch 200, Training Loss 0.47956574904491833\n",
      "2022-03-26 19:55:56.557007 Epoch 200, Training Loss 0.4803354678022892\n",
      "2022-03-26 19:55:56.580184 Epoch 200, Training Loss 0.4810100464183656\n",
      "2022-03-26 19:55:56.603532 Epoch 200, Training Loss 0.48152551836217455\n",
      "2022-03-26 19:55:56.626762 Epoch 200, Training Loss 0.4819214679396061\n",
      "2022-03-26 19:55:56.657712 Epoch 200, Training Loss 0.4826865546843585\n",
      "2022-03-26 19:55:56.681224 Epoch 200, Training Loss 0.4833215447261815\n",
      "2022-03-26 19:55:56.705695 Epoch 200, Training Loss 0.48407040993728295\n",
      "2022-03-26 19:55:56.728831 Epoch 200, Training Loss 0.48466888592218804\n",
      "2022-03-26 19:55:56.752035 Epoch 200, Training Loss 0.4855015977950352\n",
      "2022-03-26 19:55:56.775285 Epoch 200, Training Loss 0.48610767729751897\n",
      "2022-03-26 19:55:56.798215 Epoch 200, Training Loss 0.4868454415627453\n",
      "2022-03-26 19:55:56.821445 Epoch 200, Training Loss 0.4873302346071624\n",
      "2022-03-26 19:55:56.845923 Epoch 200, Training Loss 0.4879871294702715\n",
      "2022-03-26 19:55:56.874233 Epoch 200, Training Loss 0.48895525257758166\n",
      "2022-03-26 19:55:56.900596 Epoch 200, Training Loss 0.48962381562156143\n",
      "2022-03-26 19:55:56.924171 Epoch 200, Training Loss 0.4902448466290598\n",
      "2022-03-26 19:55:56.956099 Epoch 200, Training Loss 0.49095622996997346\n",
      "2022-03-26 19:55:56.979888 Epoch 200, Training Loss 0.49156878537991466\n",
      "2022-03-26 19:55:57.007008 Epoch 200, Training Loss 0.49227656934724745\n",
      "2022-03-26 19:55:57.029715 Epoch 200, Training Loss 0.49297431000815634\n",
      "2022-03-26 19:55:57.053181 Epoch 200, Training Loss 0.493568927659403\n",
      "2022-03-26 19:55:57.085963 Epoch 200, Training Loss 0.4945433031567527\n",
      "2022-03-26 19:55:57.110837 Epoch 200, Training Loss 0.49527877607308995\n",
      "2022-03-26 19:55:57.134285 Epoch 200, Training Loss 0.4959256284682037\n",
      "2022-03-26 19:55:57.161251 Epoch 200, Training Loss 0.49690585459589653\n",
      "2022-03-26 19:55:57.184457 Epoch 200, Training Loss 0.497666572137257\n",
      "2022-03-26 19:55:57.207602 Epoch 200, Training Loss 0.4983659269254836\n",
      "2022-03-26 19:55:57.230359 Epoch 200, Training Loss 0.49906314128195234\n",
      "2022-03-26 19:55:57.253612 Epoch 200, Training Loss 0.49994591587339826\n",
      "2022-03-26 19:55:57.276355 Epoch 200, Training Loss 0.5007183344467826\n",
      "2022-03-26 19:55:57.311629 Epoch 200, Training Loss 0.501442338042247\n",
      "2022-03-26 19:55:57.343141 Epoch 200, Training Loss 0.502506941404489\n",
      "2022-03-26 19:55:57.366228 Epoch 200, Training Loss 0.5035601755237336\n",
      "2022-03-26 19:55:57.390252 Epoch 200, Training Loss 0.5043909328672892\n",
      "2022-03-26 19:55:57.418349 Epoch 200, Training Loss 0.5052099372724743\n",
      "2022-03-26 19:55:57.442956 Epoch 200, Training Loss 0.5059048541823922\n",
      "2022-03-26 19:55:57.468014 Epoch 200, Training Loss 0.5065249227501852\n",
      "2022-03-26 19:55:57.491634 Epoch 200, Training Loss 0.507486863057022\n",
      "2022-03-26 19:55:57.519123 Epoch 200, Training Loss 0.5081704778744437\n",
      "2022-03-26 19:55:57.551092 Epoch 200, Training Loss 0.5089569943182913\n",
      "2022-03-26 19:55:57.579017 Epoch 200, Training Loss 0.5097296930030178\n",
      "2022-03-26 19:55:57.609637 Epoch 200, Training Loss 0.5104662683004003\n",
      "2022-03-26 19:55:57.633767 Epoch 200, Training Loss 0.511056933523444\n",
      "2022-03-26 19:55:57.657421 Epoch 200, Training Loss 0.511856708037274\n",
      "2022-03-26 19:55:57.680693 Epoch 200, Training Loss 0.5125337218308388\n",
      "2022-03-26 19:55:57.704584 Epoch 200, Training Loss 0.5132358225486467\n",
      "2022-03-26 19:55:57.731793 Epoch 200, Training Loss 0.5140743249136469\n",
      "2022-03-26 19:55:57.759661 Epoch 200, Training Loss 0.5148940192692725\n",
      "2022-03-26 19:55:57.783365 Epoch 200, Training Loss 0.5156965903232774\n",
      "2022-03-26 19:55:57.808603 Epoch 200, Training Loss 0.5163828231718229\n",
      "2022-03-26 19:55:57.832011 Epoch 200, Training Loss 0.517270022379163\n",
      "2022-03-26 19:55:57.860575 Epoch 200, Training Loss 0.5180759495862609\n",
      "2022-03-26 19:55:57.884708 Epoch 200, Training Loss 0.5185949330592095\n",
      "2022-03-26 19:55:57.908476 Epoch 200, Training Loss 0.5193438633628513\n",
      "2022-03-26 19:55:57.932323 Epoch 200, Training Loss 0.5200265429513838\n",
      "2022-03-26 19:55:57.967494 Epoch 200, Training Loss 0.5209691596153142\n",
      "2022-03-26 19:55:57.991467 Epoch 200, Training Loss 0.5219314495468383\n",
      "2022-03-26 19:55:58.014826 Epoch 200, Training Loss 0.52257816276282\n",
      "2022-03-26 19:55:58.042064 Epoch 200, Training Loss 0.5233370640393719\n",
      "2022-03-26 19:55:58.066209 Epoch 200, Training Loss 0.5238746166076806\n",
      "2022-03-26 19:55:58.089219 Epoch 200, Training Loss 0.5248892121302807\n",
      "2022-03-26 19:55:58.112833 Epoch 200, Training Loss 0.5255665128950573\n",
      "2022-03-26 19:55:58.136406 Epoch 200, Training Loss 0.5262491526963461\n",
      "2022-03-26 19:55:58.160970 Epoch 200, Training Loss 0.5270045731226196\n",
      "2022-03-26 19:55:58.193635 Epoch 200, Training Loss 0.5277497628155876\n",
      "2022-03-26 19:55:58.220098 Epoch 200, Training Loss 0.5286827853421117\n",
      "2022-03-26 19:55:58.244504 Epoch 200, Training Loss 0.5294671422227875\n",
      "2022-03-26 19:55:58.271639 Epoch 200, Training Loss 0.5301192263355645\n",
      "2022-03-26 19:55:58.295045 Epoch 200, Training Loss 0.5309502303295428\n",
      "2022-03-26 19:55:58.318100 Epoch 200, Training Loss 0.5316656795913911\n",
      "2022-03-26 19:55:58.355434 Epoch 200, Training Loss 0.5323570084846233\n",
      "2022-03-26 19:55:58.379553 Epoch 200, Training Loss 0.5331938076964424\n",
      "2022-03-26 19:55:58.407239 Epoch 200, Training Loss 0.5337146988991276\n",
      "2022-03-26 19:55:58.433329 Epoch 200, Training Loss 0.5344443158496677\n",
      "2022-03-26 19:55:58.458599 Epoch 200, Training Loss 0.5352220642368507\n",
      "2022-03-26 19:55:58.483707 Epoch 200, Training Loss 0.5359677433434045\n",
      "2022-03-26 19:55:58.508984 Epoch 200, Training Loss 0.5367274749690615\n",
      "2022-03-26 19:55:58.532774 Epoch 200, Training Loss 0.5377411852635996\n",
      "2022-03-26 19:55:58.557356 Epoch 200, Training Loss 0.538586847503167\n",
      "2022-03-26 19:55:58.581666 Epoch 200, Training Loss 0.539313778586095\n",
      "2022-03-26 19:55:58.605293 Epoch 200, Training Loss 0.540212848950225\n",
      "2022-03-26 19:55:58.638847 Epoch 200, Training Loss 0.540964019153734\n",
      "2022-03-26 19:55:58.663664 Epoch 200, Training Loss 0.541919397004425\n",
      "2022-03-26 19:55:58.687404 Epoch 200, Training Loss 0.5428159619155137\n",
      "2022-03-26 19:55:58.715501 Epoch 200, Training Loss 0.5434380247617316\n",
      "2022-03-26 19:55:58.739134 Epoch 200, Training Loss 0.5444643431154969\n",
      "2022-03-26 19:55:58.762785 Epoch 200, Training Loss 0.5451690960875557\n",
      "2022-03-26 19:55:58.786770 Epoch 200, Training Loss 0.5456415534857899\n",
      "2022-03-26 19:55:58.810743 Epoch 200, Training Loss 0.5464016740660533\n",
      "2022-03-26 19:55:58.834684 Epoch 200, Training Loss 0.5475167661447964\n",
      "2022-03-26 19:55:58.867998 Epoch 200, Training Loss 0.5481906826310146\n",
      "2022-03-26 19:55:58.892405 Epoch 200, Training Loss 0.5490542027117956\n",
      "2022-03-26 19:55:58.917165 Epoch 200, Training Loss 0.5497218604054293\n",
      "2022-03-26 19:55:58.947757 Epoch 200, Training Loss 0.5507103376605017\n",
      "2022-03-26 19:55:58.971385 Epoch 200, Training Loss 0.5513325300820343\n",
      "2022-03-26 19:55:58.995268 Epoch 200, Training Loss 0.5523324146142701\n",
      "2022-03-26 19:55:59.018567 Epoch 200, Training Loss 0.5530573855275693\n",
      "2022-03-26 19:55:59.041853 Epoch 200, Training Loss 0.5537915168058537\n",
      "2022-03-26 19:55:59.079645 Epoch 200, Training Loss 0.5547837018204467\n",
      "2022-03-26 19:55:59.105861 Epoch 200, Training Loss 0.5555654003492096\n",
      "2022-03-26 19:55:59.131372 Epoch 200, Training Loss 0.5561619432990813\n",
      "2022-03-26 19:55:59.159914 Epoch 200, Training Loss 0.5569048242648239\n",
      "2022-03-26 19:55:59.183310 Epoch 200, Training Loss 0.5575639935558104\n",
      "2022-03-26 19:55:59.207089 Epoch 200, Training Loss 0.5581887825523191\n",
      "2022-03-26 19:55:59.230839 Epoch 200, Training Loss 0.5588947762461269\n",
      "2022-03-26 19:55:59.255483 Epoch 200, Training Loss 0.5596154731557802\n",
      "2022-03-26 19:55:59.278911 Epoch 200, Training Loss 0.5604361471007852\n",
      "2022-03-26 19:55:59.310128 Epoch 200, Training Loss 0.5612685535569935\n",
      "2022-03-26 19:55:59.335992 Epoch 200, Training Loss 0.5621232734921643\n",
      "2022-03-26 19:55:59.368515 Epoch 200, Training Loss 0.5628580549336455\n",
      "2022-03-26 19:55:59.397392 Epoch 200, Training Loss 0.5636284117351102\n",
      "2022-03-26 19:55:59.421679 Epoch 200, Training Loss 0.5643085233695672\n",
      "2022-03-26 19:55:59.445340 Epoch 200, Training Loss 0.565001732095733\n",
      "2022-03-26 19:55:59.470227 Epoch 200, Training Loss 0.5657827297744848\n",
      "2022-03-26 19:55:59.492840 Epoch 200, Training Loss 0.5664048500530555\n",
      "2022-03-26 19:55:59.518119 Epoch 200, Training Loss 0.5671348779097848\n",
      "2022-03-26 19:55:59.546277 Epoch 200, Training Loss 0.5679585156233414\n",
      "2022-03-26 19:55:59.570055 Epoch 200, Training Loss 0.5686830754780099\n",
      "2022-03-26 19:55:59.593191 Epoch 200, Training Loss 0.5694555203475611\n",
      "2022-03-26 19:55:59.619106 Epoch 200, Training Loss 0.5703284703862027\n",
      "2022-03-26 19:55:59.641866 Epoch 200, Training Loss 0.5709637948466689\n",
      "2022-03-26 19:55:59.665190 Epoch 200, Training Loss 0.5716663524317924\n",
      "2022-03-26 19:55:59.687684 Epoch 200, Training Loss 0.5725822993708999\n",
      "2022-03-26 19:55:59.710474 Epoch 200, Training Loss 0.5734925960640773\n",
      "2022-03-26 19:55:59.739542 Epoch 200, Training Loss 0.574200144966545\n",
      "2022-03-26 19:55:59.766274 Epoch 200, Training Loss 0.5754981786393754\n",
      "2022-03-26 19:55:59.788536 Epoch 200, Training Loss 0.5759254004949194\n",
      "2022-03-26 19:55:59.813357 Epoch 200, Training Loss 0.5765109718074579\n",
      "2022-03-26 19:55:59.836174 Epoch 200, Training Loss 0.577294506265989\n",
      "2022-03-26 19:55:59.859428 Epoch 200, Training Loss 0.5779128751486463\n",
      "2022-03-26 19:55:59.882355 Epoch 200, Training Loss 0.5786561249467113\n",
      "2022-03-26 19:55:59.904601 Epoch 200, Training Loss 0.5792678094199856\n",
      "2022-03-26 19:55:59.927775 Epoch 200, Training Loss 0.5800284770367395\n",
      "2022-03-26 19:55:59.958142 Epoch 200, Training Loss 0.5806626322705423\n",
      "2022-03-26 19:55:59.974167 Epoch 200, Training Loss 0.5812860764277256\n",
      "2022-03-26 20:10:41.080410 Epoch 250, Training Loss 0.0007781342167378692\n",
      "2022-03-26 20:10:41.103617 Epoch 250, Training Loss 0.0014606926904614929\n",
      "2022-03-26 20:10:41.126492 Epoch 250, Training Loss 0.002010460628572937\n",
      "2022-03-26 20:10:41.149466 Epoch 250, Training Loss 0.002835163527437488\n",
      "2022-03-26 20:10:41.172381 Epoch 250, Training Loss 0.003415516117954498\n",
      "2022-03-26 20:10:41.194893 Epoch 250, Training Loss 0.003990785392653911\n",
      "2022-03-26 20:10:41.217929 Epoch 250, Training Loss 0.0045836551110153\n",
      "2022-03-26 20:10:41.240415 Epoch 250, Training Loss 0.005365573689150993\n",
      "2022-03-26 20:10:41.263408 Epoch 250, Training Loss 0.005731689465015441\n",
      "2022-03-26 20:10:41.290048 Epoch 250, Training Loss 0.006359869371289792\n",
      "2022-03-26 20:10:41.317925 Epoch 250, Training Loss 0.006963683706720162\n",
      "2022-03-26 20:10:41.344177 Epoch 250, Training Loss 0.0076298432048324425\n",
      "2022-03-26 20:10:41.367608 Epoch 250, Training Loss 0.008313114342787076\n",
      "2022-03-26 20:10:41.395369 Epoch 250, Training Loss 0.009234804593388686\n",
      "2022-03-26 20:10:41.419830 Epoch 250, Training Loss 0.009873586313803787\n",
      "2022-03-26 20:10:41.442843 Epoch 250, Training Loss 0.01069168285335726\n",
      "2022-03-26 20:10:41.465591 Epoch 250, Training Loss 0.011198805840423955\n",
      "2022-03-26 20:10:41.489585 Epoch 250, Training Loss 0.011918984829922161\n",
      "2022-03-26 20:10:41.521353 Epoch 250, Training Loss 0.012384414749072335\n",
      "2022-03-26 20:10:41.545425 Epoch 250, Training Loss 0.013305628696061157\n",
      "2022-03-26 20:10:41.569100 Epoch 250, Training Loss 0.014031319362123299\n",
      "2022-03-26 20:10:41.595573 Epoch 250, Training Loss 0.014671585672651715\n",
      "2022-03-26 20:10:41.619653 Epoch 250, Training Loss 0.015417883768106056\n",
      "2022-03-26 20:10:41.643217 Epoch 250, Training Loss 0.016127955425730752\n",
      "2022-03-26 20:10:41.666874 Epoch 250, Training Loss 0.01681680904934778\n",
      "2022-03-26 20:10:41.689721 Epoch 250, Training Loss 0.017197071400749712\n",
      "2022-03-26 20:10:41.718709 Epoch 250, Training Loss 0.017943872324645976\n",
      "2022-03-26 20:10:41.755963 Epoch 250, Training Loss 0.018712401504406843\n",
      "2022-03-26 20:10:41.780633 Epoch 250, Training Loss 0.019476797536510944\n",
      "2022-03-26 20:10:41.803857 Epoch 250, Training Loss 0.02027860348639281\n",
      "2022-03-26 20:10:41.828801 Epoch 250, Training Loss 0.020839208074847754\n",
      "2022-03-26 20:10:41.853297 Epoch 250, Training Loss 0.021540647120122106\n",
      "2022-03-26 20:10:41.888873 Epoch 250, Training Loss 0.022249988201634048\n",
      "2022-03-26 20:10:41.911632 Epoch 250, Training Loss 0.023201569648045102\n",
      "2022-03-26 20:10:41.938844 Epoch 250, Training Loss 0.023678023987413976\n",
      "2022-03-26 20:10:41.964265 Epoch 250, Training Loss 0.024417857189312615\n",
      "2022-03-26 20:10:41.995810 Epoch 250, Training Loss 0.025111280629397048\n",
      "2022-03-26 20:10:42.019046 Epoch 250, Training Loss 0.025837013902871506\n",
      "2022-03-26 20:10:42.041619 Epoch 250, Training Loss 0.026467707036706187\n",
      "2022-03-26 20:10:42.068627 Epoch 250, Training Loss 0.027152687196841326\n",
      "2022-03-26 20:10:42.091452 Epoch 250, Training Loss 0.027911008204645513\n",
      "2022-03-26 20:10:42.115650 Epoch 250, Training Loss 0.028579239230936446\n",
      "2022-03-26 20:10:42.139485 Epoch 250, Training Loss 0.029328099563908396\n",
      "2022-03-26 20:10:42.162857 Epoch 250, Training Loss 0.03021826368311177\n",
      "2022-03-26 20:10:42.194840 Epoch 250, Training Loss 0.031052095086678215\n",
      "2022-03-26 20:10:42.219306 Epoch 250, Training Loss 0.03179753844237998\n",
      "2022-03-26 20:10:42.242444 Epoch 250, Training Loss 0.03249024052906524\n",
      "2022-03-26 20:10:42.268898 Epoch 250, Training Loss 0.0331350524940759\n",
      "2022-03-26 20:10:42.292116 Epoch 250, Training Loss 0.0338095163978884\n",
      "2022-03-26 20:10:42.314940 Epoch 250, Training Loss 0.034404663135633444\n",
      "2022-03-26 20:10:42.341602 Epoch 250, Training Loss 0.03503104689938333\n",
      "2022-03-26 20:10:42.366220 Epoch 250, Training Loss 0.03583887855872474\n",
      "2022-03-26 20:10:42.390449 Epoch 250, Training Loss 0.03665397314311903\n",
      "2022-03-26 20:10:42.426246 Epoch 250, Training Loss 0.03746682477881536\n",
      "2022-03-26 20:10:42.450587 Epoch 250, Training Loss 0.0381472362657947\n",
      "2022-03-26 20:10:42.473355 Epoch 250, Training Loss 0.038766719991593715\n",
      "2022-03-26 20:10:42.500051 Epoch 250, Training Loss 0.03912691093619217\n",
      "2022-03-26 20:10:42.521796 Epoch 250, Training Loss 0.03968477371098745\n",
      "2022-03-26 20:10:42.544422 Epoch 250, Training Loss 0.04025866784860411\n",
      "2022-03-26 20:10:42.567067 Epoch 250, Training Loss 0.0409867775333507\n",
      "2022-03-26 20:10:42.590774 Epoch 250, Training Loss 0.04154036946766212\n",
      "2022-03-26 20:10:42.613764 Epoch 250, Training Loss 0.042058730331223336\n",
      "2022-03-26 20:10:42.649169 Epoch 250, Training Loss 0.04273593543892931\n",
      "2022-03-26 20:10:42.677400 Epoch 250, Training Loss 0.0439435536675441\n",
      "2022-03-26 20:10:42.700121 Epoch 250, Training Loss 0.044976997596528526\n",
      "2022-03-26 20:10:42.722447 Epoch 250, Training Loss 0.04555261222755208\n",
      "2022-03-26 20:10:42.756064 Epoch 250, Training Loss 0.04647939333982785\n",
      "2022-03-26 20:10:42.779808 Epoch 250, Training Loss 0.04726219493562303\n",
      "2022-03-26 20:10:42.802398 Epoch 250, Training Loss 0.04771443256331832\n",
      "2022-03-26 20:10:42.825009 Epoch 250, Training Loss 0.04825480895883897\n",
      "2022-03-26 20:10:42.848106 Epoch 250, Training Loss 0.048901885595468\n",
      "2022-03-26 20:10:42.878821 Epoch 250, Training Loss 0.0498761623106954\n",
      "2022-03-26 20:10:42.902131 Epoch 250, Training Loss 0.050580200713003995\n",
      "2022-03-26 20:10:42.924624 Epoch 250, Training Loss 0.0517236040833661\n",
      "2022-03-26 20:10:42.950516 Epoch 250, Training Loss 0.05258043297111531\n",
      "2022-03-26 20:10:42.973207 Epoch 250, Training Loss 0.05324112812576391\n",
      "2022-03-26 20:10:42.995491 Epoch 250, Training Loss 0.05383088708381214\n",
      "2022-03-26 20:10:43.018091 Epoch 250, Training Loss 0.054316674237665924\n",
      "2022-03-26 20:10:43.040925 Epoch 250, Training Loss 0.05485352191626263\n",
      "2022-03-26 20:10:43.065986 Epoch 250, Training Loss 0.055500810294200086\n",
      "2022-03-26 20:10:43.095348 Epoch 250, Training Loss 0.056197690956123036\n",
      "2022-03-26 20:10:43.124783 Epoch 250, Training Loss 0.05695858936937873\n",
      "2022-03-26 20:10:43.147337 Epoch 250, Training Loss 0.05758266036620226\n",
      "2022-03-26 20:10:43.170212 Epoch 250, Training Loss 0.05847398730952417\n",
      "2022-03-26 20:10:43.192614 Epoch 250, Training Loss 0.05920174134814221\n",
      "2022-03-26 20:10:43.221761 Epoch 250, Training Loss 0.0599233824045152\n",
      "2022-03-26 20:10:43.243942 Epoch 250, Training Loss 0.06034242458965467\n",
      "2022-03-26 20:10:43.266377 Epoch 250, Training Loss 0.060918882755977115\n",
      "2022-03-26 20:10:43.288466 Epoch 250, Training Loss 0.06145036761718028\n",
      "2022-03-26 20:10:43.318559 Epoch 250, Training Loss 0.06223298475870391\n",
      "2022-03-26 20:10:43.344260 Epoch 250, Training Loss 0.06277550044267075\n",
      "2022-03-26 20:10:43.367045 Epoch 250, Training Loss 0.06346392219938586\n",
      "2022-03-26 20:10:43.393385 Epoch 250, Training Loss 0.06381619571115049\n",
      "2022-03-26 20:10:43.417954 Epoch 250, Training Loss 0.06453581905121084\n",
      "2022-03-26 20:10:43.444029 Epoch 250, Training Loss 0.06554960617628854\n",
      "2022-03-26 20:10:43.468619 Epoch 250, Training Loss 0.06646248644880016\n",
      "2022-03-26 20:10:43.491972 Epoch 250, Training Loss 0.06717026889171747\n",
      "2022-03-26 20:10:43.515341 Epoch 250, Training Loss 0.06777942184444584\n",
      "2022-03-26 20:10:43.550589 Epoch 250, Training Loss 0.06853214009186191\n",
      "2022-03-26 20:10:43.574407 Epoch 250, Training Loss 0.06912203468477635\n",
      "2022-03-26 20:10:43.597615 Epoch 250, Training Loss 0.06976378272713908\n",
      "2022-03-26 20:10:43.624729 Epoch 250, Training Loss 0.0705452097575073\n",
      "2022-03-26 20:10:43.647786 Epoch 250, Training Loss 0.07106798902496962\n",
      "2022-03-26 20:10:43.671410 Epoch 250, Training Loss 0.0719723400404996\n",
      "2022-03-26 20:10:43.694615 Epoch 250, Training Loss 0.07285505639927467\n",
      "2022-03-26 20:10:43.717932 Epoch 250, Training Loss 0.0736840961839232\n",
      "2022-03-26 20:10:43.741091 Epoch 250, Training Loss 0.07415286651657671\n",
      "2022-03-26 20:10:43.773353 Epoch 250, Training Loss 0.0750452419528571\n",
      "2022-03-26 20:10:43.805513 Epoch 250, Training Loss 0.07586941336426893\n",
      "2022-03-26 20:10:43.828743 Epoch 250, Training Loss 0.07667748222265706\n",
      "2022-03-26 20:10:43.852751 Epoch 250, Training Loss 0.07741017186123392\n",
      "2022-03-26 20:10:43.878666 Epoch 250, Training Loss 0.07811243515795149\n",
      "2022-03-26 20:10:43.901758 Epoch 250, Training Loss 0.07863776423894536\n",
      "2022-03-26 20:10:43.924738 Epoch 250, Training Loss 0.07908781482587995\n",
      "2022-03-26 20:10:43.947816 Epoch 250, Training Loss 0.07954429387284057\n",
      "2022-03-26 20:10:43.981382 Epoch 250, Training Loss 0.08025535487610361\n",
      "2022-03-26 20:10:44.010549 Epoch 250, Training Loss 0.08104467243337266\n",
      "2022-03-26 20:10:44.033775 Epoch 250, Training Loss 0.08157518967185788\n",
      "2022-03-26 20:10:44.056247 Epoch 250, Training Loss 0.08221561650333502\n",
      "2022-03-26 20:10:44.081952 Epoch 250, Training Loss 0.08285008420419815\n",
      "2022-03-26 20:10:44.105997 Epoch 250, Training Loss 0.08348160265656689\n",
      "2022-03-26 20:10:44.132803 Epoch 250, Training Loss 0.08425846543458416\n",
      "2022-03-26 20:10:44.157194 Epoch 250, Training Loss 0.08497988156345494\n",
      "2022-03-26 20:10:44.180108 Epoch 250, Training Loss 0.08555220540069863\n",
      "2022-03-26 20:10:44.211022 Epoch 250, Training Loss 0.08619838156511107\n",
      "2022-03-26 20:10:44.234405 Epoch 250, Training Loss 0.0868593082022484\n",
      "2022-03-26 20:10:44.257659 Epoch 250, Training Loss 0.08734124258656026\n",
      "2022-03-26 20:10:44.286447 Epoch 250, Training Loss 0.0880471140984684\n",
      "2022-03-26 20:10:44.309034 Epoch 250, Training Loss 0.08870296199303454\n",
      "2022-03-26 20:10:44.334521 Epoch 250, Training Loss 0.08915649358268894\n",
      "2022-03-26 20:10:44.358131 Epoch 250, Training Loss 0.08977657343115648\n",
      "2022-03-26 20:10:44.381612 Epoch 250, Training Loss 0.0903298061750734\n",
      "2022-03-26 20:10:44.404647 Epoch 250, Training Loss 0.09099157672861348\n",
      "2022-03-26 20:10:44.439217 Epoch 250, Training Loss 0.09148781199741851\n",
      "2022-03-26 20:10:44.464531 Epoch 250, Training Loss 0.0923235818476933\n",
      "2022-03-26 20:10:44.489361 Epoch 250, Training Loss 0.0929427121377662\n",
      "2022-03-26 20:10:44.517037 Epoch 250, Training Loss 0.09347339534698546\n",
      "2022-03-26 20:10:44.540512 Epoch 250, Training Loss 0.09404127978150496\n",
      "2022-03-26 20:10:44.564269 Epoch 250, Training Loss 0.09456480841350068\n",
      "2022-03-26 20:10:44.587812 Epoch 250, Training Loss 0.09528797594330195\n",
      "2022-03-26 20:10:44.611513 Epoch 250, Training Loss 0.09583417796875204\n",
      "2022-03-26 20:10:44.635052 Epoch 250, Training Loss 0.09639404089097171\n",
      "2022-03-26 20:10:44.668482 Epoch 250, Training Loss 0.09710090616932306\n",
      "2022-03-26 20:10:44.692387 Epoch 250, Training Loss 0.09770069067435497\n",
      "2022-03-26 20:10:44.715495 Epoch 250, Training Loss 0.0981027622661932\n",
      "2022-03-26 20:10:44.743164 Epoch 250, Training Loss 0.09873069064391543\n",
      "2022-03-26 20:10:44.767233 Epoch 250, Training Loss 0.09951159556198608\n",
      "2022-03-26 20:10:44.790815 Epoch 250, Training Loss 0.10018935098367579\n",
      "2022-03-26 20:10:44.827403 Epoch 250, Training Loss 0.1006884659876299\n",
      "2022-03-26 20:10:44.851258 Epoch 250, Training Loss 0.10132135561360117\n",
      "2022-03-26 20:10:44.885817 Epoch 250, Training Loss 0.10174654801483349\n",
      "2022-03-26 20:10:44.910609 Epoch 250, Training Loss 0.10219577389300022\n",
      "2022-03-26 20:10:44.933928 Epoch 250, Training Loss 0.10292675671979899\n",
      "2022-03-26 20:10:44.959099 Epoch 250, Training Loss 0.10360063204679952\n",
      "2022-03-26 20:10:44.985880 Epoch 250, Training Loss 0.10431792768066192\n",
      "2022-03-26 20:10:45.011612 Epoch 250, Training Loss 0.10502762646626329\n",
      "2022-03-26 20:10:45.035133 Epoch 250, Training Loss 0.1058179401528195\n",
      "2022-03-26 20:10:45.058198 Epoch 250, Training Loss 0.10658716858195527\n",
      "2022-03-26 20:10:45.081325 Epoch 250, Training Loss 0.10727180041315611\n",
      "2022-03-26 20:10:45.114810 Epoch 250, Training Loss 0.1081409646422052\n",
      "2022-03-26 20:10:45.138737 Epoch 250, Training Loss 0.1089143160054141\n",
      "2022-03-26 20:10:45.162738 Epoch 250, Training Loss 0.10949730023246287\n",
      "2022-03-26 20:10:45.189419 Epoch 250, Training Loss 0.11030016443156221\n",
      "2022-03-26 20:10:45.213773 Epoch 250, Training Loss 0.1109256448648165\n",
      "2022-03-26 20:10:45.238020 Epoch 250, Training Loss 0.11169496056673776\n",
      "2022-03-26 20:10:45.261020 Epoch 250, Training Loss 0.11226544275765529\n",
      "2022-03-26 20:10:45.285131 Epoch 250, Training Loss 0.11318272554203677\n",
      "2022-03-26 20:10:45.308318 Epoch 250, Training Loss 0.11406692927298338\n",
      "2022-03-26 20:10:45.344464 Epoch 250, Training Loss 0.114677122220054\n",
      "2022-03-26 20:10:45.369095 Epoch 250, Training Loss 0.11531935982844409\n",
      "2022-03-26 20:10:45.392479 Epoch 250, Training Loss 0.11601344913320469\n",
      "2022-03-26 20:10:45.417237 Epoch 250, Training Loss 0.11679202375357109\n",
      "2022-03-26 20:10:45.444865 Epoch 250, Training Loss 0.11740525585153828\n",
      "2022-03-26 20:10:45.468821 Epoch 250, Training Loss 0.11809396092086802\n",
      "2022-03-26 20:10:45.492574 Epoch 250, Training Loss 0.11879127192527741\n",
      "2022-03-26 20:10:45.516980 Epoch 250, Training Loss 0.11938849236349315\n",
      "2022-03-26 20:10:45.549040 Epoch 250, Training Loss 0.12009244600830175\n",
      "2022-03-26 20:10:45.578984 Epoch 250, Training Loss 0.12081421450580783\n",
      "2022-03-26 20:10:45.601816 Epoch 250, Training Loss 0.12138112472451251\n",
      "2022-03-26 20:10:45.625376 Epoch 250, Training Loss 0.12216755968835348\n",
      "2022-03-26 20:10:45.651335 Epoch 250, Training Loss 0.12265467087326147\n",
      "2022-03-26 20:10:45.673458 Epoch 250, Training Loss 0.12308792929972528\n",
      "2022-03-26 20:10:45.695127 Epoch 250, Training Loss 0.12379923230394378\n",
      "2022-03-26 20:10:45.717351 Epoch 250, Training Loss 0.12444706466954078\n",
      "2022-03-26 20:10:45.740545 Epoch 250, Training Loss 0.12512727249461367\n",
      "2022-03-26 20:10:45.772457 Epoch 250, Training Loss 0.12561996448832705\n",
      "2022-03-26 20:10:45.796382 Epoch 250, Training Loss 0.12632191108773128\n",
      "2022-03-26 20:10:45.821292 Epoch 250, Training Loss 0.12679987029193918\n",
      "2022-03-26 20:10:45.855521 Epoch 250, Training Loss 0.12764816016644773\n",
      "2022-03-26 20:10:45.878279 Epoch 250, Training Loss 0.128270533993421\n",
      "2022-03-26 20:10:45.901418 Epoch 250, Training Loss 0.12893647344216055\n",
      "2022-03-26 20:10:45.924494 Epoch 250, Training Loss 0.1295753681217618\n",
      "2022-03-26 20:10:45.947472 Epoch 250, Training Loss 0.13020074645729016\n",
      "2022-03-26 20:10:45.970626 Epoch 250, Training Loss 0.13078715036744656\n",
      "2022-03-26 20:10:46.003391 Epoch 250, Training Loss 0.13165969719819706\n",
      "2022-03-26 20:10:46.026312 Epoch 250, Training Loss 0.13213997450478546\n",
      "2022-03-26 20:10:46.049383 Epoch 250, Training Loss 0.13274862680136396\n",
      "2022-03-26 20:10:46.076597 Epoch 250, Training Loss 0.13345886599224852\n",
      "2022-03-26 20:10:46.105656 Epoch 250, Training Loss 0.13449930912240995\n",
      "2022-03-26 20:10:46.129620 Epoch 250, Training Loss 0.1349347911756057\n",
      "2022-03-26 20:10:46.153594 Epoch 250, Training Loss 0.1355470656357763\n",
      "2022-03-26 20:10:46.178736 Epoch 250, Training Loss 0.1362263629274905\n",
      "2022-03-26 20:10:46.202034 Epoch 250, Training Loss 0.13716893027658048\n",
      "2022-03-26 20:10:46.233511 Epoch 250, Training Loss 0.1378760843554421\n",
      "2022-03-26 20:10:46.256445 Epoch 250, Training Loss 0.13862963485748261\n",
      "2022-03-26 20:10:46.279404 Epoch 250, Training Loss 0.1393058343845255\n",
      "2022-03-26 20:10:46.306060 Epoch 250, Training Loss 0.13986028509829052\n",
      "2022-03-26 20:10:46.330379 Epoch 250, Training Loss 0.14055061702380706\n",
      "2022-03-26 20:10:46.352979 Epoch 250, Training Loss 0.14141785721187397\n",
      "2022-03-26 20:10:46.376382 Epoch 250, Training Loss 0.14217016241891914\n",
      "2022-03-26 20:10:46.399662 Epoch 250, Training Loss 0.1429405391902265\n",
      "2022-03-26 20:10:46.423206 Epoch 250, Training Loss 0.1435222118483175\n",
      "2022-03-26 20:10:46.454878 Epoch 250, Training Loss 0.14428847521314841\n",
      "2022-03-26 20:10:46.480049 Epoch 250, Training Loss 0.14492453581384382\n",
      "2022-03-26 20:10:46.502231 Epoch 250, Training Loss 0.1455878050202299\n",
      "2022-03-26 20:10:46.525540 Epoch 250, Training Loss 0.14634757010680635\n",
      "2022-03-26 20:10:46.549777 Epoch 250, Training Loss 0.14689124815756707\n",
      "2022-03-26 20:10:46.576242 Epoch 250, Training Loss 0.1475396649078335\n",
      "2022-03-26 20:10:46.599098 Epoch 250, Training Loss 0.14817786026183907\n",
      "2022-03-26 20:10:46.623023 Epoch 250, Training Loss 0.1488330753715447\n",
      "2022-03-26 20:10:46.646407 Epoch 250, Training Loss 0.14963377726352428\n",
      "2022-03-26 20:10:46.675563 Epoch 250, Training Loss 0.15034204370835247\n",
      "2022-03-26 20:10:46.698975 Epoch 250, Training Loss 0.15081371919578299\n",
      "2022-03-26 20:10:46.721228 Epoch 250, Training Loss 0.1516666754584788\n",
      "2022-03-26 20:10:46.747018 Epoch 250, Training Loss 0.15230868653873045\n",
      "2022-03-26 20:10:46.769980 Epoch 250, Training Loss 0.15293441160255686\n",
      "2022-03-26 20:10:46.792967 Epoch 250, Training Loss 0.1533458638374153\n",
      "2022-03-26 20:10:46.815809 Epoch 250, Training Loss 0.1540471258980539\n",
      "2022-03-26 20:10:46.839211 Epoch 250, Training Loss 0.15461513670661564\n",
      "2022-03-26 20:10:46.873926 Epoch 250, Training Loss 0.15514770169239825\n",
      "2022-03-26 20:10:46.908902 Epoch 250, Training Loss 0.1559163319409046\n",
      "2022-03-26 20:10:46.932814 Epoch 250, Training Loss 0.1566753120297361\n",
      "2022-03-26 20:10:46.956062 Epoch 250, Training Loss 0.15732762312797635\n",
      "2022-03-26 20:10:46.983314 Epoch 250, Training Loss 0.15795361645081463\n",
      "2022-03-26 20:10:47.011277 Epoch 250, Training Loss 0.15852944183227657\n",
      "2022-03-26 20:10:47.035455 Epoch 250, Training Loss 0.15931036572931978\n",
      "2022-03-26 20:10:47.059076 Epoch 250, Training Loss 0.1598811636266806\n",
      "2022-03-26 20:10:47.082529 Epoch 250, Training Loss 0.16047466532958438\n",
      "2022-03-26 20:10:47.106477 Epoch 250, Training Loss 0.16096832132552896\n",
      "2022-03-26 20:10:47.140122 Epoch 250, Training Loss 0.16177215215647617\n",
      "2022-03-26 20:10:47.164211 Epoch 250, Training Loss 0.16225777501645294\n",
      "2022-03-26 20:10:47.187705 Epoch 250, Training Loss 0.16293897805616375\n",
      "2022-03-26 20:10:47.214449 Epoch 250, Training Loss 0.16350351197792745\n",
      "2022-03-26 20:10:47.237987 Epoch 250, Training Loss 0.16423839143932323\n",
      "2022-03-26 20:10:47.261174 Epoch 250, Training Loss 0.165084577544266\n",
      "2022-03-26 20:10:47.284226 Epoch 250, Training Loss 0.1659868205218669\n",
      "2022-03-26 20:10:47.307670 Epoch 250, Training Loss 0.16681337185070644\n",
      "2022-03-26 20:10:47.332433 Epoch 250, Training Loss 0.16785150289992848\n",
      "2022-03-26 20:10:47.363915 Epoch 250, Training Loss 0.16830687022879912\n",
      "2022-03-26 20:10:47.389014 Epoch 250, Training Loss 0.16906908649922636\n",
      "2022-03-26 20:10:47.412788 Epoch 250, Training Loss 0.16992442306045377\n",
      "2022-03-26 20:10:47.440807 Epoch 250, Training Loss 0.1708110287366316\n",
      "2022-03-26 20:10:47.464154 Epoch 250, Training Loss 0.1714675630754827\n",
      "2022-03-26 20:10:47.487693 Epoch 250, Training Loss 0.172094008578059\n",
      "2022-03-26 20:10:47.510967 Epoch 250, Training Loss 0.17291763272431807\n",
      "2022-03-26 20:10:47.534507 Epoch 250, Training Loss 0.17351332451681348\n",
      "2022-03-26 20:10:47.558646 Epoch 250, Training Loss 0.17439895395732596\n",
      "2022-03-26 20:10:47.592187 Epoch 250, Training Loss 0.1749789773122124\n",
      "2022-03-26 20:10:47.616613 Epoch 250, Training Loss 0.175687494523385\n",
      "2022-03-26 20:10:47.640336 Epoch 250, Training Loss 0.1762265750514272\n",
      "2022-03-26 20:10:47.667304 Epoch 250, Training Loss 0.17665408692701393\n",
      "2022-03-26 20:10:47.691150 Epoch 250, Training Loss 0.1772477297526796\n",
      "2022-03-26 20:10:47.714406 Epoch 250, Training Loss 0.17819891164979668\n",
      "2022-03-26 20:10:47.737561 Epoch 250, Training Loss 0.17886848728675062\n",
      "2022-03-26 20:10:47.760855 Epoch 250, Training Loss 0.17941239739165588\n",
      "2022-03-26 20:10:47.784033 Epoch 250, Training Loss 0.18010136671840687\n",
      "2022-03-26 20:10:47.815478 Epoch 250, Training Loss 0.18071254527629793\n",
      "2022-03-26 20:10:47.840110 Epoch 250, Training Loss 0.18165114464814705\n",
      "2022-03-26 20:10:47.863745 Epoch 250, Training Loss 0.18227648186256817\n",
      "2022-03-26 20:10:47.907406 Epoch 250, Training Loss 0.18292377077405106\n",
      "2022-03-26 20:10:47.931720 Epoch 250, Training Loss 0.18363765179348723\n",
      "2022-03-26 20:10:47.955516 Epoch 250, Training Loss 0.18417879638007228\n",
      "2022-03-26 20:10:47.984583 Epoch 250, Training Loss 0.18491751782577057\n",
      "2022-03-26 20:10:48.010311 Epoch 250, Training Loss 0.18555717261703422\n",
      "2022-03-26 20:10:48.041873 Epoch 250, Training Loss 0.1863856246632993\n",
      "2022-03-26 20:10:48.065066 Epoch 250, Training Loss 0.18706113141973305\n",
      "2022-03-26 20:10:48.088887 Epoch 250, Training Loss 0.18788309128540556\n",
      "2022-03-26 20:10:48.116359 Epoch 250, Training Loss 0.1886257683605794\n",
      "2022-03-26 20:10:48.139878 Epoch 250, Training Loss 0.18961960911903236\n",
      "2022-03-26 20:10:48.163514 Epoch 250, Training Loss 0.19011873574665442\n",
      "2022-03-26 20:10:48.186753 Epoch 250, Training Loss 0.19070066580229708\n",
      "2022-03-26 20:10:48.211038 Epoch 250, Training Loss 0.19198450106946405\n",
      "2022-03-26 20:10:48.234439 Epoch 250, Training Loss 0.1926160907501455\n",
      "2022-03-26 20:10:48.266659 Epoch 250, Training Loss 0.1935545542203557\n",
      "2022-03-26 20:10:48.291301 Epoch 250, Training Loss 0.1942805248453184\n",
      "2022-03-26 20:10:48.314567 Epoch 250, Training Loss 0.19476987695907388\n",
      "2022-03-26 20:10:48.341147 Epoch 250, Training Loss 0.19537680975311553\n",
      "2022-03-26 20:10:48.367602 Epoch 250, Training Loss 0.19611182435394248\n",
      "2022-03-26 20:10:48.391025 Epoch 250, Training Loss 0.196964439681119\n",
      "2022-03-26 20:10:48.414205 Epoch 250, Training Loss 0.19780412331566483\n",
      "2022-03-26 20:10:48.439305 Epoch 250, Training Loss 0.19879933810599928\n",
      "2022-03-26 20:10:48.463927 Epoch 250, Training Loss 0.19975492358207703\n",
      "2022-03-26 20:10:48.497408 Epoch 250, Training Loss 0.20036067594500148\n",
      "2022-03-26 20:10:48.522099 Epoch 250, Training Loss 0.20100883461172928\n",
      "2022-03-26 20:10:48.546343 Epoch 250, Training Loss 0.20186969131002647\n",
      "2022-03-26 20:10:48.570218 Epoch 250, Training Loss 0.2025958665115449\n",
      "2022-03-26 20:10:48.598972 Epoch 250, Training Loss 0.20306131232272634\n",
      "2022-03-26 20:10:48.623217 Epoch 250, Training Loss 0.20400693330465985\n",
      "2022-03-26 20:10:48.647127 Epoch 250, Training Loss 0.2047426963934813\n",
      "2022-03-26 20:10:48.670744 Epoch 250, Training Loss 0.2055691016070983\n",
      "2022-03-26 20:10:48.693465 Epoch 250, Training Loss 0.20624998955013196\n",
      "2022-03-26 20:10:48.727027 Epoch 250, Training Loss 0.2070570196718206\n",
      "2022-03-26 20:10:48.751294 Epoch 250, Training Loss 0.20763597635509412\n",
      "2022-03-26 20:10:48.776338 Epoch 250, Training Loss 0.20835159741856557\n",
      "2022-03-26 20:10:48.801586 Epoch 250, Training Loss 0.20930763663690719\n",
      "2022-03-26 20:10:48.824434 Epoch 250, Training Loss 0.20983809160302058\n",
      "2022-03-26 20:10:48.848181 Epoch 250, Training Loss 0.21042264452980608\n",
      "2022-03-26 20:10:48.871920 Epoch 250, Training Loss 0.21101754153018717\n",
      "2022-03-26 20:10:48.894992 Epoch 250, Training Loss 0.21186791322267878\n",
      "2022-03-26 20:10:48.926397 Epoch 250, Training Loss 0.21267605929271036\n",
      "2022-03-26 20:10:48.959411 Epoch 250, Training Loss 0.21319245297433165\n",
      "2022-03-26 20:10:48.982771 Epoch 250, Training Loss 0.21377825493093036\n",
      "2022-03-26 20:10:49.005443 Epoch 250, Training Loss 0.21459049138876482\n",
      "2022-03-26 20:10:49.031816 Epoch 250, Training Loss 0.2151577085485239\n",
      "2022-03-26 20:10:49.054467 Epoch 250, Training Loss 0.21569202131475024\n",
      "2022-03-26 20:10:49.077059 Epoch 250, Training Loss 0.21619273192437408\n",
      "2022-03-26 20:10:49.100028 Epoch 250, Training Loss 0.2168791355836727\n",
      "2022-03-26 20:10:49.127603 Epoch 250, Training Loss 0.21755344216780895\n",
      "2022-03-26 20:10:49.150327 Epoch 250, Training Loss 0.21816140005503165\n",
      "2022-03-26 20:10:49.181277 Epoch 250, Training Loss 0.21859274202448023\n",
      "2022-03-26 20:10:49.211939 Epoch 250, Training Loss 0.2192871324775164\n",
      "2022-03-26 20:10:49.236172 Epoch 250, Training Loss 0.2199399464804193\n",
      "2022-03-26 20:10:49.260398 Epoch 250, Training Loss 0.22071596336029375\n",
      "2022-03-26 20:10:49.288510 Epoch 250, Training Loss 0.22160676948707123\n",
      "2022-03-26 20:10:49.312715 Epoch 250, Training Loss 0.22222743707392223\n",
      "2022-03-26 20:10:49.339806 Epoch 250, Training Loss 0.22297752562843626\n",
      "2022-03-26 20:10:49.363553 Epoch 250, Training Loss 0.22377176034023694\n",
      "2022-03-26 20:10:49.392090 Epoch 250, Training Loss 0.2242509762344458\n",
      "2022-03-26 20:10:49.421050 Epoch 250, Training Loss 0.2248797850383212\n",
      "2022-03-26 20:10:49.445388 Epoch 250, Training Loss 0.22567635599304647\n",
      "2022-03-26 20:10:49.468052 Epoch 250, Training Loss 0.22636006844927892\n",
      "2022-03-26 20:10:49.495076 Epoch 250, Training Loss 0.22714127093324882\n",
      "2022-03-26 20:10:49.518650 Epoch 250, Training Loss 0.22793810545940837\n",
      "2022-03-26 20:10:49.541815 Epoch 250, Training Loss 0.22870325340944178\n",
      "2022-03-26 20:10:49.565145 Epoch 250, Training Loss 0.2294471627458587\n",
      "2022-03-26 20:10:49.588281 Epoch 250, Training Loss 0.23040319350369445\n",
      "2022-03-26 20:10:49.617446 Epoch 250, Training Loss 0.2313681945319066\n",
      "2022-03-26 20:10:49.643563 Epoch 250, Training Loss 0.23211402043966992\n",
      "2022-03-26 20:10:49.667007 Epoch 250, Training Loss 0.23299929850241718\n",
      "2022-03-26 20:10:49.692362 Epoch 250, Training Loss 0.2337069654708628\n",
      "2022-03-26 20:10:49.716698 Epoch 250, Training Loss 0.23455772802348027\n",
      "2022-03-26 20:10:49.740440 Epoch 250, Training Loss 0.235426504593676\n",
      "2022-03-26 20:10:49.763480 Epoch 250, Training Loss 0.23619712786296446\n",
      "2022-03-26 20:10:49.786878 Epoch 250, Training Loss 0.23717388144844329\n",
      "2022-03-26 20:10:49.809605 Epoch 250, Training Loss 0.23785487366149494\n",
      "2022-03-26 20:10:49.839104 Epoch 250, Training Loss 0.2383286497934395\n",
      "2022-03-26 20:10:49.863516 Epoch 250, Training Loss 0.2390155537658945\n",
      "2022-03-26 20:10:49.887309 Epoch 250, Training Loss 0.2396067385478398\n",
      "2022-03-26 20:10:49.911111 Epoch 250, Training Loss 0.24055636508385544\n",
      "2022-03-26 20:10:49.939053 Epoch 250, Training Loss 0.24143465133884068\n",
      "2022-03-26 20:10:49.969573 Epoch 250, Training Loss 0.242165872431777\n",
      "2022-03-26 20:10:49.994082 Epoch 250, Training Loss 0.24288755685777005\n",
      "2022-03-26 20:10:50.023426 Epoch 250, Training Loss 0.24361180893295561\n",
      "2022-03-26 20:10:50.051048 Epoch 250, Training Loss 0.2442657478782527\n",
      "2022-03-26 20:10:50.079232 Epoch 250, Training Loss 0.24501797221505733\n",
      "2022-03-26 20:10:50.103481 Epoch 250, Training Loss 0.2456607242374469\n",
      "2022-03-26 20:10:50.127566 Epoch 250, Training Loss 0.24623985081682426\n",
      "2022-03-26 20:10:50.156388 Epoch 250, Training Loss 0.24702459215508094\n",
      "2022-03-26 20:10:50.180068 Epoch 250, Training Loss 0.24761591081881462\n",
      "2022-03-26 20:10:50.202955 Epoch 250, Training Loss 0.24831922016942592\n",
      "2022-03-26 20:10:50.226147 Epoch 250, Training Loss 0.24902841765100084\n",
      "2022-03-26 20:10:50.248671 Epoch 250, Training Loss 0.2495376006950198\n",
      "2022-03-26 20:10:50.279003 Epoch 250, Training Loss 0.25014335755496986\n",
      "2022-03-26 20:10:50.303175 Epoch 250, Training Loss 0.2505875107501169\n",
      "2022-03-26 20:10:50.328145 Epoch 250, Training Loss 0.25141679825227886\n",
      "2022-03-26 20:10:50.354309 Epoch 250, Training Loss 0.25204060605877193\n",
      "2022-03-26 20:10:50.377303 Epoch 250, Training Loss 0.25265013104509515\n",
      "2022-03-26 20:10:50.401116 Epoch 250, Training Loss 0.2534323141855352\n",
      "2022-03-26 20:10:50.424695 Epoch 250, Training Loss 0.25412565858467767\n",
      "2022-03-26 20:10:50.449399 Epoch 250, Training Loss 0.2547046251385413\n",
      "2022-03-26 20:10:50.472245 Epoch 250, Training Loss 0.25553982401900277\n",
      "2022-03-26 20:10:50.504978 Epoch 250, Training Loss 0.2561285182490678\n",
      "2022-03-26 20:10:50.529532 Epoch 250, Training Loss 0.25672974172608015\n",
      "2022-03-26 20:10:50.553212 Epoch 250, Training Loss 0.25745084191984535\n",
      "2022-03-26 20:10:50.576937 Epoch 250, Training Loss 0.25817461525235336\n",
      "2022-03-26 20:10:50.604879 Epoch 250, Training Loss 0.25880785861893385\n",
      "2022-03-26 20:10:50.629329 Epoch 250, Training Loss 0.2597605439708056\n",
      "2022-03-26 20:10:50.652263 Epoch 250, Training Loss 0.2603396464644186\n",
      "2022-03-26 20:10:50.676198 Epoch 250, Training Loss 0.2610269265101694\n",
      "2022-03-26 20:10:50.700046 Epoch 250, Training Loss 0.2616409929207219\n",
      "2022-03-26 20:10:50.732418 Epoch 250, Training Loss 0.26252978095008284\n",
      "2022-03-26 20:10:50.756799 Epoch 250, Training Loss 0.26314743370046395\n",
      "2022-03-26 20:10:50.779620 Epoch 250, Training Loss 0.2638787432094974\n",
      "2022-03-26 20:10:50.802346 Epoch 250, Training Loss 0.2644332300137986\n",
      "2022-03-26 20:10:50.826570 Epoch 250, Training Loss 0.26506976867118454\n",
      "2022-03-26 20:10:50.849774 Epoch 250, Training Loss 0.265631920846222\n",
      "2022-03-26 20:10:50.872759 Epoch 250, Training Loss 0.26642532841019007\n",
      "2022-03-26 20:10:50.896694 Epoch 250, Training Loss 0.2671263714885468\n",
      "2022-03-26 20:10:50.923605 Epoch 250, Training Loss 0.2677343126453097\n",
      "2022-03-26 20:10:50.956922 Epoch 250, Training Loss 0.268358716841244\n",
      "2022-03-26 20:10:50.992501 Epoch 250, Training Loss 0.2691086745246902\n",
      "2022-03-26 20:10:51.016293 Epoch 250, Training Loss 0.269799724068788\n",
      "2022-03-26 20:10:51.044597 Epoch 250, Training Loss 0.27040280748511214\n",
      "2022-03-26 20:10:51.068193 Epoch 250, Training Loss 0.2710207946922468\n",
      "2022-03-26 20:10:51.092317 Epoch 250, Training Loss 0.27178970642406924\n",
      "2022-03-26 20:10:51.116226 Epoch 250, Training Loss 0.2724768635257126\n",
      "2022-03-26 20:10:51.140090 Epoch 250, Training Loss 0.27309065939063004\n",
      "2022-03-26 20:10:51.168226 Epoch 250, Training Loss 0.27373868916803\n",
      "2022-03-26 20:10:51.194906 Epoch 250, Training Loss 0.2742658288734953\n",
      "2022-03-26 20:10:51.218651 Epoch 250, Training Loss 0.2750107508791072\n",
      "2022-03-26 20:10:51.241844 Epoch 250, Training Loss 0.2758119265594141\n",
      "2022-03-26 20:10:51.265352 Epoch 250, Training Loss 0.27653398827823533\n",
      "2022-03-26 20:10:51.296632 Epoch 250, Training Loss 0.27738298510041687\n",
      "2022-03-26 20:10:51.319788 Epoch 250, Training Loss 0.2784050119197582\n",
      "2022-03-26 20:10:51.346471 Epoch 250, Training Loss 0.2791539217200121\n",
      "2022-03-26 20:10:51.370197 Epoch 250, Training Loss 0.2799912427392457\n",
      "2022-03-26 20:10:51.403439 Epoch 250, Training Loss 0.2806635575221323\n",
      "2022-03-26 20:10:51.426529 Epoch 250, Training Loss 0.2813865173503261\n",
      "2022-03-26 20:10:51.451132 Epoch 250, Training Loss 0.28189623538795333\n",
      "2022-03-26 20:10:51.478232 Epoch 250, Training Loss 0.2827084801538521\n",
      "2022-03-26 20:10:51.500632 Epoch 250, Training Loss 0.28362786823221486\n",
      "2022-03-26 20:10:51.523681 Epoch 250, Training Loss 0.2843814472408246\n",
      "2022-03-26 20:10:51.545929 Epoch 250, Training Loss 0.2850868301775754\n",
      "2022-03-26 20:10:51.568349 Epoch 250, Training Loss 0.2859264443749967\n",
      "2022-03-26 20:10:51.598083 Epoch 250, Training Loss 0.28646997577698946\n",
      "2022-03-26 20:10:51.622105 Epoch 250, Training Loss 0.28730833423716945\n",
      "2022-03-26 20:10:51.645146 Epoch 250, Training Loss 0.2878448927722624\n",
      "2022-03-26 20:10:51.668255 Epoch 250, Training Loss 0.2884811144274519\n",
      "2022-03-26 20:10:51.693190 Epoch 250, Training Loss 0.28917735506354086\n",
      "2022-03-26 20:10:51.716028 Epoch 250, Training Loss 0.2900570267073029\n",
      "2022-03-26 20:10:51.739756 Epoch 250, Training Loss 0.2905814136995379\n",
      "2022-03-26 20:10:51.762285 Epoch 250, Training Loss 0.29138353741382395\n",
      "2022-03-26 20:10:51.785233 Epoch 250, Training Loss 0.2919774207160296\n",
      "2022-03-26 20:10:51.814947 Epoch 250, Training Loss 0.29255938884394855\n",
      "2022-03-26 20:10:51.839705 Epoch 250, Training Loss 0.29335204952055843\n",
      "2022-03-26 20:10:51.862296 Epoch 250, Training Loss 0.2939784875153885\n",
      "2022-03-26 20:10:51.885393 Epoch 250, Training Loss 0.2947165265565028\n",
      "2022-03-26 20:10:51.911483 Epoch 250, Training Loss 0.2953050940695321\n",
      "2022-03-26 20:10:51.934163 Epoch 250, Training Loss 0.29614022762879083\n",
      "2022-03-26 20:10:51.957652 Epoch 250, Training Loss 0.29689108067766173\n",
      "2022-03-26 20:10:51.980620 Epoch 250, Training Loss 0.29747831196431307\n",
      "2022-03-26 20:10:52.014887 Epoch 250, Training Loss 0.29802932683616645\n",
      "2022-03-26 20:10:52.046709 Epoch 250, Training Loss 0.2988696103281987\n",
      "2022-03-26 20:10:52.069992 Epoch 250, Training Loss 0.2993555006087589\n",
      "2022-03-26 20:10:52.093042 Epoch 250, Training Loss 0.30004583832705417\n",
      "2022-03-26 20:10:52.123443 Epoch 250, Training Loss 0.3006895802286275\n",
      "2022-03-26 20:10:52.146799 Epoch 250, Training Loss 0.3013340658162866\n",
      "2022-03-26 20:10:52.169639 Epoch 250, Training Loss 0.3021852963644525\n",
      "2022-03-26 20:10:52.193295 Epoch 250, Training Loss 0.30298271634237234\n",
      "2022-03-26 20:10:52.219753 Epoch 250, Training Loss 0.30374807153668854\n",
      "2022-03-26 20:10:52.242855 Epoch 250, Training Loss 0.3043558893301298\n",
      "2022-03-26 20:10:52.275948 Epoch 250, Training Loss 0.30537993279869297\n",
      "2022-03-26 20:10:52.300131 Epoch 250, Training Loss 0.3060623392119737\n",
      "2022-03-26 20:10:52.322947 Epoch 250, Training Loss 0.30693621739097265\n",
      "2022-03-26 20:10:52.348056 Epoch 250, Training Loss 0.30754225550557646\n",
      "2022-03-26 20:10:52.376448 Epoch 250, Training Loss 0.30848195954509405\n",
      "2022-03-26 20:10:52.400469 Epoch 250, Training Loss 0.30936090251826265\n",
      "2022-03-26 20:10:52.424676 Epoch 250, Training Loss 0.3100302894325817\n",
      "2022-03-26 20:10:52.449126 Epoch 250, Training Loss 0.31069730862479683\n",
      "2022-03-26 20:10:52.472304 Epoch 250, Training Loss 0.3114472810188523\n",
      "2022-03-26 20:10:52.504763 Epoch 250, Training Loss 0.3121719508600967\n",
      "2022-03-26 20:10:52.529464 Epoch 250, Training Loss 0.312898672640781\n",
      "2022-03-26 20:10:52.552681 Epoch 250, Training Loss 0.3134784990030786\n",
      "2022-03-26 20:10:52.575317 Epoch 250, Training Loss 0.31416294950505963\n",
      "2022-03-26 20:10:52.601399 Epoch 250, Training Loss 0.3147814149594368\n",
      "2022-03-26 20:10:52.624093 Epoch 250, Training Loss 0.31548743807446317\n",
      "2022-03-26 20:10:52.647200 Epoch 250, Training Loss 0.31601725000402203\n",
      "2022-03-26 20:10:52.670418 Epoch 250, Training Loss 0.31679916217961274\n",
      "2022-03-26 20:10:52.693545 Epoch 250, Training Loss 0.31749685291591506\n",
      "2022-03-26 20:10:52.723018 Epoch 250, Training Loss 0.31865326511433056\n",
      "2022-03-26 20:10:52.750582 Epoch 250, Training Loss 0.3193293906310025\n",
      "2022-03-26 20:10:52.773977 Epoch 250, Training Loss 0.3199068288059186\n",
      "2022-03-26 20:10:52.797979 Epoch 250, Training Loss 0.32075047386271877\n",
      "2022-03-26 20:10:52.824857 Epoch 250, Training Loss 0.32135347396973757\n",
      "2022-03-26 20:10:52.848065 Epoch 250, Training Loss 0.32225613692379973\n",
      "2022-03-26 20:10:52.870825 Epoch 250, Training Loss 0.32281980189063664\n",
      "2022-03-26 20:10:52.893770 Epoch 250, Training Loss 0.323408778023232\n",
      "2022-03-26 20:10:52.916339 Epoch 250, Training Loss 0.324026465454065\n",
      "2022-03-26 20:10:52.947044 Epoch 250, Training Loss 0.32448565975174576\n",
      "2022-03-26 20:10:52.970140 Epoch 250, Training Loss 0.32532839305565486\n",
      "2022-03-26 20:10:52.993400 Epoch 250, Training Loss 0.326061604577867\n",
      "2022-03-26 20:10:53.026879 Epoch 250, Training Loss 0.32685653571887396\n",
      "2022-03-26 20:10:53.057426 Epoch 250, Training Loss 0.3274605152628306\n",
      "2022-03-26 20:10:53.080948 Epoch 250, Training Loss 0.3281470500218594\n",
      "2022-03-26 20:10:53.107951 Epoch 250, Training Loss 0.32859270507112487\n",
      "2022-03-26 20:10:53.130825 Epoch 250, Training Loss 0.3296080016723984\n",
      "2022-03-26 20:10:53.163374 Epoch 250, Training Loss 0.33010450851581896\n",
      "2022-03-26 20:10:53.189177 Epoch 250, Training Loss 0.33070276384158515\n",
      "2022-03-26 20:10:53.211745 Epoch 250, Training Loss 0.33140948750173954\n",
      "2022-03-26 20:10:53.234220 Epoch 250, Training Loss 0.33225994989695146\n",
      "2022-03-26 20:10:53.262108 Epoch 250, Training Loss 0.33308367152957963\n",
      "2022-03-26 20:10:53.285605 Epoch 250, Training Loss 0.33376523715150935\n",
      "2022-03-26 20:10:53.308346 Epoch 250, Training Loss 0.3344184997898843\n",
      "2022-03-26 20:10:53.334880 Epoch 250, Training Loss 0.33507925424429463\n",
      "2022-03-26 20:10:53.358716 Epoch 250, Training Loss 0.3356775733287377\n",
      "2022-03-26 20:10:53.391492 Epoch 250, Training Loss 0.3364633613306543\n",
      "2022-03-26 20:10:53.415690 Epoch 250, Training Loss 0.3371417233172585\n",
      "2022-03-26 20:10:53.440518 Epoch 250, Training Loss 0.3378555994585652\n",
      "2022-03-26 20:10:53.468742 Epoch 250, Training Loss 0.33863267237725464\n",
      "2022-03-26 20:10:53.491414 Epoch 250, Training Loss 0.3391781625006815\n",
      "2022-03-26 20:10:53.513568 Epoch 250, Training Loss 0.3400870499479801\n",
      "2022-03-26 20:10:53.542030 Epoch 250, Training Loss 0.3409203589343659\n",
      "2022-03-26 20:10:53.564554 Epoch 250, Training Loss 0.3417459440124614\n",
      "2022-03-26 20:10:53.587014 Epoch 250, Training Loss 0.34229105657628733\n",
      "2022-03-26 20:10:53.619134 Epoch 250, Training Loss 0.3428637514943662\n",
      "2022-03-26 20:10:53.643539 Epoch 250, Training Loss 0.3435299123644524\n",
      "2022-03-26 20:10:53.666041 Epoch 250, Training Loss 0.34412238859307126\n",
      "2022-03-26 20:10:53.690681 Epoch 250, Training Loss 0.34472815513305954\n",
      "2022-03-26 20:10:53.713487 Epoch 250, Training Loss 0.3454140406435408\n",
      "2022-03-26 20:10:53.736890 Epoch 250, Training Loss 0.346255920655892\n",
      "2022-03-26 20:10:53.760045 Epoch 250, Training Loss 0.34691317657680465\n",
      "2022-03-26 20:10:53.783285 Epoch 250, Training Loss 0.34759874813392033\n",
      "2022-03-26 20:10:53.806562 Epoch 250, Training Loss 0.3484148651437686\n",
      "2022-03-26 20:10:53.836841 Epoch 250, Training Loss 0.3491639643526443\n",
      "2022-03-26 20:10:53.863349 Epoch 250, Training Loss 0.34969529296126206\n",
      "2022-03-26 20:10:53.886285 Epoch 250, Training Loss 0.35022951902635874\n",
      "2022-03-26 20:10:53.910357 Epoch 250, Training Loss 0.351003128549327\n",
      "2022-03-26 20:10:53.938587 Epoch 250, Training Loss 0.35149656045619787\n",
      "2022-03-26 20:10:53.960948 Epoch 250, Training Loss 0.3521394832893406\n",
      "2022-03-26 20:10:53.988017 Epoch 250, Training Loss 0.35290975236069516\n",
      "2022-03-26 20:10:54.011577 Epoch 250, Training Loss 0.3535660764826533\n",
      "2022-03-26 20:10:54.035502 Epoch 250, Training Loss 0.3544195485313225\n",
      "2022-03-26 20:10:54.077645 Epoch 250, Training Loss 0.3552587814724354\n",
      "2022-03-26 20:10:54.109153 Epoch 250, Training Loss 0.3561724868348187\n",
      "2022-03-26 20:10:54.134314 Epoch 250, Training Loss 0.35681058451190323\n",
      "2022-03-26 20:10:54.157805 Epoch 250, Training Loss 0.35742674524064566\n",
      "2022-03-26 20:10:54.184254 Epoch 250, Training Loss 0.3579085997074766\n",
      "2022-03-26 20:10:54.208613 Epoch 250, Training Loss 0.3583435397547529\n",
      "2022-03-26 20:10:54.232580 Epoch 250, Training Loss 0.3590016802177405\n",
      "2022-03-26 20:10:54.256026 Epoch 250, Training Loss 0.35990565546485775\n",
      "2022-03-26 20:10:54.279926 Epoch 250, Training Loss 0.36078058876802244\n",
      "2022-03-26 20:10:54.311558 Epoch 250, Training Loss 0.36154849621493496\n",
      "2022-03-26 20:10:54.337779 Epoch 250, Training Loss 0.3623812401767277\n",
      "2022-03-26 20:10:54.365648 Epoch 250, Training Loss 0.363145193213697\n",
      "2022-03-26 20:10:54.389930 Epoch 250, Training Loss 0.36397654607015495\n",
      "2022-03-26 20:10:54.414110 Epoch 250, Training Loss 0.36462682596100565\n",
      "2022-03-26 20:10:54.438375 Epoch 250, Training Loss 0.36539861091110104\n",
      "2022-03-26 20:10:54.463120 Epoch 250, Training Loss 0.36626101534842226\n",
      "2022-03-26 20:10:54.493378 Epoch 250, Training Loss 0.3669299538559316\n",
      "2022-03-26 20:10:54.517290 Epoch 250, Training Loss 0.3673828554046733\n",
      "2022-03-26 20:10:54.540593 Epoch 250, Training Loss 0.3677986074439095\n",
      "2022-03-26 20:10:54.562763 Epoch 250, Training Loss 0.3684486988407876\n",
      "2022-03-26 20:10:54.586267 Epoch 250, Training Loss 0.3693824400529837\n",
      "2022-03-26 20:10:54.611618 Epoch 250, Training Loss 0.3700996321028151\n",
      "2022-03-26 20:10:54.634761 Epoch 250, Training Loss 0.37088828905464133\n",
      "2022-03-26 20:10:54.657408 Epoch 250, Training Loss 0.371528192401847\n",
      "2022-03-26 20:10:54.680232 Epoch 250, Training Loss 0.3722643350701198\n",
      "2022-03-26 20:10:54.708514 Epoch 250, Training Loss 0.3729883446108045\n",
      "2022-03-26 20:10:54.735909 Epoch 250, Training Loss 0.37361757384846583\n",
      "2022-03-26 20:10:54.758551 Epoch 250, Training Loss 0.3743771326054088\n",
      "2022-03-26 20:10:54.781683 Epoch 250, Training Loss 0.3749247934964612\n",
      "2022-03-26 20:10:54.809168 Epoch 250, Training Loss 0.37573245175354314\n",
      "2022-03-26 20:10:54.833171 Epoch 250, Training Loss 0.3763577665590569\n",
      "2022-03-26 20:10:54.856263 Epoch 250, Training Loss 0.3772378018521287\n",
      "2022-03-26 20:10:54.879325 Epoch 250, Training Loss 0.3780353492711816\n",
      "2022-03-26 20:10:54.901799 Epoch 250, Training Loss 0.378515480119554\n",
      "2022-03-26 20:10:54.932855 Epoch 250, Training Loss 0.3793343401625943\n",
      "2022-03-26 20:10:54.958056 Epoch 250, Training Loss 0.3798885169007894\n",
      "2022-03-26 20:10:54.981659 Epoch 250, Training Loss 0.3805715405498929\n",
      "2022-03-26 20:10:55.005272 Epoch 250, Training Loss 0.3810656029168907\n",
      "2022-03-26 20:10:55.029237 Epoch 250, Training Loss 0.3815950547032954\n",
      "2022-03-26 20:10:55.051396 Epoch 250, Training Loss 0.3821960407525987\n",
      "2022-03-26 20:10:55.073653 Epoch 250, Training Loss 0.38290749985696104\n",
      "2022-03-26 20:10:55.097798 Epoch 250, Training Loss 0.38348345386097804\n",
      "2022-03-26 20:10:55.128824 Epoch 250, Training Loss 0.3842545862088118\n",
      "2022-03-26 20:10:55.166952 Epoch 250, Training Loss 0.3849047217375177\n",
      "2022-03-26 20:10:55.190974 Epoch 250, Training Loss 0.3855256996953579\n",
      "2022-03-26 20:10:55.215158 Epoch 250, Training Loss 0.3862935345038733\n",
      "2022-03-26 20:10:55.243378 Epoch 250, Training Loss 0.3872232174172121\n",
      "2022-03-26 20:10:55.266546 Epoch 250, Training Loss 0.38801506527549473\n",
      "2022-03-26 20:10:55.289249 Epoch 250, Training Loss 0.38887866363501\n",
      "2022-03-26 20:10:55.312527 Epoch 250, Training Loss 0.38953469186792594\n",
      "2022-03-26 20:10:55.337829 Epoch 250, Training Loss 0.390263531900123\n",
      "2022-03-26 20:10:55.360703 Epoch 250, Training Loss 0.3910454643504394\n",
      "2022-03-26 20:10:55.390939 Epoch 250, Training Loss 0.39187691637012356\n",
      "2022-03-26 20:10:55.414840 Epoch 250, Training Loss 0.39251289213709817\n",
      "2022-03-26 20:10:55.438835 Epoch 250, Training Loss 0.39302456550433507\n",
      "2022-03-26 20:10:55.466409 Epoch 250, Training Loss 0.39368115578923385\n",
      "2022-03-26 20:10:55.489513 Epoch 250, Training Loss 0.39448064013057965\n",
      "2022-03-26 20:10:55.513959 Epoch 250, Training Loss 0.39527067500154683\n",
      "2022-03-26 20:10:55.540769 Epoch 250, Training Loss 0.395897361361767\n",
      "2022-03-26 20:10:55.563956 Epoch 250, Training Loss 0.396547334768888\n",
      "2022-03-26 20:10:55.588054 Epoch 250, Training Loss 0.39769867691390043\n",
      "2022-03-26 20:10:55.621134 Epoch 250, Training Loss 0.39830893095192094\n",
      "2022-03-26 20:10:55.645539 Epoch 250, Training Loss 0.3989491233282992\n",
      "2022-03-26 20:10:55.668053 Epoch 250, Training Loss 0.39982903590592583\n",
      "2022-03-26 20:10:55.692650 Epoch 250, Training Loss 0.40026246678188937\n",
      "2022-03-26 20:10:55.716109 Epoch 250, Training Loss 0.40118578236426233\n",
      "2022-03-26 20:10:55.739504 Epoch 250, Training Loss 0.40179239651736093\n",
      "2022-03-26 20:10:55.762558 Epoch 250, Training Loss 0.4024584316231711\n",
      "2022-03-26 20:10:55.785612 Epoch 250, Training Loss 0.4033641274018056\n",
      "2022-03-26 20:10:55.809116 Epoch 250, Training Loss 0.40412852587297443\n",
      "2022-03-26 20:10:55.838157 Epoch 250, Training Loss 0.4049565332472477\n",
      "2022-03-26 20:10:55.864907 Epoch 250, Training Loss 0.40568465794748665\n",
      "2022-03-26 20:10:55.888244 Epoch 250, Training Loss 0.40619720880637694\n",
      "2022-03-26 20:10:55.913293 Epoch 250, Training Loss 0.40684441052129505\n",
      "2022-03-26 20:10:55.937653 Epoch 250, Training Loss 0.407552481993385\n",
      "2022-03-26 20:10:55.960083 Epoch 250, Training Loss 0.4081325074443427\n",
      "2022-03-26 20:10:55.982769 Epoch 250, Training Loss 0.40879139700509093\n",
      "2022-03-26 20:10:56.005263 Epoch 250, Training Loss 0.4097056947553249\n",
      "2022-03-26 20:10:56.028574 Epoch 250, Training Loss 0.41042299435266755\n",
      "2022-03-26 20:10:56.067496 Epoch 250, Training Loss 0.4109470703641472\n",
      "2022-03-26 20:10:56.091182 Epoch 250, Training Loss 0.41180789840343357\n",
      "2022-03-26 20:10:56.115728 Epoch 250, Training Loss 0.4126604216559159\n",
      "2022-03-26 20:10:56.151371 Epoch 250, Training Loss 0.4134518429065299\n",
      "2022-03-26 20:10:56.174133 Epoch 250, Training Loss 0.4143280955150609\n",
      "2022-03-26 20:10:56.198471 Epoch 250, Training Loss 0.414916881705489\n",
      "2022-03-26 20:10:56.223118 Epoch 250, Training Loss 0.4159436960278265\n",
      "2022-03-26 20:10:56.247359 Epoch 250, Training Loss 0.4166117205720423\n",
      "2022-03-26 20:10:56.269744 Epoch 250, Training Loss 0.4172968713142683\n",
      "2022-03-26 20:10:56.301168 Epoch 250, Training Loss 0.4179445823363941\n",
      "2022-03-26 20:10:56.323373 Epoch 250, Training Loss 0.4186433612766778\n",
      "2022-03-26 20:10:56.348733 Epoch 250, Training Loss 0.41953040003928993\n",
      "2022-03-26 20:10:56.375377 Epoch 250, Training Loss 0.420371936372174\n",
      "2022-03-26 20:10:56.399768 Epoch 250, Training Loss 0.42124610457121564\n",
      "2022-03-26 20:10:56.422993 Epoch 250, Training Loss 0.422156646161738\n",
      "2022-03-26 20:10:56.448015 Epoch 250, Training Loss 0.4228426505766256\n",
      "2022-03-26 20:10:56.473858 Epoch 250, Training Loss 0.4237473910803075\n",
      "2022-03-26 20:10:56.510364 Epoch 250, Training Loss 0.4245478011229459\n",
      "2022-03-26 20:10:56.534803 Epoch 250, Training Loss 0.42524717389927497\n",
      "2022-03-26 20:10:56.558640 Epoch 250, Training Loss 0.4257829435493635\n",
      "2022-03-26 20:10:56.583184 Epoch 250, Training Loss 0.42676951055941376\n",
      "2022-03-26 20:10:56.608587 Epoch 250, Training Loss 0.4276125817500112\n",
      "2022-03-26 20:10:56.632620 Epoch 250, Training Loss 0.4283199291247541\n",
      "2022-03-26 20:10:56.655937 Epoch 250, Training Loss 0.4288793774821874\n",
      "2022-03-26 20:10:56.678866 Epoch 250, Training Loss 0.429652241947096\n",
      "2022-03-26 20:10:56.702572 Epoch 250, Training Loss 0.43064508764335263\n",
      "2022-03-26 20:10:56.732539 Epoch 250, Training Loss 0.4311498284644788\n",
      "2022-03-26 20:10:56.758328 Epoch 250, Training Loss 0.43189603265594034\n",
      "2022-03-26 20:10:56.781356 Epoch 250, Training Loss 0.4326134273005873\n",
      "2022-03-26 20:10:56.807405 Epoch 250, Training Loss 0.4331457928928268\n",
      "2022-03-26 20:10:56.831154 Epoch 250, Training Loss 0.4339074400227393\n",
      "2022-03-26 20:10:56.854014 Epoch 250, Training Loss 0.4348646352815506\n",
      "2022-03-26 20:10:56.876628 Epoch 250, Training Loss 0.4356024641057719\n",
      "2022-03-26 20:10:56.899577 Epoch 250, Training Loss 0.43631730123858925\n",
      "2022-03-26 20:10:56.922956 Epoch 250, Training Loss 0.43714813724198304\n",
      "2022-03-26 20:10:56.955860 Epoch 250, Training Loss 0.43777417553507764\n",
      "2022-03-26 20:10:56.983271 Epoch 250, Training Loss 0.4385491674742126\n",
      "2022-03-26 20:10:57.006920 Epoch 250, Training Loss 0.4389299767096634\n",
      "2022-03-26 20:10:57.031176 Epoch 250, Training Loss 0.43959679551746533\n",
      "2022-03-26 20:10:57.060731 Epoch 250, Training Loss 0.4402231442958803\n",
      "2022-03-26 20:10:57.083475 Epoch 250, Training Loss 0.44088770376751796\n",
      "2022-03-26 20:10:57.107041 Epoch 250, Training Loss 0.44169454204152003\n",
      "2022-03-26 20:10:57.130534 Epoch 250, Training Loss 0.44254925008625023\n",
      "2022-03-26 20:10:57.156041 Epoch 250, Training Loss 0.44293586364792437\n",
      "2022-03-26 20:10:57.193521 Epoch 250, Training Loss 0.4437166956227149\n",
      "2022-03-26 20:10:57.218087 Epoch 250, Training Loss 0.44470594499422156\n",
      "2022-03-26 20:10:57.240982 Epoch 250, Training Loss 0.44534502046949725\n",
      "2022-03-26 20:10:57.266318 Epoch 250, Training Loss 0.44594483782568245\n",
      "2022-03-26 20:10:57.289916 Epoch 250, Training Loss 0.44664298687749504\n",
      "2022-03-26 20:10:57.312689 Epoch 250, Training Loss 0.4472659351804372\n",
      "2022-03-26 20:10:57.338346 Epoch 250, Training Loss 0.44815807288412546\n",
      "2022-03-26 20:10:57.361622 Epoch 250, Training Loss 0.4489149563681439\n",
      "2022-03-26 20:10:57.384737 Epoch 250, Training Loss 0.44956779537146047\n",
      "2022-03-26 20:10:57.413574 Epoch 250, Training Loss 0.45018457627052544\n",
      "2022-03-26 20:10:57.439524 Epoch 250, Training Loss 0.4507530143727427\n",
      "2022-03-26 20:10:57.463839 Epoch 250, Training Loss 0.45141855884543464\n",
      "2022-03-26 20:10:57.486953 Epoch 250, Training Loss 0.4519296473706775\n",
      "2022-03-26 20:10:57.511537 Epoch 250, Training Loss 0.4524544673350156\n",
      "2022-03-26 20:10:57.533796 Epoch 250, Training Loss 0.4531732329627132\n",
      "2022-03-26 20:10:57.556890 Epoch 250, Training Loss 0.4537320485733964\n",
      "2022-03-26 20:10:57.579390 Epoch 250, Training Loss 0.45432962366687063\n",
      "2022-03-26 20:10:57.602744 Epoch 250, Training Loss 0.4548862321907297\n",
      "2022-03-26 20:10:57.631547 Epoch 250, Training Loss 0.45553980809648326\n",
      "2022-03-26 20:10:57.656316 Epoch 250, Training Loss 0.4562444407921618\n",
      "2022-03-26 20:10:57.678952 Epoch 250, Training Loss 0.45704393656662357\n",
      "2022-03-26 20:10:57.702080 Epoch 250, Training Loss 0.45798916249628874\n",
      "2022-03-26 20:10:57.728473 Epoch 250, Training Loss 0.4587600893529175\n",
      "2022-03-26 20:10:57.750864 Epoch 250, Training Loss 0.4593494822225912\n",
      "2022-03-26 20:10:57.773031 Epoch 250, Training Loss 0.4601831139277314\n",
      "2022-03-26 20:10:57.795470 Epoch 250, Training Loss 0.46097924158243875\n",
      "2022-03-26 20:10:57.818528 Epoch 250, Training Loss 0.4617813025670283\n",
      "2022-03-26 20:10:57.846711 Epoch 250, Training Loss 0.4626906372778251\n",
      "2022-03-26 20:10:57.878206 Epoch 250, Training Loss 0.4633144488191361\n",
      "2022-03-26 20:10:57.901037 Epoch 250, Training Loss 0.46402122263255935\n",
      "2022-03-26 20:10:57.923772 Epoch 250, Training Loss 0.46481881075350523\n",
      "2022-03-26 20:10:57.948925 Epoch 250, Training Loss 0.46550958994251995\n",
      "2022-03-26 20:10:57.980689 Epoch 250, Training Loss 0.46620397254481644\n",
      "2022-03-26 20:10:58.003595 Epoch 250, Training Loss 0.4668902319944118\n",
      "2022-03-26 20:10:58.026187 Epoch 250, Training Loss 0.4673829698349204\n",
      "2022-03-26 20:10:58.048650 Epoch 250, Training Loss 0.4683157586685532\n",
      "2022-03-26 20:10:58.078940 Epoch 250, Training Loss 0.4691376882745787\n",
      "2022-03-26 20:10:58.101782 Epoch 250, Training Loss 0.4696988210348827\n",
      "2022-03-26 20:10:58.125984 Epoch 250, Training Loss 0.47040760700050216\n",
      "2022-03-26 20:10:58.152892 Epoch 250, Training Loss 0.47090879772477745\n",
      "2022-03-26 20:10:58.175481 Epoch 250, Training Loss 0.47152511855525436\n",
      "2022-03-26 20:10:58.200762 Epoch 250, Training Loss 0.47226810569653427\n",
      "2022-03-26 20:10:58.230410 Epoch 250, Training Loss 0.47302304036781917\n",
      "2022-03-26 20:10:58.252890 Epoch 250, Training Loss 0.4737825212271317\n",
      "2022-03-26 20:10:58.275186 Epoch 250, Training Loss 0.474458475780609\n",
      "2022-03-26 20:10:58.307404 Epoch 250, Training Loss 0.47495987283451785\n",
      "2022-03-26 20:10:58.336573 Epoch 250, Training Loss 0.47568216009060743\n",
      "2022-03-26 20:10:58.360398 Epoch 250, Training Loss 0.47659004531095706\n",
      "2022-03-26 20:10:58.384242 Epoch 250, Training Loss 0.4772883615911464\n",
      "2022-03-26 20:10:58.411651 Epoch 250, Training Loss 0.47780379725386724\n",
      "2022-03-26 20:10:58.436420 Epoch 250, Training Loss 0.4786667700694955\n",
      "2022-03-26 20:10:58.459940 Epoch 250, Training Loss 0.4794307123593357\n",
      "2022-03-26 20:10:58.482801 Epoch 250, Training Loss 0.480148403197908\n",
      "2022-03-26 20:10:58.505771 Epoch 250, Training Loss 0.48082409951540517\n",
      "2022-03-26 20:10:58.536954 Epoch 250, Training Loss 0.4816468996388833\n",
      "2022-03-26 20:10:58.560316 Epoch 250, Training Loss 0.4823127218600734\n",
      "2022-03-26 20:10:58.583283 Epoch 250, Training Loss 0.4830396097258229\n",
      "2022-03-26 20:10:58.609851 Epoch 250, Training Loss 0.4837502528868063\n",
      "2022-03-26 20:10:58.633698 Epoch 250, Training Loss 0.48460812340764436\n",
      "2022-03-26 20:10:58.656595 Epoch 250, Training Loss 0.4852956698449981\n",
      "2022-03-26 20:10:58.679769 Epoch 250, Training Loss 0.48597630084780474\n",
      "2022-03-26 20:10:58.702588 Epoch 250, Training Loss 0.4866273928709957\n",
      "2022-03-26 20:10:58.725411 Epoch 250, Training Loss 0.4873999585504727\n",
      "2022-03-26 20:10:58.755539 Epoch 250, Training Loss 0.4881573942539942\n",
      "2022-03-26 20:10:58.781233 Epoch 250, Training Loss 0.4887098145225774\n",
      "2022-03-26 20:10:58.804421 Epoch 250, Training Loss 0.48930097056929106\n",
      "2022-03-26 20:10:58.828224 Epoch 250, Training Loss 0.4900360159633105\n",
      "2022-03-26 20:10:58.852243 Epoch 250, Training Loss 0.4907349631990618\n",
      "2022-03-26 20:10:58.874855 Epoch 250, Training Loss 0.49135417161542744\n",
      "2022-03-26 20:10:58.897464 Epoch 250, Training Loss 0.49196302841234085\n",
      "2022-03-26 20:10:58.919792 Epoch 250, Training Loss 0.4924953810470488\n",
      "2022-03-26 20:10:58.943747 Epoch 250, Training Loss 0.49316658186333256\n",
      "2022-03-26 20:10:58.972199 Epoch 250, Training Loss 0.49412088423891143\n",
      "2022-03-26 20:10:59.000129 Epoch 250, Training Loss 0.49489603608922883\n",
      "2022-03-26 20:10:59.023607 Epoch 250, Training Loss 0.4955401965190687\n",
      "2022-03-26 20:10:59.047175 Epoch 250, Training Loss 0.4960824851413517\n",
      "2022-03-26 20:10:59.079015 Epoch 250, Training Loss 0.4966619306284448\n",
      "2022-03-26 20:10:59.102635 Epoch 250, Training Loss 0.49754289444297783\n",
      "2022-03-26 20:10:59.126309 Epoch 250, Training Loss 0.49847186187191694\n",
      "2022-03-26 20:10:59.154065 Epoch 250, Training Loss 0.49913399950470155\n",
      "2022-03-26 20:10:59.179405 Epoch 250, Training Loss 0.4997893390829301\n",
      "2022-03-26 20:10:59.206727 Epoch 250, Training Loss 0.5003819303286959\n",
      "2022-03-26 20:10:59.236170 Epoch 250, Training Loss 0.5012549312065935\n",
      "2022-03-26 20:10:59.261826 Epoch 250, Training Loss 0.5022402312749487\n",
      "2022-03-26 20:10:59.286874 Epoch 250, Training Loss 0.5027666655952668\n",
      "2022-03-26 20:10:59.310643 Epoch 250, Training Loss 0.5035636284772087\n",
      "2022-03-26 20:10:59.337038 Epoch 250, Training Loss 0.5042355014844928\n",
      "2022-03-26 20:10:59.359328 Epoch 250, Training Loss 0.5049269044643168\n",
      "2022-03-26 20:10:59.381950 Epoch 250, Training Loss 0.5057500524594046\n",
      "2022-03-26 20:10:59.414500 Epoch 250, Training Loss 0.5065617363928528\n",
      "2022-03-26 20:10:59.437203 Epoch 250, Training Loss 0.5072529203141741\n",
      "2022-03-26 20:10:59.461175 Epoch 250, Training Loss 0.508074300597086\n",
      "2022-03-26 20:10:59.487375 Epoch 250, Training Loss 0.5088160582972915\n",
      "2022-03-26 20:10:59.509939 Epoch 250, Training Loss 0.5094701371077076\n",
      "2022-03-26 20:10:59.533094 Epoch 250, Training Loss 0.5101458536237097\n",
      "2022-03-26 20:10:59.556537 Epoch 250, Training Loss 0.5107685665950141\n",
      "2022-03-26 20:10:59.579275 Epoch 250, Training Loss 0.5114866425009335\n",
      "2022-03-26 20:10:59.609945 Epoch 250, Training Loss 0.5122390374198289\n",
      "2022-03-26 20:10:59.634506 Epoch 250, Training Loss 0.5131398576604741\n",
      "2022-03-26 20:10:59.657375 Epoch 250, Training Loss 0.5138523565686267\n",
      "2022-03-26 20:10:59.683686 Epoch 250, Training Loss 0.5145948033046235\n",
      "2022-03-26 20:10:59.707124 Epoch 250, Training Loss 0.5151107552487527\n",
      "2022-03-26 20:10:59.730760 Epoch 250, Training Loss 0.5159620712404056\n",
      "2022-03-26 20:10:59.753579 Epoch 250, Training Loss 0.5166487574501111\n",
      "2022-03-26 20:10:59.776619 Epoch 250, Training Loss 0.5175067611286402\n",
      "2022-03-26 20:10:59.799169 Epoch 250, Training Loss 0.5183860448086658\n",
      "2022-03-26 20:10:59.828320 Epoch 250, Training Loss 0.5190065536276459\n",
      "2022-03-26 20:10:59.852944 Epoch 250, Training Loss 0.519938292367684\n",
      "2022-03-26 20:10:59.876236 Epoch 250, Training Loss 0.5205385650286589\n",
      "2022-03-26 20:10:59.898718 Epoch 250, Training Loss 0.5214538280387668\n",
      "2022-03-26 20:10:59.926117 Epoch 250, Training Loss 0.5223033287564812\n",
      "2022-03-26 20:10:59.949159 Epoch 250, Training Loss 0.523082780373066\n",
      "2022-03-26 20:10:59.977100 Epoch 250, Training Loss 0.5237249397027218\n",
      "2022-03-26 20:11:00.000155 Epoch 250, Training Loss 0.5245023575966316\n",
      "2022-03-26 20:11:00.024283 Epoch 250, Training Loss 0.5251494740205043\n",
      "2022-03-26 20:11:00.057600 Epoch 250, Training Loss 0.5255560993081163\n",
      "2022-03-26 20:11:00.081285 Epoch 250, Training Loss 0.526517931305234\n",
      "2022-03-26 20:11:00.104379 Epoch 250, Training Loss 0.5269492164902065\n",
      "2022-03-26 20:11:00.130194 Epoch 250, Training Loss 0.527499501167051\n",
      "2022-03-26 20:11:00.152667 Epoch 250, Training Loss 0.5284083268755232\n",
      "2022-03-26 20:11:00.175463 Epoch 250, Training Loss 0.5290698496353291\n",
      "2022-03-26 20:11:00.198399 Epoch 250, Training Loss 0.5297205605546532\n",
      "2022-03-26 20:11:00.221791 Epoch 250, Training Loss 0.5305511002116801\n",
      "2022-03-26 20:11:00.244500 Epoch 250, Training Loss 0.5311962947287523\n",
      "2022-03-26 20:11:00.287427 Epoch 250, Training Loss 0.5318642679001669\n",
      "2022-03-26 20:11:00.312497 Epoch 250, Training Loss 0.5324578568758562\n",
      "2022-03-26 20:11:00.339135 Epoch 250, Training Loss 0.5331501520198324\n",
      "2022-03-26 20:11:00.370404 Epoch 250, Training Loss 0.5339907678344366\n",
      "2022-03-26 20:11:00.394185 Epoch 250, Training Loss 0.5347942068143878\n",
      "2022-03-26 20:11:00.417844 Epoch 250, Training Loss 0.5355525516030734\n",
      "2022-03-26 20:11:00.441176 Epoch 250, Training Loss 0.5360112384609554\n",
      "2022-03-26 20:11:00.464676 Epoch 250, Training Loss 0.536843075090662\n",
      "2022-03-26 20:11:00.487752 Epoch 250, Training Loss 0.5374361900112513\n",
      "2022-03-26 20:11:00.518368 Epoch 250, Training Loss 0.5381791523807798\n",
      "2022-03-26 20:11:00.541884 Epoch 250, Training Loss 0.5387656331976967\n",
      "2022-03-26 20:11:00.564496 Epoch 250, Training Loss 0.5394260774335593\n",
      "2022-03-26 20:11:00.587070 Epoch 250, Training Loss 0.539934047614522\n",
      "2022-03-26 20:11:00.612981 Epoch 250, Training Loss 0.5406410266142672\n",
      "2022-03-26 20:11:00.635774 Epoch 250, Training Loss 0.5414854129180884\n",
      "2022-03-26 20:11:00.658032 Epoch 250, Training Loss 0.5422853753161248\n",
      "2022-03-26 20:11:00.680438 Epoch 250, Training Loss 0.5429952043630278\n",
      "2022-03-26 20:11:00.703966 Epoch 250, Training Loss 0.5434875857189793\n",
      "2022-03-26 20:11:00.732649 Epoch 250, Training Loss 0.5441782482902108\n",
      "2022-03-26 20:11:00.759582 Epoch 250, Training Loss 0.5452230073454435\n",
      "2022-03-26 20:11:00.783028 Epoch 250, Training Loss 0.5459051389066155\n",
      "2022-03-26 20:11:00.805518 Epoch 250, Training Loss 0.5468153625040713\n",
      "2022-03-26 20:11:00.814142 Epoch 250, Training Loss 0.5478145727873458\n",
      "2022-03-26 20:25:38.813335 Epoch 300, Training Loss 0.0005290075717374797\n",
      "2022-03-26 20:25:38.837590 Epoch 300, Training Loss 0.0012014346278232076\n",
      "2022-03-26 20:25:38.861234 Epoch 300, Training Loss 0.0019017222820950285\n",
      "2022-03-26 20:25:38.884729 Epoch 300, Training Loss 0.002524944941710938\n",
      "2022-03-26 20:25:38.908234 Epoch 300, Training Loss 0.0032096699146968327\n",
      "2022-03-26 20:25:38.931642 Epoch 300, Training Loss 0.004238259037742225\n",
      "2022-03-26 20:25:38.955176 Epoch 300, Training Loss 0.005166252441418446\n",
      "2022-03-26 20:25:38.978367 Epoch 300, Training Loss 0.005707128654660471\n",
      "2022-03-26 20:25:39.001631 Epoch 300, Training Loss 0.006372448657174855\n",
      "2022-03-26 20:25:39.030968 Epoch 300, Training Loss 0.007132876864479631\n",
      "2022-03-26 20:25:39.059714 Epoch 300, Training Loss 0.007710927854413571\n",
      "2022-03-26 20:25:39.092317 Epoch 300, Training Loss 0.008342048624897246\n",
      "2022-03-26 20:25:39.117300 Epoch 300, Training Loss 0.00902338306922132\n",
      "2022-03-26 20:25:39.145244 Epoch 300, Training Loss 0.009615485701719514\n",
      "2022-03-26 20:25:39.169400 Epoch 300, Training Loss 0.010474310125536321\n",
      "2022-03-26 20:25:39.193498 Epoch 300, Training Loss 0.01105245562922924\n",
      "2022-03-26 20:25:39.217180 Epoch 300, Training Loss 0.011554910551251657\n",
      "2022-03-26 20:25:39.245110 Epoch 300, Training Loss 0.012228636485536385\n",
      "2022-03-26 20:25:39.272758 Epoch 300, Training Loss 0.012876978996769546\n",
      "2022-03-26 20:25:39.296398 Epoch 300, Training Loss 0.013472602190568929\n",
      "2022-03-26 20:25:39.323499 Epoch 300, Training Loss 0.014311317363968287\n",
      "2022-03-26 20:25:39.350360 Epoch 300, Training Loss 0.01499287071435348\n",
      "2022-03-26 20:25:39.374845 Epoch 300, Training Loss 0.015719326408317937\n",
      "2022-03-26 20:25:39.398208 Epoch 300, Training Loss 0.016331419005723255\n",
      "2022-03-26 20:25:39.421480 Epoch 300, Training Loss 0.01698993310294188\n",
      "2022-03-26 20:25:39.446081 Epoch 300, Training Loss 0.017789482079503482\n",
      "2022-03-26 20:25:39.479866 Epoch 300, Training Loss 0.018445184697275577\n",
      "2022-03-26 20:25:39.508164 Epoch 300, Training Loss 0.01898650680204182\n",
      "2022-03-26 20:25:39.530677 Epoch 300, Training Loss 0.01951559916938967\n",
      "2022-03-26 20:25:39.555395 Epoch 300, Training Loss 0.01977725572826917\n",
      "2022-03-26 20:25:39.579878 Epoch 300, Training Loss 0.02050497592486384\n",
      "2022-03-26 20:25:39.605528 Epoch 300, Training Loss 0.021014432296576097\n",
      "2022-03-26 20:25:39.629112 Epoch 300, Training Loss 0.02176845186125592\n",
      "2022-03-26 20:25:39.651655 Epoch 300, Training Loss 0.022451347611901706\n",
      "2022-03-26 20:25:39.679788 Epoch 300, Training Loss 0.02277390325389555\n",
      "2022-03-26 20:25:39.717375 Epoch 300, Training Loss 0.02347067240482706\n",
      "2022-03-26 20:25:39.745391 Epoch 300, Training Loss 0.024118412771950596\n",
      "2022-03-26 20:25:39.771985 Epoch 300, Training Loss 0.024713938403160066\n",
      "2022-03-26 20:25:39.795357 Epoch 300, Training Loss 0.025485646675157424\n",
      "2022-03-26 20:25:39.818400 Epoch 300, Training Loss 0.026060202637749254\n",
      "2022-03-26 20:25:39.842185 Epoch 300, Training Loss 0.026575672676038865\n",
      "2022-03-26 20:25:39.865180 Epoch 300, Training Loss 0.027191468116724887\n",
      "2022-03-26 20:25:39.888383 Epoch 300, Training Loss 0.027913131734446796\n",
      "2022-03-26 20:25:39.911354 Epoch 300, Training Loss 0.028795854571987602\n",
      "2022-03-26 20:25:39.942765 Epoch 300, Training Loss 0.029403984527606182\n",
      "2022-03-26 20:25:39.967791 Epoch 300, Training Loss 0.030029805134172024\n",
      "2022-03-26 20:25:39.997129 Epoch 300, Training Loss 0.030755125062392495\n",
      "2022-03-26 20:25:40.021765 Epoch 300, Training Loss 0.03135760080860094\n",
      "2022-03-26 20:25:40.050123 Epoch 300, Training Loss 0.03218982441117391\n",
      "2022-03-26 20:25:40.073388 Epoch 300, Training Loss 0.03300352849999962\n",
      "2022-03-26 20:25:40.097255 Epoch 300, Training Loss 0.03390543938369092\n",
      "2022-03-26 20:25:40.120501 Epoch 300, Training Loss 0.03458865015479305\n",
      "2022-03-26 20:25:40.144826 Epoch 300, Training Loss 0.03522708697620865\n",
      "2022-03-26 20:25:40.177536 Epoch 300, Training Loss 0.036048730525671674\n",
      "2022-03-26 20:25:40.201309 Epoch 300, Training Loss 0.036413798608895764\n",
      "2022-03-26 20:25:40.224438 Epoch 300, Training Loss 0.036839385262078334\n",
      "2022-03-26 20:25:40.251281 Epoch 300, Training Loss 0.037540859140246116\n",
      "2022-03-26 20:25:40.274745 Epoch 300, Training Loss 0.03810490391519673\n",
      "2022-03-26 20:25:40.298199 Epoch 300, Training Loss 0.03877751184317767\n",
      "2022-03-26 20:25:40.321050 Epoch 300, Training Loss 0.03929728183828656\n",
      "2022-03-26 20:25:40.347852 Epoch 300, Training Loss 0.04004107623377724\n",
      "2022-03-26 20:25:40.380930 Epoch 300, Training Loss 0.0408058822574213\n",
      "2022-03-26 20:25:40.404921 Epoch 300, Training Loss 0.04140252387508407\n",
      "2022-03-26 20:25:40.428106 Epoch 300, Training Loss 0.042188102422315445\n",
      "2022-03-26 20:25:40.456296 Epoch 300, Training Loss 0.042848861068868274\n",
      "2022-03-26 20:25:40.479833 Epoch 300, Training Loss 0.04360102259975565\n",
      "2022-03-26 20:25:40.502910 Epoch 300, Training Loss 0.0443467227432429\n",
      "2022-03-26 20:25:40.526631 Epoch 300, Training Loss 0.04494784800978878\n",
      "2022-03-26 20:25:40.550502 Epoch 300, Training Loss 0.045671599085830974\n",
      "2022-03-26 20:25:40.573748 Epoch 300, Training Loss 0.046663025551287415\n",
      "2022-03-26 20:25:40.604419 Epoch 300, Training Loss 0.04748632248176638\n",
      "2022-03-26 20:25:40.629165 Epoch 300, Training Loss 0.04812971674039236\n",
      "2022-03-26 20:25:40.652659 Epoch 300, Training Loss 0.048739070001313146\n",
      "2022-03-26 20:25:40.676828 Epoch 300, Training Loss 0.04922824086206953\n",
      "2022-03-26 20:25:40.702871 Epoch 300, Training Loss 0.04985339754758893\n",
      "2022-03-26 20:25:40.725455 Epoch 300, Training Loss 0.050644090889817306\n",
      "2022-03-26 20:25:40.759976 Epoch 300, Training Loss 0.05123028900388561\n",
      "2022-03-26 20:25:40.783322 Epoch 300, Training Loss 0.051888104156612436\n",
      "2022-03-26 20:25:40.806269 Epoch 300, Training Loss 0.052417406931405175\n",
      "2022-03-26 20:25:40.838802 Epoch 300, Training Loss 0.05310332803698757\n",
      "2022-03-26 20:25:40.862147 Epoch 300, Training Loss 0.053716643291818515\n",
      "2022-03-26 20:25:40.895186 Epoch 300, Training Loss 0.054580499734872445\n",
      "2022-03-26 20:25:40.918470 Epoch 300, Training Loss 0.055213653580154606\n",
      "2022-03-26 20:25:40.941319 Epoch 300, Training Loss 0.055727511892080916\n",
      "2022-03-26 20:25:40.964603 Epoch 300, Training Loss 0.05632334335914353\n",
      "2022-03-26 20:25:40.992000 Epoch 300, Training Loss 0.05715242575120438\n",
      "2022-03-26 20:25:41.017123 Epoch 300, Training Loss 0.05769614477063079\n",
      "2022-03-26 20:25:41.044884 Epoch 300, Training Loss 0.0582589330461324\n",
      "2022-03-26 20:25:41.067778 Epoch 300, Training Loss 0.05887953428280018\n",
      "2022-03-26 20:25:41.091025 Epoch 300, Training Loss 0.059372205049028175\n",
      "2022-03-26 20:25:41.118010 Epoch 300, Training Loss 0.060076663122914944\n",
      "2022-03-26 20:25:41.140739 Epoch 300, Training Loss 0.06097521648153929\n",
      "2022-03-26 20:25:41.164027 Epoch 300, Training Loss 0.061832237750520484\n",
      "2022-03-26 20:25:41.188073 Epoch 300, Training Loss 0.06253826688698795\n",
      "2022-03-26 20:25:41.211309 Epoch 300, Training Loss 0.06345172857155885\n",
      "2022-03-26 20:25:41.245407 Epoch 300, Training Loss 0.06403684682782045\n",
      "2022-03-26 20:25:41.270127 Epoch 300, Training Loss 0.06454946338901739\n",
      "2022-03-26 20:25:41.293766 Epoch 300, Training Loss 0.06497970015725212\n",
      "2022-03-26 20:25:41.320569 Epoch 300, Training Loss 0.06559384442732462\n",
      "2022-03-26 20:25:41.346746 Epoch 300, Training Loss 0.06619178160758274\n",
      "2022-03-26 20:25:41.369888 Epoch 300, Training Loss 0.0669495760822845\n",
      "2022-03-26 20:25:41.393204 Epoch 300, Training Loss 0.06763660513302859\n",
      "2022-03-26 20:25:41.415842 Epoch 300, Training Loss 0.06833590176480506\n",
      "2022-03-26 20:25:41.439136 Epoch 300, Training Loss 0.06899675904103862\n",
      "2022-03-26 20:25:41.472474 Epoch 300, Training Loss 0.06954039932440614\n",
      "2022-03-26 20:25:41.496767 Epoch 300, Training Loss 0.07014299084996933\n",
      "2022-03-26 20:25:41.519464 Epoch 300, Training Loss 0.07074156184406842\n",
      "2022-03-26 20:25:41.542792 Epoch 300, Training Loss 0.07130177839256613\n",
      "2022-03-26 20:25:41.565890 Epoch 300, Training Loss 0.07177225995780256\n",
      "2022-03-26 20:25:41.592556 Epoch 300, Training Loss 0.07276775249663521\n",
      "2022-03-26 20:25:41.615360 Epoch 300, Training Loss 0.07335440930731767\n",
      "2022-03-26 20:25:41.638577 Epoch 300, Training Loss 0.07392933059607626\n",
      "2022-03-26 20:25:41.661161 Epoch 300, Training Loss 0.07466637594696811\n",
      "2022-03-26 20:25:41.691006 Epoch 300, Training Loss 0.07517033066514812\n",
      "2022-03-26 20:25:41.716043 Epoch 300, Training Loss 0.07568392456721162\n",
      "2022-03-26 20:25:41.739265 Epoch 300, Training Loss 0.07631571638538405\n",
      "2022-03-26 20:25:41.766644 Epoch 300, Training Loss 0.07696063658389289\n",
      "2022-03-26 20:25:41.801942 Epoch 300, Training Loss 0.07782369552899504\n",
      "2022-03-26 20:25:41.825419 Epoch 300, Training Loss 0.07840606095769521\n",
      "2022-03-26 20:25:41.850057 Epoch 300, Training Loss 0.07926540711270574\n",
      "2022-03-26 20:25:41.878493 Epoch 300, Training Loss 0.07986871526598016\n",
      "2022-03-26 20:25:41.909974 Epoch 300, Training Loss 0.0805465017857454\n",
      "2022-03-26 20:25:41.934652 Epoch 300, Training Loss 0.08119390800099849\n",
      "2022-03-26 20:25:41.957851 Epoch 300, Training Loss 0.08184719891728037\n",
      "2022-03-26 20:25:41.984140 Epoch 300, Training Loss 0.08239622949563025\n",
      "2022-03-26 20:25:42.008395 Epoch 300, Training Loss 0.08307834405956975\n",
      "2022-03-26 20:25:42.031992 Epoch 300, Training Loss 0.08360830564861713\n",
      "2022-03-26 20:25:42.055151 Epoch 300, Training Loss 0.08416836567775672\n",
      "2022-03-26 20:25:42.078441 Epoch 300, Training Loss 0.08487549955811342\n",
      "2022-03-26 20:25:42.105896 Epoch 300, Training Loss 0.08558832027037125\n",
      "2022-03-26 20:25:42.140291 Epoch 300, Training Loss 0.08611100574817195\n",
      "2022-03-26 20:25:42.165633 Epoch 300, Training Loss 0.0868341741926225\n",
      "2022-03-26 20:25:42.189189 Epoch 300, Training Loss 0.08746357645143939\n",
      "2022-03-26 20:25:42.217023 Epoch 300, Training Loss 0.08795978191792203\n",
      "2022-03-26 20:25:42.240358 Epoch 300, Training Loss 0.08834026039332685\n",
      "2022-03-26 20:25:42.263650 Epoch 300, Training Loss 0.08891501475859176\n",
      "2022-03-26 20:25:42.287112 Epoch 300, Training Loss 0.08936643468983033\n",
      "2022-03-26 20:25:42.309792 Epoch 300, Training Loss 0.0900030690042869\n",
      "2022-03-26 20:25:42.335709 Epoch 300, Training Loss 0.09060912251548695\n",
      "2022-03-26 20:25:42.372206 Epoch 300, Training Loss 0.09099097128795541\n",
      "2022-03-26 20:25:42.396921 Epoch 300, Training Loss 0.0916058915617216\n",
      "2022-03-26 20:25:42.419748 Epoch 300, Training Loss 0.09219643917611188\n",
      "2022-03-26 20:25:42.444224 Epoch 300, Training Loss 0.0930445891855013\n",
      "2022-03-26 20:25:42.471606 Epoch 300, Training Loss 0.09374042280266047\n",
      "2022-03-26 20:25:42.494865 Epoch 300, Training Loss 0.09449424251647251\n",
      "2022-03-26 20:25:42.518166 Epoch 300, Training Loss 0.0951369186229718\n",
      "2022-03-26 20:25:42.542180 Epoch 300, Training Loss 0.09569315705686579\n",
      "2022-03-26 20:25:42.565251 Epoch 300, Training Loss 0.09624002353690775\n",
      "2022-03-26 20:25:42.596618 Epoch 300, Training Loss 0.096984791728999\n",
      "2022-03-26 20:25:42.622462 Epoch 300, Training Loss 0.0976100406035438\n",
      "2022-03-26 20:25:42.646026 Epoch 300, Training Loss 0.09822637728794152\n",
      "2022-03-26 20:25:42.668744 Epoch 300, Training Loss 0.09891434050048403\n",
      "2022-03-26 20:25:42.695811 Epoch 300, Training Loss 0.09952765019119852\n",
      "2022-03-26 20:25:42.718551 Epoch 300, Training Loss 0.10020832875576775\n",
      "2022-03-26 20:25:42.742390 Epoch 300, Training Loss 0.10087641115155062\n",
      "2022-03-26 20:25:42.765994 Epoch 300, Training Loss 0.10176057128421485\n",
      "2022-03-26 20:25:42.789276 Epoch 300, Training Loss 0.10232311710143638\n",
      "2022-03-26 20:25:42.829758 Epoch 300, Training Loss 0.10285890211001077\n",
      "2022-03-26 20:25:42.857161 Epoch 300, Training Loss 0.10382719884824265\n",
      "2022-03-26 20:25:42.882987 Epoch 300, Training Loss 0.10439914619297627\n",
      "2022-03-26 20:25:42.907704 Epoch 300, Training Loss 0.10519343364955214\n",
      "2022-03-26 20:25:42.931191 Epoch 300, Training Loss 0.10587041161935348\n",
      "2022-03-26 20:25:42.954618 Epoch 300, Training Loss 0.10632804914584855\n",
      "2022-03-26 20:25:42.978134 Epoch 300, Training Loss 0.10691515169561366\n",
      "2022-03-26 20:25:43.008986 Epoch 300, Training Loss 0.10773225065768527\n",
      "2022-03-26 20:25:43.032259 Epoch 300, Training Loss 0.10839517374553949\n",
      "2022-03-26 20:25:43.065136 Epoch 300, Training Loss 0.10923139992958444\n",
      "2022-03-26 20:25:43.088660 Epoch 300, Training Loss 0.10974896005581102\n",
      "2022-03-26 20:25:43.112386 Epoch 300, Training Loss 0.11027642510964743\n",
      "2022-03-26 20:25:43.140588 Epoch 300, Training Loss 0.11093205806163266\n",
      "2022-03-26 20:25:43.163489 Epoch 300, Training Loss 0.11160549146058919\n",
      "2022-03-26 20:25:43.186015 Epoch 300, Training Loss 0.11232007660752977\n",
      "2022-03-26 20:25:43.209729 Epoch 300, Training Loss 0.1130127513309574\n",
      "2022-03-26 20:25:43.232760 Epoch 300, Training Loss 0.11366742679758755\n",
      "2022-03-26 20:25:43.266749 Epoch 300, Training Loss 0.11406077648443944\n",
      "2022-03-26 20:25:43.291091 Epoch 300, Training Loss 0.11463432183579715\n",
      "2022-03-26 20:25:43.314601 Epoch 300, Training Loss 0.11527504181236867\n",
      "2022-03-26 20:25:43.343150 Epoch 300, Training Loss 0.11571226162297646\n",
      "2022-03-26 20:25:43.366109 Epoch 300, Training Loss 0.11655068753854088\n",
      "2022-03-26 20:25:43.389009 Epoch 300, Training Loss 0.11711413626704374\n",
      "2022-03-26 20:25:43.411905 Epoch 300, Training Loss 0.11785958015629093\n",
      "2022-03-26 20:25:43.435025 Epoch 300, Training Loss 0.11852859053998957\n",
      "2022-03-26 20:25:43.459448 Epoch 300, Training Loss 0.11941713451043419\n",
      "2022-03-26 20:25:43.490208 Epoch 300, Training Loss 0.12021075099554208\n",
      "2022-03-26 20:25:43.514310 Epoch 300, Training Loss 0.12094119753297942\n",
      "2022-03-26 20:25:43.537163 Epoch 300, Training Loss 0.12154411284438789\n",
      "2022-03-26 20:25:43.560606 Epoch 300, Training Loss 0.12197848207429242\n",
      "2022-03-26 20:25:43.583792 Epoch 300, Training Loss 0.1224347800588059\n",
      "2022-03-26 20:25:43.610534 Epoch 300, Training Loss 0.12315792113999881\n",
      "2022-03-26 20:25:43.633818 Epoch 300, Training Loss 0.12369988963503362\n",
      "2022-03-26 20:25:43.657253 Epoch 300, Training Loss 0.12441179081988152\n",
      "2022-03-26 20:25:43.680583 Epoch 300, Training Loss 0.1248999118538159\n",
      "2022-03-26 20:25:43.711190 Epoch 300, Training Loss 0.12538628122004705\n",
      "2022-03-26 20:25:43.737192 Epoch 300, Training Loss 0.1261939127236376\n",
      "2022-03-26 20:25:43.760236 Epoch 300, Training Loss 0.1267674781210587\n",
      "2022-03-26 20:25:43.786166 Epoch 300, Training Loss 0.12732133275026555\n",
      "2022-03-26 20:25:43.810176 Epoch 300, Training Loss 0.1279494654377708\n",
      "2022-03-26 20:25:43.833048 Epoch 300, Training Loss 0.1287647185613737\n",
      "2022-03-26 20:25:43.865259 Epoch 300, Training Loss 0.12948995784801595\n",
      "2022-03-26 20:25:43.890496 Epoch 300, Training Loss 0.13046579503113656\n",
      "2022-03-26 20:25:43.919852 Epoch 300, Training Loss 0.1310207356539224\n",
      "2022-03-26 20:25:43.947165 Epoch 300, Training Loss 0.13155802305016068\n",
      "2022-03-26 20:25:43.971555 Epoch 300, Training Loss 0.1321950876689933\n",
      "2022-03-26 20:25:43.995210 Epoch 300, Training Loss 0.13284200340356972\n",
      "2022-03-26 20:25:44.022031 Epoch 300, Training Loss 0.13355872128396998\n",
      "2022-03-26 20:25:44.045243 Epoch 300, Training Loss 0.1342779510961774\n",
      "2022-03-26 20:25:44.068166 Epoch 300, Training Loss 0.13492827717681674\n",
      "2022-03-26 20:25:44.091357 Epoch 300, Training Loss 0.1353357142537756\n",
      "2022-03-26 20:25:44.114351 Epoch 300, Training Loss 0.13599927919676236\n",
      "2022-03-26 20:25:44.147000 Epoch 300, Training Loss 0.13671253055638974\n",
      "2022-03-26 20:25:44.170424 Epoch 300, Training Loss 0.13718856057471326\n",
      "2022-03-26 20:25:44.193156 Epoch 300, Training Loss 0.13804990577194698\n",
      "2022-03-26 20:25:44.219301 Epoch 300, Training Loss 0.1387010558753672\n",
      "2022-03-26 20:25:44.242457 Epoch 300, Training Loss 0.13946796803142103\n",
      "2022-03-26 20:25:44.265437 Epoch 300, Training Loss 0.13991843680362873\n",
      "2022-03-26 20:25:44.288558 Epoch 300, Training Loss 0.14052737309880878\n",
      "2022-03-26 20:25:44.311704 Epoch 300, Training Loss 0.14139833598566787\n",
      "2022-03-26 20:25:44.336986 Epoch 300, Training Loss 0.14206954724419757\n",
      "2022-03-26 20:25:44.366278 Epoch 300, Training Loss 0.14279778429385645\n",
      "2022-03-26 20:25:44.391920 Epoch 300, Training Loss 0.14329774374775875\n",
      "2022-03-26 20:25:44.414712 Epoch 300, Training Loss 0.14390770768951577\n",
      "2022-03-26 20:25:44.438470 Epoch 300, Training Loss 0.14459842277686005\n",
      "2022-03-26 20:25:44.464372 Epoch 300, Training Loss 0.14541526578957467\n",
      "2022-03-26 20:25:44.487784 Epoch 300, Training Loss 0.14588762421513457\n",
      "2022-03-26 20:25:44.511143 Epoch 300, Training Loss 0.14634129879495983\n",
      "2022-03-26 20:25:44.539089 Epoch 300, Training Loss 0.1468758917106387\n",
      "2022-03-26 20:25:44.567425 Epoch 300, Training Loss 0.14746830791539853\n",
      "2022-03-26 20:25:44.599381 Epoch 300, Training Loss 0.14809665741289363\n",
      "2022-03-26 20:25:44.623734 Epoch 300, Training Loss 0.14881147145081663\n",
      "2022-03-26 20:25:44.647217 Epoch 300, Training Loss 0.14928943656213448\n",
      "2022-03-26 20:25:44.671647 Epoch 300, Training Loss 0.14984169680520396\n",
      "2022-03-26 20:25:44.695344 Epoch 300, Training Loss 0.15041546967557018\n",
      "2022-03-26 20:25:44.717971 Epoch 300, Training Loss 0.15100939745259712\n",
      "2022-03-26 20:25:44.740646 Epoch 300, Training Loss 0.15157832794101037\n",
      "2022-03-26 20:25:44.763654 Epoch 300, Training Loss 0.1521979822489002\n",
      "2022-03-26 20:25:44.786398 Epoch 300, Training Loss 0.15291472665412956\n",
      "2022-03-26 20:25:44.814107 Epoch 300, Training Loss 0.15356728714674026\n",
      "2022-03-26 20:25:44.841013 Epoch 300, Training Loss 0.154387065878762\n",
      "2022-03-26 20:25:44.864184 Epoch 300, Training Loss 0.15499574062235824\n",
      "2022-03-26 20:25:44.899159 Epoch 300, Training Loss 0.15570321200829942\n",
      "2022-03-26 20:25:44.923445 Epoch 300, Training Loss 0.15621571275203125\n",
      "2022-03-26 20:25:44.946671 Epoch 300, Training Loss 0.15700721319602884\n",
      "2022-03-26 20:25:44.970213 Epoch 300, Training Loss 0.15768605522106371\n",
      "2022-03-26 20:25:44.993405 Epoch 300, Training Loss 0.15839374772346843\n",
      "2022-03-26 20:25:45.016693 Epoch 300, Training Loss 0.1593893514684094\n",
      "2022-03-26 20:25:45.051392 Epoch 300, Training Loss 0.15997611273966178\n",
      "2022-03-26 20:25:45.074710 Epoch 300, Training Loss 0.16093592046548033\n",
      "2022-03-26 20:25:45.103653 Epoch 300, Training Loss 0.16166542125556171\n",
      "2022-03-26 20:25:45.128204 Epoch 300, Training Loss 0.16237965014660755\n",
      "2022-03-26 20:25:45.151038 Epoch 300, Training Loss 0.16310962442013308\n",
      "2022-03-26 20:25:45.175926 Epoch 300, Training Loss 0.16354157588899593\n",
      "2022-03-26 20:25:45.201073 Epoch 300, Training Loss 0.16412325876067058\n",
      "2022-03-26 20:25:45.224340 Epoch 300, Training Loss 0.1645634320500257\n",
      "2022-03-26 20:25:45.256107 Epoch 300, Training Loss 0.16517613910119552\n",
      "2022-03-26 20:25:45.279801 Epoch 300, Training Loss 0.16563571827567142\n",
      "2022-03-26 20:25:45.303387 Epoch 300, Training Loss 0.1660765940537843\n",
      "2022-03-26 20:25:45.331398 Epoch 300, Training Loss 0.166719124731048\n",
      "2022-03-26 20:25:45.355440 Epoch 300, Training Loss 0.16730887519048\n",
      "2022-03-26 20:25:45.379565 Epoch 300, Training Loss 0.16792144976994572\n",
      "2022-03-26 20:25:45.403478 Epoch 300, Training Loss 0.1688930506787032\n",
      "2022-03-26 20:25:45.427081 Epoch 300, Training Loss 0.1695782111771881\n",
      "2022-03-26 20:25:45.450987 Epoch 300, Training Loss 0.17024515620659075\n",
      "2022-03-26 20:25:45.486433 Epoch 300, Training Loss 0.17081923403627122\n",
      "2022-03-26 20:25:45.510553 Epoch 300, Training Loss 0.17137305954914264\n",
      "2022-03-26 20:25:45.533412 Epoch 300, Training Loss 0.17185976394378316\n",
      "2022-03-26 20:25:45.556340 Epoch 300, Training Loss 0.17251411333794484\n",
      "2022-03-26 20:25:45.583293 Epoch 300, Training Loss 0.17298441159222133\n",
      "2022-03-26 20:25:45.606419 Epoch 300, Training Loss 0.17376271631482923\n",
      "2022-03-26 20:25:45.629913 Epoch 300, Training Loss 0.17443438794683008\n",
      "2022-03-26 20:25:45.652767 Epoch 300, Training Loss 0.1749215182250418\n",
      "2022-03-26 20:25:45.675280 Epoch 300, Training Loss 0.17529223476300765\n",
      "2022-03-26 20:25:45.704558 Epoch 300, Training Loss 0.17590974498054254\n",
      "2022-03-26 20:25:45.730351 Epoch 300, Training Loss 0.1767963909775095\n",
      "2022-03-26 20:25:45.753183 Epoch 300, Training Loss 0.1772518342222704\n",
      "2022-03-26 20:25:45.779348 Epoch 300, Training Loss 0.17802137393704462\n",
      "2022-03-26 20:25:45.802107 Epoch 300, Training Loss 0.17851196543868544\n",
      "2022-03-26 20:25:45.825650 Epoch 300, Training Loss 0.17918135776467944\n",
      "2022-03-26 20:25:45.848982 Epoch 300, Training Loss 0.18008600718453718\n",
      "2022-03-26 20:25:45.871897 Epoch 300, Training Loss 0.18059772883763398\n",
      "2022-03-26 20:25:45.895205 Epoch 300, Training Loss 0.18149913841729884\n",
      "2022-03-26 20:25:45.929653 Epoch 300, Training Loss 0.18211191673489177\n",
      "2022-03-26 20:25:45.962352 Epoch 300, Training Loss 0.18277012015623814\n",
      "2022-03-26 20:25:45.986392 Epoch 300, Training Loss 0.18330751936835096\n",
      "2022-03-26 20:25:46.018302 Epoch 300, Training Loss 0.1840427235302413\n",
      "2022-03-26 20:25:46.042474 Epoch 300, Training Loss 0.18463025474563585\n",
      "2022-03-26 20:25:46.066032 Epoch 300, Training Loss 0.18524728351465578\n",
      "2022-03-26 20:25:46.093353 Epoch 300, Training Loss 0.18598419738471356\n",
      "2022-03-26 20:25:46.116385 Epoch 300, Training Loss 0.1868709386576472\n",
      "2022-03-26 20:25:46.143359 Epoch 300, Training Loss 0.18758626893886823\n",
      "2022-03-26 20:25:46.172145 Epoch 300, Training Loss 0.18800409342093236\n",
      "2022-03-26 20:25:46.197782 Epoch 300, Training Loss 0.18844749370728003\n",
      "2022-03-26 20:25:46.221071 Epoch 300, Training Loss 0.18878451243157277\n",
      "2022-03-26 20:25:46.248228 Epoch 300, Training Loss 0.1892384998976727\n",
      "2022-03-26 20:25:46.271105 Epoch 300, Training Loss 0.19006165665814945\n",
      "2022-03-26 20:25:46.294343 Epoch 300, Training Loss 0.19090054675822368\n",
      "2022-03-26 20:25:46.318486 Epoch 300, Training Loss 0.19167875102185228\n",
      "2022-03-26 20:25:46.344540 Epoch 300, Training Loss 0.1922162503499509\n",
      "2022-03-26 20:25:46.377141 Epoch 300, Training Loss 0.1928356860185523\n",
      "2022-03-26 20:25:46.402143 Epoch 300, Training Loss 0.19346166154384004\n",
      "2022-03-26 20:25:46.425567 Epoch 300, Training Loss 0.1941156183247981\n",
      "2022-03-26 20:25:46.456905 Epoch 300, Training Loss 0.19503613980606083\n",
      "2022-03-26 20:25:46.479698 Epoch 300, Training Loss 0.19562321564044488\n",
      "2022-03-26 20:25:46.502994 Epoch 300, Training Loss 0.19652050381045208\n",
      "2022-03-26 20:25:46.525598 Epoch 300, Training Loss 0.1973062873915638\n",
      "2022-03-26 20:25:46.549148 Epoch 300, Training Loss 0.19808808668418917\n",
      "2022-03-26 20:25:46.571813 Epoch 300, Training Loss 0.198825979914964\n",
      "2022-03-26 20:25:46.603236 Epoch 300, Training Loss 0.19941611769025588\n",
      "2022-03-26 20:25:46.628557 Epoch 300, Training Loss 0.20020640340378826\n",
      "2022-03-26 20:25:46.651340 Epoch 300, Training Loss 0.20068916257308878\n",
      "2022-03-26 20:25:46.674860 Epoch 300, Training Loss 0.20158091833448166\n",
      "2022-03-26 20:25:46.702010 Epoch 300, Training Loss 0.20223773438531115\n",
      "2022-03-26 20:25:46.725467 Epoch 300, Training Loss 0.2031652857847226\n",
      "2022-03-26 20:25:46.749594 Epoch 300, Training Loss 0.20366171362531155\n",
      "2022-03-26 20:25:46.772509 Epoch 300, Training Loss 0.20460540576435415\n",
      "2022-03-26 20:25:46.795590 Epoch 300, Training Loss 0.20518955956105991\n",
      "2022-03-26 20:25:46.823956 Epoch 300, Training Loss 0.20604753610499374\n",
      "2022-03-26 20:25:46.849607 Epoch 300, Training Loss 0.20663199802417584\n",
      "2022-03-26 20:25:46.873079 Epoch 300, Training Loss 0.20736599843139233\n",
      "2022-03-26 20:25:46.897055 Epoch 300, Training Loss 0.20803798241612245\n",
      "2022-03-26 20:25:46.927146 Epoch 300, Training Loss 0.20873276357684295\n",
      "2022-03-26 20:25:46.954564 Epoch 300, Training Loss 0.20950134268121037\n",
      "2022-03-26 20:25:46.987581 Epoch 300, Training Loss 0.2105016747056066\n",
      "2022-03-26 20:25:47.010656 Epoch 300, Training Loss 0.21129693252884824\n",
      "2022-03-26 20:25:47.038592 Epoch 300, Training Loss 0.21201793576978967\n",
      "2022-03-26 20:25:47.067776 Epoch 300, Training Loss 0.21274170780654453\n",
      "2022-03-26 20:25:47.091223 Epoch 300, Training Loss 0.21347711037110795\n",
      "2022-03-26 20:25:47.114078 Epoch 300, Training Loss 0.21420960610403733\n",
      "2022-03-26 20:25:47.139287 Epoch 300, Training Loss 0.21477713416833097\n",
      "2022-03-26 20:25:47.161958 Epoch 300, Training Loss 0.21543863962602128\n",
      "2022-03-26 20:25:47.185080 Epoch 300, Training Loss 0.21610549044654803\n",
      "2022-03-26 20:25:47.208375 Epoch 300, Training Loss 0.2167245892955519\n",
      "2022-03-26 20:25:47.231352 Epoch 300, Training Loss 0.21751482843819175\n",
      "2022-03-26 20:25:47.260596 Epoch 300, Training Loss 0.21805790407807016\n",
      "2022-03-26 20:25:47.286644 Epoch 300, Training Loss 0.21888556030324047\n",
      "2022-03-26 20:25:47.309020 Epoch 300, Training Loss 0.21958949717947895\n",
      "2022-03-26 20:25:47.335679 Epoch 300, Training Loss 0.22017674268130452\n",
      "2022-03-26 20:25:47.358551 Epoch 300, Training Loss 0.22126481068484924\n",
      "2022-03-26 20:25:47.381700 Epoch 300, Training Loss 0.22217492010358655\n",
      "2022-03-26 20:25:47.404379 Epoch 300, Training Loss 0.22293951331883136\n",
      "2022-03-26 20:25:47.427159 Epoch 300, Training Loss 0.22346902913068567\n",
      "2022-03-26 20:25:47.450452 Epoch 300, Training Loss 0.2243848946660071\n",
      "2022-03-26 20:25:47.484190 Epoch 300, Training Loss 0.22518921483431936\n",
      "2022-03-26 20:25:47.509279 Epoch 300, Training Loss 0.22595291070239928\n",
      "2022-03-26 20:25:47.532607 Epoch 300, Training Loss 0.22651536875140027\n",
      "2022-03-26 20:25:47.556468 Epoch 300, Training Loss 0.22719808279172235\n",
      "2022-03-26 20:25:47.580481 Epoch 300, Training Loss 0.22813522131622904\n",
      "2022-03-26 20:25:47.608207 Epoch 300, Training Loss 0.22889939409768795\n",
      "2022-03-26 20:25:47.630969 Epoch 300, Training Loss 0.22964586178436303\n",
      "2022-03-26 20:25:47.653682 Epoch 300, Training Loss 0.23050042760113013\n",
      "2022-03-26 20:25:47.677055 Epoch 300, Training Loss 0.2310503688652802\n",
      "2022-03-26 20:25:47.710628 Epoch 300, Training Loss 0.23180573814741487\n",
      "2022-03-26 20:25:47.735204 Epoch 300, Training Loss 0.2325134677693362\n",
      "2022-03-26 20:25:47.758460 Epoch 300, Training Loss 0.23329100647317175\n",
      "2022-03-26 20:25:47.786213 Epoch 300, Training Loss 0.23379976988372292\n",
      "2022-03-26 20:25:47.809174 Epoch 300, Training Loss 0.23438595664089598\n",
      "2022-03-26 20:25:47.831676 Epoch 300, Training Loss 0.23525360178993182\n",
      "2022-03-26 20:25:47.854668 Epoch 300, Training Loss 0.2360505424154079\n",
      "2022-03-26 20:25:47.877584 Epoch 300, Training Loss 0.236598970735317\n",
      "2022-03-26 20:25:47.901020 Epoch 300, Training Loss 0.2373116936563226\n",
      "2022-03-26 20:25:47.930291 Epoch 300, Training Loss 0.23850845667483556\n",
      "2022-03-26 20:25:47.955411 Epoch 300, Training Loss 0.23915777222046156\n",
      "2022-03-26 20:25:47.980409 Epoch 300, Training Loss 0.23988039332353855\n",
      "2022-03-26 20:25:48.014120 Epoch 300, Training Loss 0.24042960290637466\n",
      "2022-03-26 20:25:48.037479 Epoch 300, Training Loss 0.24106458483068535\n",
      "2022-03-26 20:25:48.061171 Epoch 300, Training Loss 0.24216193995436133\n",
      "2022-03-26 20:25:48.084188 Epoch 300, Training Loss 0.24269110598908666\n",
      "2022-03-26 20:25:48.107599 Epoch 300, Training Loss 0.24333988966615608\n",
      "2022-03-26 20:25:48.133031 Epoch 300, Training Loss 0.2437913687066044\n",
      "2022-03-26 20:25:48.167811 Epoch 300, Training Loss 0.24453247128926275\n",
      "2022-03-26 20:25:48.192512 Epoch 300, Training Loss 0.24496938133864757\n",
      "2022-03-26 20:25:48.216201 Epoch 300, Training Loss 0.24557466006568632\n",
      "2022-03-26 20:25:48.241307 Epoch 300, Training Loss 0.24607110482728695\n",
      "2022-03-26 20:25:48.270576 Epoch 300, Training Loss 0.24673287240821687\n",
      "2022-03-26 20:25:48.295030 Epoch 300, Training Loss 0.2473763457840056\n",
      "2022-03-26 20:25:48.318470 Epoch 300, Training Loss 0.24782975911713012\n",
      "2022-03-26 20:25:48.350811 Epoch 300, Training Loss 0.24894855331505655\n",
      "2022-03-26 20:25:48.378560 Epoch 300, Training Loss 0.24969267820382057\n",
      "2022-03-26 20:25:48.401649 Epoch 300, Training Loss 0.2505091273075784\n",
      "2022-03-26 20:25:48.425371 Epoch 300, Training Loss 0.25114964636619136\n",
      "2022-03-26 20:25:48.452227 Epoch 300, Training Loss 0.2516601643027247\n",
      "2022-03-26 20:25:48.477267 Epoch 300, Training Loss 0.25230218201418364\n",
      "2022-03-26 20:25:48.501144 Epoch 300, Training Loss 0.2528910925588035\n",
      "2022-03-26 20:25:48.523925 Epoch 300, Training Loss 0.2533380529269233\n",
      "2022-03-26 20:25:48.547206 Epoch 300, Training Loss 0.2538569771763309\n",
      "2022-03-26 20:25:48.578716 Epoch 300, Training Loss 0.2545300208584732\n",
      "2022-03-26 20:25:48.602660 Epoch 300, Training Loss 0.2551914721994144\n",
      "2022-03-26 20:25:48.626229 Epoch 300, Training Loss 0.255797078218454\n",
      "2022-03-26 20:25:48.654369 Epoch 300, Training Loss 0.256582290296207\n",
      "2022-03-26 20:25:48.677278 Epoch 300, Training Loss 0.2573292903849841\n",
      "2022-03-26 20:25:48.700665 Epoch 300, Training Loss 0.2578988202735591\n",
      "2022-03-26 20:25:48.724266 Epoch 300, Training Loss 0.2586364493993542\n",
      "2022-03-26 20:25:48.747388 Epoch 300, Training Loss 0.25927076141928773\n",
      "2022-03-26 20:25:48.770788 Epoch 300, Training Loss 0.2599924789250964\n",
      "2022-03-26 20:25:48.803888 Epoch 300, Training Loss 0.26064857842443545\n",
      "2022-03-26 20:25:48.828369 Epoch 300, Training Loss 0.2612419109934431\n",
      "2022-03-26 20:25:48.851650 Epoch 300, Training Loss 0.26189974992705123\n",
      "2022-03-26 20:25:48.878499 Epoch 300, Training Loss 0.2625457875983185\n",
      "2022-03-26 20:25:48.901703 Epoch 300, Training Loss 0.2632142850924331\n",
      "2022-03-26 20:25:48.924494 Epoch 300, Training Loss 0.26384843098919103\n",
      "2022-03-26 20:25:48.947796 Epoch 300, Training Loss 0.2643629887029338\n",
      "2022-03-26 20:25:48.970716 Epoch 300, Training Loss 0.2648548217266417\n",
      "2022-03-26 20:25:48.993692 Epoch 300, Training Loss 0.2654085576038836\n",
      "2022-03-26 20:25:49.038002 Epoch 300, Training Loss 0.26604619323063994\n",
      "2022-03-26 20:25:49.065410 Epoch 300, Training Loss 0.2668035469206093\n",
      "2022-03-26 20:25:49.088591 Epoch 300, Training Loss 0.2674005860296052\n",
      "2022-03-26 20:25:49.111946 Epoch 300, Training Loss 0.268218328295004\n",
      "2022-03-26 20:25:49.140922 Epoch 300, Training Loss 0.2687721890218727\n",
      "2022-03-26 20:25:49.164477 Epoch 300, Training Loss 0.2693301631171075\n",
      "2022-03-26 20:25:49.187629 Epoch 300, Training Loss 0.27002338884050586\n",
      "2022-03-26 20:25:49.210513 Epoch 300, Training Loss 0.2704879856284927\n",
      "2022-03-26 20:25:49.233717 Epoch 300, Training Loss 0.27120421470507333\n",
      "2022-03-26 20:25:49.265949 Epoch 300, Training Loss 0.2719665740418922\n",
      "2022-03-26 20:25:49.290257 Epoch 300, Training Loss 0.27293009257606227\n",
      "2022-03-26 20:25:49.313454 Epoch 300, Training Loss 0.2735976696281177\n",
      "2022-03-26 20:25:49.342910 Epoch 300, Training Loss 0.27448568493127823\n",
      "2022-03-26 20:25:49.365975 Epoch 300, Training Loss 0.2751518724786351\n",
      "2022-03-26 20:25:49.388848 Epoch 300, Training Loss 0.2756894739044597\n",
      "2022-03-26 20:25:49.412233 Epoch 300, Training Loss 0.2763591804887023\n",
      "2022-03-26 20:25:49.435673 Epoch 300, Training Loss 0.2771055704301885\n",
      "2022-03-26 20:25:49.459502 Epoch 300, Training Loss 0.27779593901789706\n",
      "2022-03-26 20:25:49.491347 Epoch 300, Training Loss 0.27857855433012213\n",
      "2022-03-26 20:25:49.516294 Epoch 300, Training Loss 0.279227787862196\n",
      "2022-03-26 20:25:49.539490 Epoch 300, Training Loss 0.2797353919166738\n",
      "2022-03-26 20:25:49.563201 Epoch 300, Training Loss 0.28056458155136277\n",
      "2022-03-26 20:25:49.588603 Epoch 300, Training Loss 0.28114436180009256\n",
      "2022-03-26 20:25:49.611473 Epoch 300, Training Loss 0.281777339156174\n",
      "2022-03-26 20:25:49.635351 Epoch 300, Training Loss 0.2825060850557159\n",
      "2022-03-26 20:25:49.658220 Epoch 300, Training Loss 0.283421514836876\n",
      "2022-03-26 20:25:49.681388 Epoch 300, Training Loss 0.28394780220354304\n",
      "2022-03-26 20:25:49.710574 Epoch 300, Training Loss 0.28468043787781233\n",
      "2022-03-26 20:25:49.738617 Epoch 300, Training Loss 0.2852398715246364\n",
      "2022-03-26 20:25:49.761620 Epoch 300, Training Loss 0.285785570569203\n",
      "2022-03-26 20:25:49.784786 Epoch 300, Training Loss 0.2863374014987665\n",
      "2022-03-26 20:25:49.811663 Epoch 300, Training Loss 0.28729304955209917\n",
      "2022-03-26 20:25:49.835316 Epoch 300, Training Loss 0.28818460325222184\n",
      "2022-03-26 20:25:49.859209 Epoch 300, Training Loss 0.28885154553767667\n",
      "2022-03-26 20:25:49.883036 Epoch 300, Training Loss 0.2896950781688361\n",
      "2022-03-26 20:25:49.905872 Epoch 300, Training Loss 0.2902977915714159\n",
      "2022-03-26 20:25:49.941410 Epoch 300, Training Loss 0.29114599394447666\n",
      "2022-03-26 20:25:49.968153 Epoch 300, Training Loss 0.291695644693149\n",
      "2022-03-26 20:25:49.991680 Epoch 300, Training Loss 0.2922086938834556\n",
      "2022-03-26 20:25:50.016955 Epoch 300, Training Loss 0.2930222397379558\n",
      "2022-03-26 20:25:50.049106 Epoch 300, Training Loss 0.293833254757897\n",
      "2022-03-26 20:25:50.079192 Epoch 300, Training Loss 0.29445793498736206\n",
      "2022-03-26 20:25:50.102872 Epoch 300, Training Loss 0.2952776075438465\n",
      "2022-03-26 20:25:50.126312 Epoch 300, Training Loss 0.2961523070778993\n",
      "2022-03-26 20:25:50.154394 Epoch 300, Training Loss 0.29680327890093067\n",
      "2022-03-26 20:25:50.181643 Epoch 300, Training Loss 0.2975901137951695\n",
      "2022-03-26 20:25:50.205225 Epoch 300, Training Loss 0.29818312730401986\n",
      "2022-03-26 20:25:50.230683 Epoch 300, Training Loss 0.2990981559352497\n",
      "2022-03-26 20:25:50.254203 Epoch 300, Training Loss 0.2996749994738023\n",
      "2022-03-26 20:25:50.277060 Epoch 300, Training Loss 0.30024389128017304\n",
      "2022-03-26 20:25:50.300166 Epoch 300, Training Loss 0.3012969465092625\n",
      "2022-03-26 20:25:50.323065 Epoch 300, Training Loss 0.30186638927749354\n",
      "2022-03-26 20:25:50.348716 Epoch 300, Training Loss 0.30268834121620564\n",
      "2022-03-26 20:25:50.381353 Epoch 300, Training Loss 0.3033610426670755\n",
      "2022-03-26 20:25:50.405424 Epoch 300, Training Loss 0.30389798783204136\n",
      "2022-03-26 20:25:50.428682 Epoch 300, Training Loss 0.30478978566730114\n",
      "2022-03-26 20:25:50.452568 Epoch 300, Training Loss 0.30591093958415033\n",
      "2022-03-26 20:25:50.480312 Epoch 300, Training Loss 0.3067877082835378\n",
      "2022-03-26 20:25:50.503567 Epoch 300, Training Loss 0.3074880283316383\n",
      "2022-03-26 20:25:50.526612 Epoch 300, Training Loss 0.30817029129742357\n",
      "2022-03-26 20:25:50.550470 Epoch 300, Training Loss 0.30892712950630263\n",
      "2022-03-26 20:25:50.574157 Epoch 300, Training Loss 0.3096456236356055\n",
      "2022-03-26 20:25:50.604896 Epoch 300, Training Loss 0.31022490848741874\n",
      "2022-03-26 20:25:50.629070 Epoch 300, Training Loss 0.31102704569278167\n",
      "2022-03-26 20:25:50.651654 Epoch 300, Training Loss 0.3115799165023562\n",
      "2022-03-26 20:25:50.674134 Epoch 300, Training Loss 0.31219638840240593\n",
      "2022-03-26 20:25:50.701814 Epoch 300, Training Loss 0.3128719216836688\n",
      "2022-03-26 20:25:50.724495 Epoch 300, Training Loss 0.3134334227427497\n",
      "2022-03-26 20:25:50.747406 Epoch 300, Training Loss 0.31401556728364866\n",
      "2022-03-26 20:25:50.770671 Epoch 300, Training Loss 0.3146027042852033\n",
      "2022-03-26 20:25:50.793648 Epoch 300, Training Loss 0.3151210612996155\n",
      "2022-03-26 20:25:50.825103 Epoch 300, Training Loss 0.31544105321778665\n",
      "2022-03-26 20:25:50.857390 Epoch 300, Training Loss 0.3160285741052664\n",
      "2022-03-26 20:25:50.881414 Epoch 300, Training Loss 0.31667103199169155\n",
      "2022-03-26 20:25:50.906750 Epoch 300, Training Loss 0.31713482407886356\n",
      "2022-03-26 20:25:50.933769 Epoch 300, Training Loss 0.31771798048863936\n",
      "2022-03-26 20:25:50.956448 Epoch 300, Training Loss 0.3182170424429352\n",
      "2022-03-26 20:25:50.979754 Epoch 300, Training Loss 0.31864929079171034\n",
      "2022-03-26 20:25:51.003076 Epoch 300, Training Loss 0.3192670143893003\n",
      "2022-03-26 20:25:51.026421 Epoch 300, Training Loss 0.3200040518132317\n",
      "2022-03-26 20:25:51.059283 Epoch 300, Training Loss 0.3206013471383573\n",
      "2022-03-26 20:25:51.093124 Epoch 300, Training Loss 0.32114046547190306\n",
      "2022-03-26 20:25:51.116562 Epoch 300, Training Loss 0.3217109823051621\n",
      "2022-03-26 20:25:51.142319 Epoch 300, Training Loss 0.3222324480600369\n",
      "2022-03-26 20:25:51.166057 Epoch 300, Training Loss 0.3229007550784389\n",
      "2022-03-26 20:25:51.189170 Epoch 300, Training Loss 0.3234158105519422\n",
      "2022-03-26 20:25:51.212029 Epoch 300, Training Loss 0.3241339512455189\n",
      "2022-03-26 20:25:51.235992 Epoch 300, Training Loss 0.3247945554687849\n",
      "2022-03-26 20:25:51.259224 Epoch 300, Training Loss 0.325497932808326\n",
      "2022-03-26 20:25:51.292318 Epoch 300, Training Loss 0.32606190014297093\n",
      "2022-03-26 20:25:51.316202 Epoch 300, Training Loss 0.3266735614451301\n",
      "2022-03-26 20:25:51.341468 Epoch 300, Training Loss 0.32706919280083285\n",
      "2022-03-26 20:25:51.369301 Epoch 300, Training Loss 0.32805768758668313\n",
      "2022-03-26 20:25:51.392294 Epoch 300, Training Loss 0.32880841113646014\n",
      "2022-03-26 20:25:51.414992 Epoch 300, Training Loss 0.3294123986073772\n",
      "2022-03-26 20:25:51.439434 Epoch 300, Training Loss 0.33003226347515346\n",
      "2022-03-26 20:25:51.463938 Epoch 300, Training Loss 0.330738761750481\n",
      "2022-03-26 20:25:51.488955 Epoch 300, Training Loss 0.3315128148021296\n",
      "2022-03-26 20:25:51.522981 Epoch 300, Training Loss 0.3320335838610254\n",
      "2022-03-26 20:25:51.548144 Epoch 300, Training Loss 0.33271451936582164\n",
      "2022-03-26 20:25:51.572726 Epoch 300, Training Loss 0.3332605548869923\n",
      "2022-03-26 20:25:51.598851 Epoch 300, Training Loss 0.3339527911885315\n",
      "2022-03-26 20:25:51.622171 Epoch 300, Training Loss 0.33476894127819545\n",
      "2022-03-26 20:25:51.645543 Epoch 300, Training Loss 0.33553449144524994\n",
      "2022-03-26 20:25:51.668565 Epoch 300, Training Loss 0.3362814440484852\n",
      "2022-03-26 20:25:51.691571 Epoch 300, Training Loss 0.33695245995317274\n",
      "2022-03-26 20:25:51.715096 Epoch 300, Training Loss 0.33757566085175783\n",
      "2022-03-26 20:25:51.747012 Epoch 300, Training Loss 0.33833841478352045\n",
      "2022-03-26 20:25:51.772221 Epoch 300, Training Loss 0.33896560351485794\n",
      "2022-03-26 20:25:51.795479 Epoch 300, Training Loss 0.33959788272676567\n",
      "2022-03-26 20:25:51.820156 Epoch 300, Training Loss 0.34005660346478145\n",
      "2022-03-26 20:25:51.846386 Epoch 300, Training Loss 0.340670132252109\n",
      "2022-03-26 20:25:51.870249 Epoch 300, Training Loss 0.34152613353470096\n",
      "2022-03-26 20:25:51.893611 Epoch 300, Training Loss 0.3421554243587472\n",
      "2022-03-26 20:25:51.917357 Epoch 300, Training Loss 0.3428809365919789\n",
      "2022-03-26 20:25:51.941554 Epoch 300, Training Loss 0.3436743131340922\n",
      "2022-03-26 20:25:51.975853 Epoch 300, Training Loss 0.34423894423734197\n",
      "2022-03-26 20:25:51.999939 Epoch 300, Training Loss 0.34504351014142753\n",
      "2022-03-26 20:25:52.023023 Epoch 300, Training Loss 0.3456410324901266\n",
      "2022-03-26 20:25:52.056940 Epoch 300, Training Loss 0.3463936096528912\n",
      "2022-03-26 20:25:52.080633 Epoch 300, Training Loss 0.34711331820777613\n",
      "2022-03-26 20:25:52.110854 Epoch 300, Training Loss 0.3477863060200916\n",
      "2022-03-26 20:25:52.140613 Epoch 300, Training Loss 0.3482934809897257\n",
      "2022-03-26 20:25:52.163814 Epoch 300, Training Loss 0.34877247630101643\n",
      "2022-03-26 20:25:52.194718 Epoch 300, Training Loss 0.3496498196288143\n",
      "2022-03-26 20:25:52.220464 Epoch 300, Training Loss 0.35027935195838095\n",
      "2022-03-26 20:25:52.243787 Epoch 300, Training Loss 0.35094744144269574\n",
      "2022-03-26 20:25:52.268033 Epoch 300, Training Loss 0.3515287413430946\n",
      "2022-03-26 20:25:52.292779 Epoch 300, Training Loss 0.35215780209473635\n",
      "2022-03-26 20:25:52.315858 Epoch 300, Training Loss 0.35274648203340636\n",
      "2022-03-26 20:25:52.342532 Epoch 300, Training Loss 0.35335486130717464\n",
      "2022-03-26 20:25:52.366346 Epoch 300, Training Loss 0.3541217421364906\n",
      "2022-03-26 20:25:52.390353 Epoch 300, Training Loss 0.35476783278119534\n",
      "2022-03-26 20:25:52.423509 Epoch 300, Training Loss 0.3554815913138487\n",
      "2022-03-26 20:25:52.448056 Epoch 300, Training Loss 0.3561537244626323\n",
      "2022-03-26 20:25:52.473037 Epoch 300, Training Loss 0.3566228603501149\n",
      "2022-03-26 20:25:52.500550 Epoch 300, Training Loss 0.3575095373689366\n",
      "2022-03-26 20:25:52.524632 Epoch 300, Training Loss 0.3582823063292162\n",
      "2022-03-26 20:25:52.549123 Epoch 300, Training Loss 0.358826369942759\n",
      "2022-03-26 20:25:52.573624 Epoch 300, Training Loss 0.3592914847271217\n",
      "2022-03-26 20:25:52.597863 Epoch 300, Training Loss 0.36010834252666635\n",
      "2022-03-26 20:25:52.622390 Epoch 300, Training Loss 0.3606075479665681\n",
      "2022-03-26 20:25:52.656631 Epoch 300, Training Loss 0.36104638441977904\n",
      "2022-03-26 20:25:52.681373 Epoch 300, Training Loss 0.36155018574365266\n",
      "2022-03-26 20:25:52.705391 Epoch 300, Training Loss 0.36227679654689093\n",
      "2022-03-26 20:25:52.730487 Epoch 300, Training Loss 0.36302844785592137\n",
      "2022-03-26 20:25:52.755905 Epoch 300, Training Loss 0.36352572544380224\n",
      "2022-03-26 20:25:52.778889 Epoch 300, Training Loss 0.3640922931263514\n",
      "2022-03-26 20:25:52.802202 Epoch 300, Training Loss 0.36495591983999437\n",
      "2022-03-26 20:25:52.825933 Epoch 300, Training Loss 0.36579006862686114\n",
      "2022-03-26 20:25:52.850152 Epoch 300, Training Loss 0.36637848628985\n",
      "2022-03-26 20:25:52.883876 Epoch 300, Training Loss 0.3669634390708126\n",
      "2022-03-26 20:25:52.908941 Epoch 300, Training Loss 0.36747643936549307\n",
      "2022-03-26 20:25:52.933830 Epoch 300, Training Loss 0.3682301484067422\n",
      "2022-03-26 20:25:52.968381 Epoch 300, Training Loss 0.36889764077751835\n",
      "2022-03-26 20:25:52.992960 Epoch 300, Training Loss 0.36959873629576717\n",
      "2022-03-26 20:25:53.020706 Epoch 300, Training Loss 0.37011229342130747\n",
      "2022-03-26 20:25:53.044635 Epoch 300, Training Loss 0.3707680271371551\n",
      "2022-03-26 20:25:53.068373 Epoch 300, Training Loss 0.37155150039040524\n",
      "2022-03-26 20:25:53.098328 Epoch 300, Training Loss 0.3722541810720778\n",
      "2022-03-26 20:25:53.125590 Epoch 300, Training Loss 0.373197828621968\n",
      "2022-03-26 20:25:53.157848 Epoch 300, Training Loss 0.37410966718516997\n",
      "2022-03-26 20:25:53.185735 Epoch 300, Training Loss 0.3748630529741192\n",
      "2022-03-26 20:25:53.209152 Epoch 300, Training Loss 0.3758623923753838\n",
      "2022-03-26 20:25:53.232901 Epoch 300, Training Loss 0.37678020740942575\n",
      "2022-03-26 20:25:53.256633 Epoch 300, Training Loss 0.37755362409383747\n",
      "2022-03-26 20:25:53.280392 Epoch 300, Training Loss 0.3783409694385955\n",
      "2022-03-26 20:25:53.309049 Epoch 300, Training Loss 0.3792183180256268\n",
      "2022-03-26 20:25:53.337384 Epoch 300, Training Loss 0.38003744011568596\n",
      "2022-03-26 20:25:53.360781 Epoch 300, Training Loss 0.380469743648301\n",
      "2022-03-26 20:25:53.384559 Epoch 300, Training Loss 0.38142647502748556\n",
      "2022-03-26 20:25:53.410818 Epoch 300, Training Loss 0.38230562844620947\n",
      "2022-03-26 20:25:53.434636 Epoch 300, Training Loss 0.3829496456572162\n",
      "2022-03-26 20:25:53.457640 Epoch 300, Training Loss 0.3834237382959222\n",
      "2022-03-26 20:25:53.482025 Epoch 300, Training Loss 0.3840430417214818\n",
      "2022-03-26 20:25:53.505261 Epoch 300, Training Loss 0.3845294248455626\n",
      "2022-03-26 20:25:53.538075 Epoch 300, Training Loss 0.38503962619911375\n",
      "2022-03-26 20:25:53.562509 Epoch 300, Training Loss 0.38594890496386286\n",
      "2022-03-26 20:25:53.585900 Epoch 300, Training Loss 0.3866516981473969\n",
      "2022-03-26 20:25:53.611797 Epoch 300, Training Loss 0.3873947519818535\n",
      "2022-03-26 20:25:53.635692 Epoch 300, Training Loss 0.3881852782481467\n",
      "2022-03-26 20:25:53.658833 Epoch 300, Training Loss 0.388934871291413\n",
      "2022-03-26 20:25:53.681702 Epoch 300, Training Loss 0.38925975513503985\n",
      "2022-03-26 20:25:53.704610 Epoch 300, Training Loss 0.38989557011429304\n",
      "2022-03-26 20:25:53.727855 Epoch 300, Training Loss 0.39064892777777693\n",
      "2022-03-26 20:25:53.758726 Epoch 300, Training Loss 0.3911706537884824\n",
      "2022-03-26 20:25:53.785538 Epoch 300, Training Loss 0.391934914452493\n",
      "2022-03-26 20:25:53.808614 Epoch 300, Training Loss 0.39247704034342484\n",
      "2022-03-26 20:25:53.833838 Epoch 300, Training Loss 0.39347320972272504\n",
      "2022-03-26 20:25:53.866840 Epoch 300, Training Loss 0.3940506662287371\n",
      "2022-03-26 20:25:53.890800 Epoch 300, Training Loss 0.39472127724867645\n",
      "2022-03-26 20:25:53.913877 Epoch 300, Training Loss 0.39526724146531367\n",
      "2022-03-26 20:25:53.940456 Epoch 300, Training Loss 0.3958776487070886\n",
      "2022-03-26 20:25:53.970445 Epoch 300, Training Loss 0.39649091476141035\n",
      "2022-03-26 20:25:53.997502 Epoch 300, Training Loss 0.39695522175801684\n",
      "2022-03-26 20:25:54.020628 Epoch 300, Training Loss 0.3975968023433405\n",
      "2022-03-26 20:25:54.043491 Epoch 300, Training Loss 0.39852515962498875\n",
      "2022-03-26 20:25:54.070245 Epoch 300, Training Loss 0.39923858667349876\n",
      "2022-03-26 20:25:54.093270 Epoch 300, Training Loss 0.39992354838820676\n",
      "2022-03-26 20:25:54.116358 Epoch 300, Training Loss 0.40048204642503765\n",
      "2022-03-26 20:25:54.139749 Epoch 300, Training Loss 0.4012471233373103\n",
      "2022-03-26 20:25:54.173294 Epoch 300, Training Loss 0.40185735881557244\n",
      "2022-03-26 20:25:54.206239 Epoch 300, Training Loss 0.40245649769254355\n",
      "2022-03-26 20:25:54.229690 Epoch 300, Training Loss 0.40317030188144015\n",
      "2022-03-26 20:25:54.253063 Epoch 300, Training Loss 0.40396267368131894\n",
      "2022-03-26 20:25:54.280590 Epoch 300, Training Loss 0.40452038458622325\n",
      "2022-03-26 20:25:54.304168 Epoch 300, Training Loss 0.40509869327859194\n",
      "2022-03-26 20:25:54.327858 Epoch 300, Training Loss 0.4059034972201528\n",
      "2022-03-26 20:25:54.351235 Epoch 300, Training Loss 0.40626104136028557\n",
      "2022-03-26 20:25:54.375223 Epoch 300, Training Loss 0.4069026782727607\n",
      "2022-03-26 20:25:54.408458 Epoch 300, Training Loss 0.40764364868859804\n",
      "2022-03-26 20:25:54.431732 Epoch 300, Training Loss 0.40831185097965744\n",
      "2022-03-26 20:25:54.454847 Epoch 300, Training Loss 0.40906521519812783\n",
      "2022-03-26 20:25:54.482248 Epoch 300, Training Loss 0.40974779272704476\n",
      "2022-03-26 20:25:54.505618 Epoch 300, Training Loss 0.4105908911284583\n",
      "2022-03-26 20:25:54.534378 Epoch 300, Training Loss 0.41141305503714115\n",
      "2022-03-26 20:25:54.558337 Epoch 300, Training Loss 0.4120464551517421\n",
      "2022-03-26 20:25:54.582806 Epoch 300, Training Loss 0.4127815740988078\n",
      "2022-03-26 20:25:54.606005 Epoch 300, Training Loss 0.4134398566373169\n",
      "2022-03-26 20:25:54.639444 Epoch 300, Training Loss 0.41403327552635044\n",
      "2022-03-26 20:25:54.663720 Epoch 300, Training Loss 0.41477927430282774\n",
      "2022-03-26 20:25:54.687267 Epoch 300, Training Loss 0.4157392441502313\n",
      "2022-03-26 20:25:54.713669 Epoch 300, Training Loss 0.41638660760563045\n",
      "2022-03-26 20:25:54.736918 Epoch 300, Training Loss 0.41694159956310717\n",
      "2022-03-26 20:25:54.760539 Epoch 300, Training Loss 0.41768330947288773\n",
      "2022-03-26 20:25:54.783767 Epoch 300, Training Loss 0.4183693703673685\n",
      "2022-03-26 20:25:54.806615 Epoch 300, Training Loss 0.4192189376258179\n",
      "2022-03-26 20:25:54.829647 Epoch 300, Training Loss 0.4203098850215183\n",
      "2022-03-26 20:25:54.862806 Epoch 300, Training Loss 0.4210434267321206\n",
      "2022-03-26 20:25:54.886907 Epoch 300, Training Loss 0.4220211667668484\n",
      "2022-03-26 20:25:54.909935 Epoch 300, Training Loss 0.42279142188980146\n",
      "2022-03-26 20:25:54.937329 Epoch 300, Training Loss 0.42362135627766706\n",
      "2022-03-26 20:25:54.960482 Epoch 300, Training Loss 0.42426513880491257\n",
      "2022-03-26 20:25:54.983920 Epoch 300, Training Loss 0.42466038284475544\n",
      "2022-03-26 20:25:55.007477 Epoch 300, Training Loss 0.42527958630677076\n",
      "2022-03-26 20:25:55.030487 Epoch 300, Training Loss 0.425937083161548\n",
      "2022-03-26 20:25:55.054763 Epoch 300, Training Loss 0.42688653507577184\n",
      "2022-03-26 20:25:55.086015 Epoch 300, Training Loss 0.42772155613316903\n",
      "2022-03-26 20:25:55.114190 Epoch 300, Training Loss 0.42828321291128996\n",
      "2022-03-26 20:25:55.137325 Epoch 300, Training Loss 0.4290636739957973\n",
      "2022-03-26 20:25:55.161821 Epoch 300, Training Loss 0.42960907638911394\n",
      "2022-03-26 20:25:55.199387 Epoch 300, Training Loss 0.43015707425220545\n",
      "2022-03-26 20:25:55.223423 Epoch 300, Training Loss 0.4310030326856982\n",
      "2022-03-26 20:25:55.246498 Epoch 300, Training Loss 0.4318639704066774\n",
      "2022-03-26 20:25:55.270054 Epoch 300, Training Loss 0.4325083304587228\n",
      "2022-03-26 20:25:55.298921 Epoch 300, Training Loss 0.4330692118429162\n",
      "2022-03-26 20:25:55.326374 Epoch 300, Training Loss 0.43399227757359404\n",
      "2022-03-26 20:25:55.351220 Epoch 300, Training Loss 0.43456440525667744\n",
      "2022-03-26 20:25:55.376929 Epoch 300, Training Loss 0.4352418692101298\n",
      "2022-03-26 20:25:55.401700 Epoch 300, Training Loss 0.43616169167068\n",
      "2022-03-26 20:25:55.424487 Epoch 300, Training Loss 0.43687101857512806\n",
      "2022-03-26 20:25:55.447225 Epoch 300, Training Loss 0.43752818874767063\n",
      "2022-03-26 20:25:55.470497 Epoch 300, Training Loss 0.4378421470103666\n",
      "2022-03-26 20:25:55.493985 Epoch 300, Training Loss 0.4385347280584638\n",
      "2022-03-26 20:25:55.525911 Epoch 300, Training Loss 0.43928715148392844\n",
      "2022-03-26 20:25:55.556966 Epoch 300, Training Loss 0.4396975336172392\n",
      "2022-03-26 20:25:55.580671 Epoch 300, Training Loss 0.4404687227495491\n",
      "2022-03-26 20:25:55.603818 Epoch 300, Training Loss 0.44110331751044146\n",
      "2022-03-26 20:25:55.628668 Epoch 300, Training Loss 0.441729701731516\n",
      "2022-03-26 20:25:55.651639 Epoch 300, Training Loss 0.44214564085464036\n",
      "2022-03-26 20:25:55.675105 Epoch 300, Training Loss 0.4426257415576969\n",
      "2022-03-26 20:25:55.698469 Epoch 300, Training Loss 0.44331329630311495\n",
      "2022-03-26 20:25:55.721392 Epoch 300, Training Loss 0.4441608838031969\n",
      "2022-03-26 20:25:55.755104 Epoch 300, Training Loss 0.44486116970439094\n",
      "2022-03-26 20:25:55.778823 Epoch 300, Training Loss 0.44575880757530634\n",
      "2022-03-26 20:25:55.801558 Epoch 300, Training Loss 0.44635394310859766\n",
      "2022-03-26 20:25:55.825159 Epoch 300, Training Loss 0.4468903565193381\n",
      "2022-03-26 20:25:55.850499 Epoch 300, Training Loss 0.44748329113968804\n",
      "2022-03-26 20:25:55.873089 Epoch 300, Training Loss 0.44835592829205495\n",
      "2022-03-26 20:25:55.895636 Epoch 300, Training Loss 0.4488573536238707\n",
      "2022-03-26 20:25:55.918151 Epoch 300, Training Loss 0.44953650228507686\n",
      "2022-03-26 20:25:55.941041 Epoch 300, Training Loss 0.4506744996208669\n",
      "2022-03-26 20:25:55.976459 Epoch 300, Training Loss 0.4514953532182347\n",
      "2022-03-26 20:25:56.003504 Epoch 300, Training Loss 0.4521245442311782\n",
      "2022-03-26 20:25:56.027085 Epoch 300, Training Loss 0.45283341594516774\n",
      "2022-03-26 20:25:56.052057 Epoch 300, Training Loss 0.4534179002732572\n",
      "2022-03-26 20:25:56.083138 Epoch 300, Training Loss 0.4540768389201835\n",
      "2022-03-26 20:25:56.106224 Epoch 300, Training Loss 0.45491956498311914\n",
      "2022-03-26 20:25:56.129668 Epoch 300, Training Loss 0.45571603014341094\n",
      "2022-03-26 20:25:56.152576 Epoch 300, Training Loss 0.45677951549934914\n",
      "2022-03-26 20:25:56.175664 Epoch 300, Training Loss 0.4575262357054464\n",
      "2022-03-26 20:25:56.209392 Epoch 300, Training Loss 0.4581582116348969\n",
      "2022-03-26 20:25:56.241985 Epoch 300, Training Loss 0.45895547223517963\n",
      "2022-03-26 20:25:56.268179 Epoch 300, Training Loss 0.4596479278238838\n",
      "2022-03-26 20:25:56.291863 Epoch 300, Training Loss 0.4606694310065121\n",
      "2022-03-26 20:25:56.315707 Epoch 300, Training Loss 0.46127680595726006\n",
      "2022-03-26 20:25:56.343082 Epoch 300, Training Loss 0.4617774544088432\n",
      "2022-03-26 20:25:56.367339 Epoch 300, Training Loss 0.4623072588306559\n",
      "2022-03-26 20:25:56.390342 Epoch 300, Training Loss 0.46270383658159114\n",
      "2022-03-26 20:25:56.416234 Epoch 300, Training Loss 0.46353516565716785\n",
      "2022-03-26 20:25:56.446462 Epoch 300, Training Loss 0.4640014792418541\n",
      "2022-03-26 20:25:56.470208 Epoch 300, Training Loss 0.4648530751161868\n",
      "2022-03-26 20:25:56.495263 Epoch 300, Training Loss 0.4653831224536042\n",
      "2022-03-26 20:25:56.522509 Epoch 300, Training Loss 0.46602844113431624\n",
      "2022-03-26 20:25:56.546428 Epoch 300, Training Loss 0.4664766368125101\n",
      "2022-03-26 20:25:56.570269 Epoch 300, Training Loss 0.4672811388435876\n",
      "2022-03-26 20:25:56.593721 Epoch 300, Training Loss 0.46789866872608205\n",
      "2022-03-26 20:25:56.617287 Epoch 300, Training Loss 0.4687772407327467\n",
      "2022-03-26 20:25:56.652502 Epoch 300, Training Loss 0.4695032708861334\n",
      "2022-03-26 20:25:56.677422 Epoch 300, Training Loss 0.4701604756629071\n",
      "2022-03-26 20:25:56.701608 Epoch 300, Training Loss 0.4707501964724582\n",
      "2022-03-26 20:25:56.725665 Epoch 300, Training Loss 0.47129150657245267\n",
      "2022-03-26 20:25:56.753756 Epoch 300, Training Loss 0.47188615227294395\n",
      "2022-03-26 20:25:56.777766 Epoch 300, Training Loss 0.47273595421515463\n",
      "2022-03-26 20:25:56.801714 Epoch 300, Training Loss 0.4734180506385501\n",
      "2022-03-26 20:25:56.825396 Epoch 300, Training Loss 0.4739524267823495\n",
      "2022-03-26 20:25:56.849417 Epoch 300, Training Loss 0.47456222597290487\n",
      "2022-03-26 20:25:56.888560 Epoch 300, Training Loss 0.475283172810474\n",
      "2022-03-26 20:25:56.913225 Epoch 300, Training Loss 0.4757572252808325\n",
      "2022-03-26 20:25:56.936890 Epoch 300, Training Loss 0.47648738225555176\n",
      "2022-03-26 20:25:56.962611 Epoch 300, Training Loss 0.47740773269739906\n",
      "2022-03-26 20:25:56.989356 Epoch 300, Training Loss 0.47806065843995577\n",
      "2022-03-26 20:25:57.013292 Epoch 300, Training Loss 0.4788405040417181\n",
      "2022-03-26 20:25:57.037135 Epoch 300, Training Loss 0.47944606013615115\n",
      "2022-03-26 20:25:57.061020 Epoch 300, Training Loss 0.4799325707394754\n",
      "2022-03-26 20:25:57.084835 Epoch 300, Training Loss 0.4804040480528951\n",
      "2022-03-26 20:25:57.121046 Epoch 300, Training Loss 0.48092586388978203\n",
      "2022-03-26 20:25:57.145006 Epoch 300, Training Loss 0.48163412964862323\n",
      "2022-03-26 20:25:57.168181 Epoch 300, Training Loss 0.4823039711436347\n",
      "2022-03-26 20:25:57.198011 Epoch 300, Training Loss 0.4829785587537624\n",
      "2022-03-26 20:25:57.221749 Epoch 300, Training Loss 0.48344027694991176\n",
      "2022-03-26 20:25:57.254841 Epoch 300, Training Loss 0.4841366258194989\n",
      "2022-03-26 20:25:57.280223 Epoch 300, Training Loss 0.4847215069148242\n",
      "2022-03-26 20:25:57.304414 Epoch 300, Training Loss 0.4853625094418026\n",
      "2022-03-26 20:25:57.339423 Epoch 300, Training Loss 0.4860825246328588\n",
      "2022-03-26 20:25:57.364866 Epoch 300, Training Loss 0.48697307092301984\n",
      "2022-03-26 20:25:57.390291 Epoch 300, Training Loss 0.48743169752838056\n",
      "2022-03-26 20:25:57.415075 Epoch 300, Training Loss 0.48844935072352513\n",
      "2022-03-26 20:25:57.439809 Epoch 300, Training Loss 0.48932683284935136\n",
      "2022-03-26 20:25:57.463644 Epoch 300, Training Loss 0.4897622369668063\n",
      "2022-03-26 20:25:57.488343 Epoch 300, Training Loss 0.49044799252086896\n",
      "2022-03-26 20:25:57.514060 Epoch 300, Training Loss 0.4912542749167708\n",
      "2022-03-26 20:25:57.536670 Epoch 300, Training Loss 0.49184634679418693\n",
      "2022-03-26 20:25:57.567563 Epoch 300, Training Loss 0.4926263904937393\n",
      "2022-03-26 20:25:57.590795 Epoch 300, Training Loss 0.4930133687337036\n",
      "2022-03-26 20:25:57.613078 Epoch 300, Training Loss 0.4934067233749058\n",
      "2022-03-26 20:25:57.639261 Epoch 300, Training Loss 0.49399709434765376\n",
      "2022-03-26 20:25:57.661546 Epoch 300, Training Loss 0.49475524576423724\n",
      "2022-03-26 20:25:57.684123 Epoch 300, Training Loss 0.4953481046592488\n",
      "2022-03-26 20:25:57.706724 Epoch 300, Training Loss 0.49616890154836124\n",
      "2022-03-26 20:25:57.729430 Epoch 300, Training Loss 0.4968884520213622\n",
      "2022-03-26 20:25:57.752554 Epoch 300, Training Loss 0.49764872161323764\n",
      "2022-03-26 20:25:57.781296 Epoch 300, Training Loss 0.4982596828657038\n",
      "2022-03-26 20:25:57.807377 Epoch 300, Training Loss 0.49895457294590945\n",
      "2022-03-26 20:25:57.830023 Epoch 300, Training Loss 0.49948399085218037\n",
      "2022-03-26 20:25:57.856350 Epoch 300, Training Loss 0.5001309822739848\n",
      "2022-03-26 20:25:57.879628 Epoch 300, Training Loss 0.5007860553081688\n",
      "2022-03-26 20:25:57.902162 Epoch 300, Training Loss 0.5016070417583446\n",
      "2022-03-26 20:25:57.924687 Epoch 300, Training Loss 0.5023094143556512\n",
      "2022-03-26 20:25:57.947377 Epoch 300, Training Loss 0.5027518943523812\n",
      "2022-03-26 20:25:57.969841 Epoch 300, Training Loss 0.5036999925094492\n",
      "2022-03-26 20:25:57.998451 Epoch 300, Training Loss 0.5043322037705376\n",
      "2022-03-26 20:25:58.027242 Epoch 300, Training Loss 0.5048672812978935\n",
      "2022-03-26 20:25:58.050441 Epoch 300, Training Loss 0.5054673870734852\n",
      "2022-03-26 20:25:58.073938 Epoch 300, Training Loss 0.506259265801181\n",
      "2022-03-26 20:25:58.103418 Epoch 300, Training Loss 0.50712971717043\n",
      "2022-03-26 20:25:58.129546 Epoch 300, Training Loss 0.5077716716948677\n",
      "2022-03-26 20:25:58.152706 Epoch 300, Training Loss 0.5083027414958495\n",
      "2022-03-26 20:25:58.175788 Epoch 300, Training Loss 0.5090477370545078\n",
      "2022-03-26 20:25:58.208437 Epoch 300, Training Loss 0.5094732605587796\n",
      "2022-03-26 20:25:58.235784 Epoch 300, Training Loss 0.5101036042965892\n",
      "2022-03-26 20:25:58.258847 Epoch 300, Training Loss 0.510746581551364\n",
      "2022-03-26 20:25:58.295657 Epoch 300, Training Loss 0.5115727991856578\n",
      "2022-03-26 20:25:58.322639 Epoch 300, Training Loss 0.512168500124646\n",
      "2022-03-26 20:25:58.348333 Epoch 300, Training Loss 0.5127700482640425\n",
      "2022-03-26 20:25:58.371742 Epoch 300, Training Loss 0.5135082946256604\n",
      "2022-03-26 20:25:58.395353 Epoch 300, Training Loss 0.5144343697049124\n",
      "2022-03-26 20:25:58.422843 Epoch 300, Training Loss 0.5152006265147567\n",
      "2022-03-26 20:25:58.455005 Epoch 300, Training Loss 0.5161215568442479\n",
      "2022-03-26 20:25:58.479036 Epoch 300, Training Loss 0.5169419050216675\n",
      "2022-03-26 20:25:58.503261 Epoch 300, Training Loss 0.5177324958469557\n",
      "2022-03-26 20:25:58.526994 Epoch 300, Training Loss 0.5184378405208783\n",
      "2022-03-26 20:25:58.551378 Epoch 300, Training Loss 0.5192427961417782\n",
      "2022-03-26 20:25:58.579548 Epoch 300, Training Loss 0.519738616769576\n",
      "2022-03-26 20:25:58.603310 Epoch 300, Training Loss 0.5203429557325895\n",
      "2022-03-26 20:25:58.632253 Epoch 300, Training Loss 0.5209525106355662\n",
      "2022-03-26 20:25:58.660048 Epoch 300, Training Loss 0.5216832695257329\n",
      "2022-03-26 20:25:58.683866 Epoch 300, Training Loss 0.5224776459319512\n",
      "2022-03-26 20:25:58.709307 Epoch 300, Training Loss 0.5234538954694558\n",
      "2022-03-26 20:25:58.719285 Epoch 300, Training Loss 0.5242721596947106\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18962,
     "status": "ok",
     "timestamp": 1648326456414,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "vWDrNgVbkbl_",
    "outputId": "6d2a435d-9f15-4e3a-ef52-2bf294cd6bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.82\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSUubb_o9sV-"
   },
   "source": [
    "Problem 2 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1648330345370,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "5kiNYhFd9x7y"
   },
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module): \n",
    "  def __init__(self, n_chans1=32):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "    self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding =1)\n",
    "    self.fc1 = nn.Linear(4*4*n_chans1 // 2, 32)\n",
    "    self.fc2 = nn.Linear(32, 2)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "    out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "    out = out.view(-1, 4*4*self.n_chans1 // 2)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1648330347197,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "xHEP8UsdAklQ"
   },
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "  def __init__(self, n_chans1=32):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "    self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "    self.fc1 = nn.Linear(4*4*n_chans1 // 2, 32)\n",
    "    self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "    out1 = out\n",
    "    out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "    out = out.view(-1, 4*4*self.n_chans1 // 2)\n",
    "    out = torch.relu(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1648333866080,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "wyEeN4eeDgYi"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, n_chans):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "    #self.btach_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "    #torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "    #torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = torch.relu(out)\n",
    "    return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1648330354255,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "gmlblRmdE6tV"
   },
   "outputs": [],
   "source": [
    "class ResNet10(nn.Module): \n",
    "  def __init__(self, n_chans1=32, n_blocks=10):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = nn.Sequential(\n",
    "        *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "    self.fc1 = nn.Linear(8*8*n_chans1, 32)\n",
    "    self.fc2 = nn.Linear(32,2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "    out = self.resblocks(out)\n",
    "    out = F.max_pool2d(out, 2)\n",
    "    out = out.view(-1, 8*8*self.n_chans1)\n",
    "    out = torch.relu(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1648333868790,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00026930402542127012"
     },
     "user_tz": 240
    },
    "id": "w_fl2vVTJWlG",
    "outputId": "274edcf1-0c9f-4419-8e91-d593b41c07ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet10(\n",
       "  (conv1): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (9): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet10()\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5jUnAJbWxZ6",
    "outputId": "a57dcc9b-2259-414c-d2c5-3b049f50db99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 20:33:13.285253 Epoch 1, Training Loss 0.002920018437573367\n",
      "2022-03-26 20:33:13.316260 Epoch 1, Training Loss 0.005855738964227155\n",
      "2022-03-26 20:33:13.346358 Epoch 1, Training Loss 0.008834523008302654\n",
      "2022-03-26 20:33:13.376524 Epoch 1, Training Loss 0.011784244681258335\n",
      "2022-03-26 20:33:13.407699 Epoch 1, Training Loss 0.014759219820846986\n",
      "2022-03-26 20:33:13.437814 Epoch 1, Training Loss 0.01771200252006121\n",
      "2022-03-26 20:33:13.468104 Epoch 1, Training Loss 0.02066094796066089\n",
      "2022-03-26 20:33:13.498466 Epoch 1, Training Loss 0.02363117942419808\n",
      "2022-03-26 20:33:13.529480 Epoch 1, Training Loss 0.026609259493210736\n",
      "2022-03-26 20:33:13.559498 Epoch 1, Training Loss 0.029566505985796604\n",
      "2022-03-26 20:33:13.585252 Epoch 1, Training Loss 0.032516887120883485\n",
      "2022-03-26 20:33:13.611608 Epoch 1, Training Loss 0.0354517271451633\n",
      "2022-03-26 20:33:13.638626 Epoch 1, Training Loss 0.03842124640179412\n",
      "2022-03-26 20:33:13.665632 Epoch 1, Training Loss 0.04138691681425285\n",
      "2022-03-26 20:33:13.691775 Epoch 1, Training Loss 0.0443231626544767\n",
      "2022-03-26 20:33:13.719782 Epoch 1, Training Loss 0.047227336622564994\n",
      "2022-03-26 20:33:13.746783 Epoch 1, Training Loss 0.05018922770419694\n",
      "2022-03-26 20:33:13.773788 Epoch 1, Training Loss 0.053089866247933236\n",
      "2022-03-26 20:33:13.800098 Epoch 1, Training Loss 0.05601851775518159\n",
      "2022-03-26 20:33:13.825004 Epoch 1, Training Loss 0.05892522408224433\n",
      "2022-03-26 20:33:13.851325 Epoch 1, Training Loss 0.061840460428496453\n",
      "2022-03-26 20:33:13.877456 Epoch 1, Training Loss 0.06473958522767362\n",
      "2022-03-26 20:33:13.903849 Epoch 1, Training Loss 0.06766711232607323\n",
      "2022-03-26 20:33:13.929842 Epoch 1, Training Loss 0.0705917973042754\n",
      "2022-03-26 20:33:13.956840 Epoch 1, Training Loss 0.07351579507598487\n",
      "2022-03-26 20:33:13.982861 Epoch 1, Training Loss 0.07642546822043027\n",
      "2022-03-26 20:33:14.009868 Epoch 1, Training Loss 0.07937382096829622\n",
      "2022-03-26 20:33:14.035292 Epoch 1, Training Loss 0.08229053050965604\n",
      "2022-03-26 20:33:14.061287 Epoch 1, Training Loss 0.08520856324364157\n",
      "2022-03-26 20:33:14.086292 Epoch 1, Training Loss 0.0881509268680192\n",
      "2022-03-26 20:33:14.114292 Epoch 1, Training Loss 0.09110085128823205\n",
      "2022-03-26 20:33:14.139316 Epoch 1, Training Loss 0.09400218679472004\n",
      "2022-03-26 20:33:14.165310 Epoch 1, Training Loss 0.09692743153828184\n",
      "2022-03-26 20:33:14.183321 Epoch 1, Training Loss 0.09986446092805594\n",
      "2022-03-26 20:33:14.201317 Epoch 1, Training Loss 0.10277352948932697\n",
      "2022-03-26 20:33:14.219329 Epoch 1, Training Loss 0.10570161421890453\n",
      "2022-03-26 20:33:14.237332 Epoch 1, Training Loss 0.10864180250241019\n",
      "2022-03-26 20:33:14.255330 Epoch 1, Training Loss 0.11157293331897472\n",
      "2022-03-26 20:33:14.274331 Epoch 1, Training Loss 0.11453521190701849\n",
      "2022-03-26 20:33:14.292351 Epoch 1, Training Loss 0.11743934593542153\n",
      "2022-03-26 20:33:14.310356 Epoch 1, Training Loss 0.12035822167116053\n",
      "2022-03-26 20:33:14.327340 Epoch 1, Training Loss 0.12326947959792584\n",
      "2022-03-26 20:33:14.346351 Epoch 1, Training Loss 0.12618331073800013\n",
      "2022-03-26 20:33:14.364355 Epoch 1, Training Loss 0.1291161132285662\n",
      "2022-03-26 20:33:14.382365 Epoch 1, Training Loss 0.13203044803551092\n",
      "2022-03-26 20:33:14.400356 Epoch 1, Training Loss 0.13492681913058777\n",
      "2022-03-26 20:33:14.418379 Epoch 1, Training Loss 0.1378331611223538\n",
      "2022-03-26 20:33:14.437377 Epoch 1, Training Loss 0.1407375122275194\n",
      "2022-03-26 20:33:14.455387 Epoch 1, Training Loss 0.1436405303838003\n",
      "2022-03-26 20:33:14.473392 Epoch 1, Training Loss 0.1465229591750123\n",
      "2022-03-26 20:33:14.491595 Epoch 1, Training Loss 0.14942247270013365\n",
      "2022-03-26 20:33:14.509678 Epoch 1, Training Loss 0.15230184412368422\n",
      "2022-03-26 20:33:14.528676 Epoch 1, Training Loss 0.15521152062184365\n",
      "2022-03-26 20:33:14.546686 Epoch 1, Training Loss 0.15814054042786893\n",
      "2022-03-26 20:33:14.564696 Epoch 1, Training Loss 0.16105073492240418\n",
      "2022-03-26 20:33:14.582700 Epoch 1, Training Loss 0.16393272041359827\n",
      "2022-03-26 20:33:14.601705 Epoch 1, Training Loss 0.16680427341509962\n",
      "2022-03-26 20:33:14.619709 Epoch 1, Training Loss 0.1696992972317864\n",
      "2022-03-26 20:33:14.638700 Epoch 1, Training Loss 0.17256601905578847\n",
      "2022-03-26 20:33:14.655790 Epoch 1, Training Loss 0.17546132275515505\n",
      "2022-03-26 20:33:14.673782 Epoch 1, Training Loss 0.17833923836193427\n",
      "2022-03-26 20:33:14.691792 Epoch 1, Training Loss 0.18122296808930613\n",
      "2022-03-26 20:33:14.709796 Epoch 1, Training Loss 0.18411548180348428\n",
      "2022-03-26 20:33:14.727794 Epoch 1, Training Loss 0.1869643283317156\n",
      "2022-03-26 20:33:14.745902 Epoch 1, Training Loss 0.1898024271211356\n",
      "2022-03-26 20:33:14.764064 Epoch 1, Training Loss 0.19270868222121998\n",
      "2022-03-26 20:33:14.782076 Epoch 1, Training Loss 0.19559472784056992\n",
      "2022-03-26 20:33:14.800071 Epoch 1, Training Loss 0.19848371100852558\n",
      "2022-03-26 20:33:14.818067 Epoch 1, Training Loss 0.20135461857251805\n",
      "2022-03-26 20:33:14.836087 Epoch 1, Training Loss 0.2042775791319435\n",
      "2022-03-26 20:33:14.854076 Epoch 1, Training Loss 0.20714481986697067\n",
      "2022-03-26 20:33:14.872094 Epoch 1, Training Loss 0.20999369261514805\n",
      "2022-03-26 20:33:14.890095 Epoch 1, Training Loss 0.21289508543965763\n",
      "2022-03-26 20:33:14.908091 Epoch 1, Training Loss 0.21578695005772974\n",
      "2022-03-26 20:33:14.927104 Epoch 1, Training Loss 0.21862510890911913\n",
      "2022-03-26 20:33:14.945104 Epoch 1, Training Loss 0.22151191338248874\n",
      "2022-03-26 20:33:14.963103 Epoch 1, Training Loss 0.22435000820842851\n",
      "2022-03-26 20:33:14.981113 Epoch 1, Training Loss 0.2271999507913809\n",
      "2022-03-26 20:33:14.999111 Epoch 1, Training Loss 0.23003045646735773\n",
      "2022-03-26 20:33:15.017127 Epoch 1, Training Loss 0.23286711590369338\n",
      "2022-03-26 20:33:15.036131 Epoch 1, Training Loss 0.23569312821263852\n",
      "2022-03-26 20:33:15.054136 Epoch 1, Training Loss 0.23853776217116723\n",
      "2022-03-26 20:33:15.072139 Epoch 1, Training Loss 0.24143238293240443\n",
      "2022-03-26 20:33:15.091144 Epoch 1, Training Loss 0.2442731046310776\n",
      "2022-03-26 20:33:15.108148 Epoch 1, Training Loss 0.2471344007555481\n",
      "2022-03-26 20:33:15.126146 Epoch 1, Training Loss 0.24998925012700698\n",
      "2022-03-26 20:33:15.144156 Epoch 1, Training Loss 0.25283843538035516\n",
      "2022-03-26 20:33:15.162161 Epoch 1, Training Loss 0.25567780492250874\n",
      "2022-03-26 20:33:15.180166 Epoch 1, Training Loss 0.2585702997339351\n",
      "2022-03-26 20:33:15.197169 Epoch 1, Training Loss 0.2614196752343336\n",
      "2022-03-26 20:33:15.215175 Epoch 1, Training Loss 0.26430202475594133\n",
      "2022-03-26 20:33:15.233177 Epoch 1, Training Loss 0.2671306971698771\n",
      "2022-03-26 20:33:15.251181 Epoch 1, Training Loss 0.2699777009846914\n",
      "2022-03-26 20:33:15.269344 Epoch 1, Training Loss 0.27279925071979727\n",
      "2022-03-26 20:33:15.287343 Epoch 1, Training Loss 0.2755929734700781\n",
      "2022-03-26 20:33:15.305351 Epoch 1, Training Loss 0.2783873758047743\n",
      "2022-03-26 20:33:15.324348 Epoch 1, Training Loss 0.2812471240377792\n",
      "2022-03-26 20:33:15.343348 Epoch 1, Training Loss 0.2840517039799019\n",
      "2022-03-26 20:33:15.361353 Epoch 1, Training Loss 0.28687316621355996\n",
      "2022-03-26 20:33:15.379357 Epoch 1, Training Loss 0.28969961298091335\n",
      "2022-03-26 20:33:15.397360 Epoch 1, Training Loss 0.29255015740309226\n",
      "2022-03-26 20:33:15.415376 Epoch 1, Training Loss 0.2953311546379343\n",
      "2022-03-26 20:33:15.433385 Epoch 1, Training Loss 0.2981823975472804\n",
      "2022-03-26 20:33:15.451384 Epoch 1, Training Loss 0.3009687610294508\n",
      "2022-03-26 20:33:15.470393 Epoch 1, Training Loss 0.30374804116271037\n",
      "2022-03-26 20:33:15.488397 Epoch 1, Training Loss 0.3065736223669613\n",
      "2022-03-26 20:33:15.507396 Epoch 1, Training Loss 0.30944228477185337\n",
      "2022-03-26 20:33:15.525406 Epoch 1, Training Loss 0.31225474746635806\n",
      "2022-03-26 20:33:15.543391 Epoch 1, Training Loss 0.3150942834746807\n",
      "2022-03-26 20:33:15.561401 Epoch 1, Training Loss 0.3179305712585254\n",
      "2022-03-26 20:33:15.579412 Epoch 1, Training Loss 0.32076214890345894\n",
      "2022-03-26 20:33:15.597422 Epoch 1, Training Loss 0.3235782348286465\n",
      "2022-03-26 20:33:15.615426 Epoch 1, Training Loss 0.326395464980084\n",
      "2022-03-26 20:33:15.634430 Epoch 1, Training Loss 0.32923744432151775\n",
      "2022-03-26 20:33:15.652424 Epoch 1, Training Loss 0.33205465160672315\n",
      "2022-03-26 20:33:15.671439 Epoch 1, Training Loss 0.3348682201122079\n",
      "2022-03-26 20:33:15.689427 Epoch 1, Training Loss 0.337682862720831\n",
      "2022-03-26 20:33:15.707447 Epoch 1, Training Loss 0.34045823699677996\n",
      "2022-03-26 20:33:15.724445 Epoch 1, Training Loss 0.34324403187198105\n",
      "2022-03-26 20:33:15.742450 Epoch 1, Training Loss 0.34599996527747423\n",
      "2022-03-26 20:33:15.760453 Epoch 1, Training Loss 0.34883249202347777\n",
      "2022-03-26 20:33:15.778271 Epoch 1, Training Loss 0.35163421460124844\n",
      "2022-03-26 20:33:15.796275 Epoch 1, Training Loss 0.35446820905446397\n",
      "2022-03-26 20:33:15.814278 Epoch 1, Training Loss 0.3572134380145451\n",
      "2022-03-26 20:33:15.831283 Epoch 1, Training Loss 0.3600366825947676\n",
      "2022-03-26 20:33:15.849285 Epoch 1, Training Loss 0.3628056226179118\n",
      "2022-03-26 20:33:15.869297 Epoch 1, Training Loss 0.36557927796297973\n",
      "2022-03-26 20:33:15.887306 Epoch 1, Training Loss 0.36837979625253114\n",
      "2022-03-26 20:33:15.905310 Epoch 1, Training Loss 0.3711802236869207\n",
      "2022-03-26 20:33:15.923315 Epoch 1, Training Loss 0.3739561412645423\n",
      "2022-03-26 20:33:15.942313 Epoch 1, Training Loss 0.3767297953900779\n",
      "2022-03-26 20:33:15.960323 Epoch 1, Training Loss 0.37948705503702773\n",
      "2022-03-26 20:33:15.979321 Epoch 1, Training Loss 0.3822835577113549\n",
      "2022-03-26 20:33:15.997325 Epoch 1, Training Loss 0.3850482421762803\n",
      "2022-03-26 20:33:16.015323 Epoch 1, Training Loss 0.3877188186816242\n",
      "2022-03-26 20:33:16.032333 Epoch 1, Training Loss 0.39055691381244706\n",
      "2022-03-26 20:33:16.051343 Epoch 1, Training Loss 0.3933795361262758\n",
      "2022-03-26 20:33:16.069335 Epoch 1, Training Loss 0.3961669935289856\n",
      "2022-03-26 20:33:16.088352 Epoch 1, Training Loss 0.39892837458559316\n",
      "2022-03-26 20:33:16.107372 Epoch 1, Training Loss 0.4017053930960653\n",
      "2022-03-26 20:33:16.125360 Epoch 1, Training Loss 0.40449424137544754\n",
      "2022-03-26 20:33:16.144353 Epoch 1, Training Loss 0.4072714967800833\n",
      "2022-03-26 20:33:16.162359 Epoch 1, Training Loss 0.4100635182827025\n",
      "2022-03-26 20:33:16.180367 Epoch 1, Training Loss 0.41285393427095146\n",
      "2022-03-26 20:33:16.197371 Epoch 1, Training Loss 0.4156349598599212\n",
      "2022-03-26 20:33:16.215363 Epoch 1, Training Loss 0.4183254934027982\n",
      "2022-03-26 20:33:16.233397 Epoch 1, Training Loss 0.4210482378445013\n",
      "2022-03-26 20:33:16.251383 Epoch 1, Training Loss 0.42380198767727906\n",
      "2022-03-26 20:33:16.269393 Epoch 1, Training Loss 0.42665079579024057\n",
      "2022-03-26 20:33:16.287385 Epoch 1, Training Loss 0.42934090432608524\n",
      "2022-03-26 20:33:16.306389 Epoch 1, Training Loss 0.43202553899086954\n",
      "2022-03-26 20:33:16.324399 Epoch 1, Training Loss 0.43480390051136847\n",
      "2022-03-26 20:33:16.342403 Epoch 1, Training Loss 0.43752157810094106\n",
      "2022-03-26 20:33:16.360401 Epoch 1, Training Loss 0.44039031184847705\n",
      "2022-03-26 20:33:16.378406 Epoch 1, Training Loss 0.44319933515680415\n",
      "2022-03-26 20:33:16.396402 Epoch 1, Training Loss 0.4458925269753732\n",
      "2022-03-26 20:33:16.414415 Epoch 1, Training Loss 0.4486614093756127\n",
      "2022-03-26 20:33:16.432430 Epoch 1, Training Loss 0.4513763064313728\n",
      "2022-03-26 20:33:16.450428 Epoch 1, Training Loss 0.45417412742019614\n",
      "2022-03-26 20:33:16.468438 Epoch 1, Training Loss 0.4568844298877375\n",
      "2022-03-26 20:33:16.487442 Epoch 1, Training Loss 0.45973816857008676\n",
      "2022-03-26 20:33:16.506441 Epoch 1, Training Loss 0.46251740845877803\n",
      "2022-03-26 20:33:16.525451 Epoch 1, Training Loss 0.46524774601392427\n",
      "2022-03-26 20:33:16.543447 Epoch 1, Training Loss 0.4679942853615412\n",
      "2022-03-26 20:33:16.561453 Epoch 1, Training Loss 0.4706769260908941\n",
      "2022-03-26 20:33:16.579457 Epoch 1, Training Loss 0.4734389745365933\n",
      "2022-03-26 20:33:16.597467 Epoch 1, Training Loss 0.4762104261866616\n",
      "2022-03-26 20:33:16.615465 Epoch 1, Training Loss 0.47896673246417815\n",
      "2022-03-26 20:33:16.633476 Epoch 1, Training Loss 0.48172791290771017\n",
      "2022-03-26 20:33:16.651480 Epoch 1, Training Loss 0.48454348967813166\n",
      "2022-03-26 20:33:16.670478 Epoch 1, Training Loss 0.4874269169614748\n",
      "2022-03-26 20:33:16.687703 Epoch 1, Training Loss 0.4901478726540685\n",
      "2022-03-26 20:33:16.706714 Epoch 1, Training Loss 0.49287757605238036\n",
      "2022-03-26 20:33:16.724725 Epoch 1, Training Loss 0.4956016388085797\n",
      "2022-03-26 20:33:16.743715 Epoch 1, Training Loss 0.4983007441396299\n",
      "2022-03-26 20:33:16.762721 Epoch 1, Training Loss 0.5009731380530941\n",
      "2022-03-26 20:33:16.781733 Epoch 1, Training Loss 0.5036711387926965\n",
      "2022-03-26 20:33:16.799729 Epoch 1, Training Loss 0.5063532051223013\n",
      "2022-03-26 20:33:16.817739 Epoch 1, Training Loss 0.5089917274387291\n",
      "2022-03-26 20:33:16.835744 Epoch 1, Training Loss 0.5116977908117387\n",
      "2022-03-26 20:33:16.854747 Epoch 1, Training Loss 0.5143671770534857\n",
      "2022-03-26 20:33:16.872759 Epoch 1, Training Loss 0.5170896370392626\n",
      "2022-03-26 20:33:16.890756 Epoch 1, Training Loss 0.5197797257577061\n",
      "2022-03-26 20:33:16.909766 Epoch 1, Training Loss 0.522486592497667\n",
      "2022-03-26 20:33:16.928758 Epoch 1, Training Loss 0.5252356288378196\n",
      "2022-03-26 20:33:16.947769 Epoch 1, Training Loss 0.5279114850036933\n",
      "2022-03-26 20:33:16.965779 Epoch 1, Training Loss 0.5307172085622998\n",
      "2022-03-26 20:33:16.983783 Epoch 1, Training Loss 0.5334209557384482\n",
      "2022-03-26 20:33:17.001787 Epoch 1, Training Loss 0.5361228291031039\n",
      "2022-03-26 20:33:17.019785 Epoch 1, Training Loss 0.538850762045292\n",
      "2022-03-26 20:33:17.037795 Epoch 1, Training Loss 0.541543073056604\n",
      "2022-03-26 20:33:17.055799 Epoch 1, Training Loss 0.5443038217856756\n",
      "2022-03-26 20:33:17.074804 Epoch 1, Training Loss 0.5470082006795937\n",
      "2022-03-26 20:33:17.092802 Epoch 1, Training Loss 0.5497031727105456\n",
      "2022-03-26 20:33:17.111806 Epoch 1, Training Loss 0.5524610488311105\n",
      "2022-03-26 20:33:17.129816 Epoch 1, Training Loss 0.5552064125495189\n",
      "2022-03-26 20:33:17.148814 Epoch 1, Training Loss 0.5578299419349416\n",
      "2022-03-26 20:33:17.167825 Epoch 1, Training Loss 0.5605273978484561\n",
      "2022-03-26 20:33:17.185829 Epoch 1, Training Loss 0.563087005749383\n",
      "2022-03-26 20:33:17.203820 Epoch 1, Training Loss 0.5658020604297024\n",
      "2022-03-26 20:33:17.221831 Epoch 1, Training Loss 0.56848021236527\n",
      "2022-03-26 20:33:17.239835 Epoch 1, Training Loss 0.5711358508185658\n",
      "2022-03-26 20:33:17.258841 Epoch 1, Training Loss 0.5737503318835402\n",
      "2022-03-26 20:33:17.276836 Epoch 1, Training Loss 0.5764296865829116\n",
      "2022-03-26 20:33:17.295849 Epoch 1, Training Loss 0.5791480181467198\n",
      "2022-03-26 20:33:17.313842 Epoch 1, Training Loss 0.5818525976537133\n",
      "2022-03-26 20:33:17.331850 Epoch 1, Training Loss 0.5844830225800615\n",
      "2022-03-26 20:33:17.350854 Epoch 1, Training Loss 0.5871736631369042\n",
      "2022-03-26 20:33:17.368870 Epoch 1, Training Loss 0.5898209514520357\n",
      "2022-03-26 20:33:17.387874 Epoch 1, Training Loss 0.5924428055048598\n",
      "2022-03-26 20:33:17.405879 Epoch 1, Training Loss 0.5951701033755642\n",
      "2022-03-26 20:33:17.431879 Epoch 1, Training Loss 0.5978030310872265\n",
      "2022-03-26 20:33:17.457890 Epoch 1, Training Loss 0.6005019818425483\n",
      "2022-03-26 20:33:17.483896 Epoch 1, Training Loss 0.6031467551770417\n",
      "2022-03-26 20:33:17.509902 Epoch 1, Training Loss 0.6057894086593862\n",
      "2022-03-26 20:33:17.535908 Epoch 1, Training Loss 0.6084693877593331\n",
      "2022-03-26 20:33:17.561896 Epoch 1, Training Loss 0.611007796529004\n",
      "2022-03-26 20:33:17.588915 Epoch 1, Training Loss 0.6136037858245927\n",
      "2022-03-26 20:33:17.614920 Epoch 1, Training Loss 0.616178557391057\n",
      "2022-03-26 20:33:17.631930 Epoch 1, Training Loss 0.6188519107716163\n",
      "2022-03-26 20:33:17.649934 Epoch 1, Training Loss 0.6215226250841185\n",
      "2022-03-26 20:33:17.667932 Epoch 1, Training Loss 0.6241743238380802\n",
      "2022-03-26 20:33:17.685938 Epoch 1, Training Loss 0.6268454488281094\n",
      "2022-03-26 20:33:17.703946 Epoch 1, Training Loss 0.6294795390590072\n",
      "2022-03-26 20:33:17.722944 Epoch 1, Training Loss 0.6321809383304527\n",
      "2022-03-26 20:33:17.740955 Epoch 1, Training Loss 0.6349841661160559\n",
      "2022-03-26 20:33:17.759940 Epoch 1, Training Loss 0.6376409417832903\n",
      "2022-03-26 20:33:17.777946 Epoch 1, Training Loss 0.640289218529411\n",
      "2022-03-26 20:33:17.795955 Epoch 1, Training Loss 0.642974899248089\n",
      "2022-03-26 20:33:17.813953 Epoch 1, Training Loss 0.6456359323028409\n",
      "2022-03-26 20:33:17.831970 Epoch 1, Training Loss 0.6482665096707356\n",
      "2022-03-26 20:33:17.848973 Epoch 1, Training Loss 0.6509620286619572\n",
      "2022-03-26 20:33:17.867977 Epoch 1, Training Loss 0.6536844746231119\n",
      "2022-03-26 20:33:17.884987 Epoch 1, Training Loss 0.6562433797685082\n",
      "2022-03-26 20:33:17.902991 Epoch 1, Training Loss 0.6589884062862152\n",
      "2022-03-26 20:33:17.921996 Epoch 1, Training Loss 0.6616319320390901\n",
      "2022-03-26 20:33:17.939994 Epoch 1, Training Loss 0.6642596392375429\n",
      "2022-03-26 20:33:17.958004 Epoch 1, Training Loss 0.6668647631354954\n",
      "2022-03-26 20:33:17.976996 Epoch 1, Training Loss 0.6694908181724646\n",
      "2022-03-26 20:33:17.995008 Epoch 1, Training Loss 0.6720856037896002\n",
      "2022-03-26 20:33:18.014004 Epoch 1, Training Loss 0.6748307656754008\n",
      "2022-03-26 20:33:18.031001 Epoch 1, Training Loss 0.6774120333859378\n",
      "2022-03-26 20:33:18.049024 Epoch 1, Training Loss 0.6801350836253837\n",
      "2022-03-26 20:33:18.067022 Epoch 1, Training Loss 0.6827326269101\n",
      "2022-03-26 20:33:18.086033 Epoch 1, Training Loss 0.6854193518533731\n",
      "2022-03-26 20:33:18.103037 Epoch 1, Training Loss 0.687983065919803\n",
      "2022-03-26 20:33:18.121035 Epoch 1, Training Loss 0.6905054382960815\n",
      "2022-03-26 20:33:18.139039 Epoch 1, Training Loss 0.6931346534463145\n",
      "2022-03-26 20:33:18.158037 Epoch 1, Training Loss 0.695751381194805\n",
      "2022-03-26 20:33:18.176053 Epoch 1, Training Loss 0.698304683960917\n",
      "2022-03-26 20:33:18.195046 Epoch 1, Training Loss 0.7008324452983145\n",
      "2022-03-26 20:33:18.213057 Epoch 1, Training Loss 0.7035132939248439\n",
      "2022-03-26 20:33:18.230053 Epoch 1, Training Loss 0.7062247397039857\n",
      "2022-03-26 20:33:18.248065 Epoch 1, Training Loss 0.7089011595986993\n",
      "2022-03-26 20:33:18.266073 Epoch 1, Training Loss 0.711723658129992\n",
      "2022-03-26 20:33:18.284072 Epoch 1, Training Loss 0.7144850705895582\n",
      "2022-03-26 20:33:18.302083 Epoch 1, Training Loss 0.7171238057143853\n",
      "2022-03-26 20:33:18.320073 Epoch 1, Training Loss 0.7198401320620876\n",
      "2022-03-26 20:33:18.339086 Epoch 1, Training Loss 0.7224678416996051\n",
      "2022-03-26 20:33:18.357094 Epoch 1, Training Loss 0.7250707176945094\n",
      "2022-03-26 20:33:18.375086 Epoch 1, Training Loss 0.7276654087978861\n",
      "2022-03-26 20:33:18.392102 Epoch 1, Training Loss 0.7302333351291354\n",
      "2022-03-26 20:33:18.411097 Epoch 1, Training Loss 0.7328022264153756\n",
      "2022-03-26 20:33:18.429098 Epoch 1, Training Loss 0.7353021749450118\n",
      "2022-03-26 20:33:18.448094 Epoch 1, Training Loss 0.7379356974835896\n",
      "2022-03-26 20:33:18.467119 Epoch 1, Training Loss 0.7404522289095632\n",
      "2022-03-26 20:33:18.485123 Epoch 1, Training Loss 0.7430129770732596\n",
      "2022-03-26 20:33:18.503127 Epoch 1, Training Loss 0.7457526669172985\n",
      "2022-03-26 20:33:18.521131 Epoch 1, Training Loss 0.7483769127779909\n",
      "2022-03-26 20:33:18.539136 Epoch 1, Training Loss 0.751133048016092\n",
      "2022-03-26 20:33:18.557133 Epoch 1, Training Loss 0.7537173331546052\n",
      "2022-03-26 20:33:18.575144 Epoch 1, Training Loss 0.7563836614189245\n",
      "2022-03-26 20:33:18.593148 Epoch 1, Training Loss 0.7590664551995904\n",
      "2022-03-26 20:33:18.611146 Epoch 1, Training Loss 0.7616639820206196\n",
      "2022-03-26 20:33:18.630144 Epoch 1, Training Loss 0.7641864774172263\n",
      "2022-03-26 20:33:18.648154 Epoch 1, Training Loss 0.7668416875097758\n",
      "2022-03-26 20:33:18.666164 Epoch 1, Training Loss 0.7695113079017385\n",
      "2022-03-26 20:33:18.684168 Epoch 1, Training Loss 0.7721390163197237\n",
      "2022-03-26 20:33:18.702881 Epoch 1, Training Loss 0.7748594875530819\n",
      "2022-03-26 20:33:18.720308 Epoch 1, Training Loss 0.7774741042910329\n",
      "2022-03-26 20:33:18.738291 Epoch 1, Training Loss 0.7800160704366387\n",
      "2022-03-26 20:33:18.756606 Epoch 1, Training Loss 0.7826539668280755\n",
      "2022-03-26 20:33:18.775188 Epoch 1, Training Loss 0.7852435447370915\n",
      "2022-03-26 20:33:18.792641 Epoch 1, Training Loss 0.7878638710207342\n",
      "2022-03-26 20:33:18.810998 Epoch 1, Training Loss 0.7905809952474921\n",
      "2022-03-26 20:33:18.828581 Epoch 1, Training Loss 0.7930932487063396\n",
      "2022-03-26 20:33:18.847572 Epoch 1, Training Loss 0.7958089966908135\n",
      "2022-03-26 20:33:18.865591 Epoch 1, Training Loss 0.7983557496534284\n",
      "2022-03-26 20:33:18.883599 Epoch 1, Training Loss 0.8008778345249498\n",
      "2022-03-26 20:33:18.900599 Epoch 1, Training Loss 0.8034362338692941\n",
      "2022-03-26 20:33:18.918602 Epoch 1, Training Loss 0.8059742034548689\n",
      "2022-03-26 20:33:18.936599 Epoch 1, Training Loss 0.8086112681252268\n",
      "2022-03-26 20:33:18.955601 Epoch 1, Training Loss 0.8112170463022979\n",
      "2022-03-26 20:33:18.973613 Epoch 1, Training Loss 0.8137792752831793\n",
      "2022-03-26 20:33:18.992618 Epoch 1, Training Loss 0.8164089596484934\n",
      "2022-03-26 20:33:19.010609 Epoch 1, Training Loss 0.8189651956948478\n",
      "2022-03-26 20:33:19.028622 Epoch 1, Training Loss 0.8216556100284352\n",
      "2022-03-26 20:33:19.047624 Epoch 1, Training Loss 0.8242128288654416\n",
      "2022-03-26 20:33:19.065634 Epoch 1, Training Loss 0.8268471023310786\n",
      "2022-03-26 20:33:19.083639 Epoch 1, Training Loss 0.8293924307274392\n",
      "2022-03-26 20:33:19.101643 Epoch 1, Training Loss 0.8319862779144132\n",
      "2022-03-26 20:33:19.119653 Epoch 1, Training Loss 0.8347717243082383\n",
      "2022-03-26 20:33:19.137657 Epoch 1, Training Loss 0.8372432289221098\n",
      "2022-03-26 20:33:19.155661 Epoch 1, Training Loss 0.8398355309615659\n",
      "2022-03-26 20:33:19.174659 Epoch 1, Training Loss 0.842482021702525\n",
      "2022-03-26 20:33:19.192669 Epoch 1, Training Loss 0.8451241102364971\n",
      "2022-03-26 20:33:19.210662 Epoch 1, Training Loss 0.8476311444016673\n",
      "2022-03-26 20:33:19.228671 Epoch 1, Training Loss 0.8502288596404483\n",
      "2022-03-26 20:33:19.247682 Epoch 1, Training Loss 0.8528560281104749\n",
      "2022-03-26 20:33:19.264685 Epoch 1, Training Loss 0.8552452978270743\n",
      "2022-03-26 20:33:19.282690 Epoch 1, Training Loss 0.8577496503929958\n",
      "2022-03-26 20:33:19.301694 Epoch 1, Training Loss 0.8605286850953651\n",
      "2022-03-26 20:33:19.319698 Epoch 1, Training Loss 0.8631280500565648\n",
      "2022-03-26 20:33:19.337696 Epoch 1, Training Loss 0.8657612372237398\n",
      "2022-03-26 20:33:19.355706 Epoch 1, Training Loss 0.8683191200961238\n",
      "2022-03-26 20:33:19.373710 Epoch 1, Training Loss 0.8709308014196508\n",
      "2022-03-26 20:33:19.392179 Epoch 1, Training Loss 0.8734942227983109\n",
      "2022-03-26 20:33:19.410501 Epoch 1, Training Loss 0.8760170575298006\n",
      "2022-03-26 20:33:19.428537 Epoch 1, Training Loss 0.8786457824280195\n",
      "2022-03-26 20:33:19.447541 Epoch 1, Training Loss 0.8811360475657236\n",
      "2022-03-26 20:33:19.466545 Epoch 1, Training Loss 0.8837089273325928\n",
      "2022-03-26 20:33:19.484640 Epoch 1, Training Loss 0.8863263395436279\n",
      "2022-03-26 20:33:19.504149 Epoch 1, Training Loss 0.8888642501343241\n",
      "2022-03-26 20:33:19.522509 Epoch 1, Training Loss 0.8914472599468573\n",
      "2022-03-26 20:33:19.540126 Epoch 1, Training Loss 0.8939230836870725\n",
      "2022-03-26 20:33:19.558453 Epoch 1, Training Loss 0.8966528450131721\n",
      "2022-03-26 20:33:19.576553 Epoch 1, Training Loss 0.8991131497466046\n",
      "2022-03-26 20:33:19.594203 Epoch 1, Training Loss 0.9016906594681313\n",
      "2022-03-26 20:33:19.612427 Epoch 1, Training Loss 0.9042756132152684\n",
      "2022-03-26 20:33:19.630806 Epoch 1, Training Loss 0.9070071114603516\n",
      "2022-03-26 20:33:19.649951 Epoch 1, Training Loss 0.909520752289716\n",
      "2022-03-26 20:33:19.667948 Epoch 1, Training Loss 0.912074969552667\n",
      "2022-03-26 20:33:19.687116 Epoch 1, Training Loss 0.9145652895693279\n",
      "2022-03-26 20:33:19.704780 Epoch 1, Training Loss 0.9171433499097215\n",
      "2022-03-26 20:33:19.723249 Epoch 1, Training Loss 0.9196392835863411\n",
      "2022-03-26 20:33:19.741678 Epoch 1, Training Loss 0.9222821919509517\n",
      "2022-03-26 20:33:19.761011 Epoch 1, Training Loss 0.9247816461126518\n",
      "2022-03-26 20:33:19.779167 Epoch 1, Training Loss 0.9273727737424319\n",
      "2022-03-26 20:33:19.797464 Epoch 1, Training Loss 0.9300340900335775\n",
      "2022-03-26 20:33:19.815823 Epoch 1, Training Loss 0.9325779129172225\n",
      "2022-03-26 20:33:19.834495 Epoch 1, Training Loss 0.9350868734862189\n",
      "2022-03-26 20:33:19.852517 Epoch 1, Training Loss 0.9375069549931284\n",
      "2022-03-26 20:33:19.871515 Epoch 1, Training Loss 0.9400664863683988\n",
      "2022-03-26 20:33:19.889525 Epoch 1, Training Loss 0.9426906889356921\n",
      "2022-03-26 20:33:19.908584 Epoch 1, Training Loss 0.9452176153507379\n",
      "2022-03-26 20:33:19.926689 Epoch 1, Training Loss 0.9476139362510818\n",
      "2022-03-26 20:33:19.945699 Epoch 1, Training Loss 0.9500462713144014\n",
      "2022-03-26 20:33:19.964696 Epoch 1, Training Loss 0.9527471181376815\n",
      "2022-03-26 20:33:19.982713 Epoch 1, Training Loss 0.9552857418499334\n",
      "2022-03-26 20:33:20.000723 Epoch 1, Training Loss 0.9578567968914881\n",
      "2022-03-26 20:33:20.017720 Epoch 1, Training Loss 0.9604125266794659\n",
      "2022-03-26 20:33:20.035730 Epoch 1, Training Loss 0.9629657073398991\n",
      "2022-03-26 20:33:20.054729 Epoch 1, Training Loss 0.9656817891713604\n",
      "2022-03-26 20:33:20.072733 Epoch 1, Training Loss 0.9682585854664483\n",
      "2022-03-26 20:33:20.091743 Epoch 1, Training Loss 0.9707270699083957\n",
      "2022-03-26 20:33:20.108741 Epoch 1, Training Loss 0.9734709982372001\n",
      "2022-03-26 20:33:20.127742 Epoch 1, Training Loss 0.9758538725735891\n",
      "2022-03-26 20:33:20.145749 Epoch 1, Training Loss 0.9785163840064612\n",
      "2022-03-26 20:33:20.163753 Epoch 1, Training Loss 0.9810194873139072\n",
      "2022-03-26 20:33:20.181764 Epoch 1, Training Loss 0.9834782944615844\n",
      "2022-03-26 20:33:20.199761 Epoch 1, Training Loss 0.9861531673794817\n",
      "2022-03-26 20:33:20.217766 Epoch 1, Training Loss 0.9886731362098928\n",
      "2022-03-26 20:33:20.235776 Epoch 1, Training Loss 0.9911713901992953\n",
      "2022-03-26 20:33:20.253780 Epoch 1, Training Loss 0.993725469350205\n",
      "2022-03-26 20:33:20.274778 Epoch 1, Training Loss 0.9962116443287686\n",
      "2022-03-26 20:33:20.293777 Epoch 1, Training Loss 0.9987607865077456\n",
      "2022-03-26 20:33:20.311787 Epoch 1, Training Loss 1.001327878983734\n",
      "2022-03-26 20:33:20.329797 Epoch 1, Training Loss 1.003818778888039\n",
      "2022-03-26 20:33:20.347795 Epoch 1, Training Loss 1.0061761534122555\n",
      "2022-03-26 20:33:20.365805 Epoch 1, Training Loss 1.0087796095996866\n",
      "2022-03-26 20:33:20.384810 Epoch 1, Training Loss 1.0113784108320465\n",
      "2022-03-26 20:33:20.402808 Epoch 1, Training Loss 1.013963969467241\n",
      "2022-03-26 20:33:20.420818 Epoch 1, Training Loss 1.01636947832449\n",
      "2022-03-26 20:33:20.439816 Epoch 1, Training Loss 1.0189887436149676\n",
      "2022-03-26 20:33:20.457826 Epoch 1, Training Loss 1.021599862429187\n",
      "2022-03-26 20:33:20.477812 Epoch 1, Training Loss 1.0243277703709615\n",
      "2022-03-26 20:33:20.496823 Epoch 1, Training Loss 1.0267315670047574\n",
      "2022-03-26 20:33:20.514829 Epoch 1, Training Loss 1.0292379098475133\n",
      "2022-03-26 20:33:20.532831 Epoch 1, Training Loss 1.0318726601503085\n",
      "2022-03-26 20:33:20.549841 Epoch 1, Training Loss 1.0344094327648583\n",
      "2022-03-26 20:33:20.568851 Epoch 1, Training Loss 1.036667928671288\n",
      "2022-03-26 20:33:20.586855 Epoch 1, Training Loss 1.0391376055110142\n",
      "2022-03-26 20:33:20.604859 Epoch 1, Training Loss 1.0417701649238995\n",
      "2022-03-26 20:33:20.623851 Epoch 1, Training Loss 1.0442341230714414\n",
      "2022-03-26 20:33:20.642868 Epoch 1, Training Loss 1.046723619141542\n",
      "2022-03-26 20:33:20.661861 Epoch 1, Training Loss 1.0493172516908182\n",
      "2022-03-26 20:33:20.680864 Epoch 1, Training Loss 1.0519072884488898\n",
      "2022-03-26 20:33:20.699881 Epoch 1, Training Loss 1.0543618046719094\n",
      "2022-03-26 20:33:20.718885 Epoch 1, Training Loss 1.056889545429698\n",
      "2022-03-26 20:33:20.736889 Epoch 1, Training Loss 1.0594734708061608\n",
      "2022-03-26 20:33:20.755894 Epoch 1, Training Loss 1.0620418425716098\n",
      "2022-03-26 20:33:20.774898 Epoch 1, Training Loss 1.0645330807437068\n",
      "2022-03-26 20:33:20.793898 Epoch 1, Training Loss 1.0669748991956491\n",
      "2022-03-26 20:33:20.811887 Epoch 1, Training Loss 1.0696915987202578\n",
      "2022-03-26 20:33:20.829899 Epoch 1, Training Loss 1.0721315550987067\n",
      "2022-03-26 20:33:20.847908 Epoch 1, Training Loss 1.074737392727981\n",
      "2022-03-26 20:33:20.865918 Epoch 1, Training Loss 1.0772591207338416\n",
      "2022-03-26 20:33:20.884910 Epoch 1, Training Loss 1.0797908828996332\n",
      "2022-03-26 20:33:20.903927 Epoch 1, Training Loss 1.0823016771879952\n",
      "2022-03-26 20:33:20.922925 Epoch 1, Training Loss 1.0848053120591146\n",
      "2022-03-26 20:33:20.940936 Epoch 1, Training Loss 1.0875274165512046\n",
      "2022-03-26 20:33:20.959940 Epoch 1, Training Loss 1.0899640561064796\n",
      "2022-03-26 20:33:20.978926 Epoch 1, Training Loss 1.0925419765055333\n",
      "2022-03-26 20:33:20.996937 Epoch 1, Training Loss 1.0950415108514868\n",
      "2022-03-26 20:33:21.014946 Epoch 1, Training Loss 1.0975162681106412\n",
      "2022-03-26 20:33:21.032956 Epoch 1, Training Loss 1.1000453834338566\n",
      "2022-03-26 20:33:21.050961 Epoch 1, Training Loss 1.1025734511787628\n",
      "2022-03-26 20:33:21.077955 Epoch 1, Training Loss 1.1049489968877924\n",
      "2022-03-26 20:33:21.103973 Epoch 1, Training Loss 1.1075449706343434\n",
      "2022-03-26 20:33:21.130966 Epoch 1, Training Loss 1.1100373286420426\n",
      "2022-03-26 20:33:21.156985 Epoch 1, Training Loss 1.112719452289669\n",
      "2022-03-26 20:33:21.182984 Epoch 1, Training Loss 1.1152264218196235\n",
      "2022-03-26 20:33:21.209978 Epoch 1, Training Loss 1.117806771832049\n",
      "2022-03-26 20:33:21.235870 Epoch 1, Training Loss 1.1201841134549406\n",
      "2022-03-26 20:33:21.261864 Epoch 1, Training Loss 1.1229982892875476\n",
      "2022-03-26 20:33:21.279868 Epoch 1, Training Loss 1.125484251457712\n",
      "2022-03-26 20:33:21.297879 Epoch 1, Training Loss 1.1280449151687915\n",
      "2022-03-26 20:33:21.316883 Epoch 1, Training Loss 1.1306829935754352\n",
      "2022-03-26 20:33:21.333893 Epoch 1, Training Loss 1.1331867911016849\n",
      "2022-03-26 20:33:21.352891 Epoch 1, Training Loss 1.1357960043965702\n",
      "2022-03-26 20:33:21.370901 Epoch 1, Training Loss 1.138249437674842\n",
      "2022-03-26 20:33:21.388905 Epoch 1, Training Loss 1.1406763444471237\n",
      "2022-03-26 20:33:21.406903 Epoch 1, Training Loss 1.143251348029622\n",
      "2022-03-26 20:33:21.425897 Epoch 1, Training Loss 1.1456958214035424\n",
      "2022-03-26 20:33:21.444906 Epoch 1, Training Loss 1.148207401832961\n",
      "2022-03-26 20:33:21.462917 Epoch 1, Training Loss 1.1507405896320977\n",
      "2022-03-26 20:33:21.480920 Epoch 1, Training Loss 1.153183008093968\n",
      "2022-03-26 20:33:21.498918 Epoch 1, Training Loss 1.1557514130916742\n",
      "2022-03-26 20:33:21.515922 Epoch 1, Training Loss 1.1582846330559773\n",
      "2022-03-26 20:33:21.534926 Epoch 1, Training Loss 1.1607535771091881\n",
      "2022-03-26 20:33:21.553943 Epoch 1, Training Loss 1.1632339701323253\n",
      "2022-03-26 20:33:21.571947 Epoch 1, Training Loss 1.1655468231881672\n",
      "2022-03-26 20:33:21.590951 Epoch 1, Training Loss 1.1681286774937758\n",
      "2022-03-26 20:33:21.609941 Epoch 1, Training Loss 1.1705975544727063\n",
      "2022-03-26 20:33:21.627948 Epoch 1, Training Loss 1.1731397463842426\n",
      "2022-03-26 20:33:21.646951 Epoch 1, Training Loss 1.1756162452880683\n",
      "2022-03-26 20:33:21.663962 Epoch 1, Training Loss 1.178020825318973\n",
      "2022-03-26 20:33:21.681966 Epoch 1, Training Loss 1.1805571611884915\n",
      "2022-03-26 20:33:21.699976 Epoch 1, Training Loss 1.183170464794959\n",
      "2022-03-26 20:33:21.717980 Epoch 1, Training Loss 1.185818638795477\n",
      "2022-03-26 20:33:21.738985 Epoch 1, Training Loss 1.1882587463959404\n",
      "2022-03-26 20:33:21.756989 Epoch 1, Training Loss 1.1906478842506019\n",
      "2022-03-26 20:33:21.774986 Epoch 1, Training Loss 1.1931709813339937\n",
      "2022-03-26 20:33:21.793985 Epoch 1, Training Loss 1.1956720172291826\n",
      "2022-03-26 20:33:21.812995 Epoch 1, Training Loss 1.1981695320295251\n",
      "2022-03-26 20:33:21.830993 Epoch 1, Training Loss 1.2007631389686213\n",
      "2022-03-26 20:33:21.849003 Epoch 1, Training Loss 1.2033136397066628\n",
      "2022-03-26 20:33:21.866007 Epoch 1, Training Loss 1.2058496334973503\n",
      "2022-03-26 20:33:21.884011 Epoch 1, Training Loss 1.2082861340259348\n",
      "2022-03-26 20:33:21.902021 Epoch 1, Training Loss 1.210802945334588\n",
      "2022-03-26 20:33:21.921020 Epoch 1, Training Loss 1.2134459018707275\n",
      "2022-03-26 20:33:21.939030 Epoch 1, Training Loss 1.2158798238505488\n",
      "2022-03-26 20:33:21.958034 Epoch 1, Training Loss 1.2182652556987674\n",
      "2022-03-26 20:33:21.977026 Epoch 1, Training Loss 1.2207175596900608\n",
      "2022-03-26 20:33:21.995043 Epoch 1, Training Loss 1.2231963019236884\n",
      "2022-03-26 20:33:22.013041 Epoch 1, Training Loss 1.2256428085629592\n",
      "2022-03-26 20:33:22.031051 Epoch 1, Training Loss 1.2281668135882033\n",
      "2022-03-26 20:33:22.049055 Epoch 1, Training Loss 1.2307112003531298\n",
      "2022-03-26 20:33:22.066047 Epoch 1, Training Loss 1.2332513692128995\n",
      "2022-03-26 20:33:22.084057 Epoch 1, Training Loss 1.2357351511640622\n",
      "2022-03-26 20:33:22.103061 Epoch 1, Training Loss 1.2381855939965114\n",
      "2022-03-26 20:33:22.121071 Epoch 1, Training Loss 1.2406621317729316\n",
      "2022-03-26 20:33:22.139069 Epoch 1, Training Loss 1.2430417339515198\n",
      "2022-03-26 20:33:22.157067 Epoch 1, Training Loss 1.2453842675289535\n",
      "2022-03-26 20:33:22.176071 Epoch 1, Training Loss 1.2480246173146436\n",
      "2022-03-26 20:33:22.194077 Epoch 1, Training Loss 1.2505076846198353\n",
      "2022-03-26 20:33:22.212080 Epoch 1, Training Loss 1.2531088453424557\n",
      "2022-03-26 20:33:22.230084 Epoch 1, Training Loss 1.255688519733946\n",
      "2022-03-26 20:33:22.248084 Epoch 1, Training Loss 1.2581075486319755\n",
      "2022-03-26 20:33:22.266104 Epoch 1, Training Loss 1.2606696492570746\n",
      "2022-03-26 20:33:22.284102 Epoch 1, Training Loss 1.2631938561149265\n",
      "2022-03-26 20:33:22.303112 Epoch 1, Training Loss 1.2655774152187436\n",
      "2022-03-26 20:33:22.321104 Epoch 1, Training Loss 1.2681206422083824\n",
      "2022-03-26 20:33:22.339115 Epoch 1, Training Loss 1.2705339232978918\n",
      "2022-03-26 20:33:22.357119 Epoch 1, Training Loss 1.273107191485822\n",
      "2022-03-26 20:33:22.376129 Epoch 1, Training Loss 1.2755156461235202\n",
      "2022-03-26 20:33:22.395121 Epoch 1, Training Loss 1.2779865674960338\n",
      "2022-03-26 20:33:22.413126 Epoch 1, Training Loss 1.280445279215303\n",
      "2022-03-26 20:33:22.431127 Epoch 1, Training Loss 1.2829012495782368\n",
      "2022-03-26 20:33:22.448131 Epoch 1, Training Loss 1.2853802078215362\n",
      "2022-03-26 20:33:22.466131 Epoch 1, Training Loss 1.287801992710289\n",
      "2022-03-26 20:33:22.484153 Epoch 1, Training Loss 1.290364177483122\n",
      "2022-03-26 20:33:22.501151 Epoch 1, Training Loss 1.2927044547732225\n",
      "2022-03-26 20:33:22.520162 Epoch 1, Training Loss 1.294854678766197\n",
      "2022-03-26 20:33:22.538165 Epoch 1, Training Loss 1.2972698812289616\n",
      "2022-03-26 20:33:22.556170 Epoch 1, Training Loss 1.2996944213462303\n",
      "2022-03-26 20:33:22.575168 Epoch 1, Training Loss 1.3021529993742629\n",
      "2022-03-26 20:33:22.593163 Epoch 1, Training Loss 1.3045714015850935\n",
      "2022-03-26 20:33:22.611171 Epoch 1, Training Loss 1.3070340362351265\n",
      "2022-03-26 20:33:22.629175 Epoch 1, Training Loss 1.3096104325235958\n",
      "2022-03-26 20:33:22.647170 Epoch 1, Training Loss 1.312082217020147\n",
      "2022-03-26 20:33:22.666194 Epoch 1, Training Loss 1.3145841416495536\n",
      "2022-03-26 20:33:22.684186 Epoch 1, Training Loss 1.3170936610692603\n",
      "2022-03-26 20:33:22.702197 Epoch 1, Training Loss 1.3195902415553626\n",
      "2022-03-26 20:33:22.720207 Epoch 1, Training Loss 1.3220316886596972\n",
      "2022-03-26 20:33:22.738197 Epoch 1, Training Loss 1.3245585644641495\n",
      "2022-03-26 20:33:22.756205 Epoch 1, Training Loss 1.3269934094775364\n",
      "2022-03-26 20:33:22.774209 Epoch 1, Training Loss 1.3295170469662112\n",
      "2022-03-26 20:33:22.792223 Epoch 1, Training Loss 1.332183255411475\n",
      "2022-03-26 20:33:22.812210 Epoch 1, Training Loss 1.3344130049581113\n",
      "2022-03-26 20:33:22.830221 Epoch 1, Training Loss 1.3369150497114566\n",
      "2022-03-26 20:33:22.848220 Epoch 1, Training Loss 1.339356373795463\n",
      "2022-03-26 20:33:22.865235 Epoch 1, Training Loss 1.341790849442982\n",
      "2022-03-26 20:33:22.883244 Epoch 1, Training Loss 1.3443610428849144\n",
      "2022-03-26 20:33:22.902248 Epoch 1, Training Loss 1.3467541132741572\n",
      "2022-03-26 20:33:22.920246 Epoch 1, Training Loss 1.3491696610170252\n",
      "2022-03-26 20:33:22.938976 Epoch 1, Training Loss 1.3516469388971548\n",
      "2022-03-26 20:33:22.956980 Epoch 1, Training Loss 1.3541126653666387\n",
      "2022-03-26 20:33:22.975984 Epoch 1, Training Loss 1.3565059845404857\n",
      "2022-03-26 20:33:22.993969 Epoch 1, Training Loss 1.3589310103365222\n",
      "2022-03-26 20:33:23.012973 Epoch 1, Training Loss 1.3614031619123181\n",
      "2022-03-26 20:33:23.030991 Epoch 1, Training Loss 1.3639052206902857\n",
      "2022-03-26 20:33:23.049001 Epoch 1, Training Loss 1.3664184888000683\n",
      "2022-03-26 20:33:23.065998 Epoch 1, Training Loss 1.368870649038983\n",
      "2022-03-26 20:33:23.084009 Epoch 1, Training Loss 1.3713242121974525\n",
      "2022-03-26 20:33:23.102013 Epoch 1, Training Loss 1.3738522098192474\n",
      "2022-03-26 20:33:23.121017 Epoch 1, Training Loss 1.376267339262511\n",
      "2022-03-26 20:33:23.139245 Epoch 1, Training Loss 1.3787016496633935\n",
      "2022-03-26 20:33:23.157249 Epoch 1, Training Loss 1.381173829288434\n",
      "2022-03-26 20:33:23.176241 Epoch 1, Training Loss 1.3836684623337767\n",
      "2022-03-26 20:33:23.194239 Epoch 1, Training Loss 1.3860665526231537\n",
      "2022-03-26 20:33:23.212255 Epoch 1, Training Loss 1.3884865104999689\n",
      "2022-03-26 20:33:23.230253 Epoch 1, Training Loss 1.3910071297984599\n",
      "2022-03-26 20:33:23.248263 Epoch 1, Training Loss 1.3935425531528796\n",
      "2022-03-26 20:33:23.266273 Epoch 1, Training Loss 1.3960261977542088\n",
      "2022-03-26 20:33:23.284278 Epoch 1, Training Loss 1.3986729775243403\n",
      "2022-03-26 20:33:23.303262 Epoch 1, Training Loss 1.4011523525428284\n",
      "2022-03-26 20:33:23.322280 Epoch 1, Training Loss 1.4034876398113378\n",
      "2022-03-26 20:33:23.340459 Epoch 1, Training Loss 1.4057103174421794\n",
      "2022-03-26 20:33:23.359446 Epoch 1, Training Loss 1.4082050402755932\n",
      "2022-03-26 20:33:23.377711 Epoch 1, Training Loss 1.4105696242178798\n",
      "2022-03-26 20:33:23.395726 Epoch 1, Training Loss 1.4129324890768435\n",
      "2022-03-26 20:33:23.414717 Epoch 1, Training Loss 1.4153649607277892\n",
      "2022-03-26 20:33:23.432735 Epoch 1, Training Loss 1.4178133298978781\n",
      "2022-03-26 20:33:23.450732 Epoch 1, Training Loss 1.4202027243116628\n",
      "2022-03-26 20:33:23.468742 Epoch 1, Training Loss 1.4226185517847691\n",
      "2022-03-26 20:33:23.487753 Epoch 1, Training Loss 1.4251237810420259\n",
      "2022-03-26 20:33:23.505758 Epoch 1, Training Loss 1.4276569334747236\n",
      "2022-03-26 20:33:23.525748 Epoch 1, Training Loss 1.4301537173178496\n",
      "2022-03-26 20:33:23.543757 Epoch 1, Training Loss 1.432566514100565\n",
      "2022-03-26 20:33:23.562758 Epoch 1, Training Loss 1.4351388462973982\n",
      "2022-03-26 20:33:23.580768 Epoch 1, Training Loss 1.4377383368704326\n",
      "2022-03-26 20:33:23.598778 Epoch 1, Training Loss 1.4400363594979582\n",
      "2022-03-26 20:33:23.616776 Epoch 1, Training Loss 1.442565921017581\n",
      "2022-03-26 20:33:23.635786 Epoch 1, Training Loss 1.4450449874943785\n",
      "2022-03-26 20:33:23.653791 Epoch 1, Training Loss 1.447522426505223\n",
      "2022-03-26 20:33:23.672795 Epoch 1, Training Loss 1.4498629127926839\n",
      "2022-03-26 20:33:23.691799 Epoch 1, Training Loss 1.452346863801522\n",
      "2022-03-26 20:33:23.710785 Epoch 1, Training Loss 1.4547808357821705\n",
      "2022-03-26 20:33:23.729647 Epoch 1, Training Loss 1.4571990242699528\n",
      "2022-03-26 20:33:23.747653 Epoch 1, Training Loss 1.4594878941545706\n",
      "2022-03-26 20:33:23.765667 Epoch 1, Training Loss 1.4620608306296952\n",
      "2022-03-26 20:33:23.784672 Epoch 1, Training Loss 1.4643919277374091\n",
      "2022-03-26 20:33:23.802676 Epoch 1, Training Loss 1.466782943824368\n",
      "2022-03-26 20:33:23.821674 Epoch 1, Training Loss 1.469343090453721\n",
      "2022-03-26 20:33:23.840684 Epoch 1, Training Loss 1.4717791621642344\n",
      "2022-03-26 20:33:23.858688 Epoch 1, Training Loss 1.4742606840170254\n",
      "2022-03-26 20:33:23.877681 Epoch 1, Training Loss 1.4768292222486432\n",
      "2022-03-26 20:33:23.895692 Epoch 1, Training Loss 1.4793940018814848\n",
      "2022-03-26 20:33:23.913696 Epoch 1, Training Loss 1.481961438570486\n",
      "2022-03-26 20:33:23.931699 Epoch 1, Training Loss 1.4844686145062946\n",
      "2022-03-26 20:33:23.950703 Epoch 1, Training Loss 1.4869690901787995\n",
      "2022-03-26 20:33:23.968713 Epoch 1, Training Loss 1.4894484430932633\n",
      "2022-03-26 20:33:23.986711 Epoch 1, Training Loss 1.491866741034076\n",
      "2022-03-26 20:33:24.005716 Epoch 1, Training Loss 1.4942920339077026\n",
      "2022-03-26 20:33:24.024726 Epoch 1, Training Loss 1.4969767655253106\n",
      "2022-03-26 20:33:24.043713 Epoch 1, Training Loss 1.499373565854319\n",
      "2022-03-26 20:33:24.062724 Epoch 1, Training Loss 1.5016353197414856\n",
      "2022-03-26 20:33:24.080726 Epoch 1, Training Loss 1.5040836065931393\n",
      "2022-03-26 20:33:24.098743 Epoch 1, Training Loss 1.5065139885753622\n",
      "2022-03-26 20:33:24.116747 Epoch 1, Training Loss 1.5088559771742662\n",
      "2022-03-26 20:33:24.135751 Epoch 1, Training Loss 1.511227493700774\n",
      "2022-03-26 20:33:24.154755 Epoch 1, Training Loss 1.5138814781632874\n",
      "2022-03-26 20:33:24.173315 Epoch 1, Training Loss 1.5163551299163447\n",
      "2022-03-26 20:33:24.192299 Epoch 1, Training Loss 1.5187836072938827\n",
      "2022-03-26 20:33:24.211312 Epoch 1, Training Loss 1.52125686681484\n",
      "2022-03-26 20:33:24.230315 Epoch 1, Training Loss 1.5237448878605346\n",
      "2022-03-26 20:33:24.248332 Epoch 1, Training Loss 1.5261827376492494\n",
      "2022-03-26 20:33:24.266323 Epoch 1, Training Loss 1.528630153754788\n",
      "2022-03-26 20:33:24.284321 Epoch 1, Training Loss 1.5311211199711656\n",
      "2022-03-26 20:33:24.310329 Epoch 1, Training Loss 1.533617153954323\n",
      "2022-03-26 20:33:24.335333 Epoch 1, Training Loss 1.5360697207548428\n",
      "2022-03-26 20:33:24.361339 Epoch 1, Training Loss 1.538457311632688\n",
      "2022-03-26 20:33:24.387363 Epoch 1, Training Loss 1.5409536822067806\n",
      "2022-03-26 20:33:24.413369 Epoch 1, Training Loss 1.5433887045096863\n",
      "2022-03-26 20:33:24.440361 Epoch 1, Training Loss 1.5457919390914996\n",
      "2022-03-26 20:33:24.466381 Epoch 1, Training Loss 1.5483079842289391\n",
      "2022-03-26 20:33:24.492368 Epoch 1, Training Loss 1.5509990019261684\n",
      "2022-03-26 20:33:24.511371 Epoch 1, Training Loss 1.553339849195212\n",
      "2022-03-26 20:33:24.529383 Epoch 1, Training Loss 1.5558243927443425\n",
      "2022-03-26 20:33:24.547387 Epoch 1, Training Loss 1.5582344362802822\n",
      "2022-03-26 20:33:24.565403 Epoch 1, Training Loss 1.5607036617405885\n",
      "2022-03-26 20:33:24.582395 Epoch 1, Training Loss 1.5632580962327436\n",
      "2022-03-26 20:33:24.601399 Epoch 1, Training Loss 1.5656851249582626\n",
      "2022-03-26 20:33:24.619416 Epoch 1, Training Loss 1.5682683728844917\n",
      "2022-03-26 20:33:24.637408 Epoch 1, Training Loss 1.5706272085609339\n",
      "2022-03-26 20:33:24.656412 Epoch 1, Training Loss 1.573096642866159\n",
      "2022-03-26 20:33:24.674416 Epoch 1, Training Loss 1.5755855552375775\n",
      "2022-03-26 20:33:24.692610 Epoch 1, Training Loss 1.5780965858103368\n",
      "2022-03-26 20:33:24.711606 Epoch 1, Training Loss 1.580515058144279\n",
      "2022-03-26 20:33:24.729616 Epoch 1, Training Loss 1.5827869687543805\n",
      "2022-03-26 20:33:24.746621 Epoch 1, Training Loss 1.585299843107648\n",
      "2022-03-26 20:33:24.764624 Epoch 1, Training Loss 1.5876845327179756\n",
      "2022-03-26 20:33:24.782641 Epoch 1, Training Loss 1.590177609335126\n",
      "2022-03-26 20:33:24.800645 Epoch 1, Training Loss 1.5926391243019982\n",
      "2022-03-26 20:33:24.819649 Epoch 1, Training Loss 1.5950945685891544\n",
      "2022-03-26 20:33:24.838641 Epoch 1, Training Loss 1.597459351924984\n",
      "2022-03-26 20:33:24.856657 Epoch 1, Training Loss 1.5999200603236323\n",
      "2022-03-26 20:33:24.874869 Epoch 1, Training Loss 1.6025213317188156\n",
      "2022-03-26 20:33:24.893871 Epoch 1, Training Loss 1.6049631623660816\n",
      "2022-03-26 20:33:24.911883 Epoch 1, Training Loss 1.607330069365099\n",
      "2022-03-26 20:33:24.928887 Epoch 1, Training Loss 1.6096572912562535\n",
      "2022-03-26 20:33:24.946890 Epoch 1, Training Loss 1.611941834697333\n",
      "2022-03-26 20:33:24.964886 Epoch 1, Training Loss 1.6142594884423649\n",
      "2022-03-26 20:33:24.982892 Epoch 1, Training Loss 1.6167433391446653\n",
      "2022-03-26 20:33:24.999902 Epoch 1, Training Loss 1.619235758586308\n",
      "2022-03-26 20:33:25.017918 Epoch 1, Training Loss 1.6216152679279943\n",
      "2022-03-26 20:33:25.036916 Epoch 1, Training Loss 1.6239541633354733\n",
      "2022-03-26 20:33:25.054917 Epoch 1, Training Loss 1.6262995317159101\n",
      "2022-03-26 20:33:25.072930 Epoch 1, Training Loss 1.6286471841280417\n",
      "2022-03-26 20:33:25.090922 Epoch 1, Training Loss 1.6310578758454384\n",
      "2022-03-26 20:33:25.109935 Epoch 1, Training Loss 1.633548747243174\n",
      "2022-03-26 20:33:25.128932 Epoch 1, Training Loss 1.6359836753372037\n",
      "2022-03-26 20:33:25.146930 Epoch 1, Training Loss 1.6384433314318547\n",
      "2022-03-26 20:33:25.163939 Epoch 1, Training Loss 1.640852419311738\n",
      "2022-03-26 20:33:25.181955 Epoch 1, Training Loss 1.643295862638127\n",
      "2022-03-26 20:33:25.199959 Epoch 1, Training Loss 1.6456793406430412\n",
      "2022-03-26 20:33:25.216951 Epoch 1, Training Loss 1.6480986684789438\n",
      "2022-03-26 20:33:25.235534 Epoch 1, Training Loss 1.6505291173830057\n",
      "2022-03-26 20:33:25.253526 Epoch 1, Training Loss 1.6529278837506423\n",
      "2022-03-26 20:33:25.272530 Epoch 1, Training Loss 1.6553577327972178\n",
      "2022-03-26 20:33:25.290547 Epoch 1, Training Loss 1.6578214979537613\n",
      "2022-03-26 20:33:25.309552 Epoch 1, Training Loss 1.66030996290924\n",
      "2022-03-26 20:33:25.327544 Epoch 1, Training Loss 1.6626653148390143\n",
      "2022-03-26 20:33:25.345547 Epoch 1, Training Loss 1.6649849486472967\n",
      "2022-03-26 20:33:25.363552 Epoch 1, Training Loss 1.6673844808812641\n",
      "2022-03-26 20:33:25.381548 Epoch 1, Training Loss 1.6698020896338441\n",
      "2022-03-26 20:33:25.398572 Epoch 1, Training Loss 1.6721775123225453\n",
      "2022-03-26 20:33:25.416575 Epoch 1, Training Loss 1.6746194565387638\n",
      "2022-03-26 20:33:25.434567 Epoch 1, Training Loss 1.6770308229624462\n",
      "2022-03-26 20:33:25.452584 Epoch 1, Training Loss 1.679387259361384\n",
      "2022-03-26 20:33:25.470587 Epoch 1, Training Loss 1.681825739648336\n",
      "2022-03-26 20:33:25.489592 Epoch 1, Training Loss 1.6840155179543264\n",
      "2022-03-26 20:33:25.507577 Epoch 1, Training Loss 1.686406960572733\n",
      "2022-03-26 20:33:25.525581 Epoch 1, Training Loss 1.688887956349746\n",
      "2022-03-26 20:33:25.543594 Epoch 1, Training Loss 1.6914779787783123\n",
      "2022-03-26 20:33:25.561596 Epoch 1, Training Loss 1.6938813340938306\n",
      "2022-03-26 20:33:25.580600 Epoch 1, Training Loss 1.6965428333148322\n",
      "2022-03-26 20:33:25.598617 Epoch 1, Training Loss 1.6989293343880598\n",
      "2022-03-26 20:33:25.615608 Epoch 1, Training Loss 1.7012300806887009\n",
      "2022-03-26 20:33:25.633613 Epoch 1, Training Loss 1.7036380202264128\n",
      "2022-03-26 20:33:25.651609 Epoch 1, Training Loss 1.7060931980457452\n",
      "2022-03-26 20:33:25.669613 Epoch 1, Training Loss 1.7084788445316617\n",
      "2022-03-26 20:33:25.689240 Epoch 1, Training Loss 1.7109949825060031\n",
      "2022-03-26 20:33:25.708251 Epoch 1, Training Loss 1.7135169477109105\n",
      "2022-03-26 20:33:25.726236 Epoch 1, Training Loss 1.7159310498505906\n",
      "2022-03-26 20:33:25.744247 Epoch 1, Training Loss 1.718438440424097\n",
      "2022-03-26 20:33:25.762252 Epoch 1, Training Loss 1.7207860124995336\n",
      "2022-03-26 20:33:25.780465 Epoch 1, Training Loss 1.7232092600649276\n",
      "2022-03-26 20:33:25.799482 Epoch 1, Training Loss 1.7255974184826512\n",
      "2022-03-26 20:33:25.818486 Epoch 1, Training Loss 1.7282363472082425\n",
      "2022-03-26 20:33:25.836484 Epoch 1, Training Loss 1.7304361895527072\n",
      "2022-03-26 20:33:25.855488 Epoch 1, Training Loss 1.7329084979908547\n",
      "2022-03-26 20:33:25.873498 Epoch 1, Training Loss 1.735214246660852\n",
      "2022-03-26 20:33:25.892486 Epoch 1, Training Loss 1.7376541042571787\n",
      "2022-03-26 20:33:25.910495 Epoch 1, Training Loss 1.740174467148988\n",
      "2022-03-26 20:33:25.929505 Epoch 1, Training Loss 1.7423409110749775\n",
      "2022-03-26 20:33:25.947509 Epoch 1, Training Loss 1.7449240550360716\n",
      "2022-03-26 20:33:25.965519 Epoch 1, Training Loss 1.7472656198474756\n",
      "2022-03-26 20:33:25.983511 Epoch 1, Training Loss 1.7498891717942475\n",
      "2022-03-26 20:33:26.002528 Epoch 1, Training Loss 1.752310306367362\n",
      "2022-03-26 20:33:26.019531 Epoch 1, Training Loss 1.7546743750572205\n",
      "2022-03-26 20:33:26.038536 Epoch 1, Training Loss 1.7570536071077332\n",
      "2022-03-26 20:33:26.056540 Epoch 1, Training Loss 1.7594175733568724\n",
      "2022-03-26 20:33:26.074532 Epoch 1, Training Loss 1.7617861041632454\n",
      "2022-03-26 20:33:26.092534 Epoch 1, Training Loss 1.764244470297528\n",
      "2022-03-26 20:33:26.111542 Epoch 1, Training Loss 1.7666197365812024\n",
      "2022-03-26 20:33:26.129551 Epoch 1, Training Loss 1.7689635681984064\n",
      "2022-03-26 20:33:26.148555 Epoch 1, Training Loss 1.7713939419487859\n",
      "2022-03-26 20:33:26.166564 Epoch 1, Training Loss 1.773793801169871\n",
      "2022-03-26 20:33:26.185564 Epoch 1, Training Loss 1.776144269482254\n",
      "2022-03-26 20:33:26.202567 Epoch 1, Training Loss 1.7785897494277076\n",
      "2022-03-26 20:33:26.221577 Epoch 1, Training Loss 1.7808801043978737\n",
      "2022-03-26 20:33:26.239581 Epoch 1, Training Loss 1.7831019612834276\n",
      "2022-03-26 20:33:26.257585 Epoch 1, Training Loss 1.785654186440246\n",
      "2022-03-26 20:33:26.275589 Epoch 1, Training Loss 1.7880239227543706\n",
      "2022-03-26 20:33:26.293042 Epoch 1, Training Loss 1.7903692682685755\n",
      "2022-03-26 20:33:26.311346 Epoch 1, Training Loss 1.79282428465231\n",
      "2022-03-26 20:33:26.328388 Epoch 1, Training Loss 1.7950809248877913\n",
      "2022-03-26 20:33:26.346026 Epoch 1, Training Loss 1.797440834210047\n",
      "2022-03-26 20:33:26.366030 Epoch 1, Training Loss 1.7997816964183622\n",
      "2022-03-26 20:33:26.384028 Epoch 1, Training Loss 1.8021204729214348\n",
      "2022-03-26 20:33:26.402043 Epoch 1, Training Loss 1.8045812817790625\n",
      "2022-03-26 20:33:26.420427 Epoch 1, Training Loss 1.8070350411298024\n",
      "2022-03-26 20:33:26.438040 Epoch 1, Training Loss 1.8095103890999504\n",
      "2022-03-26 20:33:26.457801 Epoch 1, Training Loss 1.8119034936360996\n",
      "2022-03-26 20:33:26.475805 Epoch 1, Training Loss 1.8142306034827171\n",
      "2022-03-26 20:33:26.493795 Epoch 1, Training Loss 1.8167919632418992\n",
      "2022-03-26 20:33:26.513807 Epoch 1, Training Loss 1.8191874553175533\n",
      "2022-03-26 20:33:26.530811 Epoch 1, Training Loss 1.8215194032015398\n",
      "2022-03-26 20:33:26.548827 Epoch 1, Training Loss 1.8237646026989383\n",
      "2022-03-26 20:33:26.566825 Epoch 1, Training Loss 1.8261179353879846\n",
      "2022-03-26 20:33:26.585823 Epoch 1, Training Loss 1.828449725190087\n",
      "2022-03-26 20:33:26.604840 Epoch 1, Training Loss 1.830975798999562\n",
      "2022-03-26 20:33:26.622844 Epoch 1, Training Loss 1.8335299988841767\n",
      "2022-03-26 20:33:26.641848 Epoch 1, Training Loss 1.836174696912546\n",
      "2022-03-26 20:33:26.659843 Epoch 1, Training Loss 1.8386982281494628\n",
      "2022-03-26 20:33:26.678847 Epoch 1, Training Loss 1.8413005285250865\n",
      "2022-03-26 20:33:26.696848 Epoch 1, Training Loss 1.8435437080195494\n",
      "2022-03-26 20:33:26.714859 Epoch 1, Training Loss 1.8460403673179315\n",
      "2022-03-26 20:33:26.732869 Epoch 1, Training Loss 1.8484454507108234\n",
      "2022-03-26 20:33:26.750873 Epoch 1, Training Loss 1.850832080902041\n",
      "2022-03-26 20:33:26.768877 Epoch 1, Training Loss 1.8531457234526534\n",
      "2022-03-26 20:33:26.787869 Epoch 1, Training Loss 1.8555824553875058\n",
      "2022-03-26 20:33:26.806886 Epoch 1, Training Loss 1.8581135526032704\n",
      "2022-03-26 20:33:26.824890 Epoch 1, Training Loss 1.8604898368915939\n",
      "2022-03-26 20:33:26.843875 Epoch 1, Training Loss 1.8628552685613218\n",
      "2022-03-26 20:33:26.861886 Epoch 1, Training Loss 1.8652014790288627\n",
      "2022-03-26 20:33:26.879890 Epoch 1, Training Loss 1.8674050133551479\n",
      "2022-03-26 20:33:26.897929 Epoch 1, Training Loss 1.8698239328001467\n",
      "2022-03-26 20:33:26.915904 Epoch 1, Training Loss 1.8721621227081475\n",
      "2022-03-26 20:33:26.933914 Epoch 1, Training Loss 1.8744318706300251\n",
      "2022-03-26 20:33:26.951919 Epoch 1, Training Loss 1.8766918944580782\n",
      "2022-03-26 20:33:26.969910 Epoch 1, Training Loss 1.879167222915708\n",
      "2022-03-26 20:33:26.987921 Epoch 1, Training Loss 1.8816073146622505\n",
      "2022-03-26 20:33:27.006928 Epoch 1, Training Loss 1.8840865934901225\n",
      "2022-03-26 20:33:27.024935 Epoch 1, Training Loss 1.8865457501862666\n",
      "2022-03-26 20:33:27.043928 Epoch 1, Training Loss 1.888991408335888\n",
      "2022-03-26 20:33:27.062939 Epoch 1, Training Loss 1.89146756683774\n",
      "2022-03-26 20:33:27.080935 Epoch 1, Training Loss 1.8939993758030864\n",
      "2022-03-26 20:33:27.098946 Epoch 1, Training Loss 1.8962549902593997\n",
      "2022-03-26 20:33:27.117950 Epoch 1, Training Loss 1.8986176479503016\n",
      "2022-03-26 20:33:27.135954 Epoch 1, Training Loss 1.90083328140971\n",
      "2022-03-26 20:33:27.154964 Epoch 1, Training Loss 1.9031422240349947\n",
      "2022-03-26 20:33:27.173963 Epoch 1, Training Loss 1.9054814246304506\n",
      "2022-03-26 20:33:27.191953 Epoch 1, Training Loss 1.9078825527749708\n",
      "2022-03-26 20:33:27.209968 Epoch 1, Training Loss 1.9103426116201885\n",
      "2022-03-26 20:33:27.227969 Epoch 1, Training Loss 1.9127252778738661\n",
      "2022-03-26 20:33:27.246985 Epoch 1, Training Loss 1.9151443614984107\n",
      "2022-03-26 20:33:27.264984 Epoch 1, Training Loss 1.9175808387034385\n",
      "2022-03-26 20:33:27.282979 Epoch 1, Training Loss 1.9199872353802556\n",
      "2022-03-26 20:33:27.300992 Epoch 1, Training Loss 1.9225697939658104\n",
      "2022-03-26 20:33:27.318990 Epoch 1, Training Loss 1.924970171640596\n",
      "2022-03-26 20:33:27.337006 Epoch 1, Training Loss 1.9273243921492107\n",
      "2022-03-26 20:33:27.356004 Epoch 1, Training Loss 1.9296272746132463\n",
      "2022-03-26 20:33:27.374014 Epoch 1, Training Loss 1.9319808835263752\n",
      "2022-03-26 20:33:27.392002 Epoch 1, Training Loss 1.9343050172566758\n",
      "2022-03-26 20:33:27.411012 Epoch 1, Training Loss 1.9367341556207602\n",
      "2022-03-26 20:33:27.430021 Epoch 1, Training Loss 1.9392118313733269\n",
      "2022-03-26 20:33:27.448019 Epoch 1, Training Loss 1.9415332243570587\n",
      "2022-03-26 20:33:27.466035 Epoch 1, Training Loss 1.94400382224861\n",
      "2022-03-26 20:33:27.484039 Epoch 1, Training Loss 1.9464249528582445\n",
      "2022-03-26 20:33:27.503037 Epoch 1, Training Loss 1.9490150664468555\n",
      "2022-03-26 20:33:27.521048 Epoch 1, Training Loss 1.9514403725828966\n",
      "2022-03-26 20:33:27.538838 Epoch 1, Training Loss 1.9538752875669534\n",
      "2022-03-26 20:33:27.557842 Epoch 1, Training Loss 1.9563843030149064\n",
      "2022-03-26 20:33:27.576833 Epoch 1, Training Loss 1.9587077265200408\n",
      "2022-03-26 20:33:27.594837 Epoch 1, Training Loss 1.9610616843718702\n",
      "2022-03-26 20:33:27.613849 Epoch 1, Training Loss 1.9634824767137122\n",
      "2022-03-26 20:33:27.631852 Epoch 1, Training Loss 1.965852265162846\n",
      "2022-03-26 20:33:27.649856 Epoch 1, Training Loss 1.9683013448629842\n",
      "2022-03-26 20:33:27.668877 Epoch 1, Training Loss 1.9707003927901579\n",
      "2022-03-26 20:33:27.686877 Epoch 1, Training Loss 1.9731370498762106\n",
      "2022-03-26 20:33:27.705876 Epoch 1, Training Loss 1.9754452493489552\n",
      "2022-03-26 20:33:27.723886 Epoch 1, Training Loss 1.9777366715623899\n",
      "2022-03-26 20:33:27.749892 Epoch 1, Training Loss 1.9801704927783488\n",
      "2022-03-26 20:33:27.775887 Epoch 1, Training Loss 1.9824509859999733\n",
      "2022-03-26 20:33:27.801898 Epoch 1, Training Loss 1.9848507796711934\n",
      "2022-03-26 20:33:27.827899 Epoch 1, Training Loss 1.9872467016320094\n",
      "2022-03-26 20:33:27.853916 Epoch 1, Training Loss 1.9897532517952687\n",
      "2022-03-26 20:33:27.879921 Epoch 1, Training Loss 1.9921396717695934\n",
      "2022-03-26 20:33:27.905927 Epoch 1, Training Loss 1.9946092859558437\n",
      "2022-03-26 20:33:27.932933 Epoch 1, Training Loss 1.997039602540643\n",
      "2022-03-26 20:33:27.950937 Epoch 1, Training Loss 1.9993871265969922\n",
      "2022-03-26 20:33:27.968941 Epoch 1, Training Loss 2.0017729163779627\n",
      "2022-03-26 20:33:27.986945 Epoch 1, Training Loss 2.004163052877197\n",
      "2022-03-26 20:33:28.004949 Epoch 1, Training Loss 2.0065318286571356\n",
      "2022-03-26 20:33:28.022948 Epoch 1, Training Loss 2.0087551628537192\n",
      "2022-03-26 20:33:28.040164 Epoch 1, Training Loss 2.011312267054682\n",
      "2022-03-26 20:33:28.059319 Epoch 1, Training Loss 2.0137697950653406\n",
      "2022-03-26 20:33:28.077322 Epoch 1, Training Loss 2.016217253702071\n",
      "2022-03-26 20:33:28.085324 Epoch 1, Training Loss 2.01885808825188\n",
      "2022-03-26 20:44:32.737441 Epoch 50, Training Loss 0.0009678846887310448\n",
      "2022-03-26 20:44:32.755339 Epoch 50, Training Loss 0.0020492377183626374\n",
      "2022-03-26 20:44:32.773819 Epoch 50, Training Loss 0.0031750327181023406\n",
      "2022-03-26 20:44:32.791284 Epoch 50, Training Loss 0.0045402905215387755\n",
      "2022-03-26 20:44:32.809803 Epoch 50, Training Loss 0.005590497380327386\n",
      "2022-03-26 20:44:32.827812 Epoch 50, Training Loss 0.006552396680388\n",
      "2022-03-26 20:44:32.845629 Epoch 50, Training Loss 0.007543360180866993\n",
      "2022-03-26 20:44:32.863639 Epoch 50, Training Loss 0.008280645703415737\n",
      "2022-03-26 20:44:32.881645 Epoch 50, Training Loss 0.009338906125339401\n",
      "2022-03-26 20:44:32.899654 Epoch 50, Training Loss 0.010326359964087796\n",
      "2022-03-26 20:44:32.918665 Epoch 50, Training Loss 0.011335885204622507\n",
      "2022-03-26 20:44:32.937687 Epoch 50, Training Loss 0.012582300569090392\n",
      "2022-03-26 20:44:32.957688 Epoch 50, Training Loss 0.013717198615793682\n",
      "2022-03-26 20:44:32.975710 Epoch 50, Training Loss 0.01468334592821653\n",
      "2022-03-26 20:44:32.993756 Epoch 50, Training Loss 0.0157981157455298\n",
      "2022-03-26 20:44:33.011695 Epoch 50, Training Loss 0.01683597918361654\n",
      "2022-03-26 20:44:33.029733 Epoch 50, Training Loss 0.01789346017191172\n",
      "2022-03-26 20:44:33.048491 Epoch 50, Training Loss 0.018991621863811523\n",
      "2022-03-26 20:44:33.067533 Epoch 50, Training Loss 0.02001165421417607\n",
      "2022-03-26 20:44:33.085558 Epoch 50, Training Loss 0.021231253967260767\n",
      "2022-03-26 20:44:33.104252 Epoch 50, Training Loss 0.022496597007717316\n",
      "2022-03-26 20:44:33.122888 Epoch 50, Training Loss 0.02372531428971254\n",
      "2022-03-26 20:44:33.140871 Epoch 50, Training Loss 0.024772688327238078\n",
      "2022-03-26 20:44:33.158927 Epoch 50, Training Loss 0.025526572523824394\n",
      "2022-03-26 20:44:33.178049 Epoch 50, Training Loss 0.026639138722358763\n",
      "2022-03-26 20:44:33.196083 Epoch 50, Training Loss 0.027597034831181207\n",
      "2022-03-26 20:44:33.215090 Epoch 50, Training Loss 0.02868557578462469\n",
      "2022-03-26 20:44:33.233158 Epoch 50, Training Loss 0.029860513899332423\n",
      "2022-03-26 20:44:33.252225 Epoch 50, Training Loss 0.03104646957438925\n",
      "2022-03-26 20:44:33.270715 Epoch 50, Training Loss 0.03209840359590243\n",
      "2022-03-26 20:44:33.288723 Epoch 50, Training Loss 0.03315718254774733\n",
      "2022-03-26 20:44:33.307029 Epoch 50, Training Loss 0.03435853923982976\n",
      "2022-03-26 20:44:33.324934 Epoch 50, Training Loss 0.03519114394626959\n",
      "2022-03-26 20:44:33.342481 Epoch 50, Training Loss 0.036168429628967325\n",
      "2022-03-26 20:44:33.362127 Epoch 50, Training Loss 0.03724923752762777\n",
      "2022-03-26 20:44:33.380254 Epoch 50, Training Loss 0.038164238338275334\n",
      "2022-03-26 20:44:33.399328 Epoch 50, Training Loss 0.03890376962969065\n",
      "2022-03-26 20:44:33.418556 Epoch 50, Training Loss 0.04003886470709311\n",
      "2022-03-26 20:44:33.436940 Epoch 50, Training Loss 0.041121930417502325\n",
      "2022-03-26 20:44:33.454966 Epoch 50, Training Loss 0.04208222801423134\n",
      "2022-03-26 20:44:33.473898 Epoch 50, Training Loss 0.04318173843271592\n",
      "2022-03-26 20:44:33.491950 Epoch 50, Training Loss 0.04399563513143593\n",
      "2022-03-26 20:44:33.510322 Epoch 50, Training Loss 0.04511327450842504\n",
      "2022-03-26 20:44:33.528951 Epoch 50, Training Loss 0.046216016077934324\n",
      "2022-03-26 20:44:33.547913 Epoch 50, Training Loss 0.04732066659671266\n",
      "2022-03-26 20:44:33.565876 Epoch 50, Training Loss 0.04822490693967971\n",
      "2022-03-26 20:44:33.584886 Epoch 50, Training Loss 0.04923185805225616\n",
      "2022-03-26 20:44:33.602904 Epoch 50, Training Loss 0.05015380021251376\n",
      "2022-03-26 20:44:33.620959 Epoch 50, Training Loss 0.051140543140108935\n",
      "2022-03-26 20:44:33.640766 Epoch 50, Training Loss 0.052376574033971335\n",
      "2022-03-26 20:44:33.659032 Epoch 50, Training Loss 0.053216212088494655\n",
      "2022-03-26 20:44:33.677090 Epoch 50, Training Loss 0.05422637552556479\n",
      "2022-03-26 20:44:33.695165 Epoch 50, Training Loss 0.05508092022917765\n",
      "2022-03-26 20:44:33.713154 Epoch 50, Training Loss 0.05604459532081624\n",
      "2022-03-26 20:44:33.731301 Epoch 50, Training Loss 0.0570254478308246\n",
      "2022-03-26 20:44:33.750270 Epoch 50, Training Loss 0.05778962571907531\n",
      "2022-03-26 20:44:33.769882 Epoch 50, Training Loss 0.059090224678254186\n",
      "2022-03-26 20:44:33.788870 Epoch 50, Training Loss 0.060218488697505665\n",
      "2022-03-26 20:44:33.807833 Epoch 50, Training Loss 0.061267630675869524\n",
      "2022-03-26 20:44:33.825863 Epoch 50, Training Loss 0.062456504387014056\n",
      "2022-03-26 20:44:33.843975 Epoch 50, Training Loss 0.06346928120574073\n",
      "2022-03-26 20:44:33.862515 Epoch 50, Training Loss 0.06452127399347017\n",
      "2022-03-26 20:44:33.881529 Epoch 50, Training Loss 0.06589557432457614\n",
      "2022-03-26 20:44:33.899495 Epoch 50, Training Loss 0.06691567237724734\n",
      "2022-03-26 20:44:33.918313 Epoch 50, Training Loss 0.0680010291316625\n",
      "2022-03-26 20:44:33.937372 Epoch 50, Training Loss 0.06897217363042904\n",
      "2022-03-26 20:44:33.956772 Epoch 50, Training Loss 0.0697555669280879\n",
      "2022-03-26 20:44:33.974799 Epoch 50, Training Loss 0.07096575631205078\n",
      "2022-03-26 20:44:33.992859 Epoch 50, Training Loss 0.07193500253245654\n",
      "2022-03-26 20:44:34.018715 Epoch 50, Training Loss 0.07312513678275107\n",
      "2022-03-26 20:44:34.044747 Epoch 50, Training Loss 0.0741208929692388\n",
      "2022-03-26 20:44:34.070780 Epoch 50, Training Loss 0.07524968634176132\n",
      "2022-03-26 20:44:34.097835 Epoch 50, Training Loss 0.07626482547091706\n",
      "2022-03-26 20:44:34.123714 Epoch 50, Training Loss 0.07745827265712611\n",
      "2022-03-26 20:44:34.148980 Epoch 50, Training Loss 0.07828370193996088\n",
      "2022-03-26 20:44:34.176392 Epoch 50, Training Loss 0.07909749817970159\n",
      "2022-03-26 20:44:34.203487 Epoch 50, Training Loss 0.08013875481417722\n",
      "2022-03-26 20:44:34.222502 Epoch 50, Training Loss 0.08129736178976191\n",
      "2022-03-26 20:44:34.240540 Epoch 50, Training Loss 0.08252420465049841\n",
      "2022-03-26 20:44:34.258941 Epoch 50, Training Loss 0.08382607603926792\n",
      "2022-03-26 20:44:34.275968 Epoch 50, Training Loss 0.08474830318899716\n",
      "2022-03-26 20:44:34.294233 Epoch 50, Training Loss 0.08574131679961748\n",
      "2022-03-26 20:44:34.313236 Epoch 50, Training Loss 0.0868173772111878\n",
      "2022-03-26 20:44:34.331511 Epoch 50, Training Loss 0.08780845443306066\n",
      "2022-03-26 20:44:34.350476 Epoch 50, Training Loss 0.0887910701582194\n",
      "2022-03-26 20:44:34.368521 Epoch 50, Training Loss 0.09010867595367725\n",
      "2022-03-26 20:44:34.386531 Epoch 50, Training Loss 0.09110685672296588\n",
      "2022-03-26 20:44:34.405805 Epoch 50, Training Loss 0.09224273565480166\n",
      "2022-03-26 20:44:34.423815 Epoch 50, Training Loss 0.0931576406559371\n",
      "2022-03-26 20:44:34.441878 Epoch 50, Training Loss 0.09399631299326182\n",
      "2022-03-26 20:44:34.459823 Epoch 50, Training Loss 0.09498863451925994\n",
      "2022-03-26 20:44:34.477808 Epoch 50, Training Loss 0.09598883361462741\n",
      "2022-03-26 20:44:34.495768 Epoch 50, Training Loss 0.09707445012943824\n",
      "2022-03-26 20:44:34.514809 Epoch 50, Training Loss 0.09806742258084095\n",
      "2022-03-26 20:44:34.532754 Epoch 50, Training Loss 0.09923902314032435\n",
      "2022-03-26 20:44:34.551758 Epoch 50, Training Loss 0.10009667894724385\n",
      "2022-03-26 20:44:34.569645 Epoch 50, Training Loss 0.1009936632250276\n",
      "2022-03-26 20:44:34.588768 Epoch 50, Training Loss 0.10218519353500717\n",
      "2022-03-26 20:44:34.606760 Epoch 50, Training Loss 0.1032699582064548\n",
      "2022-03-26 20:44:34.624810 Epoch 50, Training Loss 0.10441215690749381\n",
      "2022-03-26 20:44:34.642767 Epoch 50, Training Loss 0.10536754802059944\n",
      "2022-03-26 20:44:34.660036 Epoch 50, Training Loss 0.10636391755565049\n",
      "2022-03-26 20:44:34.679043 Epoch 50, Training Loss 0.10756458056247448\n",
      "2022-03-26 20:44:34.696976 Epoch 50, Training Loss 0.10848894715309143\n",
      "2022-03-26 20:44:34.716738 Epoch 50, Training Loss 0.10963230174215859\n",
      "2022-03-26 20:44:34.735617 Epoch 50, Training Loss 0.11078362506064003\n",
      "2022-03-26 20:44:34.754655 Epoch 50, Training Loss 0.11216570006307129\n",
      "2022-03-26 20:44:34.772684 Epoch 50, Training Loss 0.11320963944010723\n",
      "2022-03-26 20:44:34.792700 Epoch 50, Training Loss 0.11435047226488743\n",
      "2022-03-26 20:44:34.810722 Epoch 50, Training Loss 0.11535410426766671\n",
      "2022-03-26 20:44:34.827774 Epoch 50, Training Loss 0.1167336177947881\n",
      "2022-03-26 20:44:34.846777 Epoch 50, Training Loss 0.118049700699194\n",
      "2022-03-26 20:44:34.865392 Epoch 50, Training Loss 0.11898781324896361\n",
      "2022-03-26 20:44:34.883402 Epoch 50, Training Loss 0.11997002934860757\n",
      "2022-03-26 20:44:34.901423 Epoch 50, Training Loss 0.12077584527337643\n",
      "2022-03-26 20:44:34.919378 Epoch 50, Training Loss 0.1217788304666729\n",
      "2022-03-26 20:44:34.938400 Epoch 50, Training Loss 0.12315916809279595\n",
      "2022-03-26 20:44:34.957387 Epoch 50, Training Loss 0.12411958406038602\n",
      "2022-03-26 20:44:34.976428 Epoch 50, Training Loss 0.12525995917942212\n",
      "2022-03-26 20:44:34.995749 Epoch 50, Training Loss 0.12624670927177\n",
      "2022-03-26 20:44:35.013614 Epoch 50, Training Loss 0.12707560012102737\n",
      "2022-03-26 20:44:35.031631 Epoch 50, Training Loss 0.1281845170975951\n",
      "2022-03-26 20:44:35.050438 Epoch 50, Training Loss 0.129444199373655\n",
      "2022-03-26 20:44:35.067835 Epoch 50, Training Loss 0.13052635104454996\n",
      "2022-03-26 20:44:35.086897 Epoch 50, Training Loss 0.1314373227488964\n",
      "2022-03-26 20:44:35.104816 Epoch 50, Training Loss 0.13266588804666954\n",
      "2022-03-26 20:44:35.123680 Epoch 50, Training Loss 0.1338195348792064\n",
      "2022-03-26 20:44:35.141692 Epoch 50, Training Loss 0.13494861232655128\n",
      "2022-03-26 20:44:35.160615 Epoch 50, Training Loss 0.13596567381983218\n",
      "2022-03-26 20:44:35.177966 Epoch 50, Training Loss 0.13710142020374308\n",
      "2022-03-26 20:44:35.197014 Epoch 50, Training Loss 0.13822216054667597\n",
      "2022-03-26 20:44:35.215807 Epoch 50, Training Loss 0.13907006390564278\n",
      "2022-03-26 20:44:35.234228 Epoch 50, Training Loss 0.14014181738619305\n",
      "2022-03-26 20:44:35.252733 Epoch 50, Training Loss 0.1410575999170923\n",
      "2022-03-26 20:44:35.270748 Epoch 50, Training Loss 0.14211669876752303\n",
      "2022-03-26 20:44:35.289722 Epoch 50, Training Loss 0.14309835967505374\n",
      "2022-03-26 20:44:35.307803 Epoch 50, Training Loss 0.14422168969498267\n",
      "2022-03-26 20:44:35.325786 Epoch 50, Training Loss 0.14537527898083563\n",
      "2022-03-26 20:44:35.343842 Epoch 50, Training Loss 0.14641644064422762\n",
      "2022-03-26 20:44:35.361803 Epoch 50, Training Loss 0.14734819866812138\n",
      "2022-03-26 20:44:35.379754 Epoch 50, Training Loss 0.1483115050036584\n",
      "2022-03-26 20:44:35.398759 Epoch 50, Training Loss 0.1493626624117117\n",
      "2022-03-26 20:44:35.417031 Epoch 50, Training Loss 0.1504420545857276\n",
      "2022-03-26 20:44:35.435189 Epoch 50, Training Loss 0.15136808606669727\n",
      "2022-03-26 20:44:35.453207 Epoch 50, Training Loss 0.15200907045313158\n",
      "2022-03-26 20:44:35.471814 Epoch 50, Training Loss 0.1534354233223459\n",
      "2022-03-26 20:44:35.490392 Epoch 50, Training Loss 0.15435135669415564\n",
      "2022-03-26 20:44:35.508851 Epoch 50, Training Loss 0.1556396684073426\n",
      "2022-03-26 20:44:35.526883 Epoch 50, Training Loss 0.1564916556753466\n",
      "2022-03-26 20:44:35.544869 Epoch 50, Training Loss 0.15761971778576941\n",
      "2022-03-26 20:44:35.562866 Epoch 50, Training Loss 0.15866351546838764\n",
      "2022-03-26 20:44:35.580964 Epoch 50, Training Loss 0.15971905900084454\n",
      "2022-03-26 20:44:35.599875 Epoch 50, Training Loss 0.16058722961589197\n",
      "2022-03-26 20:44:35.618530 Epoch 50, Training Loss 0.16164543782658589\n",
      "2022-03-26 20:44:35.637558 Epoch 50, Training Loss 0.16283019614951386\n",
      "2022-03-26 20:44:35.655597 Epoch 50, Training Loss 0.1640167271389681\n",
      "2022-03-26 20:44:35.673603 Epoch 50, Training Loss 0.16498374443529817\n",
      "2022-03-26 20:44:35.691921 Epoch 50, Training Loss 0.16609849900845677\n",
      "2022-03-26 20:44:35.709960 Epoch 50, Training Loss 0.1670970721622867\n",
      "2022-03-26 20:44:35.727918 Epoch 50, Training Loss 0.16818296101392077\n",
      "2022-03-26 20:44:35.746952 Epoch 50, Training Loss 0.1691807368222405\n",
      "2022-03-26 20:44:35.764936 Epoch 50, Training Loss 0.1704030444707407\n",
      "2022-03-26 20:44:35.783933 Epoch 50, Training Loss 0.1714716062826269\n",
      "2022-03-26 20:44:35.801981 Epoch 50, Training Loss 0.17267087887010307\n",
      "2022-03-26 20:44:35.820499 Epoch 50, Training Loss 0.17361050181071777\n",
      "2022-03-26 20:44:35.839540 Epoch 50, Training Loss 0.1745754536003103\n",
      "2022-03-26 20:44:35.857720 Epoch 50, Training Loss 0.17539789274220577\n",
      "2022-03-26 20:44:35.876777 Epoch 50, Training Loss 0.17644489390770798\n",
      "2022-03-26 20:44:35.894776 Epoch 50, Training Loss 0.17755508849687893\n",
      "2022-03-26 20:44:35.913674 Epoch 50, Training Loss 0.17842582638001503\n",
      "2022-03-26 20:44:35.931678 Epoch 50, Training Loss 0.17966695461431734\n",
      "2022-03-26 20:44:35.949733 Epoch 50, Training Loss 0.18072185484344697\n",
      "2022-03-26 20:44:35.967987 Epoch 50, Training Loss 0.18191130364032657\n",
      "2022-03-26 20:44:35.986109 Epoch 50, Training Loss 0.18300845212948597\n",
      "2022-03-26 20:44:36.004148 Epoch 50, Training Loss 0.18396470758616162\n",
      "2022-03-26 20:44:36.023130 Epoch 50, Training Loss 0.18502804903727968\n",
      "2022-03-26 20:44:36.042234 Epoch 50, Training Loss 0.18614031347777227\n",
      "2022-03-26 20:44:36.060250 Epoch 50, Training Loss 0.1869182981493528\n",
      "2022-03-26 20:44:36.079317 Epoch 50, Training Loss 0.1880855976468157\n",
      "2022-03-26 20:44:36.098376 Epoch 50, Training Loss 0.18902497309857927\n",
      "2022-03-26 20:44:36.117408 Epoch 50, Training Loss 0.19007968117513924\n",
      "2022-03-26 20:44:36.137549 Epoch 50, Training Loss 0.19094331124249628\n",
      "2022-03-26 20:44:36.157567 Epoch 50, Training Loss 0.19186848805993414\n",
      "2022-03-26 20:44:36.175617 Epoch 50, Training Loss 0.19306817978544308\n",
      "2022-03-26 20:44:36.194673 Epoch 50, Training Loss 0.19412146527748889\n",
      "2022-03-26 20:44:36.213696 Epoch 50, Training Loss 0.1948611689040728\n",
      "2022-03-26 20:44:36.233657 Epoch 50, Training Loss 0.19595410977787983\n",
      "2022-03-26 20:44:36.252146 Epoch 50, Training Loss 0.19693719647119723\n",
      "2022-03-26 20:44:36.270615 Epoch 50, Training Loss 0.1975227590183468\n",
      "2022-03-26 20:44:36.290649 Epoch 50, Training Loss 0.19857452992740496\n",
      "2022-03-26 20:44:36.308677 Epoch 50, Training Loss 0.1995714546545692\n",
      "2022-03-26 20:44:36.326707 Epoch 50, Training Loss 0.20061644321055058\n",
      "2022-03-26 20:44:36.345742 Epoch 50, Training Loss 0.2015841408535038\n",
      "2022-03-26 20:44:36.364686 Epoch 50, Training Loss 0.20284041880494189\n",
      "2022-03-26 20:44:36.383266 Epoch 50, Training Loss 0.20386362270168637\n",
      "2022-03-26 20:44:36.401307 Epoch 50, Training Loss 0.20500798470071516\n",
      "2022-03-26 20:44:36.420455 Epoch 50, Training Loss 0.20616384254544592\n",
      "2022-03-26 20:44:36.440474 Epoch 50, Training Loss 0.20720813733994808\n",
      "2022-03-26 20:44:36.459544 Epoch 50, Training Loss 0.2084744370273312\n",
      "2022-03-26 20:44:36.477647 Epoch 50, Training Loss 0.20962865330526592\n",
      "2022-03-26 20:44:36.497655 Epoch 50, Training Loss 0.21053443475604972\n",
      "2022-03-26 20:44:36.515664 Epoch 50, Training Loss 0.211781915458267\n",
      "2022-03-26 20:44:36.534684 Epoch 50, Training Loss 0.21310489237918268\n",
      "2022-03-26 20:44:36.553647 Epoch 50, Training Loss 0.21394775266690022\n",
      "2022-03-26 20:44:36.573658 Epoch 50, Training Loss 0.2146288077620899\n",
      "2022-03-26 20:44:36.591687 Epoch 50, Training Loss 0.21536938099147718\n",
      "2022-03-26 20:44:36.610708 Epoch 50, Training Loss 0.21634824360575516\n",
      "2022-03-26 20:44:36.628764 Epoch 50, Training Loss 0.21725671755535828\n",
      "2022-03-26 20:44:36.647717 Epoch 50, Training Loss 0.21812449166994266\n",
      "2022-03-26 20:44:36.666748 Epoch 50, Training Loss 0.219076446674364\n",
      "2022-03-26 20:44:36.684785 Epoch 50, Training Loss 0.2200486193913633\n",
      "2022-03-26 20:44:36.703726 Epoch 50, Training Loss 0.22147539574319444\n",
      "2022-03-26 20:44:36.722504 Epoch 50, Training Loss 0.22269767709552785\n",
      "2022-03-26 20:44:36.740545 Epoch 50, Training Loss 0.2236998351409917\n",
      "2022-03-26 20:44:36.759473 Epoch 50, Training Loss 0.22484890583073697\n",
      "2022-03-26 20:44:36.777513 Epoch 50, Training Loss 0.22603212136899115\n",
      "2022-03-26 20:44:36.796211 Epoch 50, Training Loss 0.2273163728015807\n",
      "2022-03-26 20:44:36.814933 Epoch 50, Training Loss 0.22819139101468694\n",
      "2022-03-26 20:44:36.833945 Epoch 50, Training Loss 0.22914476174375284\n",
      "2022-03-26 20:44:36.852179 Epoch 50, Training Loss 0.230418502560357\n",
      "2022-03-26 20:44:36.870617 Epoch 50, Training Loss 0.23148276895055991\n",
      "2022-03-26 20:44:36.890633 Epoch 50, Training Loss 0.23252652997098616\n",
      "2022-03-26 20:44:36.909694 Epoch 50, Training Loss 0.23355307119429264\n",
      "2022-03-26 20:44:36.927708 Epoch 50, Training Loss 0.23475100397301452\n",
      "2022-03-26 20:44:36.946767 Epoch 50, Training Loss 0.23605603898120353\n",
      "2022-03-26 20:44:36.965937 Epoch 50, Training Loss 0.23695620639092477\n",
      "2022-03-26 20:44:36.984017 Epoch 50, Training Loss 0.237860403288051\n",
      "2022-03-26 20:44:37.003093 Epoch 50, Training Loss 0.2388510734147733\n",
      "2022-03-26 20:44:37.021204 Epoch 50, Training Loss 0.23977448594996997\n",
      "2022-03-26 20:44:37.047355 Epoch 50, Training Loss 0.24129830841975444\n",
      "2022-03-26 20:44:37.074699 Epoch 50, Training Loss 0.24225537029221234\n",
      "2022-03-26 20:44:37.100753 Epoch 50, Training Loss 0.24372125399844422\n",
      "2022-03-26 20:44:37.127628 Epoch 50, Training Loss 0.24487675726413727\n",
      "2022-03-26 20:44:37.153633 Epoch 50, Training Loss 0.24585384096178559\n",
      "2022-03-26 20:44:37.180694 Epoch 50, Training Loss 0.24710439747709143\n",
      "2022-03-26 20:44:37.207099 Epoch 50, Training Loss 0.24838844326603443\n",
      "2022-03-26 20:44:37.233197 Epoch 50, Training Loss 0.24948731884169761\n",
      "2022-03-26 20:44:37.247217 Epoch 50, Training Loss 0.2503561691173812\n",
      "2022-03-26 20:44:37.262155 Epoch 50, Training Loss 0.2513444392806124\n",
      "2022-03-26 20:44:37.277187 Epoch 50, Training Loss 0.25235617606688643\n",
      "2022-03-26 20:44:37.291137 Epoch 50, Training Loss 0.2535169149375023\n",
      "2022-03-26 20:44:37.305158 Epoch 50, Training Loss 0.25465821019371454\n",
      "2022-03-26 20:44:37.319113 Epoch 50, Training Loss 0.25566748032332076\n",
      "2022-03-26 20:44:37.334223 Epoch 50, Training Loss 0.25645321329383897\n",
      "2022-03-26 20:44:37.348210 Epoch 50, Training Loss 0.25771731020087174\n",
      "2022-03-26 20:44:37.362654 Epoch 50, Training Loss 0.25877723246431716\n",
      "2022-03-26 20:44:37.377204 Epoch 50, Training Loss 0.25988941367172524\n",
      "2022-03-26 20:44:37.391236 Epoch 50, Training Loss 0.2608323242429577\n",
      "2022-03-26 20:44:37.406251 Epoch 50, Training Loss 0.261922767514463\n",
      "2022-03-26 20:44:37.420168 Epoch 50, Training Loss 0.26299965865624225\n",
      "2022-03-26 20:44:37.434198 Epoch 50, Training Loss 0.2640445520886985\n",
      "2022-03-26 20:44:37.449071 Epoch 50, Training Loss 0.2649262330644881\n",
      "2022-03-26 20:44:37.463074 Epoch 50, Training Loss 0.26607340989667744\n",
      "2022-03-26 20:44:37.478670 Epoch 50, Training Loss 0.2670390742742802\n",
      "2022-03-26 20:44:37.493108 Epoch 50, Training Loss 0.268212174020155\n",
      "2022-03-26 20:44:37.507335 Epoch 50, Training Loss 0.2695762004007769\n",
      "2022-03-26 20:44:37.521343 Epoch 50, Training Loss 0.27051433795096014\n",
      "2022-03-26 20:44:37.536345 Epoch 50, Training Loss 0.271914496484315\n",
      "2022-03-26 20:44:37.549799 Epoch 50, Training Loss 0.27282305191392486\n",
      "2022-03-26 20:44:37.563857 Epoch 50, Training Loss 0.27382223597725336\n",
      "2022-03-26 20:44:37.578818 Epoch 50, Training Loss 0.275058786391907\n",
      "2022-03-26 20:44:37.592821 Epoch 50, Training Loss 0.27614924437402155\n",
      "2022-03-26 20:44:37.607005 Epoch 50, Training Loss 0.2772336844211954\n",
      "2022-03-26 20:44:37.620865 Epoch 50, Training Loss 0.278050335738665\n",
      "2022-03-26 20:44:37.635894 Epoch 50, Training Loss 0.2790615251835655\n",
      "2022-03-26 20:44:37.650837 Epoch 50, Training Loss 0.2800449465622987\n",
      "2022-03-26 20:44:37.664849 Epoch 50, Training Loss 0.2810108678801285\n",
      "2022-03-26 20:44:37.679809 Epoch 50, Training Loss 0.2822329337563356\n",
      "2022-03-26 20:44:37.694811 Epoch 50, Training Loss 0.28334707566691786\n",
      "2022-03-26 20:44:37.708830 Epoch 50, Training Loss 0.28447617087370297\n",
      "2022-03-26 20:44:37.722834 Epoch 50, Training Loss 0.2856070748756609\n",
      "2022-03-26 20:44:37.736887 Epoch 50, Training Loss 0.2865215002194695\n",
      "2022-03-26 20:44:37.752824 Epoch 50, Training Loss 0.28743884024565175\n",
      "2022-03-26 20:44:37.767729 Epoch 50, Training Loss 0.2884880761280084\n",
      "2022-03-26 20:44:37.781786 Epoch 50, Training Loss 0.28956230442084924\n",
      "2022-03-26 20:44:37.795790 Epoch 50, Training Loss 0.2905495749486377\n",
      "2022-03-26 20:44:37.810753 Epoch 50, Training Loss 0.29147421303764937\n",
      "2022-03-26 20:44:37.824762 Epoch 50, Training Loss 0.292752982062452\n",
      "2022-03-26 20:44:37.839124 Epoch 50, Training Loss 0.29365942853948346\n",
      "2022-03-26 20:44:37.853153 Epoch 50, Training Loss 0.2949252693778109\n",
      "2022-03-26 20:44:37.867504 Epoch 50, Training Loss 0.29606652446567555\n",
      "2022-03-26 20:44:37.883507 Epoch 50, Training Loss 0.29713018124213303\n",
      "2022-03-26 20:44:37.897542 Epoch 50, Training Loss 0.29825821941923303\n",
      "2022-03-26 20:44:37.912546 Epoch 50, Training Loss 0.29925060908660256\n",
      "2022-03-26 20:44:37.926639 Epoch 50, Training Loss 0.3001373288652781\n",
      "2022-03-26 20:44:37.940625 Epoch 50, Training Loss 0.30106165483022285\n",
      "2022-03-26 20:44:37.955629 Epoch 50, Training Loss 0.3023408070930739\n",
      "2022-03-26 20:44:37.969608 Epoch 50, Training Loss 0.30389246737103326\n",
      "2022-03-26 20:44:37.983611 Epoch 50, Training Loss 0.30509390165586303\n",
      "2022-03-26 20:44:37.998906 Epoch 50, Training Loss 0.3063870951571428\n",
      "2022-03-26 20:44:38.013777 Epoch 50, Training Loss 0.3075586433529549\n",
      "2022-03-26 20:44:38.027815 Epoch 50, Training Loss 0.3087478352858282\n",
      "2022-03-26 20:44:38.041820 Epoch 50, Training Loss 0.3097781810690375\n",
      "2022-03-26 20:44:38.055711 Epoch 50, Training Loss 0.31085725273470133\n",
      "2022-03-26 20:44:38.070609 Epoch 50, Training Loss 0.31202540285599506\n",
      "2022-03-26 20:44:38.084613 Epoch 50, Training Loss 0.31324698556871977\n",
      "2022-03-26 20:44:38.099651 Epoch 50, Training Loss 0.314204823902196\n",
      "2022-03-26 20:44:38.113654 Epoch 50, Training Loss 0.31524712018802037\n",
      "2022-03-26 20:44:38.127895 Epoch 50, Training Loss 0.31617312788810875\n",
      "2022-03-26 20:44:38.141924 Epoch 50, Training Loss 0.3172311444797784\n",
      "2022-03-26 20:44:38.156510 Epoch 50, Training Loss 0.3184745654349437\n",
      "2022-03-26 20:44:38.170431 Epoch 50, Training Loss 0.31943714576761434\n",
      "2022-03-26 20:44:38.186476 Epoch 50, Training Loss 0.32032760074528893\n",
      "2022-03-26 20:44:38.200026 Epoch 50, Training Loss 0.3213839548094498\n",
      "2022-03-26 20:44:38.215504 Epoch 50, Training Loss 0.3224878745996739\n",
      "2022-03-26 20:44:38.229512 Epoch 50, Training Loss 0.32343838156184274\n",
      "2022-03-26 20:44:38.243515 Epoch 50, Training Loss 0.32479351282577074\n",
      "2022-03-26 20:44:38.257665 Epoch 50, Training Loss 0.3260340942522449\n",
      "2022-03-26 20:44:38.271664 Epoch 50, Training Loss 0.32701291364934437\n",
      "2022-03-26 20:44:38.286693 Epoch 50, Training Loss 0.32784308813264607\n",
      "2022-03-26 20:44:38.300715 Epoch 50, Training Loss 0.3288766205539484\n",
      "2022-03-26 20:44:38.315743 Epoch 50, Training Loss 0.33019103407097594\n",
      "2022-03-26 20:44:38.329751 Epoch 50, Training Loss 0.33119295011548433\n",
      "2022-03-26 20:44:38.344759 Epoch 50, Training Loss 0.33222378912331807\n",
      "2022-03-26 20:44:38.358122 Epoch 50, Training Loss 0.33338393522497944\n",
      "2022-03-26 20:44:38.373112 Epoch 50, Training Loss 0.3343002506153053\n",
      "2022-03-26 20:44:38.387052 Epoch 50, Training Loss 0.3351159573287305\n",
      "2022-03-26 20:44:38.401111 Epoch 50, Training Loss 0.33618199234575874\n",
      "2022-03-26 20:44:38.415694 Epoch 50, Training Loss 0.33728072897095207\n",
      "2022-03-26 20:44:38.430715 Epoch 50, Training Loss 0.3380641731840875\n",
      "2022-03-26 20:44:38.444746 Epoch 50, Training Loss 0.3391340857042986\n",
      "2022-03-26 20:44:38.458773 Epoch 50, Training Loss 0.3401333586410488\n",
      "2022-03-26 20:44:38.473802 Epoch 50, Training Loss 0.3412161883719437\n",
      "2022-03-26 20:44:38.488505 Epoch 50, Training Loss 0.3423582795254715\n",
      "2022-03-26 20:44:38.502526 Epoch 50, Training Loss 0.34368100232632876\n",
      "2022-03-26 20:44:38.517487 Epoch 50, Training Loss 0.34516216159019325\n",
      "2022-03-26 20:44:38.531492 Epoch 50, Training Loss 0.34634246130276214\n",
      "2022-03-26 20:44:38.546494 Epoch 50, Training Loss 0.3473559872954703\n",
      "2022-03-26 20:44:38.560044 Epoch 50, Training Loss 0.3485150927549128\n",
      "2022-03-26 20:44:38.574119 Epoch 50, Training Loss 0.34974144170503785\n",
      "2022-03-26 20:44:38.589115 Epoch 50, Training Loss 0.3508533423437792\n",
      "2022-03-26 20:44:38.603143 Epoch 50, Training Loss 0.35211466809215447\n",
      "2022-03-26 20:44:38.617094 Epoch 50, Training Loss 0.3531799222273595\n",
      "2022-03-26 20:44:38.631211 Epoch 50, Training Loss 0.35416039119443626\n",
      "2022-03-26 20:44:38.646278 Epoch 50, Training Loss 0.3554606492943166\n",
      "2022-03-26 20:44:38.659989 Epoch 50, Training Loss 0.3565083426206618\n",
      "2022-03-26 20:44:38.673974 Epoch 50, Training Loss 0.3574312139883676\n",
      "2022-03-26 20:44:38.688006 Epoch 50, Training Loss 0.358638346690656\n",
      "2022-03-26 20:44:38.703056 Epoch 50, Training Loss 0.3597298006496161\n",
      "2022-03-26 20:44:38.716930 Epoch 50, Training Loss 0.36065144196648125\n",
      "2022-03-26 20:44:38.731998 Epoch 50, Training Loss 0.3614988118562552\n",
      "2022-03-26 20:44:38.746019 Epoch 50, Training Loss 0.36250279683743597\n",
      "2022-03-26 20:44:38.760278 Epoch 50, Training Loss 0.36375038074258037\n",
      "2022-03-26 20:44:38.774389 Epoch 50, Training Loss 0.36464000826753923\n",
      "2022-03-26 20:44:38.788323 Epoch 50, Training Loss 0.36556462486229285\n",
      "2022-03-26 20:44:38.802362 Epoch 50, Training Loss 0.366811723545994\n",
      "2022-03-26 20:44:38.816375 Epoch 50, Training Loss 0.3679534657608213\n",
      "2022-03-26 20:44:38.830476 Epoch 50, Training Loss 0.3694149344549764\n",
      "2022-03-26 20:44:38.845480 Epoch 50, Training Loss 0.37048206308766096\n",
      "2022-03-26 20:44:38.859700 Epoch 50, Training Loss 0.37147255203760493\n",
      "2022-03-26 20:44:38.873729 Epoch 50, Training Loss 0.3725486352391865\n",
      "2022-03-26 20:44:38.887743 Epoch 50, Training Loss 0.37368068765953677\n",
      "2022-03-26 20:44:38.902746 Epoch 50, Training Loss 0.37456161648873476\n",
      "2022-03-26 20:44:38.916749 Epoch 50, Training Loss 0.37583124801478424\n",
      "2022-03-26 20:44:38.931101 Epoch 50, Training Loss 0.37691132808127975\n",
      "2022-03-26 20:44:38.946130 Epoch 50, Training Loss 0.3777752737026385\n",
      "2022-03-26 20:44:38.960218 Epoch 50, Training Loss 0.3787097682047378\n",
      "2022-03-26 20:44:38.974190 Epoch 50, Training Loss 0.37976989512095977\n",
      "2022-03-26 20:44:38.988241 Epoch 50, Training Loss 0.3807686659914758\n",
      "2022-03-26 20:44:39.002342 Epoch 50, Training Loss 0.38176260949553126\n",
      "2022-03-26 20:44:39.016920 Epoch 50, Training Loss 0.3827278827081251\n",
      "2022-03-26 20:44:39.030805 Epoch 50, Training Loss 0.3836711166078782\n",
      "2022-03-26 20:44:39.049779 Epoch 50, Training Loss 0.384712631478334\n",
      "2022-03-26 20:44:39.069225 Epoch 50, Training Loss 0.38593608320064254\n",
      "2022-03-26 20:44:39.089022 Epoch 50, Training Loss 0.38702150184632567\n",
      "2022-03-26 20:44:39.107921 Epoch 50, Training Loss 0.3879646562478122\n",
      "2022-03-26 20:44:39.126973 Epoch 50, Training Loss 0.38905090051691243\n",
      "2022-03-26 20:44:39.147095 Epoch 50, Training Loss 0.3901014587534663\n",
      "2022-03-26 20:44:39.165060 Epoch 50, Training Loss 0.3909931934779257\n",
      "2022-03-26 20:44:39.184089 Epoch 50, Training Loss 0.39202022205685716\n",
      "2022-03-26 20:44:39.203944 Epoch 50, Training Loss 0.3928612404695862\n",
      "2022-03-26 20:44:39.223627 Epoch 50, Training Loss 0.393922736615781\n",
      "2022-03-26 20:44:39.242631 Epoch 50, Training Loss 0.3952510608812732\n",
      "2022-03-26 20:44:39.261551 Epoch 50, Training Loss 0.396383070084445\n",
      "2022-03-26 20:44:39.281566 Epoch 50, Training Loss 0.3971581053169792\n",
      "2022-03-26 20:44:39.300886 Epoch 50, Training Loss 0.3985559578670565\n",
      "2022-03-26 20:44:39.320787 Epoch 50, Training Loss 0.39956426609050283\n",
      "2022-03-26 20:44:39.339784 Epoch 50, Training Loss 0.4002846475985959\n",
      "2022-03-26 20:44:39.359440 Epoch 50, Training Loss 0.40141117355555217\n",
      "2022-03-26 20:44:39.378493 Epoch 50, Training Loss 0.40234653255366304\n",
      "2022-03-26 20:44:39.398522 Epoch 50, Training Loss 0.40338796045621644\n",
      "2022-03-26 20:44:39.417537 Epoch 50, Training Loss 0.40456236533039364\n",
      "2022-03-26 20:44:39.435738 Epoch 50, Training Loss 0.4057660680978804\n",
      "2022-03-26 20:44:39.455602 Epoch 50, Training Loss 0.4066810139533504\n",
      "2022-03-26 20:44:39.474638 Epoch 50, Training Loss 0.40750937278160965\n",
      "2022-03-26 20:44:39.493668 Epoch 50, Training Loss 0.4085490075142487\n",
      "2022-03-26 20:44:39.512740 Epoch 50, Training Loss 0.40977929364842225\n",
      "2022-03-26 20:44:39.532654 Epoch 50, Training Loss 0.41086734717002\n",
      "2022-03-26 20:44:39.551767 Epoch 50, Training Loss 0.41214211208893514\n",
      "2022-03-26 20:44:39.570950 Epoch 50, Training Loss 0.4133688020508003\n",
      "2022-03-26 20:44:39.590689 Epoch 50, Training Loss 0.41462218262198025\n",
      "2022-03-26 20:44:39.609820 Epoch 50, Training Loss 0.4153892296125822\n",
      "2022-03-26 20:44:39.628789 Epoch 50, Training Loss 0.4165314420333604\n",
      "2022-03-26 20:44:39.647843 Epoch 50, Training Loss 0.4175140377124557\n",
      "2022-03-26 20:44:39.666888 Epoch 50, Training Loss 0.4186909926288268\n",
      "2022-03-26 20:44:39.686933 Epoch 50, Training Loss 0.41980646062842414\n",
      "2022-03-26 20:44:39.705894 Epoch 50, Training Loss 0.4210268246472034\n",
      "2022-03-26 20:44:39.725758 Epoch 50, Training Loss 0.42203764922326176\n",
      "2022-03-26 20:44:39.745662 Epoch 50, Training Loss 0.4230342069474023\n",
      "2022-03-26 20:44:39.764696 Epoch 50, Training Loss 0.4240496624689883\n",
      "2022-03-26 20:44:39.784743 Epoch 50, Training Loss 0.42514987267039317\n",
      "2022-03-26 20:44:39.803638 Epoch 50, Training Loss 0.4262821957507097\n",
      "2022-03-26 20:44:39.823656 Epoch 50, Training Loss 0.4273554938452323\n",
      "2022-03-26 20:44:39.842957 Epoch 50, Training Loss 0.42829413506228603\n",
      "2022-03-26 20:44:39.861833 Epoch 50, Training Loss 0.42955535059542305\n",
      "2022-03-26 20:44:39.880875 Epoch 50, Training Loss 0.43042885186269764\n",
      "2022-03-26 20:44:39.900894 Epoch 50, Training Loss 0.43147860238771607\n",
      "2022-03-26 20:44:39.920551 Epoch 50, Training Loss 0.4326079175676531\n",
      "2022-03-26 20:44:39.939195 Epoch 50, Training Loss 0.4334807471774728\n",
      "2022-03-26 20:44:39.959107 Epoch 50, Training Loss 0.43466854846233605\n",
      "2022-03-26 20:44:39.977913 Epoch 50, Training Loss 0.4359482798506232\n",
      "2022-03-26 20:44:39.996810 Epoch 50, Training Loss 0.4370139901671568\n",
      "2022-03-26 20:44:40.017773 Epoch 50, Training Loss 0.43792710253192335\n",
      "2022-03-26 20:44:40.038384 Epoch 50, Training Loss 0.4388348592440491\n",
      "2022-03-26 20:44:40.057080 Epoch 50, Training Loss 0.4397338995009737\n",
      "2022-03-26 20:44:40.077006 Epoch 50, Training Loss 0.44079582903848585\n",
      "2022-03-26 20:44:40.096719 Epoch 50, Training Loss 0.44174422346569997\n",
      "2022-03-26 20:44:40.116629 Epoch 50, Training Loss 0.4429150272894393\n",
      "2022-03-26 20:44:40.136643 Epoch 50, Training Loss 0.44385992607954516\n",
      "2022-03-26 20:44:40.156475 Epoch 50, Training Loss 0.4449184599816037\n",
      "2022-03-26 20:44:40.176501 Epoch 50, Training Loss 0.4459985737376811\n",
      "2022-03-26 20:44:40.196498 Epoch 50, Training Loss 0.4471770699143105\n",
      "2022-03-26 20:44:40.216441 Epoch 50, Training Loss 0.44822753226513146\n",
      "2022-03-26 20:44:40.235443 Epoch 50, Training Loss 0.44914301037026183\n",
      "2022-03-26 20:44:40.255415 Epoch 50, Training Loss 0.4500796867682196\n",
      "2022-03-26 20:44:40.275444 Epoch 50, Training Loss 0.4508684316788183\n",
      "2022-03-26 20:44:40.294458 Epoch 50, Training Loss 0.4520781009322237\n",
      "2022-03-26 20:44:40.312650 Epoch 50, Training Loss 0.45331519967912104\n",
      "2022-03-26 20:44:40.332655 Epoch 50, Training Loss 0.4543907877505588\n",
      "2022-03-26 20:44:40.352613 Epoch 50, Training Loss 0.45538390319213234\n",
      "2022-03-26 20:44:40.373643 Epoch 50, Training Loss 0.4562423796681187\n",
      "2022-03-26 20:44:40.393661 Epoch 50, Training Loss 0.4573246423164597\n",
      "2022-03-26 20:44:40.413504 Epoch 50, Training Loss 0.4582226168545311\n",
      "2022-03-26 20:44:40.433536 Epoch 50, Training Loss 0.4593707729712167\n",
      "2022-03-26 20:44:40.453569 Epoch 50, Training Loss 0.4604454274144014\n",
      "2022-03-26 20:44:40.474495 Epoch 50, Training Loss 0.4619396326258359\n",
      "2022-03-26 20:44:40.495510 Epoch 50, Training Loss 0.46307169960435396\n",
      "2022-03-26 20:44:40.515400 Epoch 50, Training Loss 0.46433626999483085\n",
      "2022-03-26 20:44:40.535429 Epoch 50, Training Loss 0.4652787563974595\n",
      "2022-03-26 20:44:40.555513 Epoch 50, Training Loss 0.46641692469644425\n",
      "2022-03-26 20:44:40.575431 Epoch 50, Training Loss 0.4672709434767208\n",
      "2022-03-26 20:44:40.595550 Epoch 50, Training Loss 0.46855093005215726\n",
      "2022-03-26 20:44:40.615521 Epoch 50, Training Loss 0.4697146216393127\n",
      "2022-03-26 20:44:40.636535 Epoch 50, Training Loss 0.4706969036699256\n",
      "2022-03-26 20:44:40.657630 Epoch 50, Training Loss 0.4717631838891817\n",
      "2022-03-26 20:44:40.682627 Epoch 50, Training Loss 0.47265869077971523\n",
      "2022-03-26 20:44:40.704629 Epoch 50, Training Loss 0.4739064698100395\n",
      "2022-03-26 20:44:40.731634 Epoch 50, Training Loss 0.47488782895952847\n",
      "2022-03-26 20:44:40.753478 Epoch 50, Training Loss 0.47596703355422104\n",
      "2022-03-26 20:44:40.773413 Epoch 50, Training Loss 0.47679873306275633\n",
      "2022-03-26 20:44:40.794673 Epoch 50, Training Loss 0.4781481979219505\n",
      "2022-03-26 20:44:40.815633 Epoch 50, Training Loss 0.47929092460428663\n",
      "2022-03-26 20:44:40.834521 Epoch 50, Training Loss 0.4802314784292065\n",
      "2022-03-26 20:44:40.854104 Epoch 50, Training Loss 0.4813370558688098\n",
      "2022-03-26 20:44:40.873010 Epoch 50, Training Loss 0.4824723911178691\n",
      "2022-03-26 20:44:40.893051 Epoch 50, Training Loss 0.48346993170888225\n",
      "2022-03-26 20:44:40.911409 Epoch 50, Training Loss 0.4844342346310311\n",
      "2022-03-26 20:44:40.930417 Epoch 50, Training Loss 0.48520571355472136\n",
      "2022-03-26 20:44:40.950390 Epoch 50, Training Loss 0.486355632657895\n",
      "2022-03-26 20:44:40.970419 Epoch 50, Training Loss 0.48737511881019757\n",
      "2022-03-26 20:44:40.989769 Epoch 50, Training Loss 0.4886291103670969\n",
      "2022-03-26 20:44:41.009690 Epoch 50, Training Loss 0.4895985860120305\n",
      "2022-03-26 20:44:41.028970 Epoch 50, Training Loss 0.49054697030188177\n",
      "2022-03-26 20:44:41.047899 Epoch 50, Training Loss 0.4914899806461066\n",
      "2022-03-26 20:44:41.066887 Epoch 50, Training Loss 0.4928585182294211\n",
      "2022-03-26 20:44:41.086794 Epoch 50, Training Loss 0.4939368100803527\n",
      "2022-03-26 20:44:41.105737 Epoch 50, Training Loss 0.49504532468745777\n",
      "2022-03-26 20:44:41.125043 Epoch 50, Training Loss 0.4959533073179557\n",
      "2022-03-26 20:44:41.144061 Epoch 50, Training Loss 0.4967967218831372\n",
      "2022-03-26 20:44:41.163063 Epoch 50, Training Loss 0.49786181298210797\n",
      "2022-03-26 20:44:41.182111 Epoch 50, Training Loss 0.49880341697684333\n",
      "2022-03-26 20:44:41.201188 Epoch 50, Training Loss 0.4997543970413525\n",
      "2022-03-26 20:44:41.221594 Epoch 50, Training Loss 0.5008067171973036\n",
      "2022-03-26 20:44:41.241202 Epoch 50, Training Loss 0.501992041383253\n",
      "2022-03-26 20:44:41.260303 Epoch 50, Training Loss 0.5028029147087766\n",
      "2022-03-26 20:44:41.279371 Epoch 50, Training Loss 0.5041333542531713\n",
      "2022-03-26 20:44:41.298459 Epoch 50, Training Loss 0.5051730620053113\n",
      "2022-03-26 20:44:41.317485 Epoch 50, Training Loss 0.5064008044236151\n",
      "2022-03-26 20:44:41.337538 Epoch 50, Training Loss 0.5075624975783136\n",
      "2022-03-26 20:44:41.356546 Epoch 50, Training Loss 0.5085870637689405\n",
      "2022-03-26 20:44:41.375909 Epoch 50, Training Loss 0.5095222547764668\n",
      "2022-03-26 20:44:41.394911 Epoch 50, Training Loss 0.5107727676172695\n",
      "2022-03-26 20:44:41.414774 Epoch 50, Training Loss 0.5119316438808466\n",
      "2022-03-26 20:44:41.434726 Epoch 50, Training Loss 0.5129967567027377\n",
      "2022-03-26 20:44:41.453714 Epoch 50, Training Loss 0.5142561120679007\n",
      "2022-03-26 20:44:41.472661 Epoch 50, Training Loss 0.515134117449336\n",
      "2022-03-26 20:44:41.492677 Epoch 50, Training Loss 0.5161292707295064\n",
      "2022-03-26 20:44:41.511707 Epoch 50, Training Loss 0.5172344586809577\n",
      "2022-03-26 20:44:41.531985 Epoch 50, Training Loss 0.518724610624106\n",
      "2022-03-26 20:44:41.550955 Epoch 50, Training Loss 0.5198422926847283\n",
      "2022-03-26 20:44:41.570578 Epoch 50, Training Loss 0.5209969097314893\n",
      "2022-03-26 20:44:41.590575 Epoch 50, Training Loss 0.5219935534326622\n",
      "2022-03-26 20:44:41.609583 Epoch 50, Training Loss 0.5233175759501469\n",
      "2022-03-26 20:44:41.630596 Epoch 50, Training Loss 0.5247462201682503\n",
      "2022-03-26 20:44:41.651621 Epoch 50, Training Loss 0.5256632072160311\n",
      "2022-03-26 20:44:41.670649 Epoch 50, Training Loss 0.52662236843725\n",
      "2022-03-26 20:44:41.690654 Epoch 50, Training Loss 0.5278466569874293\n",
      "2022-03-26 20:44:41.709707 Epoch 50, Training Loss 0.5288722712899108\n",
      "2022-03-26 20:44:41.729155 Epoch 50, Training Loss 0.53003786046944\n",
      "2022-03-26 20:44:41.748272 Epoch 50, Training Loss 0.5310262167621451\n",
      "2022-03-26 20:44:41.767748 Epoch 50, Training Loss 0.5322174193227992\n",
      "2022-03-26 20:44:41.787664 Epoch 50, Training Loss 0.5331960017495143\n",
      "2022-03-26 20:44:41.806690 Epoch 50, Training Loss 0.5339468109714406\n",
      "2022-03-26 20:44:41.826644 Epoch 50, Training Loss 0.5351417547525348\n",
      "2022-03-26 20:44:41.846945 Epoch 50, Training Loss 0.536188770704867\n",
      "2022-03-26 20:44:41.865750 Epoch 50, Training Loss 0.5373261569787169\n",
      "2022-03-26 20:44:41.885821 Epoch 50, Training Loss 0.538413951425906\n",
      "2022-03-26 20:44:41.906261 Epoch 50, Training Loss 0.5395582329357982\n",
      "2022-03-26 20:44:41.926275 Epoch 50, Training Loss 0.5408630924837668\n",
      "2022-03-26 20:44:41.945276 Epoch 50, Training Loss 0.5420401172183663\n",
      "2022-03-26 20:44:41.965277 Epoch 50, Training Loss 0.5430567024080345\n",
      "2022-03-26 20:44:41.983558 Epoch 50, Training Loss 0.5441558074844463\n",
      "2022-03-26 20:44:42.003672 Epoch 50, Training Loss 0.5453692880814033\n",
      "2022-03-26 20:44:42.022615 Epoch 50, Training Loss 0.5464603749611189\n",
      "2022-03-26 20:44:42.042629 Epoch 50, Training Loss 0.5474700331306823\n",
      "2022-03-26 20:44:42.061152 Epoch 50, Training Loss 0.5484201459552321\n",
      "2022-03-26 20:44:42.081506 Epoch 50, Training Loss 0.5494633860828931\n",
      "2022-03-26 20:44:42.100529 Epoch 50, Training Loss 0.5505260295803894\n",
      "2022-03-26 20:44:42.120548 Epoch 50, Training Loss 0.5513204428012414\n",
      "2022-03-26 20:44:42.139318 Epoch 50, Training Loss 0.5522193600378378\n",
      "2022-03-26 20:44:42.159598 Epoch 50, Training Loss 0.5534965965677711\n",
      "2022-03-26 20:44:42.178827 Epoch 50, Training Loss 0.5548502541792667\n",
      "2022-03-26 20:44:42.197800 Epoch 50, Training Loss 0.5557129506945915\n",
      "2022-03-26 20:44:42.216705 Epoch 50, Training Loss 0.556759381347605\n",
      "2022-03-26 20:44:42.236628 Epoch 50, Training Loss 0.5580641058323633\n",
      "2022-03-26 20:44:42.257584 Epoch 50, Training Loss 0.5591091086416293\n",
      "2022-03-26 20:44:42.277475 Epoch 50, Training Loss 0.5599428996481859\n",
      "2022-03-26 20:44:42.296417 Epoch 50, Training Loss 0.5611847929866113\n",
      "2022-03-26 20:44:42.317445 Epoch 50, Training Loss 0.5621661990118758\n",
      "2022-03-26 20:44:42.337475 Epoch 50, Training Loss 0.5634082873992603\n",
      "2022-03-26 20:44:42.356637 Epoch 50, Training Loss 0.5643816227117158\n",
      "2022-03-26 20:44:42.376639 Epoch 50, Training Loss 0.5653284281644675\n",
      "2022-03-26 20:44:42.396655 Epoch 50, Training Loss 0.5666161080074432\n",
      "2022-03-26 20:44:42.416510 Epoch 50, Training Loss 0.567904829788391\n",
      "2022-03-26 20:44:42.437622 Epoch 50, Training Loss 0.5691311197436374\n",
      "2022-03-26 20:44:42.457611 Epoch 50, Training Loss 0.5701720734767597\n",
      "2022-03-26 20:44:42.477508 Epoch 50, Training Loss 0.5711871738095418\n",
      "2022-03-26 20:44:42.497523 Epoch 50, Training Loss 0.5723111552121999\n",
      "2022-03-26 20:44:42.517292 Epoch 50, Training Loss 0.5733001613632187\n",
      "2022-03-26 20:44:42.538294 Epoch 50, Training Loss 0.5746364263088807\n",
      "2022-03-26 20:44:42.558434 Epoch 50, Training Loss 0.575583909897853\n",
      "2022-03-26 20:44:42.577532 Epoch 50, Training Loss 0.5767646586269979\n",
      "2022-03-26 20:44:42.598558 Epoch 50, Training Loss 0.5778217113307674\n",
      "2022-03-26 20:44:42.618582 Epoch 50, Training Loss 0.5790268420182225\n",
      "2022-03-26 20:44:42.638593 Epoch 50, Training Loss 0.5798959134865904\n",
      "2022-03-26 20:44:42.658502 Epoch 50, Training Loss 0.5809174681182407\n",
      "2022-03-26 20:44:42.677553 Epoch 50, Training Loss 0.5818757422820992\n",
      "2022-03-26 20:44:42.698209 Epoch 50, Training Loss 0.5829264035691386\n",
      "2022-03-26 20:44:42.717176 Epoch 50, Training Loss 0.5841092622417319\n",
      "2022-03-26 20:44:42.737365 Epoch 50, Training Loss 0.5853454898614103\n",
      "2022-03-26 20:44:42.757500 Epoch 50, Training Loss 0.5863165121020564\n",
      "2022-03-26 20:44:42.777530 Epoch 50, Training Loss 0.5871474161706007\n",
      "2022-03-26 20:44:42.796557 Epoch 50, Training Loss 0.5882386329305141\n",
      "2022-03-26 20:44:42.816106 Epoch 50, Training Loss 0.5893445313357941\n",
      "2022-03-26 20:44:42.835901 Epoch 50, Training Loss 0.5905664819661919\n",
      "2022-03-26 20:44:42.854801 Epoch 50, Training Loss 0.5917532685238992\n",
      "2022-03-26 20:44:42.874636 Epoch 50, Training Loss 0.59265777731643\n",
      "2022-03-26 20:44:42.894660 Epoch 50, Training Loss 0.5938489184431408\n",
      "2022-03-26 20:44:42.915664 Epoch 50, Training Loss 0.5949400884034993\n",
      "2022-03-26 20:44:42.936601 Epoch 50, Training Loss 0.5959297332083783\n",
      "2022-03-26 20:44:42.956491 Epoch 50, Training Loss 0.5971400660398366\n",
      "2022-03-26 20:44:42.976436 Epoch 50, Training Loss 0.597969941966369\n",
      "2022-03-26 20:44:42.996466 Epoch 50, Training Loss 0.5988097599019175\n",
      "2022-03-26 20:44:43.016291 Epoch 50, Training Loss 0.5996364328791114\n",
      "2022-03-26 20:44:43.037275 Epoch 50, Training Loss 0.6007323299756135\n",
      "2022-03-26 20:44:43.057796 Epoch 50, Training Loss 0.6017145373860894\n",
      "2022-03-26 20:44:43.077672 Epoch 50, Training Loss 0.6028129621920988\n",
      "2022-03-26 20:44:43.097695 Epoch 50, Training Loss 0.6038312650931156\n",
      "2022-03-26 20:44:43.117716 Epoch 50, Training Loss 0.6048099652809256\n",
      "2022-03-26 20:44:43.138486 Epoch 50, Training Loss 0.6060053396331685\n",
      "2022-03-26 20:44:43.158509 Epoch 50, Training Loss 0.6071923947166604\n",
      "2022-03-26 20:44:43.178519 Epoch 50, Training Loss 0.6081881050182425\n",
      "2022-03-26 20:44:43.199548 Epoch 50, Training Loss 0.6093613232111992\n",
      "2022-03-26 20:44:43.220492 Epoch 50, Training Loss 0.6104186873530488\n",
      "2022-03-26 20:44:43.240462 Epoch 50, Training Loss 0.611374674893706\n",
      "2022-03-26 20:44:43.260315 Epoch 50, Training Loss 0.6125371112771656\n",
      "2022-03-26 20:44:43.280247 Epoch 50, Training Loss 0.6136088165099663\n",
      "2022-03-26 20:44:43.301768 Epoch 50, Training Loss 0.6147338962539688\n",
      "2022-03-26 20:44:43.322176 Epoch 50, Training Loss 0.6158916217363094\n",
      "2022-03-26 20:44:43.342479 Epoch 50, Training Loss 0.6169499944314323\n",
      "2022-03-26 20:44:43.363617 Epoch 50, Training Loss 0.6184013878064387\n",
      "2022-03-26 20:44:43.383639 Epoch 50, Training Loss 0.6195258375857492\n",
      "2022-03-26 20:44:43.403543 Epoch 50, Training Loss 0.6204485060537562\n",
      "2022-03-26 20:44:43.423560 Epoch 50, Training Loss 0.6217574966152001\n",
      "2022-03-26 20:44:43.443495 Epoch 50, Training Loss 0.622654686689072\n",
      "2022-03-26 20:44:43.463518 Epoch 50, Training Loss 0.6234903215523571\n",
      "2022-03-26 20:44:43.483547 Epoch 50, Training Loss 0.6247570535639668\n",
      "2022-03-26 20:44:43.510540 Epoch 50, Training Loss 0.6257004278623844\n",
      "2022-03-26 20:44:43.538590 Epoch 50, Training Loss 0.6268905408470832\n",
      "2022-03-26 20:44:43.566511 Epoch 50, Training Loss 0.6279945142967317\n",
      "2022-03-26 20:44:43.593535 Epoch 50, Training Loss 0.6288208369632511\n",
      "2022-03-26 20:44:43.619647 Epoch 50, Training Loss 0.6298876652860885\n",
      "2022-03-26 20:44:43.646714 Epoch 50, Training Loss 0.6309232214451446\n",
      "2022-03-26 20:44:43.673180 Epoch 50, Training Loss 0.6319698223753658\n",
      "2022-03-26 20:44:43.699678 Epoch 50, Training Loss 0.6331021484282925\n",
      "2022-03-26 20:44:43.725631 Epoch 50, Training Loss 0.6341147946808344\n",
      "2022-03-26 20:44:43.751696 Epoch 50, Training Loss 0.6353185793856526\n",
      "2022-03-26 20:44:43.777524 Epoch 50, Training Loss 0.6364667967076192\n",
      "2022-03-26 20:44:43.804485 Epoch 50, Training Loss 0.6376253495664548\n",
      "2022-03-26 20:44:43.830452 Epoch 50, Training Loss 0.6390006131756946\n",
      "2022-03-26 20:44:43.857443 Epoch 50, Training Loss 0.6400456360310239\n",
      "2022-03-26 20:44:43.883474 Epoch 50, Training Loss 0.6410177414450804\n",
      "2022-03-26 20:44:43.909028 Epoch 50, Training Loss 0.6421367471556529\n",
      "2022-03-26 20:44:43.929106 Epoch 50, Training Loss 0.6431870883154442\n",
      "2022-03-26 20:44:43.948186 Epoch 50, Training Loss 0.6441574471304788\n",
      "2022-03-26 20:44:43.966207 Epoch 50, Training Loss 0.6452098912976282\n",
      "2022-03-26 20:44:43.985203 Epoch 50, Training Loss 0.6465261670405907\n",
      "2022-03-26 20:44:44.004619 Epoch 50, Training Loss 0.6472798438404527\n",
      "2022-03-26 20:44:44.022669 Epoch 50, Training Loss 0.6484869854605716\n",
      "2022-03-26 20:44:44.041685 Epoch 50, Training Loss 0.649497045923377\n",
      "2022-03-26 20:44:44.059217 Epoch 50, Training Loss 0.6505937245876893\n",
      "2022-03-26 20:44:44.078283 Epoch 50, Training Loss 0.6513675339615254\n",
      "2022-03-26 20:44:44.097332 Epoch 50, Training Loss 0.6523773864178402\n",
      "2022-03-26 20:44:44.115354 Epoch 50, Training Loss 0.653553917318049\n",
      "2022-03-26 20:44:44.134429 Epoch 50, Training Loss 0.65454972015165\n",
      "2022-03-26 20:44:44.151427 Epoch 50, Training Loss 0.6555910649735605\n",
      "2022-03-26 20:44:44.170008 Epoch 50, Training Loss 0.6567203094206198\n",
      "2022-03-26 20:44:44.189035 Epoch 50, Training Loss 0.6574750419544138\n",
      "2022-03-26 20:44:44.208087 Epoch 50, Training Loss 0.6582873765083835\n",
      "2022-03-26 20:44:44.226119 Epoch 50, Training Loss 0.6591895304601211\n",
      "2022-03-26 20:44:44.244466 Epoch 50, Training Loss 0.660307725867652\n",
      "2022-03-26 20:44:44.262083 Epoch 50, Training Loss 0.6616801891256782\n",
      "2022-03-26 20:44:44.281129 Epoch 50, Training Loss 0.6625205928940907\n",
      "2022-03-26 20:44:44.299182 Epoch 50, Training Loss 0.6635230784602177\n",
      "2022-03-26 20:44:44.318238 Epoch 50, Training Loss 0.6646707030894506\n",
      "2022-03-26 20:44:44.335731 Epoch 50, Training Loss 0.6656582557103213\n",
      "2022-03-26 20:44:44.354752 Epoch 50, Training Loss 0.6666807412262767\n",
      "2022-03-26 20:44:44.372347 Epoch 50, Training Loss 0.667534211902972\n",
      "2022-03-26 20:44:44.391752 Epoch 50, Training Loss 0.6684421994878204\n",
      "2022-03-26 20:44:44.411145 Epoch 50, Training Loss 0.6693075865964451\n",
      "2022-03-26 20:44:44.429058 Epoch 50, Training Loss 0.6703442048157573\n",
      "2022-03-26 20:44:44.448090 Epoch 50, Training Loss 0.6713515884050018\n",
      "2022-03-26 20:44:44.466514 Epoch 50, Training Loss 0.6722196671740174\n",
      "2022-03-26 20:44:44.485484 Epoch 50, Training Loss 0.6731371556020453\n",
      "2022-03-26 20:44:44.504530 Epoch 50, Training Loss 0.6742809775768949\n",
      "2022-03-26 20:44:44.522544 Epoch 50, Training Loss 0.6754071621410073\n",
      "2022-03-26 20:44:44.541657 Epoch 50, Training Loss 0.676551152487545\n",
      "2022-03-26 20:44:44.559704 Epoch 50, Training Loss 0.6778547214653791\n",
      "2022-03-26 20:44:44.577732 Epoch 50, Training Loss 0.6788933699774315\n",
      "2022-03-26 20:44:44.596094 Epoch 50, Training Loss 0.6799903885101723\n",
      "2022-03-26 20:44:44.615051 Epoch 50, Training Loss 0.6813670135748661\n",
      "2022-03-26 20:44:44.632933 Epoch 50, Training Loss 0.6827246782648594\n",
      "2022-03-26 20:44:44.651986 Epoch 50, Training Loss 0.6839415917692282\n",
      "2022-03-26 20:44:44.670088 Epoch 50, Training Loss 0.684907602570246\n",
      "2022-03-26 20:44:44.689212 Epoch 50, Training Loss 0.6862394977789705\n",
      "2022-03-26 20:44:44.707264 Epoch 50, Training Loss 0.6871512042515723\n",
      "2022-03-26 20:44:44.726294 Epoch 50, Training Loss 0.6880080425906974\n",
      "2022-03-26 20:44:44.744284 Epoch 50, Training Loss 0.689067376277331\n",
      "2022-03-26 20:44:44.763333 Epoch 50, Training Loss 0.690351184257461\n",
      "2022-03-26 20:44:44.781601 Epoch 50, Training Loss 0.6913219879731498\n",
      "2022-03-26 20:44:44.801615 Epoch 50, Training Loss 0.6926013506053353\n",
      "2022-03-26 20:44:44.820537 Epoch 50, Training Loss 0.6936066338549489\n",
      "2022-03-26 20:44:44.839551 Epoch 50, Training Loss 0.6948985779453116\n",
      "2022-03-26 20:44:44.857573 Epoch 50, Training Loss 0.6960109091552017\n",
      "2022-03-26 20:44:44.876600 Epoch 50, Training Loss 0.6971917626116891\n",
      "2022-03-26 20:44:44.894623 Epoch 50, Training Loss 0.6981565667616437\n",
      "2022-03-26 20:44:44.912662 Epoch 50, Training Loss 0.699048664068322\n",
      "2022-03-26 20:44:44.931677 Epoch 50, Training Loss 0.6999506432458263\n",
      "2022-03-26 20:44:44.950692 Epoch 50, Training Loss 0.7007731100558625\n",
      "2022-03-26 20:44:44.970700 Epoch 50, Training Loss 0.7017962765282072\n",
      "2022-03-26 20:44:44.988729 Epoch 50, Training Loss 0.7028548962548565\n",
      "2022-03-26 20:44:45.007679 Epoch 50, Training Loss 0.7037185198434478\n",
      "2022-03-26 20:44:45.026689 Epoch 50, Training Loss 0.7049895161786652\n",
      "2022-03-26 20:44:45.044753 Epoch 50, Training Loss 0.7061272496381379\n",
      "2022-03-26 20:44:45.064761 Epoch 50, Training Loss 0.7072753900533442\n",
      "2022-03-26 20:44:45.083790 Epoch 50, Training Loss 0.7086328102270966\n",
      "2022-03-26 20:44:45.102749 Epoch 50, Training Loss 0.7096491401533946\n",
      "2022-03-26 20:44:45.121318 Epoch 50, Training Loss 0.7107721034751828\n",
      "2022-03-26 20:44:45.139762 Epoch 50, Training Loss 0.7119061748313782\n",
      "2022-03-26 20:44:45.158637 Epoch 50, Training Loss 0.7129972561469773\n",
      "2022-03-26 20:44:45.177652 Epoch 50, Training Loss 0.714069931670223\n",
      "2022-03-26 20:44:45.195459 Epoch 50, Training Loss 0.7150432036432159\n",
      "2022-03-26 20:44:45.215541 Epoch 50, Training Loss 0.7158967801143447\n",
      "2022-03-26 20:44:45.234539 Epoch 50, Training Loss 0.7171397027380936\n",
      "2022-03-26 20:44:45.253948 Epoch 50, Training Loss 0.7181327020954293\n",
      "2022-03-26 20:44:45.272914 Epoch 50, Training Loss 0.7192837985046684\n",
      "2022-03-26 20:44:45.290944 Epoch 50, Training Loss 0.7205886800042198\n",
      "2022-03-26 20:44:45.309885 Epoch 50, Training Loss 0.7220193637377771\n",
      "2022-03-26 20:44:45.329791 Epoch 50, Training Loss 0.7234515182273772\n",
      "2022-03-26 20:44:45.348826 Epoch 50, Training Loss 0.7243204590533395\n",
      "2022-03-26 20:44:45.368734 Epoch 50, Training Loss 0.7252095768137661\n",
      "2022-03-26 20:44:45.387773 Epoch 50, Training Loss 0.7262565231765322\n",
      "2022-03-26 20:44:45.406606 Epoch 50, Training Loss 0.7274851564632352\n",
      "2022-03-26 20:44:45.425633 Epoch 50, Training Loss 0.7282187767574549\n",
      "2022-03-26 20:44:45.444648 Epoch 50, Training Loss 0.7295468133657484\n",
      "2022-03-26 20:44:45.463664 Epoch 50, Training Loss 0.7307021841597374\n",
      "2022-03-26 20:44:45.482696 Epoch 50, Training Loss 0.7316095519172566\n",
      "2022-03-26 20:44:45.500733 Epoch 50, Training Loss 0.7326213587504213\n",
      "2022-03-26 20:44:45.519769 Epoch 50, Training Loss 0.733572604672988\n",
      "2022-03-26 20:44:45.537802 Epoch 50, Training Loss 0.7346416309742672\n",
      "2022-03-26 20:44:45.556612 Epoch 50, Training Loss 0.7352951536017001\n",
      "2022-03-26 20:44:45.574624 Epoch 50, Training Loss 0.7365950670312432\n",
      "2022-03-26 20:44:45.592665 Epoch 50, Training Loss 0.7375327724858624\n",
      "2022-03-26 20:44:45.611663 Epoch 50, Training Loss 0.7386078713723766\n",
      "2022-03-26 20:44:45.630684 Epoch 50, Training Loss 0.7396349677115756\n",
      "2022-03-26 20:44:45.648728 Epoch 50, Training Loss 0.7406941637434923\n",
      "2022-03-26 20:44:45.666317 Epoch 50, Training Loss 0.74168486298655\n",
      "2022-03-26 20:44:45.685795 Epoch 50, Training Loss 0.7426091486307056\n",
      "2022-03-26 20:44:45.704727 Epoch 50, Training Loss 0.7434307003341367\n",
      "2022-03-26 20:44:45.722709 Epoch 50, Training Loss 0.7446313497736631\n",
      "2022-03-26 20:44:45.747736 Epoch 50, Training Loss 0.7453391934980822\n",
      "2022-03-26 20:44:45.773374 Epoch 50, Training Loss 0.7465282480049011\n",
      "2022-03-26 20:44:45.799336 Epoch 50, Training Loss 0.7476864289063627\n",
      "2022-03-26 20:44:45.825281 Epoch 50, Training Loss 0.7488418971867208\n",
      "2022-03-26 20:44:45.853405 Epoch 50, Training Loss 0.7497013794720325\n",
      "2022-03-26 20:44:45.879491 Epoch 50, Training Loss 0.7506968509739317\n",
      "2022-03-26 20:44:45.905497 Epoch 50, Training Loss 0.7516224807714258\n",
      "2022-03-26 20:44:45.931391 Epoch 50, Training Loss 0.7527945261553425\n",
      "2022-03-26 20:44:45.950416 Epoch 50, Training Loss 0.7538435454182613\n",
      "2022-03-26 20:44:45.968496 Epoch 50, Training Loss 0.7550196303507252\n",
      "2022-03-26 20:44:45.987526 Epoch 50, Training Loss 0.7562985512073083\n",
      "2022-03-26 20:44:46.005530 Epoch 50, Training Loss 0.7571578546786857\n",
      "2022-03-26 20:44:46.025520 Epoch 50, Training Loss 0.7581509615835327\n",
      "2022-03-26 20:44:46.042558 Epoch 50, Training Loss 0.7593297176729993\n",
      "2022-03-26 20:44:46.061584 Epoch 50, Training Loss 0.7604416539068417\n",
      "2022-03-26 20:44:46.081515 Epoch 50, Training Loss 0.7616390759301612\n",
      "2022-03-26 20:44:46.099543 Epoch 50, Training Loss 0.7627525606652354\n",
      "2022-03-26 20:44:46.118557 Epoch 50, Training Loss 0.7636812241257304\n",
      "2022-03-26 20:44:46.136567 Epoch 50, Training Loss 0.7648537066357824\n",
      "2022-03-26 20:44:46.155214 Epoch 50, Training Loss 0.7658703043256574\n",
      "2022-03-26 20:44:46.173249 Epoch 50, Training Loss 0.7671096891622105\n",
      "2022-03-26 20:44:46.192545 Epoch 50, Training Loss 0.768148102228294\n",
      "2022-03-26 20:44:46.210339 Epoch 50, Training Loss 0.7692179413479002\n",
      "2022-03-26 20:44:46.228390 Epoch 50, Training Loss 0.7702031943880384\n",
      "2022-03-26 20:44:46.246400 Epoch 50, Training Loss 0.7711316384851475\n",
      "2022-03-26 20:44:46.265287 Epoch 50, Training Loss 0.7722817750461876\n",
      "2022-03-26 20:44:46.283490 Epoch 50, Training Loss 0.7733386300332711\n",
      "2022-03-26 20:44:46.301539 Epoch 50, Training Loss 0.7741489721000042\n",
      "2022-03-26 20:44:46.319554 Epoch 50, Training Loss 0.7751994685215109\n",
      "2022-03-26 20:44:46.337981 Epoch 50, Training Loss 0.7762158652172064\n",
      "2022-03-26 20:44:46.355879 Epoch 50, Training Loss 0.7775099512637423\n",
      "2022-03-26 20:44:46.374799 Epoch 50, Training Loss 0.7783454618109461\n",
      "2022-03-26 20:44:46.392733 Epoch 50, Training Loss 0.7796731211264115\n",
      "2022-03-26 20:44:46.410476 Epoch 50, Training Loss 0.7810080144030359\n",
      "2022-03-26 20:44:46.429449 Epoch 50, Training Loss 0.7820372791470164\n",
      "2022-03-26 20:44:46.447467 Epoch 50, Training Loss 0.7828326823994937\n",
      "2022-03-26 20:44:46.466488 Epoch 50, Training Loss 0.7840175057387413\n",
      "2022-03-26 20:44:46.484686 Epoch 50, Training Loss 0.7851117249111386\n",
      "2022-03-26 20:44:46.503637 Epoch 50, Training Loss 0.7863086639234173\n",
      "2022-03-26 20:44:46.521980 Epoch 50, Training Loss 0.7874132081142167\n",
      "2022-03-26 20:44:46.539923 Epoch 50, Training Loss 0.7883345906615562\n",
      "2022-03-26 20:44:46.558755 Epoch 50, Training Loss 0.7893803881486053\n",
      "2022-03-26 20:44:46.576795 Epoch 50, Training Loss 0.7902121682987189\n",
      "2022-03-26 20:44:46.595669 Epoch 50, Training Loss 0.791053547235706\n",
      "2022-03-26 20:44:46.613647 Epoch 50, Training Loss 0.7920638513001029\n",
      "2022-03-26 20:44:46.631737 Epoch 50, Training Loss 0.7929582331720215\n",
      "2022-03-26 20:44:46.650788 Epoch 50, Training Loss 0.7940620982357304\n",
      "2022-03-26 20:44:46.669734 Epoch 50, Training Loss 0.7951690273745285\n",
      "2022-03-26 20:44:46.688570 Epoch 50, Training Loss 0.7966240689806316\n",
      "2022-03-26 20:44:46.706323 Epoch 50, Training Loss 0.797758165565903\n",
      "2022-03-26 20:44:46.724302 Epoch 50, Training Loss 0.7989193788346123\n",
      "2022-03-26 20:44:46.742333 Epoch 50, Training Loss 0.800189283002368\n",
      "2022-03-26 20:44:46.759305 Epoch 50, Training Loss 0.8011912248094978\n",
      "2022-03-26 20:44:46.777747 Epoch 50, Training Loss 0.8022508259548251\n",
      "2022-03-26 20:44:46.795771 Epoch 50, Training Loss 0.8033164444253268\n",
      "2022-03-26 20:44:46.814682 Epoch 50, Training Loss 0.8043363536791424\n",
      "2022-03-26 20:44:46.833705 Epoch 50, Training Loss 0.8051998034843704\n",
      "2022-03-26 20:44:46.851734 Epoch 50, Training Loss 0.8062073668784193\n",
      "2022-03-26 20:44:46.871060 Epoch 50, Training Loss 0.8074751763468813\n",
      "2022-03-26 20:44:46.889677 Epoch 50, Training Loss 0.8085588561680616\n",
      "2022-03-26 20:44:46.907626 Epoch 50, Training Loss 0.8095619255853126\n",
      "2022-03-26 20:44:46.924637 Epoch 50, Training Loss 0.810395560567946\n",
      "2022-03-26 20:44:46.943686 Epoch 50, Training Loss 0.8115219677729375\n",
      "2022-03-26 20:44:46.961520 Epoch 50, Training Loss 0.8124731798534808\n",
      "2022-03-26 20:44:46.978834 Epoch 50, Training Loss 0.8139167372756602\n",
      "2022-03-26 20:44:46.997743 Epoch 50, Training Loss 0.8149878486724156\n",
      "2022-03-26 20:44:47.015768 Epoch 50, Training Loss 0.8161788768399402\n",
      "2022-03-26 20:44:47.034693 Epoch 50, Training Loss 0.8171234945278338\n",
      "2022-03-26 20:44:47.052704 Epoch 50, Training Loss 0.8179906517876994\n",
      "2022-03-26 20:44:47.071745 Epoch 50, Training Loss 0.818949824823138\n",
      "2022-03-26 20:44:47.089793 Epoch 50, Training Loss 0.8199415482447275\n",
      "2022-03-26 20:44:47.108678 Epoch 50, Training Loss 0.8207782569824887\n",
      "2022-03-26 20:44:47.126751 Epoch 50, Training Loss 0.8216638381752517\n",
      "2022-03-26 20:44:47.144765 Epoch 50, Training Loss 0.823082865618379\n",
      "2022-03-26 20:44:47.162773 Epoch 50, Training Loss 0.8241544450869036\n",
      "2022-03-26 20:44:47.180937 Epoch 50, Training Loss 0.8252614698065516\n",
      "2022-03-26 20:44:47.198820 Epoch 50, Training Loss 0.8263893825242586\n",
      "2022-03-26 20:44:47.216871 Epoch 50, Training Loss 0.8276495263933221\n",
      "2022-03-26 20:44:47.234810 Epoch 50, Training Loss 0.8288507708884261\n",
      "2022-03-26 20:44:47.252698 Epoch 50, Training Loss 0.8297042722820931\n",
      "2022-03-26 20:44:47.272725 Epoch 50, Training Loss 0.830856103612029\n",
      "2022-03-26 20:44:47.280707 Epoch 50, Training Loss 0.8314967940911613\n",
      "2022-03-26 20:56:07.584869 Epoch 100, Training Loss 0.001083370898385792\n",
      "2022-03-26 20:56:07.598865 Epoch 100, Training Loss 0.0021253904265820826\n",
      "2022-03-26 20:56:07.612701 Epoch 100, Training Loss 0.0030253403022161224\n",
      "2022-03-26 20:56:07.626609 Epoch 100, Training Loss 0.004136134825094276\n",
      "2022-03-26 20:56:07.641040 Epoch 100, Training Loss 0.004948874873578396\n",
      "2022-03-26 20:56:07.654933 Epoch 100, Training Loss 0.00606197972431817\n",
      "2022-03-26 20:56:07.668955 Epoch 100, Training Loss 0.0070472690455444026\n",
      "2022-03-26 20:56:07.681970 Epoch 100, Training Loss 0.008000306384947599\n",
      "2022-03-26 20:56:07.697063 Epoch 100, Training Loss 0.008722871618197701\n",
      "2022-03-26 20:56:07.711068 Epoch 100, Training Loss 0.009689161920791391\n",
      "2022-03-26 20:56:07.725077 Epoch 100, Training Loss 0.010412418857559828\n",
      "2022-03-26 20:56:07.738195 Epoch 100, Training Loss 0.011316795025945015\n",
      "2022-03-26 20:56:07.753207 Epoch 100, Training Loss 0.012405668835505805\n",
      "2022-03-26 20:56:07.766473 Epoch 100, Training Loss 0.013388608880055225\n",
      "2022-03-26 20:56:07.781478 Epoch 100, Training Loss 0.014149928946629205\n",
      "2022-03-26 20:56:07.796244 Epoch 100, Training Loss 0.014861404972003243\n",
      "2022-03-26 20:56:07.809217 Epoch 100, Training Loss 0.015640740504350197\n",
      "2022-03-26 20:56:07.823618 Epoch 100, Training Loss 0.016437938527377974\n",
      "2022-03-26 20:56:07.838584 Epoch 100, Training Loss 0.017461172378886385\n",
      "2022-03-26 20:56:07.852034 Epoch 100, Training Loss 0.018083208738385564\n",
      "2022-03-26 20:56:07.866117 Epoch 100, Training Loss 0.018934610630850047\n",
      "2022-03-26 20:56:07.880189 Epoch 100, Training Loss 0.01992182513637006\n",
      "2022-03-26 20:56:07.895333 Epoch 100, Training Loss 0.020639676191008\n",
      "2022-03-26 20:56:07.909413 Epoch 100, Training Loss 0.021360230186711186\n",
      "2022-03-26 20:56:07.923452 Epoch 100, Training Loss 0.02240357123067617\n",
      "2022-03-26 20:56:07.937396 Epoch 100, Training Loss 0.023577746375442465\n",
      "2022-03-26 20:56:07.951425 Epoch 100, Training Loss 0.02475746757234149\n",
      "2022-03-26 20:56:07.965751 Epoch 100, Training Loss 0.025669697986539366\n",
      "2022-03-26 20:56:07.979753 Epoch 100, Training Loss 0.026653028357669216\n",
      "2022-03-26 20:56:07.994916 Epoch 100, Training Loss 0.027267740701165652\n",
      "2022-03-26 20:56:08.009430 Epoch 100, Training Loss 0.028279512320333124\n",
      "2022-03-26 20:56:08.023468 Epoch 100, Training Loss 0.029234699237986904\n",
      "2022-03-26 20:56:08.037390 Epoch 100, Training Loss 0.03004941454781291\n",
      "2022-03-26 20:56:08.051418 Epoch 100, Training Loss 0.030806679516802053\n",
      "2022-03-26 20:56:08.065649 Epoch 100, Training Loss 0.03189757127133782\n",
      "2022-03-26 20:56:08.078651 Epoch 100, Training Loss 0.03295125425471674\n",
      "2022-03-26 20:56:08.093052 Epoch 100, Training Loss 0.033575177688123015\n",
      "2022-03-26 20:56:08.108055 Epoch 100, Training Loss 0.03453475759004998\n",
      "2022-03-26 20:56:08.122339 Epoch 100, Training Loss 0.03548627341037516\n",
      "2022-03-26 20:56:08.135310 Epoch 100, Training Loss 0.03648482232599917\n",
      "2022-03-26 20:56:08.150330 Epoch 100, Training Loss 0.037396872020743384\n",
      "2022-03-26 20:56:08.163445 Epoch 100, Training Loss 0.03843306683365951\n",
      "2022-03-26 20:56:08.178353 Epoch 100, Training Loss 0.03916346779107438\n",
      "2022-03-26 20:56:08.193180 Epoch 100, Training Loss 0.03987226610445915\n",
      "2022-03-26 20:56:08.207196 Epoch 100, Training Loss 0.04078837592735925\n",
      "2022-03-26 20:56:08.221209 Epoch 100, Training Loss 0.041961131460221526\n",
      "2022-03-26 20:56:08.235363 Epoch 100, Training Loss 0.04258465019943159\n",
      "2022-03-26 20:56:08.248415 Epoch 100, Training Loss 0.043482393864780436\n",
      "2022-03-26 20:56:08.263436 Epoch 100, Training Loss 0.044349250738578073\n",
      "2022-03-26 20:56:08.277429 Epoch 100, Training Loss 0.045297139212298575\n",
      "2022-03-26 20:56:08.291228 Epoch 100, Training Loss 0.045920147196106285\n",
      "2022-03-26 20:56:08.305276 Epoch 100, Training Loss 0.04672880722281268\n",
      "2022-03-26 20:56:08.319262 Epoch 100, Training Loss 0.04779551862298375\n",
      "2022-03-26 20:56:08.333395 Epoch 100, Training Loss 0.04858069190436312\n",
      "2022-03-26 20:56:08.348336 Epoch 100, Training Loss 0.049490769767700254\n",
      "2022-03-26 20:56:08.361323 Epoch 100, Training Loss 0.050323736522813585\n",
      "2022-03-26 20:56:08.376369 Epoch 100, Training Loss 0.05107851067314977\n",
      "2022-03-26 20:56:08.390226 Epoch 100, Training Loss 0.05198045227381275\n",
      "2022-03-26 20:56:08.405555 Epoch 100, Training Loss 0.0529393043435748\n",
      "2022-03-26 20:56:08.418789 Epoch 100, Training Loss 0.053905594569947715\n",
      "2022-03-26 20:56:08.433834 Epoch 100, Training Loss 0.054603380048671345\n",
      "2022-03-26 20:56:08.447408 Epoch 100, Training Loss 0.05556732938265252\n",
      "2022-03-26 20:56:08.461535 Epoch 100, Training Loss 0.0567485453451381\n",
      "2022-03-26 20:56:08.475589 Epoch 100, Training Loss 0.057827078358596545\n",
      "2022-03-26 20:56:08.489588 Epoch 100, Training Loss 0.05863858492630522\n",
      "2022-03-26 20:56:08.503405 Epoch 100, Training Loss 0.059396708446085605\n",
      "2022-03-26 20:56:08.517406 Epoch 100, Training Loss 0.06029943649268821\n",
      "2022-03-26 20:56:08.531435 Epoch 100, Training Loss 0.061090223662688604\n",
      "2022-03-26 20:56:08.545652 Epoch 100, Training Loss 0.06191078922175385\n",
      "2022-03-26 20:56:08.559655 Epoch 100, Training Loss 0.06314599929410783\n",
      "2022-03-26 20:56:08.574566 Epoch 100, Training Loss 0.06399907766248258\n",
      "2022-03-26 20:56:08.588580 Epoch 100, Training Loss 0.06483931458362228\n",
      "2022-03-26 20:56:08.603594 Epoch 100, Training Loss 0.06581976861142746\n",
      "2022-03-26 20:56:08.617502 Epoch 100, Training Loss 0.06666144389478142\n",
      "2022-03-26 20:56:08.631531 Epoch 100, Training Loss 0.06725864440126492\n",
      "2022-03-26 20:56:08.646037 Epoch 100, Training Loss 0.06807824691085865\n",
      "2022-03-26 20:56:08.660040 Epoch 100, Training Loss 0.06901993673018482\n",
      "2022-03-26 20:56:08.674275 Epoch 100, Training Loss 0.06986015208084564\n",
      "2022-03-26 20:56:08.689463 Epoch 100, Training Loss 0.07070604298273316\n",
      "2022-03-26 20:56:08.702433 Epoch 100, Training Loss 0.07151491901911128\n",
      "2022-03-26 20:56:08.717373 Epoch 100, Training Loss 0.07222702561894341\n",
      "2022-03-26 20:56:08.730426 Epoch 100, Training Loss 0.07341799357205706\n",
      "2022-03-26 20:56:08.745545 Epoch 100, Training Loss 0.07390302240543659\n",
      "2022-03-26 20:56:08.759548 Epoch 100, Training Loss 0.07494238617322634\n",
      "2022-03-26 20:56:08.774119 Epoch 100, Training Loss 0.0760938162007905\n",
      "2022-03-26 20:56:08.788169 Epoch 100, Training Loss 0.07706389162698975\n",
      "2022-03-26 20:56:08.803071 Epoch 100, Training Loss 0.07786786735362713\n",
      "2022-03-26 20:56:08.817104 Epoch 100, Training Loss 0.07862369220732424\n",
      "2022-03-26 20:56:08.831432 Epoch 100, Training Loss 0.07925613274049881\n",
      "2022-03-26 20:56:08.846675 Epoch 100, Training Loss 0.08021733362961303\n",
      "2022-03-26 20:56:08.860674 Epoch 100, Training Loss 0.08108593329139378\n",
      "2022-03-26 20:56:08.875043 Epoch 100, Training Loss 0.08195439903327571\n",
      "2022-03-26 20:56:08.889046 Epoch 100, Training Loss 0.08283166743605339\n",
      "2022-03-26 20:56:08.903060 Epoch 100, Training Loss 0.08350823427100315\n",
      "2022-03-26 20:56:08.917080 Epoch 100, Training Loss 0.08448874698880383\n",
      "2022-03-26 20:56:08.931435 Epoch 100, Training Loss 0.08533934017886287\n",
      "2022-03-26 20:56:08.950368 Epoch 100, Training Loss 0.08635675899512932\n",
      "2022-03-26 20:56:08.968431 Epoch 100, Training Loss 0.08728129090860372\n",
      "2022-03-26 20:56:08.987411 Epoch 100, Training Loss 0.08808870091462684\n",
      "2022-03-26 20:56:09.006433 Epoch 100, Training Loss 0.08878183052362994\n",
      "2022-03-26 20:56:09.025359 Epoch 100, Training Loss 0.08977269638529824\n",
      "2022-03-26 20:56:09.044740 Epoch 100, Training Loss 0.09079797546881849\n",
      "2022-03-26 20:56:09.063621 Epoch 100, Training Loss 0.09187074748756331\n",
      "2022-03-26 20:56:09.082615 Epoch 100, Training Loss 0.09268532994458133\n",
      "2022-03-26 20:56:09.101315 Epoch 100, Training Loss 0.09346758793382083\n",
      "2022-03-26 20:56:09.119291 Epoch 100, Training Loss 0.09481323702865854\n",
      "2022-03-26 20:56:09.138345 Epoch 100, Training Loss 0.0958281846150108\n",
      "2022-03-26 20:56:09.157393 Epoch 100, Training Loss 0.09650070732816711\n",
      "2022-03-26 20:56:09.176275 Epoch 100, Training Loss 0.09708398923544628\n",
      "2022-03-26 20:56:09.195514 Epoch 100, Training Loss 0.09797468834825794\n",
      "2022-03-26 20:56:09.214699 Epoch 100, Training Loss 0.09874857257089346\n",
      "2022-03-26 20:56:09.234728 Epoch 100, Training Loss 0.09953340110571487\n",
      "2022-03-26 20:56:09.254035 Epoch 100, Training Loss 0.10055492029470556\n",
      "2022-03-26 20:56:09.272408 Epoch 100, Training Loss 0.10142978271255103\n",
      "2022-03-26 20:56:09.292177 Epoch 100, Training Loss 0.10260264640269072\n",
      "2022-03-26 20:56:09.311023 Epoch 100, Training Loss 0.10347061388937713\n",
      "2022-03-26 20:56:09.330124 Epoch 100, Training Loss 0.1043786620697402\n",
      "2022-03-26 20:56:09.349114 Epoch 100, Training Loss 0.10538828883634503\n",
      "2022-03-26 20:56:09.368319 Epoch 100, Training Loss 0.10605120643630357\n",
      "2022-03-26 20:56:09.387233 Epoch 100, Training Loss 0.10739570551211267\n",
      "2022-03-26 20:56:09.406237 Epoch 100, Training Loss 0.10868769868865343\n",
      "2022-03-26 20:56:09.424903 Epoch 100, Training Loss 0.10963509530972337\n",
      "2022-03-26 20:56:09.444943 Epoch 100, Training Loss 0.1106978088541104\n",
      "2022-03-26 20:56:09.463964 Epoch 100, Training Loss 0.11161060978079695\n",
      "2022-03-26 20:56:09.483384 Epoch 100, Training Loss 0.11259792162024457\n",
      "2022-03-26 20:56:09.502308 Epoch 100, Training Loss 0.1137369677538762\n",
      "2022-03-26 20:56:09.521415 Epoch 100, Training Loss 0.11473790375168061\n",
      "2022-03-26 20:56:09.539295 Epoch 100, Training Loss 0.11565110727649211\n",
      "2022-03-26 20:56:09.558371 Epoch 100, Training Loss 0.11637312120488842\n",
      "2022-03-26 20:56:09.577063 Epoch 100, Training Loss 0.11728394130611663\n",
      "2022-03-26 20:56:09.597093 Epoch 100, Training Loss 0.1184797031647714\n",
      "2022-03-26 20:56:09.616561 Epoch 100, Training Loss 0.11916523798347434\n",
      "2022-03-26 20:56:09.636059 Epoch 100, Training Loss 0.11999524097003596\n",
      "2022-03-26 20:56:09.655464 Epoch 100, Training Loss 0.12074630423579984\n",
      "2022-03-26 20:56:09.674606 Epoch 100, Training Loss 0.12165755345998212\n",
      "2022-03-26 20:56:09.693634 Epoch 100, Training Loss 0.12244818331030628\n",
      "2022-03-26 20:56:09.712718 Epoch 100, Training Loss 0.12333070179995369\n",
      "2022-03-26 20:56:09.731848 Epoch 100, Training Loss 0.1242403409365193\n",
      "2022-03-26 20:56:09.750583 Epoch 100, Training Loss 0.12500804502640844\n",
      "2022-03-26 20:56:09.769895 Epoch 100, Training Loss 0.12598333845053183\n",
      "2022-03-26 20:56:09.789080 Epoch 100, Training Loss 0.1269344200411111\n",
      "2022-03-26 20:56:09.807414 Epoch 100, Training Loss 0.12799947607852613\n",
      "2022-03-26 20:56:09.826414 Epoch 100, Training Loss 0.12895303697842161\n",
      "2022-03-26 20:56:09.845582 Epoch 100, Training Loss 0.12984865019693398\n",
      "2022-03-26 20:56:09.865639 Epoch 100, Training Loss 0.13066596974192374\n",
      "2022-03-26 20:56:09.884537 Epoch 100, Training Loss 0.13162612876928675\n",
      "2022-03-26 20:56:09.903984 Epoch 100, Training Loss 0.13236650961744206\n",
      "2022-03-26 20:56:09.923345 Epoch 100, Training Loss 0.13330208515877004\n",
      "2022-03-26 20:56:09.942240 Epoch 100, Training Loss 0.1342880916412529\n",
      "2022-03-26 20:56:09.961320 Epoch 100, Training Loss 0.13513987799129829\n",
      "2022-03-26 20:56:09.980335 Epoch 100, Training Loss 0.13608049736608324\n",
      "2022-03-26 20:56:09.998813 Epoch 100, Training Loss 0.13720047313843847\n",
      "2022-03-26 20:56:10.018762 Epoch 100, Training Loss 0.138014960243269\n",
      "2022-03-26 20:56:10.037044 Epoch 100, Training Loss 0.13864776869411663\n",
      "2022-03-26 20:56:10.056416 Epoch 100, Training Loss 0.13925814106488776\n",
      "2022-03-26 20:56:10.076551 Epoch 100, Training Loss 0.14032541489814554\n",
      "2022-03-26 20:56:10.095796 Epoch 100, Training Loss 0.141290804271198\n",
      "2022-03-26 20:56:10.113824 Epoch 100, Training Loss 0.1421454859435406\n",
      "2022-03-26 20:56:10.134091 Epoch 100, Training Loss 0.14301771363791296\n",
      "2022-03-26 20:56:10.152119 Epoch 100, Training Loss 0.14356208598369832\n",
      "2022-03-26 20:56:10.172192 Epoch 100, Training Loss 0.14475261566736508\n",
      "2022-03-26 20:56:10.190291 Epoch 100, Training Loss 0.1459515898505135\n",
      "2022-03-26 20:56:10.209860 Epoch 100, Training Loss 0.14693234251130877\n",
      "2022-03-26 20:56:10.228898 Epoch 100, Training Loss 0.14781850069532615\n",
      "2022-03-26 20:56:10.248896 Epoch 100, Training Loss 0.14868005961560837\n",
      "2022-03-26 20:56:10.268002 Epoch 100, Training Loss 0.14954054557606386\n",
      "2022-03-26 20:56:10.287681 Epoch 100, Training Loss 0.15035129054580504\n",
      "2022-03-26 20:56:10.306608 Epoch 100, Training Loss 0.15118651335958935\n",
      "2022-03-26 20:56:10.326151 Epoch 100, Training Loss 0.15229264629618897\n",
      "2022-03-26 20:56:10.344786 Epoch 100, Training Loss 0.15347152296691904\n",
      "2022-03-26 20:56:10.364802 Epoch 100, Training Loss 0.15440820431922708\n",
      "2022-03-26 20:56:10.383843 Epoch 100, Training Loss 0.1550990889977921\n",
      "2022-03-26 20:56:10.402866 Epoch 100, Training Loss 0.15586692868443705\n",
      "2022-03-26 20:56:10.421266 Epoch 100, Training Loss 0.1568823354247281\n",
      "2022-03-26 20:56:10.440795 Epoch 100, Training Loss 0.1579148910005989\n",
      "2022-03-26 20:56:10.459810 Epoch 100, Training Loss 0.15888698452421465\n",
      "2022-03-26 20:56:10.478824 Epoch 100, Training Loss 0.15960497948367272\n",
      "2022-03-26 20:56:10.498761 Epoch 100, Training Loss 0.16041818432643284\n",
      "2022-03-26 20:56:10.518001 Epoch 100, Training Loss 0.16155908841763617\n",
      "2022-03-26 20:56:10.537048 Epoch 100, Training Loss 0.16254245895711358\n",
      "2022-03-26 20:56:10.556171 Epoch 100, Training Loss 0.1634588056741773\n",
      "2022-03-26 20:56:10.575239 Epoch 100, Training Loss 0.16414769291115539\n",
      "2022-03-26 20:56:10.595037 Epoch 100, Training Loss 0.16506750104220017\n",
      "2022-03-26 20:56:10.614186 Epoch 100, Training Loss 0.16599768076253973\n",
      "2022-03-26 20:56:10.633305 Epoch 100, Training Loss 0.16677640218411566\n",
      "2022-03-26 20:56:10.653293 Epoch 100, Training Loss 0.16753454209136232\n",
      "2022-03-26 20:56:10.672279 Epoch 100, Training Loss 0.16816798462282362\n",
      "2022-03-26 20:56:10.690375 Epoch 100, Training Loss 0.16934173521788223\n",
      "2022-03-26 20:56:10.710572 Epoch 100, Training Loss 0.17025139592492672\n",
      "2022-03-26 20:56:10.729595 Epoch 100, Training Loss 0.17116255604702493\n",
      "2022-03-26 20:56:10.748617 Epoch 100, Training Loss 0.17192521744676867\n",
      "2022-03-26 20:56:10.767612 Epoch 100, Training Loss 0.172793418702567\n",
      "2022-03-26 20:56:10.786512 Epoch 100, Training Loss 0.1738167168081874\n",
      "2022-03-26 20:56:10.805569 Epoch 100, Training Loss 0.17495477024246664\n",
      "2022-03-26 20:56:10.825595 Epoch 100, Training Loss 0.17572024243566997\n",
      "2022-03-26 20:56:10.844683 Epoch 100, Training Loss 0.17669990239545819\n",
      "2022-03-26 20:56:10.864103 Epoch 100, Training Loss 0.17755171778561818\n",
      "2022-03-26 20:56:10.882822 Epoch 100, Training Loss 0.17835341016654774\n",
      "2022-03-26 20:56:10.902210 Epoch 100, Training Loss 0.17927720090922186\n",
      "2022-03-26 20:56:10.921733 Epoch 100, Training Loss 0.17997725548036872\n",
      "2022-03-26 20:56:10.941762 Epoch 100, Training Loss 0.18087482231352336\n",
      "2022-03-26 20:56:10.960695 Epoch 100, Training Loss 0.18151217192182761\n",
      "2022-03-26 20:56:10.980702 Epoch 100, Training Loss 0.18234051870720466\n",
      "2022-03-26 20:56:11.000534 Epoch 100, Training Loss 0.18315035131428858\n",
      "2022-03-26 20:56:11.019546 Epoch 100, Training Loss 0.18383263295416333\n",
      "2022-03-26 20:56:11.038990 Epoch 100, Training Loss 0.18481541522171185\n",
      "2022-03-26 20:56:11.059038 Epoch 100, Training Loss 0.1857547647584125\n",
      "2022-03-26 20:56:11.079093 Epoch 100, Training Loss 0.18680268057319513\n",
      "2022-03-26 20:56:11.098080 Epoch 100, Training Loss 0.18759989848984476\n",
      "2022-03-26 20:56:11.118040 Epoch 100, Training Loss 0.1884982953672214\n",
      "2022-03-26 20:56:11.138079 Epoch 100, Training Loss 0.18927540182305114\n",
      "2022-03-26 20:56:11.157734 Epoch 100, Training Loss 0.18997937680967628\n",
      "2022-03-26 20:56:11.177624 Epoch 100, Training Loss 0.1910478758156452\n",
      "2022-03-26 20:56:11.197651 Epoch 100, Training Loss 0.19199753619368423\n",
      "2022-03-26 20:56:11.217533 Epoch 100, Training Loss 0.1931146671018942\n",
      "2022-03-26 20:56:11.237319 Epoch 100, Training Loss 0.19378871102924541\n",
      "2022-03-26 20:56:11.257830 Epoch 100, Training Loss 0.19466004694056938\n",
      "2022-03-26 20:56:11.276846 Epoch 100, Training Loss 0.1958536740078036\n",
      "2022-03-26 20:56:11.296878 Epoch 100, Training Loss 0.19690728671562946\n",
      "2022-03-26 20:56:11.315785 Epoch 100, Training Loss 0.19763993473766406\n",
      "2022-03-26 20:56:11.335807 Epoch 100, Training Loss 0.19836115375961488\n",
      "2022-03-26 20:56:11.355836 Epoch 100, Training Loss 0.19900408604413347\n",
      "2022-03-26 20:56:11.375872 Epoch 100, Training Loss 0.19996307622593687\n",
      "2022-03-26 20:56:11.394912 Epoch 100, Training Loss 0.20100392218288557\n",
      "2022-03-26 20:56:11.413946 Epoch 100, Training Loss 0.20174383351107691\n",
      "2022-03-26 20:56:11.433648 Epoch 100, Training Loss 0.20281705180244983\n",
      "2022-03-26 20:56:11.452678 Epoch 100, Training Loss 0.203571228305702\n",
      "2022-03-26 20:56:11.471671 Epoch 100, Training Loss 0.20483959273761496\n",
      "2022-03-26 20:56:11.491690 Epoch 100, Training Loss 0.20564708734869652\n",
      "2022-03-26 20:56:11.511657 Epoch 100, Training Loss 0.20633104935174099\n",
      "2022-03-26 20:56:11.532554 Epoch 100, Training Loss 0.20708451151390514\n",
      "2022-03-26 20:56:11.553574 Epoch 100, Training Loss 0.20793817167544304\n",
      "2022-03-26 20:56:11.573657 Epoch 100, Training Loss 0.2088010477669099\n",
      "2022-03-26 20:56:11.592674 Epoch 100, Training Loss 0.20952943420928458\n",
      "2022-03-26 20:56:11.611368 Epoch 100, Training Loss 0.21056273926401992\n",
      "2022-03-26 20:56:11.630419 Epoch 100, Training Loss 0.211475031493265\n",
      "2022-03-26 20:56:11.649101 Epoch 100, Training Loss 0.2122822758715476\n",
      "2022-03-26 20:56:11.668243 Epoch 100, Training Loss 0.21329606368261225\n",
      "2022-03-26 20:56:11.688066 Epoch 100, Training Loss 0.21430467358787955\n",
      "2022-03-26 20:56:11.707054 Epoch 100, Training Loss 0.2152275083696141\n",
      "2022-03-26 20:56:11.726069 Epoch 100, Training Loss 0.21633371706966245\n",
      "2022-03-26 20:56:11.744968 Epoch 100, Training Loss 0.21734072184166336\n",
      "2022-03-26 20:56:11.765798 Epoch 100, Training Loss 0.2183266997413562\n",
      "2022-03-26 20:56:11.784842 Epoch 100, Training Loss 0.21921430162304198\n",
      "2022-03-26 20:56:11.803389 Epoch 100, Training Loss 0.22033638810105335\n",
      "2022-03-26 20:56:11.822376 Epoch 100, Training Loss 0.22151689902138527\n",
      "2022-03-26 20:56:11.842382 Epoch 100, Training Loss 0.22214830722040532\n",
      "2022-03-26 20:56:11.861742 Epoch 100, Training Loss 0.2231749938729474\n",
      "2022-03-26 20:56:11.881756 Epoch 100, Training Loss 0.2240294173855306\n",
      "2022-03-26 20:56:11.900619 Epoch 100, Training Loss 0.22485903019795334\n",
      "2022-03-26 20:56:11.919654 Epoch 100, Training Loss 0.22556267057538337\n",
      "2022-03-26 20:56:11.939060 Epoch 100, Training Loss 0.22680015095969294\n",
      "2022-03-26 20:56:11.959021 Epoch 100, Training Loss 0.22768072460008704\n",
      "2022-03-26 20:56:11.978813 Epoch 100, Training Loss 0.22865365838150845\n",
      "2022-03-26 20:56:11.998834 Epoch 100, Training Loss 0.22961673940843938\n",
      "2022-03-26 20:56:12.017847 Epoch 100, Training Loss 0.23060336579447208\n",
      "2022-03-26 20:56:12.037004 Epoch 100, Training Loss 0.23167189772781507\n",
      "2022-03-26 20:56:12.056099 Epoch 100, Training Loss 0.23250450693128055\n",
      "2022-03-26 20:56:12.075189 Epoch 100, Training Loss 0.23329763872849058\n",
      "2022-03-26 20:56:12.094374 Epoch 100, Training Loss 0.23426159537966598\n",
      "2022-03-26 20:56:12.114452 Epoch 100, Training Loss 0.23514312932558376\n",
      "2022-03-26 20:56:12.133595 Epoch 100, Training Loss 0.23601305149400326\n",
      "2022-03-26 20:56:12.152783 Epoch 100, Training Loss 0.2371586297479127\n",
      "2022-03-26 20:56:12.172117 Epoch 100, Training Loss 0.2379202606428005\n",
      "2022-03-26 20:56:12.191066 Epoch 100, Training Loss 0.23864047698047766\n",
      "2022-03-26 20:56:12.210707 Epoch 100, Training Loss 0.23948742483582947\n",
      "2022-03-26 20:56:12.229729 Epoch 100, Training Loss 0.24034829327212576\n",
      "2022-03-26 20:56:12.248350 Epoch 100, Training Loss 0.2411895978938588\n",
      "2022-03-26 20:56:12.267887 Epoch 100, Training Loss 0.241909127787251\n",
      "2022-03-26 20:56:12.286250 Epoch 100, Training Loss 0.24290616821754923\n",
      "2022-03-26 20:56:12.305240 Epoch 100, Training Loss 0.24372737273535766\n",
      "2022-03-26 20:56:12.324334 Epoch 100, Training Loss 0.24440713352559473\n",
      "2022-03-26 20:56:12.344359 Epoch 100, Training Loss 0.24518683575608236\n",
      "2022-03-26 20:56:12.363430 Epoch 100, Training Loss 0.24605115722207463\n",
      "2022-03-26 20:56:12.384117 Epoch 100, Training Loss 0.24694471529987463\n",
      "2022-03-26 20:56:12.404187 Epoch 100, Training Loss 0.24768319322020196\n",
      "2022-03-26 20:56:12.423406 Epoch 100, Training Loss 0.24845230259248974\n",
      "2022-03-26 20:56:12.443438 Epoch 100, Training Loss 0.2493267570767561\n",
      "2022-03-26 20:56:12.462481 Epoch 100, Training Loss 0.2501896558820134\n",
      "2022-03-26 20:56:12.481550 Epoch 100, Training Loss 0.2512187574373182\n",
      "2022-03-26 20:56:12.500517 Epoch 100, Training Loss 0.2522508923507408\n",
      "2022-03-26 20:56:12.520564 Epoch 100, Training Loss 0.25295577329747815\n",
      "2022-03-26 20:56:12.539579 Epoch 100, Training Loss 0.25397711473962536\n",
      "2022-03-26 20:56:12.558429 Epoch 100, Training Loss 0.25501004638879193\n",
      "2022-03-26 20:56:12.577462 Epoch 100, Training Loss 0.2557788524786225\n",
      "2022-03-26 20:56:12.597349 Epoch 100, Training Loss 0.25664198368101776\n",
      "2022-03-26 20:56:12.617993 Epoch 100, Training Loss 0.25739679822836387\n",
      "2022-03-26 20:56:12.636235 Epoch 100, Training Loss 0.25814907889232\n",
      "2022-03-26 20:56:12.656237 Epoch 100, Training Loss 0.2589111998867806\n",
      "2022-03-26 20:56:12.675321 Epoch 100, Training Loss 0.25956723482712457\n",
      "2022-03-26 20:56:12.694549 Epoch 100, Training Loss 0.2605447407116366\n",
      "2022-03-26 20:56:12.713585 Epoch 100, Training Loss 0.26128729827263775\n",
      "2022-03-26 20:56:12.733548 Epoch 100, Training Loss 0.2619758224700723\n",
      "2022-03-26 20:56:12.752551 Epoch 100, Training Loss 0.26300315257838314\n",
      "2022-03-26 20:56:12.771992 Epoch 100, Training Loss 0.26408697676170817\n",
      "2022-03-26 20:56:12.791008 Epoch 100, Training Loss 0.2651495916764145\n",
      "2022-03-26 20:56:12.810410 Epoch 100, Training Loss 0.26613286953143145\n",
      "2022-03-26 20:56:12.829243 Epoch 100, Training Loss 0.266870014883978\n",
      "2022-03-26 20:56:12.849191 Epoch 100, Training Loss 0.2676953575800142\n",
      "2022-03-26 20:56:12.868006 Epoch 100, Training Loss 0.26853082353806557\n",
      "2022-03-26 20:56:12.887235 Epoch 100, Training Loss 0.269638349256857\n",
      "2022-03-26 20:56:12.905329 Epoch 100, Training Loss 0.2707862357044464\n",
      "2022-03-26 20:56:12.925442 Epoch 100, Training Loss 0.27176387528019486\n",
      "2022-03-26 20:56:12.944563 Epoch 100, Training Loss 0.273104387247349\n",
      "2022-03-26 20:56:12.964083 Epoch 100, Training Loss 0.2737458394768903\n",
      "2022-03-26 20:56:12.982387 Epoch 100, Training Loss 0.2749425896140925\n",
      "2022-03-26 20:56:13.002326 Epoch 100, Training Loss 0.27590146012928174\n",
      "2022-03-26 20:56:13.021271 Epoch 100, Training Loss 0.27667756687344797\n",
      "2022-03-26 20:56:13.041806 Epoch 100, Training Loss 0.27764118761967516\n",
      "2022-03-26 20:56:13.061451 Epoch 100, Training Loss 0.27869481229416243\n",
      "2022-03-26 20:56:13.081401 Epoch 100, Training Loss 0.2797912341706893\n",
      "2022-03-26 20:56:13.100321 Epoch 100, Training Loss 0.2805313570877475\n",
      "2022-03-26 20:56:13.119365 Epoch 100, Training Loss 0.2813220163592902\n",
      "2022-03-26 20:56:13.138551 Epoch 100, Training Loss 0.2822122806325898\n",
      "2022-03-26 20:56:13.157789 Epoch 100, Training Loss 0.2834354286913372\n",
      "2022-03-26 20:56:13.176792 Epoch 100, Training Loss 0.28427829835420987\n",
      "2022-03-26 20:56:13.202798 Epoch 100, Training Loss 0.2848756956246198\n",
      "2022-03-26 20:56:13.229419 Epoch 100, Training Loss 0.2855484816805481\n",
      "2022-03-26 20:56:13.255975 Epoch 100, Training Loss 0.2862097515398279\n",
      "2022-03-26 20:56:13.283086 Epoch 100, Training Loss 0.2874045389158951\n",
      "2022-03-26 20:56:13.308717 Epoch 100, Training Loss 0.28840338311079516\n",
      "2022-03-26 20:56:13.334725 Epoch 100, Training Loss 0.2892520452475609\n",
      "2022-03-26 20:56:13.361168 Epoch 100, Training Loss 0.29016056393875794\n",
      "2022-03-26 20:56:13.389101 Epoch 100, Training Loss 0.2912999042845748\n",
      "2022-03-26 20:56:13.415457 Epoch 100, Training Loss 0.2921358338173698\n",
      "2022-03-26 20:56:13.440121 Epoch 100, Training Loss 0.2930399396306718\n",
      "2022-03-26 20:56:13.467146 Epoch 100, Training Loss 0.2937053856261246\n",
      "2022-03-26 20:56:13.493200 Epoch 100, Training Loss 0.2944793584172988\n",
      "2022-03-26 20:56:13.519616 Epoch 100, Training Loss 0.2952947505675923\n",
      "2022-03-26 20:56:13.545187 Epoch 100, Training Loss 0.2960617925276232\n",
      "2022-03-26 20:56:13.571232 Epoch 100, Training Loss 0.29687453295721117\n",
      "2022-03-26 20:56:13.597123 Epoch 100, Training Loss 0.29789473284083556\n",
      "2022-03-26 20:56:13.616134 Epoch 100, Training Loss 0.2987671012387556\n",
      "2022-03-26 20:56:13.634049 Epoch 100, Training Loss 0.2999355355110925\n",
      "2022-03-26 20:56:13.653068 Epoch 100, Training Loss 0.3006923305790138\n",
      "2022-03-26 20:56:13.671547 Epoch 100, Training Loss 0.3014147355199775\n",
      "2022-03-26 20:56:13.689445 Epoch 100, Training Loss 0.302421814073687\n",
      "2022-03-26 20:56:13.708013 Epoch 100, Training Loss 0.30358866615521024\n",
      "2022-03-26 20:56:13.727556 Epoch 100, Training Loss 0.3043457090168658\n",
      "2022-03-26 20:56:13.745417 Epoch 100, Training Loss 0.3052036027850397\n",
      "2022-03-26 20:56:13.764546 Epoch 100, Training Loss 0.30593579714103125\n",
      "2022-03-26 20:56:13.782553 Epoch 100, Training Loss 0.3068573334256706\n",
      "2022-03-26 20:56:13.800872 Epoch 100, Training Loss 0.3077569316948771\n",
      "2022-03-26 20:56:13.818899 Epoch 100, Training Loss 0.30861416760155613\n",
      "2022-03-26 20:56:13.836828 Epoch 100, Training Loss 0.3095391215494527\n",
      "2022-03-26 20:56:13.854855 Epoch 100, Training Loss 0.31036607547641715\n",
      "2022-03-26 20:56:13.873867 Epoch 100, Training Loss 0.31092922416184565\n",
      "2022-03-26 20:56:13.890856 Epoch 100, Training Loss 0.31171702431595844\n",
      "2022-03-26 20:56:13.909878 Epoch 100, Training Loss 0.31286020153928595\n",
      "2022-03-26 20:56:13.928740 Epoch 100, Training Loss 0.31393454271509214\n",
      "2022-03-26 20:56:13.947871 Epoch 100, Training Loss 0.3149063449229121\n",
      "2022-03-26 20:56:13.966668 Epoch 100, Training Loss 0.31597590659890334\n",
      "2022-03-26 20:56:13.985705 Epoch 100, Training Loss 0.3167820727581258\n",
      "2022-03-26 20:56:14.002695 Epoch 100, Training Loss 0.31776062454409004\n",
      "2022-03-26 20:56:14.021185 Epoch 100, Training Loss 0.3186936613238986\n",
      "2022-03-26 20:56:14.039684 Epoch 100, Training Loss 0.31967760755887725\n",
      "2022-03-26 20:56:14.058354 Epoch 100, Training Loss 0.3206320730469111\n",
      "2022-03-26 20:56:14.076333 Epoch 100, Training Loss 0.32150958566104665\n",
      "2022-03-26 20:56:14.095295 Epoch 100, Training Loss 0.3224079707242034\n",
      "2022-03-26 20:56:14.113402 Epoch 100, Training Loss 0.3231995512762338\n",
      "2022-03-26 20:56:14.133443 Epoch 100, Training Loss 0.3240501954579902\n",
      "2022-03-26 20:56:14.151415 Epoch 100, Training Loss 0.3250707872688313\n",
      "2022-03-26 20:56:14.169414 Epoch 100, Training Loss 0.32586013028383864\n",
      "2022-03-26 20:56:14.187540 Epoch 100, Training Loss 0.32684495595410046\n",
      "2022-03-26 20:56:14.206570 Epoch 100, Training Loss 0.32787782037654495\n",
      "2022-03-26 20:56:14.224560 Epoch 100, Training Loss 0.32868476482608433\n",
      "2022-03-26 20:56:14.242522 Epoch 100, Training Loss 0.32950118961541547\n",
      "2022-03-26 20:56:14.260438 Epoch 100, Training Loss 0.3304604745429495\n",
      "2022-03-26 20:56:14.279415 Epoch 100, Training Loss 0.33095537248017537\n",
      "2022-03-26 20:56:14.297455 Epoch 100, Training Loss 0.3317305391935436\n",
      "2022-03-26 20:56:14.316542 Epoch 100, Training Loss 0.33288294797205864\n",
      "2022-03-26 20:56:14.334575 Epoch 100, Training Loss 0.3338357241790923\n",
      "2022-03-26 20:56:14.353610 Epoch 100, Training Loss 0.3347166810575349\n",
      "2022-03-26 20:56:14.370710 Epoch 100, Training Loss 0.3356335945522694\n",
      "2022-03-26 20:56:14.389718 Epoch 100, Training Loss 0.33658039588909927\n",
      "2022-03-26 20:56:14.407837 Epoch 100, Training Loss 0.33755608352706257\n",
      "2022-03-26 20:56:14.426748 Epoch 100, Training Loss 0.33854263895155523\n",
      "2022-03-26 20:56:14.444633 Epoch 100, Training Loss 0.3393055406753974\n",
      "2022-03-26 20:56:14.463665 Epoch 100, Training Loss 0.3398963930204396\n",
      "2022-03-26 20:56:14.481983 Epoch 100, Training Loss 0.34063690778849376\n",
      "2022-03-26 20:56:14.501104 Epoch 100, Training Loss 0.3414989443081419\n",
      "2022-03-26 20:56:14.518687 Epoch 100, Training Loss 0.34231958692640907\n",
      "2022-03-26 20:56:14.537127 Epoch 100, Training Loss 0.3429136386003031\n",
      "2022-03-26 20:56:14.556135 Epoch 100, Training Loss 0.3438422591485026\n",
      "2022-03-26 20:56:14.575259 Epoch 100, Training Loss 0.34440635354317667\n",
      "2022-03-26 20:56:14.593203 Epoch 100, Training Loss 0.3453177245681548\n",
      "2022-03-26 20:56:14.611771 Epoch 100, Training Loss 0.34606685335069054\n",
      "2022-03-26 20:56:14.630262 Epoch 100, Training Loss 0.34690340804626874\n",
      "2022-03-26 20:56:14.648833 Epoch 100, Training Loss 0.34778391468860304\n",
      "2022-03-26 20:56:14.667301 Epoch 100, Training Loss 0.34880589524193495\n",
      "2022-03-26 20:56:14.686352 Epoch 100, Training Loss 0.34988152874095363\n",
      "2022-03-26 20:56:14.703947 Epoch 100, Training Loss 0.35082373952926577\n",
      "2022-03-26 20:56:14.722361 Epoch 100, Training Loss 0.3517320379424278\n",
      "2022-03-26 20:56:14.740374 Epoch 100, Training Loss 0.35251777319956923\n",
      "2022-03-26 20:56:14.759411 Epoch 100, Training Loss 0.3535002310715063\n",
      "2022-03-26 20:56:14.778458 Epoch 100, Training Loss 0.3544243058890028\n",
      "2022-03-26 20:56:14.796390 Epoch 100, Training Loss 0.3553576849000838\n",
      "2022-03-26 20:56:14.814960 Epoch 100, Training Loss 0.3559052356902291\n",
      "2022-03-26 20:56:14.833410 Epoch 100, Training Loss 0.3568599368528942\n",
      "2022-03-26 20:56:14.851446 Epoch 100, Training Loss 0.3576680823893803\n",
      "2022-03-26 20:56:14.869577 Epoch 100, Training Loss 0.3586013027088112\n",
      "2022-03-26 20:56:14.888080 Epoch 100, Training Loss 0.35930517761756087\n",
      "2022-03-26 20:56:14.906135 Epoch 100, Training Loss 0.36021388495517204\n",
      "2022-03-26 20:56:14.924247 Epoch 100, Training Loss 0.3611671012609511\n",
      "2022-03-26 20:56:14.943151 Epoch 100, Training Loss 0.3620510367709962\n",
      "2022-03-26 20:56:14.961182 Epoch 100, Training Loss 0.36295508343697813\n",
      "2022-03-26 20:56:14.980082 Epoch 100, Training Loss 0.36388647262854956\n",
      "2022-03-26 20:56:14.998021 Epoch 100, Training Loss 0.3647567749861866\n",
      "2022-03-26 20:56:15.016926 Epoch 100, Training Loss 0.3655135655570823\n",
      "2022-03-26 20:56:15.034773 Epoch 100, Training Loss 0.36622009012857665\n",
      "2022-03-26 20:56:15.053107 Epoch 100, Training Loss 0.3671145168945308\n",
      "2022-03-26 20:56:15.071217 Epoch 100, Training Loss 0.3681149922139809\n",
      "2022-03-26 20:56:15.089277 Epoch 100, Training Loss 0.3692213439804209\n",
      "2022-03-26 20:56:15.107806 Epoch 100, Training Loss 0.3701900124473645\n",
      "2022-03-26 20:56:15.126679 Epoch 100, Training Loss 0.3709000610100949\n",
      "2022-03-26 20:56:15.144703 Epoch 100, Training Loss 0.3718350410385205\n",
      "2022-03-26 20:56:15.164707 Epoch 100, Training Loss 0.3728358539397759\n",
      "2022-03-26 20:56:15.182323 Epoch 100, Training Loss 0.3737155509650555\n",
      "2022-03-26 20:56:15.200387 Epoch 100, Training Loss 0.3747642106945862\n",
      "2022-03-26 20:56:15.219458 Epoch 100, Training Loss 0.375609186718531\n",
      "2022-03-26 20:56:15.237367 Epoch 100, Training Loss 0.37662924391686764\n",
      "2022-03-26 20:56:15.255354 Epoch 100, Training Loss 0.3777783175792231\n",
      "2022-03-26 20:56:15.273529 Epoch 100, Training Loss 0.37863411237974\n",
      "2022-03-26 20:56:15.291550 Epoch 100, Training Loss 0.37959398062485256\n",
      "2022-03-26 20:56:15.310123 Epoch 100, Training Loss 0.380732783995321\n",
      "2022-03-26 20:56:15.329188 Epoch 100, Training Loss 0.3815416876236191\n",
      "2022-03-26 20:56:15.347680 Epoch 100, Training Loss 0.38241843871600795\n",
      "2022-03-26 20:56:15.366506 Epoch 100, Training Loss 0.3835724309048689\n",
      "2022-03-26 20:56:15.385417 Epoch 100, Training Loss 0.38458609257055365\n",
      "2022-03-26 20:56:15.403364 Epoch 100, Training Loss 0.3851388488965266\n",
      "2022-03-26 20:56:15.428772 Epoch 100, Training Loss 0.3858875755382621\n",
      "2022-03-26 20:56:15.454272 Epoch 100, Training Loss 0.3866416164447584\n",
      "2022-03-26 20:56:15.480329 Epoch 100, Training Loss 0.3874142649762161\n",
      "2022-03-26 20:56:15.507223 Epoch 100, Training Loss 0.38813873908251445\n",
      "2022-03-26 20:56:15.533301 Epoch 100, Training Loss 0.38900244201693085\n",
      "2022-03-26 20:56:15.558385 Epoch 100, Training Loss 0.38975377236028463\n",
      "2022-03-26 20:56:15.585356 Epoch 100, Training Loss 0.3906524536935875\n",
      "2022-03-26 20:56:15.610442 Epoch 100, Training Loss 0.391375888834524\n",
      "2022-03-26 20:56:15.628389 Epoch 100, Training Loss 0.39205697815284096\n",
      "2022-03-26 20:56:15.645998 Epoch 100, Training Loss 0.39306501358213936\n",
      "2022-03-26 20:56:15.666036 Epoch 100, Training Loss 0.3939545946124265\n",
      "2022-03-26 20:56:15.683394 Epoch 100, Training Loss 0.39485360041756157\n",
      "2022-03-26 20:56:15.701326 Epoch 100, Training Loss 0.3960139362327278\n",
      "2022-03-26 20:56:15.719300 Epoch 100, Training Loss 0.3969537965248308\n",
      "2022-03-26 20:56:15.737783 Epoch 100, Training Loss 0.39779579292630296\n",
      "2022-03-26 20:56:15.755776 Epoch 100, Training Loss 0.39857524050318677\n",
      "2022-03-26 20:56:15.773810 Epoch 100, Training Loss 0.3997833627035551\n",
      "2022-03-26 20:56:15.792820 Epoch 100, Training Loss 0.40076260741256997\n",
      "2022-03-26 20:56:15.810161 Epoch 100, Training Loss 0.40176601273476925\n",
      "2022-03-26 20:56:15.828170 Epoch 100, Training Loss 0.402466878218724\n",
      "2022-03-26 20:56:15.846746 Epoch 100, Training Loss 0.4029667915590584\n",
      "2022-03-26 20:56:15.864660 Epoch 100, Training Loss 0.40404965787592445\n",
      "2022-03-26 20:56:15.882783 Epoch 100, Training Loss 0.40494870895619894\n",
      "2022-03-26 20:56:15.900801 Epoch 100, Training Loss 0.40586719313241026\n",
      "2022-03-26 20:56:15.918780 Epoch 100, Training Loss 0.4064851993566279\n",
      "2022-03-26 20:56:15.937179 Epoch 100, Training Loss 0.40757703137062395\n",
      "2022-03-26 20:56:15.955329 Epoch 100, Training Loss 0.40843936659948293\n",
      "2022-03-26 20:56:15.973341 Epoch 100, Training Loss 0.4094469640642176\n",
      "2022-03-26 20:56:15.992388 Epoch 100, Training Loss 0.4102328534397628\n",
      "2022-03-26 20:56:16.011377 Epoch 100, Training Loss 0.41109829618955207\n",
      "2022-03-26 20:56:16.029434 Epoch 100, Training Loss 0.41215669053137455\n",
      "2022-03-26 20:56:16.048211 Epoch 100, Training Loss 0.41283897598228797\n",
      "2022-03-26 20:56:16.066124 Epoch 100, Training Loss 0.4139138662525455\n",
      "2022-03-26 20:56:16.084048 Epoch 100, Training Loss 0.414897264002839\n",
      "2022-03-26 20:56:16.102403 Epoch 100, Training Loss 0.41574652363424713\n",
      "2022-03-26 20:56:16.120452 Epoch 100, Training Loss 0.41666724504259844\n",
      "2022-03-26 20:56:16.138473 Epoch 100, Training Loss 0.4175969583680258\n",
      "2022-03-26 20:56:16.156410 Epoch 100, Training Loss 0.41847249831232575\n",
      "2022-03-26 20:56:16.175295 Epoch 100, Training Loss 0.41944976372029774\n",
      "2022-03-26 20:56:16.193346 Epoch 100, Training Loss 0.42009038133237064\n",
      "2022-03-26 20:56:16.211414 Epoch 100, Training Loss 0.42112055721947605\n",
      "2022-03-26 20:56:16.230403 Epoch 100, Training Loss 0.42209157031362926\n",
      "2022-03-26 20:56:16.249065 Epoch 100, Training Loss 0.4230499440980384\n",
      "2022-03-26 20:56:16.267216 Epoch 100, Training Loss 0.42410238521635685\n",
      "2022-03-26 20:56:16.285190 Epoch 100, Training Loss 0.4251410024778922\n",
      "2022-03-26 20:56:16.303248 Epoch 100, Training Loss 0.4258594828112351\n",
      "2022-03-26 20:56:16.321279 Epoch 100, Training Loss 0.4269152684208682\n",
      "2022-03-26 20:56:16.339194 Epoch 100, Training Loss 0.42761938334883326\n",
      "2022-03-26 20:56:16.357961 Epoch 100, Training Loss 0.4284881610623406\n",
      "2022-03-26 20:56:16.376316 Epoch 100, Training Loss 0.4294869735493989\n",
      "2022-03-26 20:56:16.394238 Epoch 100, Training Loss 0.43032479457690587\n",
      "2022-03-26 20:56:16.413286 Epoch 100, Training Loss 0.4308757423744787\n",
      "2022-03-26 20:56:16.431402 Epoch 100, Training Loss 0.43187953268780427\n",
      "2022-03-26 20:56:16.449456 Epoch 100, Training Loss 0.43284180295436886\n",
      "2022-03-26 20:56:16.467555 Epoch 100, Training Loss 0.4337513630499925\n",
      "2022-03-26 20:56:16.485525 Epoch 100, Training Loss 0.43475019695508815\n",
      "2022-03-26 20:56:16.503813 Epoch 100, Training Loss 0.4357183218154761\n",
      "2022-03-26 20:56:16.521442 Epoch 100, Training Loss 0.43643066340395253\n",
      "2022-03-26 20:56:16.539564 Epoch 100, Training Loss 0.4373605089724216\n",
      "2022-03-26 20:56:16.558593 Epoch 100, Training Loss 0.43831380210874027\n",
      "2022-03-26 20:56:16.576622 Epoch 100, Training Loss 0.4391596361499308\n",
      "2022-03-26 20:56:16.595631 Epoch 100, Training Loss 0.4398759025746904\n",
      "2022-03-26 20:56:16.612660 Epoch 100, Training Loss 0.44085072160072036\n",
      "2022-03-26 20:56:16.632188 Epoch 100, Training Loss 0.4416668778642669\n",
      "2022-03-26 20:56:16.649747 Epoch 100, Training Loss 0.44232569730190363\n",
      "2022-03-26 20:56:16.667767 Epoch 100, Training Loss 0.443315151935953\n",
      "2022-03-26 20:56:16.686135 Epoch 100, Training Loss 0.44440905189575136\n",
      "2022-03-26 20:56:16.704190 Epoch 100, Training Loss 0.4454678545522568\n",
      "2022-03-26 20:56:16.722370 Epoch 100, Training Loss 0.44622864229294956\n",
      "2022-03-26 20:56:16.740307 Epoch 100, Training Loss 0.44699880236859824\n",
      "2022-03-26 20:56:16.758398 Epoch 100, Training Loss 0.4480875116175093\n",
      "2022-03-26 20:56:16.776964 Epoch 100, Training Loss 0.4488619091108327\n",
      "2022-03-26 20:56:16.794978 Epoch 100, Training Loss 0.4500715017623609\n",
      "2022-03-26 20:56:16.813020 Epoch 100, Training Loss 0.4508637872041034\n",
      "2022-03-26 20:56:16.831517 Epoch 100, Training Loss 0.4515146228968335\n",
      "2022-03-26 20:56:16.850542 Epoch 100, Training Loss 0.4523689190445044\n",
      "2022-03-26 20:56:16.868537 Epoch 100, Training Loss 0.45323215291628144\n",
      "2022-03-26 20:56:16.886843 Epoch 100, Training Loss 0.45427695678932895\n",
      "2022-03-26 20:56:16.904130 Epoch 100, Training Loss 0.45537051253611477\n",
      "2022-03-26 20:56:16.922199 Epoch 100, Training Loss 0.4562744720055319\n",
      "2022-03-26 20:56:16.941114 Epoch 100, Training Loss 0.45702530813339115\n",
      "2022-03-26 20:56:16.959177 Epoch 100, Training Loss 0.45800915528136443\n",
      "2022-03-26 20:56:16.977312 Epoch 100, Training Loss 0.4587073975511829\n",
      "2022-03-26 20:56:16.995343 Epoch 100, Training Loss 0.45957997852883986\n",
      "2022-03-26 20:56:17.014480 Epoch 100, Training Loss 0.4603846977129007\n",
      "2022-03-26 20:56:17.032553 Epoch 100, Training Loss 0.4610854594603829\n",
      "2022-03-26 20:56:17.050561 Epoch 100, Training Loss 0.4620775190155829\n",
      "2022-03-26 20:56:17.068564 Epoch 100, Training Loss 0.4627658438956951\n",
      "2022-03-26 20:56:17.086029 Epoch 100, Training Loss 0.46381792258423615\n",
      "2022-03-26 20:56:17.104188 Epoch 100, Training Loss 0.46472085124391427\n",
      "2022-03-26 20:56:17.123240 Epoch 100, Training Loss 0.46558520129269654\n",
      "2022-03-26 20:56:17.141192 Epoch 100, Training Loss 0.4663678735418393\n",
      "2022-03-26 20:56:17.159248 Epoch 100, Training Loss 0.4672726994127874\n",
      "2022-03-26 20:56:17.177621 Epoch 100, Training Loss 0.46805080984864394\n",
      "2022-03-26 20:56:17.195548 Epoch 100, Training Loss 0.4689788629331857\n",
      "2022-03-26 20:56:17.212887 Epoch 100, Training Loss 0.47019325551169605\n",
      "2022-03-26 20:56:17.232880 Epoch 100, Training Loss 0.4710576187466721\n",
      "2022-03-26 20:56:17.249895 Epoch 100, Training Loss 0.47176491695901623\n",
      "2022-03-26 20:56:17.268474 Epoch 100, Training Loss 0.4727612442677588\n",
      "2022-03-26 20:56:17.287025 Epoch 100, Training Loss 0.47362852874009503\n",
      "2022-03-26 20:56:17.305467 Epoch 100, Training Loss 0.4745533971682839\n",
      "2022-03-26 20:56:17.323545 Epoch 100, Training Loss 0.47532498798406947\n",
      "2022-03-26 20:56:17.341558 Epoch 100, Training Loss 0.4763410252225978\n",
      "2022-03-26 20:56:17.359541 Epoch 100, Training Loss 0.4772060811519623\n",
      "2022-03-26 20:56:17.378957 Epoch 100, Training Loss 0.4781108144908915\n",
      "2022-03-26 20:56:17.396870 Epoch 100, Training Loss 0.47901514713721505\n",
      "2022-03-26 20:56:17.414758 Epoch 100, Training Loss 0.48001716630842983\n",
      "2022-03-26 20:56:17.431769 Epoch 100, Training Loss 0.48088224098810456\n",
      "2022-03-26 20:56:17.497897 Epoch 100, Training Loss 0.4818278253840668\n",
      "2022-03-26 20:56:17.516932 Epoch 100, Training Loss 0.48257478164589923\n",
      "2022-03-26 20:56:17.535314 Epoch 100, Training Loss 0.4837071684467823\n",
      "2022-03-26 20:56:17.553373 Epoch 100, Training Loss 0.48432511415170587\n",
      "2022-03-26 20:56:17.572475 Epoch 100, Training Loss 0.4852970786335523\n",
      "2022-03-26 20:56:17.590365 Epoch 100, Training Loss 0.4863697156653075\n",
      "2022-03-26 20:56:17.608397 Epoch 100, Training Loss 0.4872108131951993\n",
      "2022-03-26 20:56:17.626438 Epoch 100, Training Loss 0.4880867471246768\n",
      "2022-03-26 20:56:17.645338 Epoch 100, Training Loss 0.4890357060429385\n",
      "2022-03-26 20:56:17.663473 Epoch 100, Training Loss 0.4897521385527633\n",
      "2022-03-26 20:56:17.681621 Epoch 100, Training Loss 0.49054465387635826\n",
      "2022-03-26 20:56:17.700643 Epoch 100, Training Loss 0.4916003426856092\n",
      "2022-03-26 20:56:17.718625 Epoch 100, Training Loss 0.4925066803955971\n",
      "2022-03-26 20:56:17.737310 Epoch 100, Training Loss 0.49355568151797174\n",
      "2022-03-26 20:56:17.755394 Epoch 100, Training Loss 0.4944793560239665\n",
      "2022-03-26 20:56:17.773313 Epoch 100, Training Loss 0.49524164790540093\n",
      "2022-03-26 20:56:17.791389 Epoch 100, Training Loss 0.4959824441567711\n",
      "2022-03-26 20:56:17.810367 Epoch 100, Training Loss 0.49705696384162856\n",
      "2022-03-26 20:56:17.829387 Epoch 100, Training Loss 0.49801341106977\n",
      "2022-03-26 20:56:17.847543 Epoch 100, Training Loss 0.4988657317274367\n",
      "2022-03-26 20:56:17.865521 Epoch 100, Training Loss 0.49965942877790204\n",
      "2022-03-26 20:56:17.883564 Epoch 100, Training Loss 0.5004721895584365\n",
      "2022-03-26 20:56:17.901574 Epoch 100, Training Loss 0.5018214756036963\n",
      "2022-03-26 20:56:17.920589 Epoch 100, Training Loss 0.5026950422683945\n",
      "2022-03-26 20:56:17.938851 Epoch 100, Training Loss 0.5037061759196889\n",
      "2022-03-26 20:56:17.957020 Epoch 100, Training Loss 0.5047604778919683\n",
      "2022-03-26 20:56:17.975292 Epoch 100, Training Loss 0.5058621534377413\n",
      "2022-03-26 20:56:17.994285 Epoch 100, Training Loss 0.5068861938574735\n",
      "2022-03-26 20:56:18.012409 Epoch 100, Training Loss 0.5076450209712129\n",
      "2022-03-26 20:56:18.031446 Epoch 100, Training Loss 0.5086698111746927\n",
      "2022-03-26 20:56:18.050457 Epoch 100, Training Loss 0.5093680367521618\n",
      "2022-03-26 20:56:18.068473 Epoch 100, Training Loss 0.5102268336602794\n",
      "2022-03-26 20:56:18.086408 Epoch 100, Training Loss 0.5112801510126085\n",
      "2022-03-26 20:56:18.104413 Epoch 100, Training Loss 0.512237301834709\n",
      "2022-03-26 20:56:18.123475 Epoch 100, Training Loss 0.513270774803808\n",
      "2022-03-26 20:56:18.141587 Epoch 100, Training Loss 0.5142950230775891\n",
      "2022-03-26 20:56:18.160538 Epoch 100, Training Loss 0.5148824042523913\n",
      "2022-03-26 20:56:18.179553 Epoch 100, Training Loss 0.5156574561010541\n",
      "2022-03-26 20:56:18.198430 Epoch 100, Training Loss 0.5164303656886605\n",
      "2022-03-26 20:56:18.217491 Epoch 100, Training Loss 0.5176395381350651\n",
      "2022-03-26 20:56:18.235545 Epoch 100, Training Loss 0.5188584656971494\n",
      "2022-03-26 20:56:18.262562 Epoch 100, Training Loss 0.5196714542253548\n",
      "2022-03-26 20:56:18.287986 Epoch 100, Training Loss 0.5206999342002527\n",
      "2022-03-26 20:56:18.314073 Epoch 100, Training Loss 0.5214526218069179\n",
      "2022-03-26 20:56:18.340476 Epoch 100, Training Loss 0.5221193704916083\n",
      "2022-03-26 20:56:18.366534 Epoch 100, Training Loss 0.5232508181763427\n",
      "2022-03-26 20:56:18.392310 Epoch 100, Training Loss 0.5237936873722564\n",
      "2022-03-26 20:56:18.418420 Epoch 100, Training Loss 0.5248365534845826\n",
      "2022-03-26 20:56:18.444528 Epoch 100, Training Loss 0.5259190402219972\n",
      "2022-03-26 20:56:18.458547 Epoch 100, Training Loss 0.5266010552416067\n",
      "2022-03-26 20:56:18.473561 Epoch 100, Training Loss 0.5276968428850783\n",
      "2022-03-26 20:56:18.487419 Epoch 100, Training Loss 0.5286247711199934\n",
      "2022-03-26 20:56:18.501446 Epoch 100, Training Loss 0.5294994912336549\n",
      "2022-03-26 20:56:18.515430 Epoch 100, Training Loss 0.5306323178284004\n",
      "2022-03-26 20:56:18.530456 Epoch 100, Training Loss 0.5314898906308977\n",
      "2022-03-26 20:56:18.545548 Epoch 100, Training Loss 0.5324774748071686\n",
      "2022-03-26 20:56:18.559855 Epoch 100, Training Loss 0.5332864569428631\n",
      "2022-03-26 20:56:18.574869 Epoch 100, Training Loss 0.5340336832548956\n",
      "2022-03-26 20:56:18.588891 Epoch 100, Training Loss 0.5348131828143469\n",
      "2022-03-26 20:56:18.602895 Epoch 100, Training Loss 0.5355947263862776\n",
      "2022-03-26 20:56:18.617796 Epoch 100, Training Loss 0.5363419484299468\n",
      "2022-03-26 20:56:18.631800 Epoch 100, Training Loss 0.5375294184593289\n",
      "2022-03-26 20:56:18.645826 Epoch 100, Training Loss 0.5384982892924257\n",
      "2022-03-26 20:56:18.659828 Epoch 100, Training Loss 0.539445793079903\n",
      "2022-03-26 20:56:18.674814 Epoch 100, Training Loss 0.5404978875461441\n",
      "2022-03-26 20:56:18.688818 Epoch 100, Training Loss 0.5412524683243783\n",
      "2022-03-26 20:56:18.703113 Epoch 100, Training Loss 0.5423534421054909\n",
      "2022-03-26 20:56:18.717186 Epoch 100, Training Loss 0.5431791636187707\n",
      "2022-03-26 20:56:18.731215 Epoch 100, Training Loss 0.544301273649001\n",
      "2022-03-26 20:56:18.746221 Epoch 100, Training Loss 0.5451029473558411\n",
      "2022-03-26 20:56:18.761249 Epoch 100, Training Loss 0.5461098820047305\n",
      "2022-03-26 20:56:18.776227 Epoch 100, Training Loss 0.5470929657254378\n",
      "2022-03-26 20:56:18.790248 Epoch 100, Training Loss 0.5478204541346606\n",
      "2022-03-26 20:56:18.804331 Epoch 100, Training Loss 0.5487058539219829\n",
      "2022-03-26 20:56:18.817901 Epoch 100, Training Loss 0.5497925865375782\n",
      "2022-03-26 20:56:18.832197 Epoch 100, Training Loss 0.5507668943508811\n",
      "2022-03-26 20:56:18.847149 Epoch 100, Training Loss 0.5516690460922163\n",
      "2022-03-26 20:56:18.861177 Epoch 100, Training Loss 0.5525241262467621\n",
      "2022-03-26 20:56:18.876180 Epoch 100, Training Loss 0.5533428908613942\n",
      "2022-03-26 20:56:18.890195 Epoch 100, Training Loss 0.5546073303808032\n",
      "2022-03-26 20:56:18.904233 Epoch 100, Training Loss 0.5552872894974925\n",
      "2022-03-26 20:56:18.918785 Epoch 100, Training Loss 0.5560825084481398\n",
      "2022-03-26 20:56:18.933107 Epoch 100, Training Loss 0.5571424955754634\n",
      "2022-03-26 20:56:18.948144 Epoch 100, Training Loss 0.558360238209405\n",
      "2022-03-26 20:56:18.962211 Epoch 100, Training Loss 0.5594590776564216\n",
      "2022-03-26 20:56:18.976218 Epoch 100, Training Loss 0.5607027667562675\n",
      "2022-03-26 20:56:18.990127 Epoch 100, Training Loss 0.5619379785816992\n",
      "2022-03-26 20:56:19.004809 Epoch 100, Training Loss 0.5627382965496434\n",
      "2022-03-26 20:56:19.018829 Epoch 100, Training Loss 0.5634121255344137\n",
      "2022-03-26 20:56:19.032858 Epoch 100, Training Loss 0.5643498052264113\n",
      "2022-03-26 20:56:19.046872 Epoch 100, Training Loss 0.5655256967867732\n",
      "2022-03-26 20:56:19.061809 Epoch 100, Training Loss 0.5665110932744067\n",
      "2022-03-26 20:56:19.075823 Epoch 100, Training Loss 0.5672460084071245\n",
      "2022-03-26 20:56:19.089844 Epoch 100, Training Loss 0.5678921613242011\n",
      "2022-03-26 20:56:19.103874 Epoch 100, Training Loss 0.5688566914604752\n",
      "2022-03-26 20:56:19.117822 Epoch 100, Training Loss 0.5698408535526841\n",
      "2022-03-26 20:56:19.132214 Epoch 100, Training Loss 0.5707813121016373\n",
      "2022-03-26 20:56:19.147255 Epoch 100, Training Loss 0.5716635688491489\n",
      "2022-03-26 20:56:19.161284 Epoch 100, Training Loss 0.5727154947912602\n",
      "2022-03-26 20:56:19.175325 Epoch 100, Training Loss 0.5734650240377392\n",
      "2022-03-26 20:56:19.190306 Epoch 100, Training Loss 0.5743202652467791\n",
      "2022-03-26 20:56:19.204375 Epoch 100, Training Loss 0.5751137541383123\n",
      "2022-03-26 20:56:19.218427 Epoch 100, Training Loss 0.5760135312214532\n",
      "2022-03-26 20:56:19.232553 Epoch 100, Training Loss 0.577017780536276\n",
      "2022-03-26 20:56:19.246573 Epoch 100, Training Loss 0.5777544953176738\n",
      "2022-03-26 20:56:19.261602 Epoch 100, Training Loss 0.5787196865929362\n",
      "2022-03-26 20:56:19.276602 Epoch 100, Training Loss 0.5793152552507722\n",
      "2022-03-26 20:56:19.290616 Epoch 100, Training Loss 0.5802101093866027\n",
      "2022-03-26 20:56:19.304630 Epoch 100, Training Loss 0.5813090149932505\n",
      "2022-03-26 20:56:19.319647 Epoch 100, Training Loss 0.5820964729160909\n",
      "2022-03-26 20:56:19.333249 Epoch 100, Training Loss 0.5831886111851543\n",
      "2022-03-26 20:56:19.348326 Epoch 100, Training Loss 0.5842039431528667\n",
      "2022-03-26 20:56:19.362293 Epoch 100, Training Loss 0.5852271230400675\n",
      "2022-03-26 20:56:19.377349 Epoch 100, Training Loss 0.5860367900956317\n",
      "2022-03-26 20:56:19.391319 Epoch 100, Training Loss 0.587090452118298\n",
      "2022-03-26 20:56:19.406338 Epoch 100, Training Loss 0.5879108743060886\n",
      "2022-03-26 20:56:19.419421 Epoch 100, Training Loss 0.5890427221498831\n",
      "2022-03-26 20:56:19.434345 Epoch 100, Training Loss 0.5898305757347581\n",
      "2022-03-26 20:56:19.448331 Epoch 100, Training Loss 0.590882207464684\n",
      "2022-03-26 20:56:19.462429 Epoch 100, Training Loss 0.5916540552969174\n",
      "2022-03-26 20:56:19.476541 Epoch 100, Training Loss 0.5924262815652905\n",
      "2022-03-26 20:56:19.491561 Epoch 100, Training Loss 0.5933778560756112\n",
      "2022-03-26 20:56:19.505533 Epoch 100, Training Loss 0.5941358260105333\n",
      "2022-03-26 20:56:19.519554 Epoch 100, Training Loss 0.595119233440865\n",
      "2022-03-26 20:56:19.533568 Epoch 100, Training Loss 0.5959547208169537\n",
      "2022-03-26 20:56:19.547526 Epoch 100, Training Loss 0.5968378492252296\n",
      "2022-03-26 20:56:19.561547 Epoch 100, Training Loss 0.5976433053117274\n",
      "2022-03-26 20:56:19.577000 Epoch 100, Training Loss 0.5983801207807667\n",
      "2022-03-26 20:56:19.591004 Epoch 100, Training Loss 0.5991841447551537\n",
      "2022-03-26 20:56:19.605033 Epoch 100, Training Loss 0.6003507180973087\n",
      "2022-03-26 20:56:19.620063 Epoch 100, Training Loss 0.60131938633559\n",
      "2022-03-26 20:56:19.634077 Epoch 100, Training Loss 0.6022268478446604\n",
      "2022-03-26 20:56:19.648103 Epoch 100, Training Loss 0.6029910933407371\n",
      "2022-03-26 20:56:19.663150 Epoch 100, Training Loss 0.6037930541712305\n",
      "2022-03-26 20:56:19.677273 Epoch 100, Training Loss 0.6045664644530971\n",
      "2022-03-26 20:56:19.691286 Epoch 100, Training Loss 0.6054555265723592\n",
      "2022-03-26 20:56:19.705334 Epoch 100, Training Loss 0.6063814819849971\n",
      "2022-03-26 20:56:19.720362 Epoch 100, Training Loss 0.6078166499390931\n",
      "2022-03-26 20:56:19.734356 Epoch 100, Training Loss 0.6090146634356141\n",
      "2022-03-26 20:56:19.748434 Epoch 100, Training Loss 0.6096172430326262\n",
      "2022-03-26 20:56:19.762456 Epoch 100, Training Loss 0.6105217187453413\n",
      "2022-03-26 20:56:19.777417 Epoch 100, Training Loss 0.6113154646533224\n",
      "2022-03-26 20:56:19.791439 Epoch 100, Training Loss 0.6120037286330366\n",
      "2022-03-26 20:56:19.806444 Epoch 100, Training Loss 0.6129098261713677\n",
      "2022-03-26 20:56:19.820472 Epoch 100, Training Loss 0.6141139093567344\n",
      "2022-03-26 20:56:19.835072 Epoch 100, Training Loss 0.6152391609785807\n",
      "2022-03-26 20:56:19.849086 Epoch 100, Training Loss 0.6161227334490822\n",
      "2022-03-26 20:56:19.863163 Epoch 100, Training Loss 0.6169096156764213\n",
      "2022-03-26 20:56:19.877172 Epoch 100, Training Loss 0.6177198037009715\n",
      "2022-03-26 20:56:19.891074 Epoch 100, Training Loss 0.6189757490249546\n",
      "2022-03-26 20:56:19.906125 Epoch 100, Training Loss 0.6198980295292252\n",
      "2022-03-26 20:56:19.920154 Epoch 100, Training Loss 0.6209615992615595\n",
      "2022-03-26 20:56:19.934770 Epoch 100, Training Loss 0.6221237499695604\n",
      "2022-03-26 20:56:19.948773 Epoch 100, Training Loss 0.622877160225378\n",
      "2022-03-26 20:56:19.962780 Epoch 100, Training Loss 0.6240185410775188\n",
      "2022-03-26 20:56:19.977791 Epoch 100, Training Loss 0.6248718768434451\n",
      "2022-03-26 20:56:19.991758 Epoch 100, Training Loss 0.6257659178560652\n",
      "2022-03-26 20:56:20.005770 Epoch 100, Training Loss 0.626547666233214\n",
      "2022-03-26 20:56:20.020130 Epoch 100, Training Loss 0.6273614779457717\n",
      "2022-03-26 20:56:20.034691 Epoch 100, Training Loss 0.6283049298369366\n",
      "2022-03-26 20:56:20.048694 Epoch 100, Training Loss 0.6291039278135275\n",
      "2022-03-26 20:56:20.062588 Epoch 100, Training Loss 0.6302150096125005\n",
      "2022-03-26 20:56:20.076606 Epoch 100, Training Loss 0.6310744723090735\n",
      "2022-03-26 20:56:20.091557 Epoch 100, Training Loss 0.6319880613585567\n",
      "2022-03-26 20:56:20.105570 Epoch 100, Training Loss 0.633141984872501\n",
      "2022-03-26 20:56:20.119584 Epoch 100, Training Loss 0.6341534067145393\n",
      "2022-03-26 20:56:20.133889 Epoch 100, Training Loss 0.6348378611800006\n",
      "2022-03-26 20:56:20.148135 Epoch 100, Training Loss 0.6355819433851315\n",
      "2022-03-26 20:56:20.163083 Epoch 100, Training Loss 0.6362550052078179\n",
      "2022-03-26 20:56:20.177105 Epoch 100, Training Loss 0.637270054091578\n",
      "2022-03-26 20:56:20.191021 Epoch 100, Training Loss 0.6381547244460991\n",
      "2022-03-26 20:56:20.206103 Epoch 100, Training Loss 0.6391567590139101\n",
      "2022-03-26 20:56:20.220037 Epoch 100, Training Loss 0.6403013678920239\n",
      "2022-03-26 20:56:20.233758 Epoch 100, Training Loss 0.6410800298613966\n",
      "2022-03-26 20:56:20.252106 Epoch 100, Training Loss 0.6418448848187771\n",
      "2022-03-26 20:56:20.271039 Epoch 100, Training Loss 0.642676366183459\n",
      "2022-03-26 20:56:20.289770 Epoch 100, Training Loss 0.643492493025787\n",
      "2022-03-26 20:56:20.308565 Epoch 100, Training Loss 0.6445441951074868\n",
      "2022-03-26 20:56:20.326523 Epoch 100, Training Loss 0.6454954054349523\n",
      "2022-03-26 20:56:20.346394 Epoch 100, Training Loss 0.6461957357728573\n",
      "2022-03-26 20:56:20.365365 Epoch 100, Training Loss 0.647213756504571\n",
      "2022-03-26 20:56:20.384462 Epoch 100, Training Loss 0.648283288683123\n",
      "2022-03-26 20:56:20.404421 Epoch 100, Training Loss 0.6490654914123019\n",
      "2022-03-26 20:56:20.424537 Epoch 100, Training Loss 0.6498047193450391\n",
      "2022-03-26 20:56:20.443456 Epoch 100, Training Loss 0.6507630644704375\n",
      "2022-03-26 20:56:20.462460 Epoch 100, Training Loss 0.6516711199679948\n",
      "2022-03-26 20:56:20.481677 Epoch 100, Training Loss 0.6529574810391496\n",
      "2022-03-26 20:56:20.500685 Epoch 100, Training Loss 0.6539740363502746\n",
      "2022-03-26 20:56:20.519583 Epoch 100, Training Loss 0.6551278479721235\n",
      "2022-03-26 20:56:20.539605 Epoch 100, Training Loss 0.6561500167907657\n",
      "2022-03-26 20:56:20.558902 Epoch 100, Training Loss 0.6567786278779549\n",
      "2022-03-26 20:56:20.577897 Epoch 100, Training Loss 0.6574240657679565\n",
      "2022-03-26 20:56:20.597896 Epoch 100, Training Loss 0.6583954744479236\n",
      "2022-03-26 20:56:20.616913 Epoch 100, Training Loss 0.6591593583526514\n",
      "2022-03-26 20:56:20.636087 Epoch 100, Training Loss 0.6601508595906865\n",
      "2022-03-26 20:56:20.655137 Epoch 100, Training Loss 0.660970909821103\n",
      "2022-03-26 20:56:20.674191 Epoch 100, Training Loss 0.6618304803700703\n",
      "2022-03-26 20:56:20.692322 Epoch 100, Training Loss 0.6627636482496091\n",
      "2022-03-26 20:56:20.711399 Epoch 100, Training Loss 0.6636688613220859\n",
      "2022-03-26 20:56:20.731440 Epoch 100, Training Loss 0.6645451412938744\n",
      "2022-03-26 20:56:20.749474 Epoch 100, Training Loss 0.665514104430328\n",
      "2022-03-26 20:56:20.768479 Epoch 100, Training Loss 0.6666814689441105\n",
      "2022-03-26 20:56:20.787519 Epoch 100, Training Loss 0.6676615816552925\n",
      "2022-03-26 20:56:20.806577 Epoch 100, Training Loss 0.6687300145016302\n",
      "2022-03-26 20:56:20.826538 Epoch 100, Training Loss 0.6697554596702157\n",
      "2022-03-26 20:56:20.845459 Epoch 100, Training Loss 0.6705186821310721\n",
      "2022-03-26 20:56:20.865412 Epoch 100, Training Loss 0.6713311136378657\n",
      "2022-03-26 20:56:20.883465 Epoch 100, Training Loss 0.6721633292372574\n",
      "2022-03-26 20:56:20.902461 Epoch 100, Training Loss 0.6729971726837061\n",
      "2022-03-26 20:56:20.921470 Epoch 100, Training Loss 0.6742417304717061\n",
      "2022-03-26 20:56:20.940436 Epoch 100, Training Loss 0.6751997085940807\n",
      "2022-03-26 20:56:20.958301 Epoch 100, Training Loss 0.6766282229319863\n",
      "2022-03-26 20:56:20.978322 Epoch 100, Training Loss 0.6776305035404537\n",
      "2022-03-26 20:56:20.996968 Epoch 100, Training Loss 0.6785850153707177\n",
      "2022-03-26 20:56:21.016978 Epoch 100, Training Loss 0.6796749428562496\n",
      "2022-03-26 20:56:21.036572 Epoch 100, Training Loss 0.6803309001276255\n",
      "2022-03-26 20:56:21.056591 Epoch 100, Training Loss 0.6811316728287036\n",
      "2022-03-26 20:56:21.075560 Epoch 100, Training Loss 0.681919633503765\n",
      "2022-03-26 20:56:21.094607 Epoch 100, Training Loss 0.6827819800132986\n",
      "2022-03-26 20:56:21.113646 Epoch 100, Training Loss 0.6837849024768985\n",
      "2022-03-26 20:56:21.133003 Epoch 100, Training Loss 0.6846995588458712\n",
      "2022-03-26 20:56:21.152409 Epoch 100, Training Loss 0.6858380808854652\n",
      "2022-03-26 20:56:21.172410 Epoch 100, Training Loss 0.6867476410573096\n",
      "2022-03-26 20:56:21.192376 Epoch 100, Training Loss 0.6876310552172649\n",
      "2022-03-26 20:56:21.211576 Epoch 100, Training Loss 0.6885163601859451\n",
      "2022-03-26 20:56:21.230658 Epoch 100, Training Loss 0.6893155173877316\n",
      "2022-03-26 20:56:21.249680 Epoch 100, Training Loss 0.6901376417378331\n",
      "2022-03-26 20:56:21.267702 Epoch 100, Training Loss 0.6908846016582626\n",
      "2022-03-26 20:56:21.287015 Epoch 100, Training Loss 0.6916203132218413\n",
      "2022-03-26 20:56:21.306062 Epoch 100, Training Loss 0.6923514500146022\n",
      "2022-03-26 20:56:21.325150 Epoch 100, Training Loss 0.6933006930839071\n",
      "2022-03-26 20:56:21.345075 Epoch 100, Training Loss 0.6941177258101265\n",
      "2022-03-26 20:56:21.364145 Epoch 100, Training Loss 0.6949772830204586\n",
      "2022-03-26 20:56:21.383191 Epoch 100, Training Loss 0.6956722465012689\n",
      "2022-03-26 20:56:21.402193 Epoch 100, Training Loss 0.696622820995043\n",
      "2022-03-26 20:56:21.422243 Epoch 100, Training Loss 0.6973308718113034\n",
      "2022-03-26 20:56:21.442218 Epoch 100, Training Loss 0.6980723119757669\n",
      "2022-03-26 20:56:21.461698 Epoch 100, Training Loss 0.6987009195949111\n",
      "2022-03-26 20:56:21.480325 Epoch 100, Training Loss 0.6997484552966969\n",
      "2022-03-26 20:56:21.499076 Epoch 100, Training Loss 0.7007942136824893\n",
      "2022-03-26 20:56:21.518137 Epoch 100, Training Loss 0.7017580901494112\n",
      "2022-03-26 20:56:21.537133 Epoch 100, Training Loss 0.7027025618745238\n",
      "2022-03-26 20:56:21.556238 Epoch 100, Training Loss 0.7035141410044087\n",
      "2022-03-26 20:56:21.564018 Epoch 100, Training Loss 0.7042097348310149\n",
      "2022-03-26 21:07:43.639850 Epoch 150, Training Loss 0.0009702466180562363\n",
      "2022-03-26 21:07:43.653853 Epoch 150, Training Loss 0.0019053361757332102\n",
      "2022-03-26 21:07:43.667857 Epoch 150, Training Loss 0.002620357000614371\n",
      "2022-03-26 21:07:43.682860 Epoch 150, Training Loss 0.0033663702590386274\n",
      "2022-03-26 21:07:43.696863 Epoch 150, Training Loss 0.004127359222573087\n",
      "2022-03-26 21:07:43.710867 Epoch 150, Training Loss 0.004842833027510387\n",
      "2022-03-26 21:07:43.724871 Epoch 150, Training Loss 0.00571759986450605\n",
      "2022-03-26 21:07:43.741470 Epoch 150, Training Loss 0.006385225392973331\n",
      "2022-03-26 21:07:43.755400 Epoch 150, Training Loss 0.006968892596261885\n",
      "2022-03-26 21:07:43.769406 Epoch 150, Training Loss 0.0076547391579279206\n",
      "2022-03-26 21:07:43.786284 Epoch 150, Training Loss 0.008512314628152287\n",
      "2022-03-26 21:07:43.801176 Epoch 150, Training Loss 0.009306603532922847\n",
      "2022-03-26 21:07:43.817185 Epoch 150, Training Loss 0.010008483515371142\n",
      "2022-03-26 21:07:43.833194 Epoch 150, Training Loss 0.010544892314754788\n",
      "2022-03-26 21:07:43.849067 Epoch 150, Training Loss 0.011252507757957634\n",
      "2022-03-26 21:07:43.863074 Epoch 150, Training Loss 0.01221327853324773\n",
      "2022-03-26 21:07:43.876957 Epoch 150, Training Loss 0.013270964631644051\n",
      "2022-03-26 21:07:43.890987 Epoch 150, Training Loss 0.013990163421996719\n",
      "2022-03-26 21:07:43.906009 Epoch 150, Training Loss 0.01481546861741244\n",
      "2022-03-26 21:07:43.920718 Epoch 150, Training Loss 0.015403951868376769\n",
      "2022-03-26 21:07:43.935686 Epoch 150, Training Loss 0.016118802111167126\n",
      "2022-03-26 21:07:43.950651 Epoch 150, Training Loss 0.017058226725329525\n",
      "2022-03-26 21:07:43.964556 Epoch 150, Training Loss 0.01786693073142215\n",
      "2022-03-26 21:07:43.979663 Epoch 150, Training Loss 0.018736826146350187\n",
      "2022-03-26 21:07:43.994690 Epoch 150, Training Loss 0.019246598643719998\n",
      "2022-03-26 21:07:44.008533 Epoch 150, Training Loss 0.01992781929042943\n",
      "2022-03-26 21:07:44.023536 Epoch 150, Training Loss 0.020940288024790147\n",
      "2022-03-26 21:07:44.038303 Epoch 150, Training Loss 0.02160237283657884\n",
      "2022-03-26 21:07:44.052306 Epoch 150, Training Loss 0.02240649315402331\n",
      "2022-03-26 21:07:44.066446 Epoch 150, Training Loss 0.0234036576717406\n",
      "2022-03-26 21:07:44.080427 Epoch 150, Training Loss 0.024009777152019997\n",
      "2022-03-26 21:07:44.094450 Epoch 150, Training Loss 0.024571424516875422\n",
      "2022-03-26 21:07:44.110464 Epoch 150, Training Loss 0.025483767737817888\n",
      "2022-03-26 21:07:44.124684 Epoch 150, Training Loss 0.02634659596263905\n",
      "2022-03-26 21:07:44.139618 Epoch 150, Training Loss 0.02689726075248035\n",
      "2022-03-26 21:07:44.155665 Epoch 150, Training Loss 0.027553339214885935\n",
      "2022-03-26 21:07:44.169564 Epoch 150, Training Loss 0.028603245306502826\n",
      "2022-03-26 21:07:44.183566 Epoch 150, Training Loss 0.02935019638532263\n",
      "2022-03-26 21:07:44.198579 Epoch 150, Training Loss 0.030360301437280367\n",
      "2022-03-26 21:07:44.212607 Epoch 150, Training Loss 0.03094468084747529\n",
      "2022-03-26 21:07:44.226435 Epoch 150, Training Loss 0.031765807391432546\n",
      "2022-03-26 21:07:44.241439 Epoch 150, Training Loss 0.0327755173911219\n",
      "2022-03-26 21:07:44.256301 Epoch 150, Training Loss 0.03372035932053081\n",
      "2022-03-26 21:07:44.270840 Epoch 150, Training Loss 0.03453667008358499\n",
      "2022-03-26 21:07:44.284850 Epoch 150, Training Loss 0.03536411647296622\n",
      "2022-03-26 21:07:44.299742 Epoch 150, Training Loss 0.03630511382656634\n",
      "2022-03-26 21:07:44.318674 Epoch 150, Training Loss 0.03681489634696785\n",
      "2022-03-26 21:07:44.337618 Epoch 150, Training Loss 0.03739691908707094\n",
      "2022-03-26 21:07:44.356485 Epoch 150, Training Loss 0.038385174463472095\n",
      "2022-03-26 21:07:44.375497 Epoch 150, Training Loss 0.03925018862385274\n",
      "2022-03-26 21:07:44.394665 Epoch 150, Training Loss 0.039959269868748266\n",
      "2022-03-26 21:07:44.413927 Epoch 150, Training Loss 0.041056056919000335\n",
      "2022-03-26 21:07:44.432754 Epoch 150, Training Loss 0.04177962506518645\n",
      "2022-03-26 21:07:44.452435 Epoch 150, Training Loss 0.04257594106142478\n",
      "2022-03-26 21:07:44.471451 Epoch 150, Training Loss 0.04344263848136453\n",
      "2022-03-26 21:07:44.490462 Epoch 150, Training Loss 0.04428366825098882\n",
      "2022-03-26 21:07:44.510431 Epoch 150, Training Loss 0.044988318020120606\n",
      "2022-03-26 21:07:44.529452 Epoch 150, Training Loss 0.04579786922010924\n",
      "2022-03-26 21:07:44.548148 Epoch 150, Training Loss 0.046733105045450314\n",
      "2022-03-26 21:07:44.567231 Epoch 150, Training Loss 0.047512676252428526\n",
      "2022-03-26 21:07:44.585816 Epoch 150, Training Loss 0.048176498211863095\n",
      "2022-03-26 21:07:44.605964 Epoch 150, Training Loss 0.04894688214792315\n",
      "2022-03-26 21:07:44.625161 Epoch 150, Training Loss 0.04964208465707881\n",
      "2022-03-26 21:07:44.645311 Epoch 150, Training Loss 0.05019604287031666\n",
      "2022-03-26 21:07:44.664580 Epoch 150, Training Loss 0.05106320904801264\n",
      "2022-03-26 21:07:44.684092 Epoch 150, Training Loss 0.052013087722346606\n",
      "2022-03-26 21:07:44.703932 Epoch 150, Training Loss 0.052787393522079645\n",
      "2022-03-26 21:07:44.722967 Epoch 150, Training Loss 0.05341415240636567\n",
      "2022-03-26 21:07:44.741147 Epoch 150, Training Loss 0.05427770754870247\n",
      "2022-03-26 21:07:44.760193 Epoch 150, Training Loss 0.055423019639671306\n",
      "2022-03-26 21:07:44.780145 Epoch 150, Training Loss 0.05608707262426996\n",
      "2022-03-26 21:07:44.799177 Epoch 150, Training Loss 0.056898495577790247\n",
      "2022-03-26 21:07:44.817249 Epoch 150, Training Loss 0.05772071436542989\n",
      "2022-03-26 21:07:44.836225 Epoch 150, Training Loss 0.0585044910535788\n",
      "2022-03-26 21:07:44.856347 Epoch 150, Training Loss 0.05918928226241675\n",
      "2022-03-26 21:07:44.875324 Epoch 150, Training Loss 0.059857654068476096\n",
      "2022-03-26 21:07:44.894447 Epoch 150, Training Loss 0.06062269630029683\n",
      "2022-03-26 21:07:44.914469 Epoch 150, Training Loss 0.06162119765415826\n",
      "2022-03-26 21:07:44.932511 Epoch 150, Training Loss 0.062222982306614556\n",
      "2022-03-26 21:07:44.953078 Epoch 150, Training Loss 0.06309776469264798\n",
      "2022-03-26 21:07:44.972150 Epoch 150, Training Loss 0.06381203962104096\n",
      "2022-03-26 21:07:44.991895 Epoch 150, Training Loss 0.06445179479506315\n",
      "2022-03-26 21:07:45.010797 Epoch 150, Training Loss 0.06507070983767205\n",
      "2022-03-26 21:07:45.029727 Epoch 150, Training Loss 0.06588006446428617\n",
      "2022-03-26 21:07:45.048812 Epoch 150, Training Loss 0.06660691673493446\n",
      "2022-03-26 21:07:45.067809 Epoch 150, Training Loss 0.06780779422701472\n",
      "2022-03-26 21:07:45.087914 Epoch 150, Training Loss 0.0686748642140947\n",
      "2022-03-26 21:07:45.107909 Epoch 150, Training Loss 0.06964146252483358\n",
      "2022-03-26 21:07:45.127621 Epoch 150, Training Loss 0.07025837620048572\n",
      "2022-03-26 21:07:45.147663 Epoch 150, Training Loss 0.07126401098030608\n",
      "2022-03-26 21:07:45.166773 Epoch 150, Training Loss 0.07197136940706111\n",
      "2022-03-26 21:07:45.186801 Epoch 150, Training Loss 0.07276076768212916\n",
      "2022-03-26 21:07:45.205796 Epoch 150, Training Loss 0.07350586773946767\n",
      "2022-03-26 21:07:45.225646 Epoch 150, Training Loss 0.07425423515269823\n",
      "2022-03-26 21:07:45.244536 Epoch 150, Training Loss 0.07508457130026025\n",
      "2022-03-26 21:07:45.263575 Epoch 150, Training Loss 0.07590458753621182\n",
      "2022-03-26 21:07:45.282488 Epoch 150, Training Loss 0.07684497832489745\n",
      "2022-03-26 21:07:45.301510 Epoch 150, Training Loss 0.07765957968466727\n",
      "2022-03-26 21:07:45.320539 Epoch 150, Training Loss 0.0785710700713765\n",
      "2022-03-26 21:07:45.339444 Epoch 150, Training Loss 0.07939238343244928\n",
      "2022-03-26 21:07:45.358412 Epoch 150, Training Loss 0.08029782379527226\n",
      "2022-03-26 21:07:45.378301 Epoch 150, Training Loss 0.08108990755684846\n",
      "2022-03-26 21:07:45.396337 Epoch 150, Training Loss 0.08185970184900572\n",
      "2022-03-26 21:07:45.416385 Epoch 150, Training Loss 0.08264151585224035\n",
      "2022-03-26 21:07:45.435372 Epoch 150, Training Loss 0.0833736647806509\n",
      "2022-03-26 21:07:45.455308 Epoch 150, Training Loss 0.0842802408329971\n",
      "2022-03-26 21:07:45.474356 Epoch 150, Training Loss 0.084917273743988\n",
      "2022-03-26 21:07:45.494448 Epoch 150, Training Loss 0.08576197499204474\n",
      "2022-03-26 21:07:45.513656 Epoch 150, Training Loss 0.08642934358028499\n",
      "2022-03-26 21:07:45.532664 Epoch 150, Training Loss 0.08721018211006204\n",
      "2022-03-26 21:07:45.551700 Epoch 150, Training Loss 0.0878108817979198\n",
      "2022-03-26 21:07:45.571723 Epoch 150, Training Loss 0.08864853338664755\n",
      "2022-03-26 21:07:45.589644 Epoch 150, Training Loss 0.0896920018717456\n",
      "2022-03-26 21:07:45.609678 Epoch 150, Training Loss 0.09066954533309887\n",
      "2022-03-26 21:07:45.628986 Epoch 150, Training Loss 0.0915885075660008\n",
      "2022-03-26 21:07:45.647122 Epoch 150, Training Loss 0.0923711983367915\n",
      "2022-03-26 21:07:45.666176 Epoch 150, Training Loss 0.09325908879032525\n",
      "2022-03-26 21:07:45.685214 Epoch 150, Training Loss 0.09411241685795357\n",
      "2022-03-26 21:07:45.705278 Epoch 150, Training Loss 0.0949801687922929\n",
      "2022-03-26 21:07:45.725257 Epoch 150, Training Loss 0.0957231983123228\n",
      "2022-03-26 21:07:45.744283 Epoch 150, Training Loss 0.09661614327022182\n",
      "2022-03-26 21:07:45.764268 Epoch 150, Training Loss 0.09750986385071064\n",
      "2022-03-26 21:07:45.783249 Epoch 150, Training Loss 0.09828084116549138\n",
      "2022-03-26 21:07:45.802268 Epoch 150, Training Loss 0.09874289987794578\n",
      "2022-03-26 21:07:45.822344 Epoch 150, Training Loss 0.09956211892082868\n",
      "2022-03-26 21:07:45.842066 Epoch 150, Training Loss 0.10058064080412736\n",
      "2022-03-26 21:07:45.861785 Epoch 150, Training Loss 0.10119193098734101\n",
      "2022-03-26 21:07:45.880343 Epoch 150, Training Loss 0.102018286428793\n",
      "2022-03-26 21:07:45.900323 Epoch 150, Training Loss 0.1026849396088544\n",
      "2022-03-26 21:07:45.919251 Epoch 150, Training Loss 0.10334817695495722\n",
      "2022-03-26 21:07:45.939928 Epoch 150, Training Loss 0.10438330841186406\n",
      "2022-03-26 21:07:45.958912 Epoch 150, Training Loss 0.10513297889543616\n",
      "2022-03-26 21:07:45.977474 Epoch 150, Training Loss 0.10585417665179124\n",
      "2022-03-26 21:07:45.996486 Epoch 150, Training Loss 0.10664584561991874\n",
      "2022-03-26 21:07:46.015494 Epoch 150, Training Loss 0.10731062697022772\n",
      "2022-03-26 21:07:46.035485 Epoch 150, Training Loss 0.10791733963867588\n",
      "2022-03-26 21:07:46.053483 Epoch 150, Training Loss 0.10854799900670796\n",
      "2022-03-26 21:07:46.073575 Epoch 150, Training Loss 0.10932413707761203\n",
      "2022-03-26 21:07:46.093594 Epoch 150, Training Loss 0.10996364895492564\n",
      "2022-03-26 21:07:46.112629 Epoch 150, Training Loss 0.11078969882729718\n",
      "2022-03-26 21:07:46.132468 Epoch 150, Training Loss 0.11169138059134373\n",
      "2022-03-26 21:07:46.152497 Epoch 150, Training Loss 0.11233922103634271\n",
      "2022-03-26 21:07:46.170756 Epoch 150, Training Loss 0.11307595586380385\n",
      "2022-03-26 21:07:46.190617 Epoch 150, Training Loss 0.11387582618714598\n",
      "2022-03-26 21:07:46.209623 Epoch 150, Training Loss 0.11470208132206022\n",
      "2022-03-26 21:07:46.229974 Epoch 150, Training Loss 0.11525624643658738\n",
      "2022-03-26 21:07:46.248315 Epoch 150, Training Loss 0.11623029433705313\n",
      "2022-03-26 21:07:46.268343 Epoch 150, Training Loss 0.1170042494450079\n",
      "2022-03-26 21:07:46.287470 Epoch 150, Training Loss 0.1178728904184478\n",
      "2022-03-26 21:07:46.306493 Epoch 150, Training Loss 0.11897855589304433\n",
      "2022-03-26 21:07:46.325328 Epoch 150, Training Loss 0.11968355223803265\n",
      "2022-03-26 21:07:46.345305 Epoch 150, Training Loss 0.12048071306532301\n",
      "2022-03-26 21:07:46.364336 Epoch 150, Training Loss 0.12147964438056702\n",
      "2022-03-26 21:07:46.384177 Epoch 150, Training Loss 0.12242029622540145\n",
      "2022-03-26 21:07:46.403516 Epoch 150, Training Loss 0.12335834665524076\n",
      "2022-03-26 21:07:46.422547 Epoch 150, Training Loss 0.12413275497191398\n",
      "2022-03-26 21:07:46.441571 Epoch 150, Training Loss 0.12498805357519623\n",
      "2022-03-26 21:07:46.460593 Epoch 150, Training Loss 0.12578296002067263\n",
      "2022-03-26 21:07:46.480554 Epoch 150, Training Loss 0.12655037142279202\n",
      "2022-03-26 21:07:46.499568 Epoch 150, Training Loss 0.1272320345310909\n",
      "2022-03-26 21:07:46.519086 Epoch 150, Training Loss 0.1283096064768179\n",
      "2022-03-26 21:07:46.538050 Epoch 150, Training Loss 0.1292332969129543\n",
      "2022-03-26 21:07:46.557194 Epoch 150, Training Loss 0.1299786182391979\n",
      "2022-03-26 21:07:46.576484 Epoch 150, Training Loss 0.13081460333693667\n",
      "2022-03-26 21:07:46.596331 Epoch 150, Training Loss 0.13162674794873924\n",
      "2022-03-26 21:07:46.615327 Epoch 150, Training Loss 0.13211747973471347\n",
      "2022-03-26 21:07:46.634856 Epoch 150, Training Loss 0.13283622051443894\n",
      "2022-03-26 21:07:46.654786 Epoch 150, Training Loss 0.13380465696534843\n",
      "2022-03-26 21:07:46.673989 Epoch 150, Training Loss 0.13442443726617662\n",
      "2022-03-26 21:07:46.693539 Epoch 150, Training Loss 0.13559025434581826\n",
      "2022-03-26 21:07:46.712583 Epoch 150, Training Loss 0.13655845733249888\n",
      "2022-03-26 21:07:46.731469 Epoch 150, Training Loss 0.13725740609266568\n",
      "2022-03-26 21:07:46.751491 Epoch 150, Training Loss 0.13815758150556814\n",
      "2022-03-26 21:07:46.771467 Epoch 150, Training Loss 0.1393421792313266\n",
      "2022-03-26 21:07:46.790509 Epoch 150, Training Loss 0.13989652113993759\n",
      "2022-03-26 21:07:46.810526 Epoch 150, Training Loss 0.14062469980448408\n",
      "2022-03-26 21:07:46.830547 Epoch 150, Training Loss 0.14128807427175819\n",
      "2022-03-26 21:07:46.848974 Epoch 150, Training Loss 0.14207899589520281\n",
      "2022-03-26 21:07:46.868918 Epoch 150, Training Loss 0.14285574129322912\n",
      "2022-03-26 21:07:46.888095 Epoch 150, Training Loss 0.14365826752941932\n",
      "2022-03-26 21:07:46.908164 Epoch 150, Training Loss 0.14477491176799132\n",
      "2022-03-26 21:07:46.927862 Epoch 150, Training Loss 0.14546859443492596\n",
      "2022-03-26 21:07:46.946931 Epoch 150, Training Loss 0.14617140065221226\n",
      "2022-03-26 21:07:46.967053 Epoch 150, Training Loss 0.14695661074822516\n",
      "2022-03-26 21:07:46.986073 Epoch 150, Training Loss 0.14772506858534215\n",
      "2022-03-26 21:07:47.006216 Epoch 150, Training Loss 0.14848203979947072\n",
      "2022-03-26 21:07:47.025235 Epoch 150, Training Loss 0.14935983496401317\n",
      "2022-03-26 21:07:47.045387 Epoch 150, Training Loss 0.1502509620564673\n",
      "2022-03-26 21:07:47.064342 Epoch 150, Training Loss 0.15095306772862555\n",
      "2022-03-26 21:07:47.085446 Epoch 150, Training Loss 0.15166741278013\n",
      "2022-03-26 21:07:47.104476 Epoch 150, Training Loss 0.15243182779120668\n",
      "2022-03-26 21:07:47.125502 Epoch 150, Training Loss 0.15334942689179765\n",
      "2022-03-26 21:07:47.144483 Epoch 150, Training Loss 0.15415052619888958\n",
      "2022-03-26 21:07:47.164467 Epoch 150, Training Loss 0.15480767556316102\n",
      "2022-03-26 21:07:47.183434 Epoch 150, Training Loss 0.15587804632266158\n",
      "2022-03-26 21:07:47.203456 Epoch 150, Training Loss 0.1567815022014291\n",
      "2022-03-26 21:07:47.222577 Epoch 150, Training Loss 0.15741941085099564\n",
      "2022-03-26 21:07:47.242032 Epoch 150, Training Loss 0.1581961334590107\n",
      "2022-03-26 21:07:47.261080 Epoch 150, Training Loss 0.15910144051170105\n",
      "2022-03-26 21:07:47.280039 Epoch 150, Training Loss 0.15964441618803518\n",
      "2022-03-26 21:07:47.299646 Epoch 150, Training Loss 0.1604703364088712\n",
      "2022-03-26 21:07:47.319698 Epoch 150, Training Loss 0.16127372691241068\n",
      "2022-03-26 21:07:47.338917 Epoch 150, Training Loss 0.1620187965576606\n",
      "2022-03-26 21:07:47.358181 Epoch 150, Training Loss 0.16279367751934948\n",
      "2022-03-26 21:07:47.377967 Epoch 150, Training Loss 0.16331360986470567\n",
      "2022-03-26 21:07:47.398021 Epoch 150, Training Loss 0.163899874900613\n",
      "2022-03-26 21:07:47.417195 Epoch 150, Training Loss 0.16482432815424927\n",
      "2022-03-26 21:07:47.437022 Epoch 150, Training Loss 0.16599933540119843\n",
      "2022-03-26 21:07:47.456157 Epoch 150, Training Loss 0.16694157103748272\n",
      "2022-03-26 21:07:47.475207 Epoch 150, Training Loss 0.16766443185489197\n",
      "2022-03-26 21:07:47.494321 Epoch 150, Training Loss 0.168492757450894\n",
      "2022-03-26 21:07:47.513332 Epoch 150, Training Loss 0.16926134577797503\n",
      "2022-03-26 21:07:47.533169 Epoch 150, Training Loss 0.1699276023051318\n",
      "2022-03-26 21:07:47.552176 Epoch 150, Training Loss 0.17063432726103936\n",
      "2022-03-26 21:07:47.571768 Epoch 150, Training Loss 0.17151720604628248\n",
      "2022-03-26 21:07:47.590815 Epoch 150, Training Loss 0.17234048460755508\n",
      "2022-03-26 21:07:47.610832 Epoch 150, Training Loss 0.17299809404041455\n",
      "2022-03-26 21:07:47.629931 Epoch 150, Training Loss 0.17396942390810194\n",
      "2022-03-26 21:07:47.649785 Epoch 150, Training Loss 0.17456316486801332\n",
      "2022-03-26 21:07:47.669712 Epoch 150, Training Loss 0.17529280689518775\n",
      "2022-03-26 21:07:47.688732 Epoch 150, Training Loss 0.17603906070637276\n",
      "2022-03-26 21:07:47.707748 Epoch 150, Training Loss 0.17674447336922522\n",
      "2022-03-26 21:07:47.727860 Epoch 150, Training Loss 0.17736680813305214\n",
      "2022-03-26 21:07:47.746965 Epoch 150, Training Loss 0.17833870820834508\n",
      "2022-03-26 21:07:47.765984 Epoch 150, Training Loss 0.17901823949783355\n",
      "2022-03-26 21:07:47.786950 Epoch 150, Training Loss 0.17988811456181508\n",
      "2022-03-26 21:07:47.805993 Epoch 150, Training Loss 0.18068386317061647\n",
      "2022-03-26 21:07:47.826070 Epoch 150, Training Loss 0.18157334069309333\n",
      "2022-03-26 21:07:47.844051 Epoch 150, Training Loss 0.18209993583924325\n",
      "2022-03-26 21:07:47.863494 Epoch 150, Training Loss 0.18307741001591354\n",
      "2022-03-26 21:07:47.882502 Epoch 150, Training Loss 0.1838530523469076\n",
      "2022-03-26 21:07:47.902556 Epoch 150, Training Loss 0.18447247738271114\n",
      "2022-03-26 21:07:47.921585 Epoch 150, Training Loss 0.18514887211115463\n",
      "2022-03-26 21:07:47.941554 Epoch 150, Training Loss 0.18612969222733433\n",
      "2022-03-26 21:07:47.960570 Epoch 150, Training Loss 0.1871063913149602\n",
      "2022-03-26 21:07:47.979592 Epoch 150, Training Loss 0.18794075740725183\n",
      "2022-03-26 21:07:47.999634 Epoch 150, Training Loss 0.18867308210076578\n",
      "2022-03-26 21:07:48.019547 Epoch 150, Training Loss 0.18967864980630558\n",
      "2022-03-26 21:07:48.038465 Epoch 150, Training Loss 0.19057983727863684\n",
      "2022-03-26 21:07:48.058441 Epoch 150, Training Loss 0.19125032786975432\n",
      "2022-03-26 21:07:48.077911 Epoch 150, Training Loss 0.19193484956193763\n",
      "2022-03-26 21:07:48.097041 Epoch 150, Training Loss 0.1925923382610921\n",
      "2022-03-26 21:07:48.116787 Epoch 150, Training Loss 0.19331544260387226\n",
      "2022-03-26 21:07:48.137049 Epoch 150, Training Loss 0.19401652333529099\n",
      "2022-03-26 21:07:48.156107 Epoch 150, Training Loss 0.19512432977519073\n",
      "2022-03-26 21:07:48.175187 Epoch 150, Training Loss 0.19586258985654778\n",
      "2022-03-26 21:07:48.195340 Epoch 150, Training Loss 0.19661104621942085\n",
      "2022-03-26 21:07:48.214333 Epoch 150, Training Loss 0.19729501500611415\n",
      "2022-03-26 21:07:48.233347 Epoch 150, Training Loss 0.1979489211002579\n",
      "2022-03-26 21:07:48.252361 Epoch 150, Training Loss 0.1986831654139492\n",
      "2022-03-26 21:07:48.271389 Epoch 150, Training Loss 0.1993586852041352\n",
      "2022-03-26 21:07:48.291405 Epoch 150, Training Loss 0.20004760635935742\n",
      "2022-03-26 21:07:48.310468 Epoch 150, Training Loss 0.2007190598475049\n",
      "2022-03-26 21:07:48.329172 Epoch 150, Training Loss 0.20141434566596586\n",
      "2022-03-26 21:07:48.348323 Epoch 150, Training Loss 0.20244303547665285\n",
      "2022-03-26 21:07:48.367316 Epoch 150, Training Loss 0.20309044336876297\n",
      "2022-03-26 21:07:48.386760 Epoch 150, Training Loss 0.2037286496223391\n",
      "2022-03-26 21:07:48.406691 Epoch 150, Training Loss 0.2046484651467989\n",
      "2022-03-26 21:07:48.426540 Epoch 150, Training Loss 0.20538156271895483\n",
      "2022-03-26 21:07:48.445469 Epoch 150, Training Loss 0.2060350895385303\n",
      "2022-03-26 21:07:48.464441 Epoch 150, Training Loss 0.20675514017224617\n",
      "2022-03-26 21:07:48.484300 Epoch 150, Training Loss 0.20737482096685472\n",
      "2022-03-26 21:07:48.504266 Epoch 150, Training Loss 0.20821877791905952\n",
      "2022-03-26 21:07:48.523800 Epoch 150, Training Loss 0.2091304641931563\n",
      "2022-03-26 21:07:48.543551 Epoch 150, Training Loss 0.2098968584290551\n",
      "2022-03-26 21:07:48.563474 Epoch 150, Training Loss 0.21068752829528525\n",
      "2022-03-26 21:07:48.582300 Epoch 150, Training Loss 0.21177198533969158\n",
      "2022-03-26 21:07:48.603202 Epoch 150, Training Loss 0.2125968486070633\n",
      "2022-03-26 21:07:48.623190 Epoch 150, Training Loss 0.21366174259911414\n",
      "2022-03-26 21:07:48.643539 Epoch 150, Training Loss 0.21452769961046136\n",
      "2022-03-26 21:07:48.663418 Epoch 150, Training Loss 0.2153219791400768\n",
      "2022-03-26 21:07:48.684321 Epoch 150, Training Loss 0.2162093112764456\n",
      "2022-03-26 21:07:48.704322 Epoch 150, Training Loss 0.21711425399383927\n",
      "2022-03-26 21:07:48.723298 Epoch 150, Training Loss 0.21824292754730606\n",
      "2022-03-26 21:07:48.744706 Epoch 150, Training Loss 0.21907825722261462\n",
      "2022-03-26 21:07:48.763292 Epoch 150, Training Loss 0.21979021275287394\n",
      "2022-03-26 21:07:48.783296 Epoch 150, Training Loss 0.22083584232555936\n",
      "2022-03-26 21:07:48.803318 Epoch 150, Training Loss 0.22167037678953935\n",
      "2022-03-26 21:07:48.823267 Epoch 150, Training Loss 0.22231007014851437\n",
      "2022-03-26 21:07:48.843252 Epoch 150, Training Loss 0.22310241450891471\n",
      "2022-03-26 21:07:48.863185 Epoch 150, Training Loss 0.22419249428355176\n",
      "2022-03-26 21:07:48.883130 Epoch 150, Training Loss 0.22513366115215183\n",
      "2022-03-26 21:07:48.902408 Epoch 150, Training Loss 0.2259405831165631\n",
      "2022-03-26 21:07:48.922408 Epoch 150, Training Loss 0.22684915843979478\n",
      "2022-03-26 21:07:48.941997 Epoch 150, Training Loss 0.22765243872809593\n",
      "2022-03-26 21:07:48.962048 Epoch 150, Training Loss 0.22840521726614374\n",
      "2022-03-26 21:07:48.988231 Epoch 150, Training Loss 0.229373419033292\n",
      "2022-03-26 21:07:49.014206 Epoch 150, Training Loss 0.23012735574599116\n",
      "2022-03-26 21:07:49.042604 Epoch 150, Training Loss 0.23110782974363897\n",
      "2022-03-26 21:07:49.068980 Epoch 150, Training Loss 0.2319280433151728\n",
      "2022-03-26 21:07:49.094773 Epoch 150, Training Loss 0.2326388362499759\n",
      "2022-03-26 21:07:49.121103 Epoch 150, Training Loss 0.2334895855401788\n",
      "2022-03-26 21:07:49.147108 Epoch 150, Training Loss 0.23425980704977079\n",
      "2022-03-26 21:07:49.174097 Epoch 150, Training Loss 0.2351292059625811\n",
      "2022-03-26 21:07:49.199966 Epoch 150, Training Loss 0.23618125957448768\n",
      "2022-03-26 21:07:49.227069 Epoch 150, Training Loss 0.23728141367740338\n",
      "2022-03-26 21:07:49.252134 Epoch 150, Training Loss 0.2379218155846876\n",
      "2022-03-26 21:07:49.279086 Epoch 150, Training Loss 0.2387800089767217\n",
      "2022-03-26 21:07:49.305404 Epoch 150, Training Loss 0.23950505733032665\n",
      "2022-03-26 21:07:49.332143 Epoch 150, Training Loss 0.2400171000634313\n",
      "2022-03-26 21:07:49.357356 Epoch 150, Training Loss 0.24107309840524288\n",
      "2022-03-26 21:07:49.375454 Epoch 150, Training Loss 0.24178132209021722\n",
      "2022-03-26 21:07:49.393488 Epoch 150, Training Loss 0.24245686474663522\n",
      "2022-03-26 21:07:49.411488 Epoch 150, Training Loss 0.2430994881845801\n",
      "2022-03-26 21:07:49.429432 Epoch 150, Training Loss 0.24398259502237715\n",
      "2022-03-26 21:07:49.447481 Epoch 150, Training Loss 0.2448629792541494\n",
      "2022-03-26 21:07:49.466783 Epoch 150, Training Loss 0.24579620399438512\n",
      "2022-03-26 21:07:49.484798 Epoch 150, Training Loss 0.2467431883372919\n",
      "2022-03-26 21:07:49.503722 Epoch 150, Training Loss 0.24765183530805057\n",
      "2022-03-26 21:07:49.522202 Epoch 150, Training Loss 0.248938054608567\n",
      "2022-03-26 21:07:49.540707 Epoch 150, Training Loss 0.24971622693569154\n",
      "2022-03-26 21:07:49.559705 Epoch 150, Training Loss 0.2506222352194969\n",
      "2022-03-26 21:07:49.577740 Epoch 150, Training Loss 0.2514048631081496\n",
      "2022-03-26 21:07:49.595794 Epoch 150, Training Loss 0.2522850997765046\n",
      "2022-03-26 21:07:49.613753 Epoch 150, Training Loss 0.2533192521013567\n",
      "2022-03-26 21:07:49.631988 Epoch 150, Training Loss 0.25405772003676275\n",
      "2022-03-26 21:07:49.651482 Epoch 150, Training Loss 0.25479886065358703\n",
      "2022-03-26 21:07:49.670504 Epoch 150, Training Loss 0.25534327431103154\n",
      "2022-03-26 21:07:49.688519 Epoch 150, Training Loss 0.2562976913226535\n",
      "2022-03-26 21:07:49.708521 Epoch 150, Training Loss 0.25685300939070904\n",
      "2022-03-26 21:07:49.726549 Epoch 150, Training Loss 0.25765439620255814\n",
      "2022-03-26 21:07:49.745566 Epoch 150, Training Loss 0.2584269845958256\n",
      "2022-03-26 21:07:49.763606 Epoch 150, Training Loss 0.2590195292325886\n",
      "2022-03-26 21:07:49.782622 Epoch 150, Training Loss 0.25963475034974726\n",
      "2022-03-26 21:07:49.800651 Epoch 150, Training Loss 0.26059416271841435\n",
      "2022-03-26 21:07:49.819685 Epoch 150, Training Loss 0.2613381859286667\n",
      "2022-03-26 21:07:49.837553 Epoch 150, Training Loss 0.2621524979543808\n",
      "2022-03-26 21:07:49.856766 Epoch 150, Training Loss 0.2629927419640524\n",
      "2022-03-26 21:07:49.874803 Epoch 150, Training Loss 0.2639067778197091\n",
      "2022-03-26 21:07:49.893896 Epoch 150, Training Loss 0.2647786264681755\n",
      "2022-03-26 21:07:49.911896 Epoch 150, Training Loss 0.26549034495182966\n",
      "2022-03-26 21:07:49.929874 Epoch 150, Training Loss 0.26638677350395473\n",
      "2022-03-26 21:07:49.947643 Epoch 150, Training Loss 0.26710945748916975\n",
      "2022-03-26 21:07:49.965677 Epoch 150, Training Loss 0.26791020183612013\n",
      "2022-03-26 21:07:49.984537 Epoch 150, Training Loss 0.2687243401546917\n",
      "2022-03-26 21:07:50.002541 Epoch 150, Training Loss 0.269555503907411\n",
      "2022-03-26 21:07:50.021048 Epoch 150, Training Loss 0.2702880742604775\n",
      "2022-03-26 21:07:50.039650 Epoch 150, Training Loss 0.2710510005274087\n",
      "2022-03-26 21:07:50.058683 Epoch 150, Training Loss 0.2716700787586934\n",
      "2022-03-26 21:07:50.077430 Epoch 150, Training Loss 0.2722727628162755\n",
      "2022-03-26 21:07:50.094520 Epoch 150, Training Loss 0.27286636909408035\n",
      "2022-03-26 21:07:50.113439 Epoch 150, Training Loss 0.273628638033062\n",
      "2022-03-26 21:07:50.132454 Epoch 150, Training Loss 0.274374453955904\n",
      "2022-03-26 21:07:50.150475 Epoch 150, Training Loss 0.27531658524594954\n",
      "2022-03-26 21:07:50.171339 Epoch 150, Training Loss 0.27625748572294667\n",
      "2022-03-26 21:07:50.197342 Epoch 150, Training Loss 0.2769138801966787\n",
      "2022-03-26 21:07:50.220348 Epoch 150, Training Loss 0.27765253372966786\n",
      "2022-03-26 21:07:50.240187 Epoch 150, Training Loss 0.27842543775315787\n",
      "2022-03-26 21:07:50.258145 Epoch 150, Training Loss 0.2791528946069805\n",
      "2022-03-26 21:07:50.278256 Epoch 150, Training Loss 0.2799559435652345\n",
      "2022-03-26 21:07:50.297262 Epoch 150, Training Loss 0.2807134285073756\n",
      "2022-03-26 21:07:50.316295 Epoch 150, Training Loss 0.2814156799136525\n",
      "2022-03-26 21:07:50.334702 Epoch 150, Training Loss 0.28214392397562255\n",
      "2022-03-26 21:07:50.353637 Epoch 150, Training Loss 0.28295657911416516\n",
      "2022-03-26 21:07:50.372670 Epoch 150, Training Loss 0.2835809848726253\n",
      "2022-03-26 21:07:50.390978 Epoch 150, Training Loss 0.284465338910937\n",
      "2022-03-26 21:07:50.409653 Epoch 150, Training Loss 0.28513195672455954\n",
      "2022-03-26 21:07:50.427688 Epoch 150, Training Loss 0.28614906913331706\n",
      "2022-03-26 21:07:50.445574 Epoch 150, Training Loss 0.2870081540797373\n",
      "2022-03-26 21:07:50.464773 Epoch 150, Training Loss 0.2877839203457088\n",
      "2022-03-26 21:07:50.482652 Epoch 150, Training Loss 0.28842434702474445\n",
      "2022-03-26 21:07:50.500935 Epoch 150, Training Loss 0.28909300084766526\n",
      "2022-03-26 21:07:50.518915 Epoch 150, Training Loss 0.28994063743392523\n",
      "2022-03-26 21:07:50.537670 Epoch 150, Training Loss 0.290763138695751\n",
      "2022-03-26 21:07:50.555728 Epoch 150, Training Loss 0.2915635756443224\n",
      "2022-03-26 21:07:50.574735 Epoch 150, Training Loss 0.2923389627119464\n",
      "2022-03-26 21:07:50.592710 Epoch 150, Training Loss 0.2930621569952392\n",
      "2022-03-26 21:07:50.610714 Epoch 150, Training Loss 0.29374274180825716\n",
      "2022-03-26 21:07:50.628640 Epoch 150, Training Loss 0.2945969118105481\n",
      "2022-03-26 21:07:50.647476 Epoch 150, Training Loss 0.2954169491596539\n",
      "2022-03-26 21:07:50.666508 Epoch 150, Training Loss 0.29617839117946526\n",
      "2022-03-26 21:07:50.685074 Epoch 150, Training Loss 0.29690297694919665\n",
      "2022-03-26 21:07:50.703102 Epoch 150, Training Loss 0.2975927190783689\n",
      "2022-03-26 21:07:50.722124 Epoch 150, Training Loss 0.2980529436903536\n",
      "2022-03-26 21:07:50.739949 Epoch 150, Training Loss 0.29880816914388897\n",
      "2022-03-26 21:07:50.758998 Epoch 150, Training Loss 0.2995483208723995\n",
      "2022-03-26 21:07:50.777673 Epoch 150, Training Loss 0.3004498803021048\n",
      "2022-03-26 21:07:50.796801 Epoch 150, Training Loss 0.3012520364102195\n",
      "2022-03-26 21:07:50.814784 Epoch 150, Training Loss 0.3021541566723753\n",
      "2022-03-26 21:07:50.834467 Epoch 150, Training Loss 0.30305053552855615\n",
      "2022-03-26 21:07:50.852474 Epoch 150, Training Loss 0.30391454837663706\n",
      "2022-03-26 21:07:50.871334 Epoch 150, Training Loss 0.3047722209521267\n",
      "2022-03-26 21:07:50.890901 Epoch 150, Training Loss 0.30568036524688497\n",
      "2022-03-26 21:07:50.909779 Epoch 150, Training Loss 0.3066197249971692\n",
      "2022-03-26 21:07:50.927699 Epoch 150, Training Loss 0.307296390385579\n",
      "2022-03-26 21:07:50.946611 Epoch 150, Training Loss 0.30855881035937677\n",
      "2022-03-26 21:07:50.964617 Epoch 150, Training Loss 0.30950343566934774\n",
      "2022-03-26 21:07:50.983424 Epoch 150, Training Loss 0.31044912768904204\n",
      "2022-03-26 21:07:51.002334 Epoch 150, Training Loss 0.3111067898666767\n",
      "2022-03-26 21:07:51.020347 Epoch 150, Training Loss 0.31179701058608494\n",
      "2022-03-26 21:07:51.038327 Epoch 150, Training Loss 0.3123296886072744\n",
      "2022-03-26 21:07:51.056343 Epoch 150, Training Loss 0.31296811963591126\n",
      "2022-03-26 21:07:51.076279 Epoch 150, Training Loss 0.31371901430132443\n",
      "2022-03-26 21:07:51.095281 Epoch 150, Training Loss 0.31476369256253744\n",
      "2022-03-26 21:07:51.113292 Epoch 150, Training Loss 0.31538974133598835\n",
      "2022-03-26 21:07:51.132264 Epoch 150, Training Loss 0.31600043089950786\n",
      "2022-03-26 21:07:51.150215 Epoch 150, Training Loss 0.3167746239687171\n",
      "2022-03-26 21:07:51.168617 Epoch 150, Training Loss 0.3174976875333835\n",
      "2022-03-26 21:07:51.185578 Epoch 150, Training Loss 0.31823048731097786\n",
      "2022-03-26 21:07:51.212135 Epoch 150, Training Loss 0.3193480554977646\n",
      "2022-03-26 21:07:51.238185 Epoch 150, Training Loss 0.3202334456812695\n",
      "2022-03-26 21:07:51.263675 Epoch 150, Training Loss 0.32141963291503584\n",
      "2022-03-26 21:07:51.291451 Epoch 150, Training Loss 0.32222436132181026\n",
      "2022-03-26 21:07:51.316794 Epoch 150, Training Loss 0.3231723370683163\n",
      "2022-03-26 21:07:51.343598 Epoch 150, Training Loss 0.32404283462735395\n",
      "2022-03-26 21:07:51.369647 Epoch 150, Training Loss 0.3247977650302755\n",
      "2022-03-26 21:07:51.395626 Epoch 150, Training Loss 0.3256641943817553\n",
      "2022-03-26 21:07:51.417619 Epoch 150, Training Loss 0.3264341804453784\n",
      "2022-03-26 21:07:51.433625 Epoch 150, Training Loss 0.32720270367991894\n",
      "2022-03-26 21:07:51.450584 Epoch 150, Training Loss 0.3280330263364041\n",
      "2022-03-26 21:07:51.465595 Epoch 150, Training Loss 0.3288082169068744\n",
      "2022-03-26 21:07:51.480609 Epoch 150, Training Loss 0.329831528046247\n",
      "2022-03-26 21:07:51.494498 Epoch 150, Training Loss 0.3304619714046073\n",
      "2022-03-26 21:07:51.509512 Epoch 150, Training Loss 0.33119464435083484\n",
      "2022-03-26 21:07:51.523522 Epoch 150, Training Loss 0.3321427540934604\n",
      "2022-03-26 21:07:51.538537 Epoch 150, Training Loss 0.3332039632684434\n",
      "2022-03-26 21:07:51.552566 Epoch 150, Training Loss 0.3341767916365353\n",
      "2022-03-26 21:07:51.567580 Epoch 150, Training Loss 0.3348987354418201\n",
      "2022-03-26 21:07:51.581583 Epoch 150, Training Loss 0.33570363854660706\n",
      "2022-03-26 21:07:51.596614 Epoch 150, Training Loss 0.3363448703456718\n",
      "2022-03-26 21:07:51.609644 Epoch 150, Training Loss 0.33724809321753507\n",
      "2022-03-26 21:07:51.624561 Epoch 150, Training Loss 0.3379278092661782\n",
      "2022-03-26 21:07:51.638574 Epoch 150, Training Loss 0.33884634092793137\n",
      "2022-03-26 21:07:51.652594 Epoch 150, Training Loss 0.3394058166486223\n",
      "2022-03-26 21:07:51.666723 Epoch 150, Training Loss 0.3400666762876998\n",
      "2022-03-26 21:07:51.682087 Epoch 150, Training Loss 0.34054855163902276\n",
      "2022-03-26 21:07:51.697062 Epoch 150, Training Loss 0.3415940393648489\n",
      "2022-03-26 21:07:51.711088 Epoch 150, Training Loss 0.34235720046798285\n",
      "2022-03-26 21:07:51.725872 Epoch 150, Training Loss 0.3432575610211438\n",
      "2022-03-26 21:07:51.739728 Epoch 150, Training Loss 0.34412859349756897\n",
      "2022-03-26 21:07:51.754714 Epoch 150, Training Loss 0.3448952897963926\n",
      "2022-03-26 21:07:51.769724 Epoch 150, Training Loss 0.3457342023053742\n",
      "2022-03-26 21:07:51.783671 Epoch 150, Training Loss 0.3466714288648742\n",
      "2022-03-26 21:07:51.798608 Epoch 150, Training Loss 0.3472790636522386\n",
      "2022-03-26 21:07:51.812331 Epoch 150, Training Loss 0.34833187406020394\n",
      "2022-03-26 21:07:51.826468 Epoch 150, Training Loss 0.34908510062395764\n",
      "2022-03-26 21:07:51.840489 Epoch 150, Training Loss 0.35027494119561237\n",
      "2022-03-26 21:07:51.855517 Epoch 150, Training Loss 0.35119130468124626\n",
      "2022-03-26 21:07:51.869546 Epoch 150, Training Loss 0.3519961506204532\n",
      "2022-03-26 21:07:51.884560 Epoch 150, Training Loss 0.35270893284122046\n",
      "2022-03-26 21:07:51.900574 Epoch 150, Training Loss 0.35331528109815113\n",
      "2022-03-26 21:07:51.914595 Epoch 150, Training Loss 0.3541551757880184\n",
      "2022-03-26 21:07:51.930608 Epoch 150, Training Loss 0.3549518185427122\n",
      "2022-03-26 21:07:51.943921 Epoch 150, Training Loss 0.35576288889893487\n",
      "2022-03-26 21:07:51.958397 Epoch 150, Training Loss 0.35645975004833985\n",
      "2022-03-26 21:07:51.972480 Epoch 150, Training Loss 0.35713057593463937\n",
      "2022-03-26 21:07:51.986745 Epoch 150, Training Loss 0.3577623862363493\n",
      "2022-03-26 21:07:52.001660 Epoch 150, Training Loss 0.3588418688082024\n",
      "2022-03-26 21:07:52.015560 Epoch 150, Training Loss 0.35970654896915416\n",
      "2022-03-26 21:07:52.030573 Epoch 150, Training Loss 0.3604252296869102\n",
      "2022-03-26 21:07:52.044956 Epoch 150, Training Loss 0.36096145056397716\n",
      "2022-03-26 21:07:52.059006 Epoch 150, Training Loss 0.36165459534091415\n",
      "2022-03-26 21:07:52.073058 Epoch 150, Training Loss 0.36241132920355446\n",
      "2022-03-26 21:07:52.087947 Epoch 150, Training Loss 0.3631591193206475\n",
      "2022-03-26 21:07:52.102986 Epoch 150, Training Loss 0.3639994641704023\n",
      "2022-03-26 21:07:52.117785 Epoch 150, Training Loss 0.36472712796362466\n",
      "2022-03-26 21:07:52.131693 Epoch 150, Training Loss 0.3656267766909831\n",
      "2022-03-26 21:07:52.145888 Epoch 150, Training Loss 0.3665337614391161\n",
      "2022-03-26 21:07:52.159925 Epoch 150, Training Loss 0.3676464746675223\n",
      "2022-03-26 21:07:52.174023 Epoch 150, Training Loss 0.36831606661572175\n",
      "2022-03-26 21:07:52.188477 Epoch 150, Training Loss 0.3692301229747665\n",
      "2022-03-26 21:07:52.203491 Epoch 150, Training Loss 0.369955134818621\n",
      "2022-03-26 21:07:52.217698 Epoch 150, Training Loss 0.3708702115451588\n",
      "2022-03-26 21:07:52.231712 Epoch 150, Training Loss 0.3716509758359026\n",
      "2022-03-26 21:07:52.245776 Epoch 150, Training Loss 0.37255278237335515\n",
      "2022-03-26 21:07:52.259679 Epoch 150, Training Loss 0.37348417918700394\n",
      "2022-03-26 21:07:52.274879 Epoch 150, Training Loss 0.3741303424701056\n",
      "2022-03-26 21:07:52.288498 Epoch 150, Training Loss 0.3750568564285708\n",
      "2022-03-26 21:07:52.302526 Epoch 150, Training Loss 0.37576822704061524\n",
      "2022-03-26 21:07:52.317529 Epoch 150, Training Loss 0.376760581646429\n",
      "2022-03-26 21:07:52.332438 Epoch 150, Training Loss 0.3776242268054991\n",
      "2022-03-26 21:07:52.346459 Epoch 150, Training Loss 0.3786183774013958\n",
      "2022-03-26 21:07:52.359946 Epoch 150, Training Loss 0.37937332746927693\n",
      "2022-03-26 21:07:52.375576 Epoch 150, Training Loss 0.3800701283280502\n",
      "2022-03-26 21:07:52.388574 Epoch 150, Training Loss 0.3808589321573067\n",
      "2022-03-26 21:07:52.403603 Epoch 150, Training Loss 0.3816802905648566\n",
      "2022-03-26 21:07:52.416917 Epoch 150, Training Loss 0.3824085019281148\n",
      "2022-03-26 21:07:52.431914 Epoch 150, Training Loss 0.38331351758878857\n",
      "2022-03-26 21:07:52.445622 Epoch 150, Training Loss 0.3841453853165707\n",
      "2022-03-26 21:07:52.459946 Epoch 150, Training Loss 0.385200318625516\n",
      "2022-03-26 21:07:52.474065 Epoch 150, Training Loss 0.38592092086896873\n",
      "2022-03-26 21:07:52.489096 Epoch 150, Training Loss 0.38691747165701884\n",
      "2022-03-26 21:07:52.503199 Epoch 150, Training Loss 0.38793623058692267\n",
      "2022-03-26 21:07:52.519275 Epoch 150, Training Loss 0.3886362747920444\n",
      "2022-03-26 21:07:52.533030 Epoch 150, Training Loss 0.38928488232290653\n",
      "2022-03-26 21:07:52.547922 Epoch 150, Training Loss 0.38998872857264544\n",
      "2022-03-26 21:07:52.562008 Epoch 150, Training Loss 0.39081947341599427\n",
      "2022-03-26 21:07:52.576562 Epoch 150, Training Loss 0.39202788418821055\n",
      "2022-03-26 21:07:52.590592 Epoch 150, Training Loss 0.39270083549077556\n",
      "2022-03-26 21:07:52.605595 Epoch 150, Training Loss 0.39332639595584185\n",
      "2022-03-26 21:07:52.618957 Epoch 150, Training Loss 0.39403365971639637\n",
      "2022-03-26 21:07:52.633866 Epoch 150, Training Loss 0.39463128427715255\n",
      "2022-03-26 21:07:52.647932 Epoch 150, Training Loss 0.3954439474950971\n",
      "2022-03-26 21:07:52.662759 Epoch 150, Training Loss 0.3961979470137135\n",
      "2022-03-26 21:07:52.676126 Epoch 150, Training Loss 0.3972721476384136\n",
      "2022-03-26 21:07:52.691074 Epoch 150, Training Loss 0.3980034069941782\n",
      "2022-03-26 21:07:52.705095 Epoch 150, Training Loss 0.3987605323267105\n",
      "2022-03-26 21:07:52.719182 Epoch 150, Training Loss 0.3995507309961197\n",
      "2022-03-26 21:07:52.734259 Epoch 150, Training Loss 0.4005596729952966\n",
      "2022-03-26 21:07:52.748925 Epoch 150, Training Loss 0.4012616679948919\n",
      "2022-03-26 21:07:52.762923 Epoch 150, Training Loss 0.4019337608228864\n",
      "2022-03-26 21:07:52.776854 Epoch 150, Training Loss 0.4024520613195951\n",
      "2022-03-26 21:07:52.790954 Epoch 150, Training Loss 0.4036691092774081\n",
      "2022-03-26 21:07:52.804986 Epoch 150, Training Loss 0.40474973180714774\n",
      "2022-03-26 21:07:52.819162 Epoch 150, Training Loss 0.4054655557703179\n",
      "2022-03-26 21:07:52.834175 Epoch 150, Training Loss 0.40643922607307237\n",
      "2022-03-26 21:07:52.848277 Epoch 150, Training Loss 0.4072607699257638\n",
      "2022-03-26 21:07:52.863304 Epoch 150, Training Loss 0.4082162331437211\n",
      "2022-03-26 21:07:52.876253 Epoch 150, Training Loss 0.40905831171118695\n",
      "2022-03-26 21:07:52.891278 Epoch 150, Training Loss 0.4099319037574027\n",
      "2022-03-26 21:07:52.904860 Epoch 150, Training Loss 0.41079714101598697\n",
      "2022-03-26 21:07:52.919896 Epoch 150, Training Loss 0.4117501971057004\n",
      "2022-03-26 21:07:52.934515 Epoch 150, Training Loss 0.4126515082843468\n",
      "2022-03-26 21:07:52.948529 Epoch 150, Training Loss 0.41363650369827093\n",
      "2022-03-26 21:07:52.963532 Epoch 150, Training Loss 0.4144959144884973\n",
      "2022-03-26 21:07:52.977573 Epoch 150, Training Loss 0.41518810170385845\n",
      "2022-03-26 21:07:52.991570 Epoch 150, Training Loss 0.41597451189594803\n",
      "2022-03-26 21:07:53.005597 Epoch 150, Training Loss 0.41710568015532723\n",
      "2022-03-26 21:07:53.019612 Epoch 150, Training Loss 0.41806676945722926\n",
      "2022-03-26 21:07:53.033444 Epoch 150, Training Loss 0.41899644269052977\n",
      "2022-03-26 21:07:53.048463 Epoch 150, Training Loss 0.41979117733438304\n",
      "2022-03-26 21:07:53.062454 Epoch 150, Training Loss 0.42050175822299457\n",
      "2022-03-26 21:07:53.076431 Epoch 150, Training Loss 0.4210191000910366\n",
      "2022-03-26 21:07:53.090435 Epoch 150, Training Loss 0.42162316111500003\n",
      "2022-03-26 21:07:53.104455 Epoch 150, Training Loss 0.4225222098324305\n",
      "2022-03-26 21:07:53.119463 Epoch 150, Training Loss 0.42340393806509957\n",
      "2022-03-26 21:07:53.134315 Epoch 150, Training Loss 0.42406289226106364\n",
      "2022-03-26 21:07:53.148343 Epoch 150, Training Loss 0.4248063932827976\n",
      "2022-03-26 21:07:53.163324 Epoch 150, Training Loss 0.42605513886874896\n",
      "2022-03-26 21:07:53.176353 Epoch 150, Training Loss 0.4267613773836809\n",
      "2022-03-26 21:07:53.191335 Epoch 150, Training Loss 0.4275894244689771\n",
      "2022-03-26 21:07:53.210356 Epoch 150, Training Loss 0.42850067979082124\n",
      "2022-03-26 21:07:53.229329 Epoch 150, Training Loss 0.4294267358911007\n",
      "2022-03-26 21:07:53.248338 Epoch 150, Training Loss 0.430155766856335\n",
      "2022-03-26 21:07:53.268465 Epoch 150, Training Loss 0.4310946044943217\n",
      "2022-03-26 21:07:53.288472 Epoch 150, Training Loss 0.43195037772435974\n",
      "2022-03-26 21:07:53.307491 Epoch 150, Training Loss 0.4327282854891799\n",
      "2022-03-26 21:07:53.327541 Epoch 150, Training Loss 0.4336796506972569\n",
      "2022-03-26 21:07:53.346575 Epoch 150, Training Loss 0.4344163548077464\n",
      "2022-03-26 21:07:53.366037 Epoch 150, Training Loss 0.4349959150833242\n",
      "2022-03-26 21:07:53.384777 Epoch 150, Training Loss 0.43593246785118756\n",
      "2022-03-26 21:07:53.404702 Epoch 150, Training Loss 0.4364571272183562\n",
      "2022-03-26 21:07:53.424069 Epoch 150, Training Loss 0.4371382325430355\n",
      "2022-03-26 21:07:53.442994 Epoch 150, Training Loss 0.4380510420826695\n",
      "2022-03-26 21:07:53.461751 Epoch 150, Training Loss 0.4387766993640329\n",
      "2022-03-26 21:07:53.480557 Epoch 150, Training Loss 0.43960298902695744\n",
      "2022-03-26 21:07:53.499586 Epoch 150, Training Loss 0.4404209734457533\n",
      "2022-03-26 21:07:53.519662 Epoch 150, Training Loss 0.44139920472336547\n",
      "2022-03-26 21:07:53.538703 Epoch 150, Training Loss 0.44219206540328465\n",
      "2022-03-26 21:07:53.558567 Epoch 150, Training Loss 0.4429037105625548\n",
      "2022-03-26 21:07:53.577428 Epoch 150, Training Loss 0.4437363104670859\n",
      "2022-03-26 21:07:53.596451 Epoch 150, Training Loss 0.44434758506315136\n",
      "2022-03-26 21:07:53.615473 Epoch 150, Training Loss 0.4453875760516852\n",
      "2022-03-26 21:07:53.634498 Epoch 150, Training Loss 0.44618731618994645\n",
      "2022-03-26 21:07:53.653436 Epoch 150, Training Loss 0.44685530978852833\n",
      "2022-03-26 21:07:53.672455 Epoch 150, Training Loss 0.44781101477877866\n",
      "2022-03-26 21:07:53.691337 Epoch 150, Training Loss 0.4485664463332852\n",
      "2022-03-26 21:07:53.711104 Epoch 150, Training Loss 0.4493376594751387\n",
      "2022-03-26 21:07:53.729097 Epoch 150, Training Loss 0.4500861650766314\n",
      "2022-03-26 21:07:53.750092 Epoch 150, Training Loss 0.4508075334150773\n",
      "2022-03-26 21:07:53.768989 Epoch 150, Training Loss 0.45155057200537924\n",
      "2022-03-26 21:07:53.788564 Epoch 150, Training Loss 0.4523842914406296\n",
      "2022-03-26 21:07:53.807591 Epoch 150, Training Loss 0.45326710719129315\n",
      "2022-03-26 21:07:53.827217 Epoch 150, Training Loss 0.453860490332784\n",
      "2022-03-26 21:07:53.846132 Epoch 150, Training Loss 0.4547115624941828\n",
      "2022-03-26 21:07:53.865265 Epoch 150, Training Loss 0.4556823775667669\n",
      "2022-03-26 21:07:53.884040 Epoch 150, Training Loss 0.4564939249125893\n",
      "2022-03-26 21:07:53.903106 Epoch 150, Training Loss 0.45708126885354367\n",
      "2022-03-26 21:07:53.922069 Epoch 150, Training Loss 0.45797884330877564\n",
      "2022-03-26 21:07:53.941022 Epoch 150, Training Loss 0.45904014981768626\n",
      "2022-03-26 21:07:53.960043 Epoch 150, Training Loss 0.4598610972428261\n",
      "2022-03-26 21:07:53.980039 Epoch 150, Training Loss 0.46055823446387223\n",
      "2022-03-26 21:07:53.998326 Epoch 150, Training Loss 0.4613843834994699\n",
      "2022-03-26 21:07:54.017342 Epoch 150, Training Loss 0.46238463815978115\n",
      "2022-03-26 21:07:54.036248 Epoch 150, Training Loss 0.46312354985252974\n",
      "2022-03-26 21:07:54.056318 Epoch 150, Training Loss 0.46383180166296945\n",
      "2022-03-26 21:07:54.075474 Epoch 150, Training Loss 0.46491346666422645\n",
      "2022-03-26 21:07:54.094502 Epoch 150, Training Loss 0.4656649220096486\n",
      "2022-03-26 21:07:54.113541 Epoch 150, Training Loss 0.4664421433301838\n",
      "2022-03-26 21:07:54.133450 Epoch 150, Training Loss 0.4673363672726599\n",
      "2022-03-26 21:07:54.152472 Epoch 150, Training Loss 0.4681549388200731\n",
      "2022-03-26 21:07:54.172651 Epoch 150, Training Loss 0.46924761002478393\n",
      "2022-03-26 21:07:54.192661 Epoch 150, Training Loss 0.47008325190038025\n",
      "2022-03-26 21:07:54.212111 Epoch 150, Training Loss 0.47094156739809323\n",
      "2022-03-26 21:07:54.230717 Epoch 150, Training Loss 0.47191334803543433\n",
      "2022-03-26 21:07:54.249732 Epoch 150, Training Loss 0.4726482208656228\n",
      "2022-03-26 21:07:54.269694 Epoch 150, Training Loss 0.47339707350029664\n",
      "2022-03-26 21:07:54.288609 Epoch 150, Training Loss 0.4740524204719402\n",
      "2022-03-26 21:07:54.307662 Epoch 150, Training Loss 0.4751027652141078\n",
      "2022-03-26 21:07:54.328431 Epoch 150, Training Loss 0.47598644203084817\n",
      "2022-03-26 21:07:54.346466 Epoch 150, Training Loss 0.4766753301824755\n",
      "2022-03-26 21:07:54.366559 Epoch 150, Training Loss 0.4774326354722538\n",
      "2022-03-26 21:07:54.386478 Epoch 150, Training Loss 0.47838447241069715\n",
      "2022-03-26 21:07:54.406561 Epoch 150, Training Loss 0.47936287251732235\n",
      "2022-03-26 21:07:54.426601 Epoch 150, Training Loss 0.4802031109628775\n",
      "2022-03-26 21:07:54.445599 Epoch 150, Training Loss 0.4809400483089335\n",
      "2022-03-26 21:07:54.464636 Epoch 150, Training Loss 0.4819697628316977\n",
      "2022-03-26 21:07:54.483481 Epoch 150, Training Loss 0.4828035405758397\n",
      "2022-03-26 21:07:54.502495 Epoch 150, Training Loss 0.48353105779651484\n",
      "2022-03-26 21:07:54.522445 Epoch 150, Training Loss 0.4843247413559033\n",
      "2022-03-26 21:07:54.541438 Epoch 150, Training Loss 0.48520798554353395\n",
      "2022-03-26 21:07:54.560634 Epoch 150, Training Loss 0.4858784512104586\n",
      "2022-03-26 21:07:54.578869 Epoch 150, Training Loss 0.48643900614107966\n",
      "2022-03-26 21:07:54.598204 Epoch 150, Training Loss 0.48715657258734985\n",
      "2022-03-26 21:07:54.617209 Epoch 150, Training Loss 0.4879873373243205\n",
      "2022-03-26 21:07:54.637248 Epoch 150, Training Loss 0.488763899030283\n",
      "2022-03-26 21:07:54.656373 Epoch 150, Training Loss 0.4897372732153329\n",
      "2022-03-26 21:07:54.676349 Epoch 150, Training Loss 0.4905841857042459\n",
      "2022-03-26 21:07:54.694904 Epoch 150, Training Loss 0.4915368802026105\n",
      "2022-03-26 21:07:54.712940 Epoch 150, Training Loss 0.4921586233026841\n",
      "2022-03-26 21:07:54.733027 Epoch 150, Training Loss 0.49310230164576674\n",
      "2022-03-26 21:07:54.752016 Epoch 150, Training Loss 0.4941040561784564\n",
      "2022-03-26 21:07:54.771503 Epoch 150, Training Loss 0.49485066418757523\n",
      "2022-03-26 21:07:54.790514 Epoch 150, Training Loss 0.49563652398946034\n",
      "2022-03-26 21:07:54.811454 Epoch 150, Training Loss 0.4963363485263132\n",
      "2022-03-26 21:07:54.829934 Epoch 150, Training Loss 0.49753236839228576\n",
      "2022-03-26 21:07:54.850051 Epoch 150, Training Loss 0.49844323559795195\n",
      "2022-03-26 21:07:54.870078 Epoch 150, Training Loss 0.499491490290293\n",
      "2022-03-26 21:07:54.889006 Epoch 150, Training Loss 0.5004631109402308\n",
      "2022-03-26 21:07:54.908561 Epoch 150, Training Loss 0.5014254187074159\n",
      "2022-03-26 21:07:54.928465 Epoch 150, Training Loss 0.5021814934890289\n",
      "2022-03-26 21:07:54.947500 Epoch 150, Training Loss 0.5029657289500127\n",
      "2022-03-26 21:07:54.965871 Epoch 150, Training Loss 0.5037721880256673\n",
      "2022-03-26 21:07:54.985983 Epoch 150, Training Loss 0.504418723266143\n",
      "2022-03-26 21:07:55.005453 Epoch 150, Training Loss 0.5051041536623865\n",
      "2022-03-26 21:07:55.025599 Epoch 150, Training Loss 0.5059262316702576\n",
      "2022-03-26 21:07:55.044473 Epoch 150, Training Loss 0.5066763942351427\n",
      "2022-03-26 21:07:55.063333 Epoch 150, Training Loss 0.5076657904078589\n",
      "2022-03-26 21:07:55.083357 Epoch 150, Training Loss 0.5084839330609802\n",
      "2022-03-26 21:07:55.102445 Epoch 150, Training Loss 0.5097433649517996\n",
      "2022-03-26 21:07:55.121469 Epoch 150, Training Loss 0.510429360067753\n",
      "2022-03-26 21:07:55.141497 Epoch 150, Training Loss 0.5111950334837979\n",
      "2022-03-26 21:07:55.160506 Epoch 150, Training Loss 0.5118591184049006\n",
      "2022-03-26 21:07:55.180570 Epoch 150, Training Loss 0.5125988513764823\n",
      "2022-03-26 21:07:55.199698 Epoch 150, Training Loss 0.5135254598487063\n",
      "2022-03-26 21:07:55.219620 Epoch 150, Training Loss 0.5143154398407168\n",
      "2022-03-26 21:07:55.239648 Epoch 150, Training Loss 0.5150498175407614\n",
      "2022-03-26 21:07:55.259556 Epoch 150, Training Loss 0.5160377174234756\n",
      "2022-03-26 21:07:55.279432 Epoch 150, Training Loss 0.5167587433019867\n",
      "2022-03-26 21:07:55.299462 Epoch 150, Training Loss 0.5176969775763314\n",
      "2022-03-26 21:07:55.318533 Epoch 150, Training Loss 0.5185400604287072\n",
      "2022-03-26 21:07:55.338439 Epoch 150, Training Loss 0.5191259589570257\n",
      "2022-03-26 21:07:55.357955 Epoch 150, Training Loss 0.5197749161125754\n",
      "2022-03-26 21:07:55.377934 Epoch 150, Training Loss 0.5204305541713524\n",
      "2022-03-26 21:07:55.396911 Epoch 150, Training Loss 0.5211026328222831\n",
      "2022-03-26 21:07:55.416604 Epoch 150, Training Loss 0.5219049965557845\n",
      "2022-03-26 21:07:55.436626 Epoch 150, Training Loss 0.522566722215289\n",
      "2022-03-26 21:07:55.456672 Epoch 150, Training Loss 0.523262143554285\n",
      "2022-03-26 21:07:55.475713 Epoch 150, Training Loss 0.5240152087586615\n",
      "2022-03-26 21:07:55.495537 Epoch 150, Training Loss 0.5250238859668717\n",
      "2022-03-26 21:07:55.515560 Epoch 150, Training Loss 0.5259518777699117\n",
      "2022-03-26 21:07:55.535583 Epoch 150, Training Loss 0.5267161330984681\n",
      "2022-03-26 21:07:55.555473 Epoch 150, Training Loss 0.5276898592710495\n",
      "2022-03-26 21:07:55.575490 Epoch 150, Training Loss 0.5282834339553438\n",
      "2022-03-26 21:07:55.594485 Epoch 150, Training Loss 0.5290334354657347\n",
      "2022-03-26 21:07:55.614508 Epoch 150, Training Loss 0.5300259790228455\n",
      "2022-03-26 21:07:55.634321 Epoch 150, Training Loss 0.5309829681806857\n",
      "2022-03-26 21:07:55.655341 Epoch 150, Training Loss 0.5316502232761944\n",
      "2022-03-26 21:07:55.674624 Epoch 150, Training Loss 0.5321863892361941\n",
      "2022-03-26 21:07:55.695548 Epoch 150, Training Loss 0.5330728245589434\n",
      "2022-03-26 21:07:55.715096 Epoch 150, Training Loss 0.5336707907793162\n",
      "2022-03-26 21:07:55.735444 Epoch 150, Training Loss 0.5344200579025556\n",
      "2022-03-26 21:07:55.755468 Epoch 150, Training Loss 0.5355354124094214\n",
      "2022-03-26 21:07:55.774357 Epoch 150, Training Loss 0.5361993402395102\n",
      "2022-03-26 21:07:55.793380 Epoch 150, Training Loss 0.5369031201771763\n",
      "2022-03-26 21:07:55.813372 Epoch 150, Training Loss 0.5375530123329528\n",
      "2022-03-26 21:07:55.832273 Epoch 150, Training Loss 0.5386193850842278\n",
      "2022-03-26 21:07:55.852297 Epoch 150, Training Loss 0.5394757170887554\n",
      "2022-03-26 21:07:55.872356 Epoch 150, Training Loss 0.5404183874121102\n",
      "2022-03-26 21:07:55.891466 Epoch 150, Training Loss 0.5414789931472305\n",
      "2022-03-26 21:07:55.910146 Epoch 150, Training Loss 0.5420983847602249\n",
      "2022-03-26 21:07:55.929107 Epoch 150, Training Loss 0.5430173431820882\n",
      "2022-03-26 21:07:55.949173 Epoch 150, Training Loss 0.5436956563873974\n",
      "2022-03-26 21:07:55.968537 Epoch 150, Training Loss 0.5445970206919228\n",
      "2022-03-26 21:07:55.988553 Epoch 150, Training Loss 0.545262846952814\n",
      "2022-03-26 21:07:56.007971 Epoch 150, Training Loss 0.5459421043048429\n",
      "2022-03-26 21:07:56.026995 Epoch 150, Training Loss 0.5466558622277301\n",
      "2022-03-26 21:07:56.046096 Epoch 150, Training Loss 0.5474014388935645\n",
      "2022-03-26 21:07:56.066098 Epoch 150, Training Loss 0.5481139368870679\n",
      "2022-03-26 21:07:56.085205 Epoch 150, Training Loss 0.5488536809868825\n",
      "2022-03-26 21:07:56.105439 Epoch 150, Training Loss 0.5494516176717056\n",
      "2022-03-26 21:07:56.124339 Epoch 150, Training Loss 0.5504517258356905\n",
      "2022-03-26 21:07:56.143279 Epoch 150, Training Loss 0.5511852780647595\n",
      "2022-03-26 21:07:56.162363 Epoch 150, Training Loss 0.5522682260902946\n",
      "2022-03-26 21:07:56.181386 Epoch 150, Training Loss 0.5533276215157545\n",
      "2022-03-26 21:07:56.201549 Epoch 150, Training Loss 0.554159575906556\n",
      "2022-03-26 21:07:56.221553 Epoch 150, Training Loss 0.555121799297345\n",
      "2022-03-26 21:07:56.240576 Epoch 150, Training Loss 0.5560235596450088\n",
      "2022-03-26 21:07:56.260722 Epoch 150, Training Loss 0.5566555772672224\n",
      "2022-03-26 21:07:56.279767 Epoch 150, Training Loss 0.5576481184614893\n",
      "2022-03-26 21:07:56.299784 Epoch 150, Training Loss 0.5587630106131439\n",
      "2022-03-26 21:07:56.318636 Epoch 150, Training Loss 0.5596371045731523\n",
      "2022-03-26 21:07:56.338663 Epoch 150, Training Loss 0.5604559422835059\n",
      "2022-03-26 21:07:56.357573 Epoch 150, Training Loss 0.56128329926592\n",
      "2022-03-26 21:07:56.376585 Epoch 150, Training Loss 0.5621575862169266\n",
      "2022-03-26 21:07:56.396456 Epoch 150, Training Loss 0.5629194357129924\n",
      "2022-03-26 21:07:56.415493 Epoch 150, Training Loss 0.5635165286338543\n",
      "2022-03-26 21:07:56.436398 Epoch 150, Training Loss 0.5642726662213845\n",
      "2022-03-26 21:07:56.456392 Epoch 150, Training Loss 0.5650196269040217\n",
      "2022-03-26 21:07:56.477385 Epoch 150, Training Loss 0.5658421051471739\n",
      "2022-03-26 21:07:56.496415 Epoch 150, Training Loss 0.5667369753655875\n",
      "2022-03-26 21:07:56.515323 Epoch 150, Training Loss 0.5674083545384809\n",
      "2022-03-26 21:07:56.535427 Epoch 150, Training Loss 0.56815946955815\n",
      "2022-03-26 21:07:56.554437 Epoch 150, Training Loss 0.5691932097572805\n",
      "2022-03-26 21:07:56.575467 Epoch 150, Training Loss 0.5702631589396835\n",
      "2022-03-26 21:07:56.595492 Epoch 150, Training Loss 0.5710517370030094\n",
      "2022-03-26 21:07:56.614542 Epoch 150, Training Loss 0.5719292698918707\n",
      "2022-03-26 21:07:56.634572 Epoch 150, Training Loss 0.5728211941773934\n",
      "2022-03-26 21:07:56.653606 Epoch 150, Training Loss 0.5733319779338739\n",
      "2022-03-26 21:07:56.673487 Epoch 150, Training Loss 0.574304582898879\n",
      "2022-03-26 21:07:56.693399 Epoch 150, Training Loss 0.5751309030501129\n",
      "2022-03-26 21:07:56.712495 Epoch 150, Training Loss 0.5758941965487302\n",
      "2022-03-26 21:07:56.731517 Epoch 150, Training Loss 0.5766354414355724\n",
      "2022-03-26 21:07:56.751825 Epoch 150, Training Loss 0.5771971942899782\n",
      "2022-03-26 21:07:56.771119 Epoch 150, Training Loss 0.5779288000691577\n",
      "2022-03-26 21:07:56.791057 Epoch 150, Training Loss 0.5787200593506284\n",
      "2022-03-26 21:07:56.810145 Epoch 150, Training Loss 0.5796371774219186\n",
      "2022-03-26 21:07:56.829049 Epoch 150, Training Loss 0.5804414873004264\n",
      "2022-03-26 21:07:56.848887 Epoch 150, Training Loss 0.5812892562059491\n",
      "2022-03-26 21:07:56.868874 Epoch 150, Training Loss 0.5823060208193177\n",
      "2022-03-26 21:07:56.888559 Epoch 150, Training Loss 0.5831083358477449\n",
      "2022-03-26 21:07:56.908587 Epoch 150, Training Loss 0.583985843202647\n",
      "2022-03-26 21:07:56.927619 Epoch 150, Training Loss 0.5848012977777539\n",
      "2022-03-26 21:07:56.946679 Epoch 150, Training Loss 0.5855704372572472\n",
      "2022-03-26 21:07:56.966677 Epoch 150, Training Loss 0.5864848527685761\n",
      "2022-03-26 21:07:56.986551 Epoch 150, Training Loss 0.5871484408826779\n",
      "2022-03-26 21:07:57.006576 Epoch 150, Training Loss 0.5879960823470675\n",
      "2022-03-26 21:07:57.025489 Epoch 150, Training Loss 0.5884050801968026\n",
      "2022-03-26 21:07:57.044518 Epoch 150, Training Loss 0.5891541537192776\n",
      "2022-03-26 21:07:57.064526 Epoch 150, Training Loss 0.5898216835906743\n",
      "2022-03-26 21:07:57.083534 Epoch 150, Training Loss 0.5907521149920075\n",
      "2022-03-26 21:07:57.103474 Epoch 150, Training Loss 0.5914239745844355\n",
      "2022-03-26 21:07:57.123504 Epoch 150, Training Loss 0.592221087316418\n",
      "2022-03-26 21:07:57.143532 Epoch 150, Training Loss 0.5929132898521545\n",
      "2022-03-26 21:07:57.162491 Epoch 150, Training Loss 0.5937940894871416\n",
      "2022-03-26 21:07:57.182520 Epoch 150, Training Loss 0.5946670920418962\n",
      "2022-03-26 21:07:57.202550 Epoch 150, Training Loss 0.5956468815769991\n",
      "2022-03-26 21:07:57.221583 Epoch 150, Training Loss 0.5965670856749615\n",
      "2022-03-26 21:07:57.241587 Epoch 150, Training Loss 0.5974371637910834\n",
      "2022-03-26 21:07:57.260579 Epoch 150, Training Loss 0.5982865286833795\n",
      "2022-03-26 21:07:57.279472 Epoch 150, Training Loss 0.5992250345323397\n",
      "2022-03-26 21:07:57.299500 Epoch 150, Training Loss 0.6000781115287405\n",
      "2022-03-26 21:07:57.318533 Epoch 150, Training Loss 0.6008490159764619\n",
      "2022-03-26 21:07:57.339559 Epoch 150, Training Loss 0.6019373227796896\n",
      "2022-03-26 21:07:57.359552 Epoch 150, Training Loss 0.6027776657239251\n",
      "2022-03-26 21:07:57.379597 Epoch 150, Training Loss 0.6035506878896137\n",
      "2022-03-26 21:07:57.398626 Epoch 150, Training Loss 0.6042619014868651\n",
      "2022-03-26 21:07:57.417670 Epoch 150, Training Loss 0.6049933854652487\n",
      "2022-03-26 21:07:57.437589 Epoch 150, Training Loss 0.605912566223108\n",
      "2022-03-26 21:07:57.464632 Epoch 150, Training Loss 0.60677679896812\n",
      "2022-03-26 21:07:57.490643 Epoch 150, Training Loss 0.6074531786810712\n",
      "2022-03-26 21:07:57.516692 Epoch 150, Training Loss 0.6081437109910008\n",
      "2022-03-26 21:07:57.543538 Epoch 150, Training Loss 0.6090654299768341\n",
      "2022-03-26 21:07:57.570593 Epoch 150, Training Loss 0.6101076548437938\n",
      "2022-03-26 21:07:57.596634 Epoch 150, Training Loss 0.6110693505581688\n",
      "2022-03-26 21:07:57.624039 Epoch 150, Training Loss 0.6117393854252823\n",
      "2022-03-26 21:07:57.650558 Epoch 150, Training Loss 0.6125078382318282\n",
      "2022-03-26 21:07:57.675996 Epoch 150, Training Loss 0.613293988739743\n",
      "2022-03-26 21:07:57.701890 Epoch 150, Training Loss 0.6140796205652949\n",
      "2022-03-26 21:07:57.728692 Epoch 150, Training Loss 0.6148289687112164\n",
      "2022-03-26 21:07:57.755601 Epoch 150, Training Loss 0.6159724604976756\n",
      "2022-03-26 21:07:57.781893 Epoch 150, Training Loss 0.6167997823423131\n",
      "2022-03-26 21:07:57.808779 Epoch 150, Training Loss 0.6175036396440643\n",
      "2022-03-26 21:07:57.834124 Epoch 150, Training Loss 0.6182607993521654\n",
      "2022-03-26 21:07:57.860304 Epoch 150, Training Loss 0.6190222864184538\n",
      "2022-03-26 21:07:57.878365 Epoch 150, Training Loss 0.6198852231816563\n",
      "2022-03-26 21:07:57.897328 Epoch 150, Training Loss 0.6206273128995505\n",
      "2022-03-26 21:07:57.915326 Epoch 150, Training Loss 0.6214403031808337\n",
      "2022-03-26 21:07:57.933214 Epoch 150, Training Loss 0.6222312075783835\n",
      "2022-03-26 21:07:57.952305 Epoch 150, Training Loss 0.6229523164613168\n",
      "2022-03-26 21:07:57.970819 Epoch 150, Training Loss 0.6238768874760479\n",
      "2022-03-26 21:07:57.989486 Epoch 150, Training Loss 0.6243931743723657\n",
      "2022-03-26 21:07:58.008563 Epoch 150, Training Loss 0.6252497404508883\n",
      "2022-03-26 21:07:58.025592 Epoch 150, Training Loss 0.6260370517249607\n",
      "2022-03-26 21:07:58.044202 Epoch 150, Training Loss 0.6268283119973015\n",
      "2022-03-26 21:07:58.062163 Epoch 150, Training Loss 0.6278624470962588\n",
      "2022-03-26 21:07:58.080196 Epoch 150, Training Loss 0.6287693789471751\n",
      "2022-03-26 21:07:58.099308 Epoch 150, Training Loss 0.6296328725793477\n",
      "2022-03-26 21:07:58.117365 Epoch 150, Training Loss 0.6303246929441266\n",
      "2022-03-26 21:07:58.136387 Epoch 150, Training Loss 0.6312744126981481\n",
      "2022-03-26 21:07:58.154363 Epoch 150, Training Loss 0.6318809943811973\n",
      "2022-03-26 21:07:58.162378 Epoch 150, Training Loss 0.632415558409203\n",
      "2022-03-26 21:19:18.232501 Epoch 200, Training Loss 0.0010338804453535154\n",
      "2022-03-26 21:19:18.258507 Epoch 200, Training Loss 0.00163263513151642\n",
      "2022-03-26 21:19:18.283513 Epoch 200, Training Loss 0.002377726575907539\n",
      "2022-03-26 21:19:18.309519 Epoch 200, Training Loss 0.00294200653005439\n",
      "2022-03-26 21:19:18.335525 Epoch 200, Training Loss 0.0036701451786948593\n",
      "2022-03-26 21:19:18.361531 Epoch 200, Training Loss 0.004498320162448737\n",
      "2022-03-26 21:19:18.379534 Epoch 200, Training Loss 0.005131179986097623\n",
      "2022-03-26 21:19:18.397542 Epoch 200, Training Loss 0.005807220897711147\n",
      "2022-03-26 21:19:18.416543 Epoch 200, Training Loss 0.006721951886820976\n",
      "2022-03-26 21:19:18.437562 Epoch 200, Training Loss 0.007250257503346104\n",
      "2022-03-26 21:19:18.455566 Epoch 200, Training Loss 0.007908855290973888\n",
      "2022-03-26 21:19:18.473576 Epoch 200, Training Loss 0.008566821208390434\n",
      "2022-03-26 21:19:18.490580 Epoch 200, Training Loss 0.009203850994329623\n",
      "2022-03-26 21:19:18.509578 Epoch 200, Training Loss 0.010239725908659913\n",
      "2022-03-26 21:19:18.527583 Epoch 200, Training Loss 0.011036900379468717\n",
      "2022-03-26 21:19:18.545586 Epoch 200, Training Loss 0.011622229691051766\n",
      "2022-03-26 21:19:18.563597 Epoch 200, Training Loss 0.012537448493111164\n",
      "2022-03-26 21:19:18.582601 Epoch 200, Training Loss 0.013262839001767775\n",
      "2022-03-26 21:19:18.600605 Epoch 200, Training Loss 0.014004498529617134\n",
      "2022-03-26 21:19:18.619597 Epoch 200, Training Loss 0.014822395446965151\n",
      "2022-03-26 21:19:18.637607 Epoch 200, Training Loss 0.01552375484153133\n",
      "2022-03-26 21:19:18.655617 Epoch 200, Training Loss 0.01620250677361208\n",
      "2022-03-26 21:19:18.673621 Epoch 200, Training Loss 0.016956265205922333\n",
      "2022-03-26 21:19:18.692626 Epoch 200, Training Loss 0.01763518779631466\n",
      "2022-03-26 21:19:18.710630 Epoch 200, Training Loss 0.01833875896528249\n",
      "2022-03-26 21:19:18.728634 Epoch 200, Training Loss 0.019192442755260126\n",
      "2022-03-26 21:19:18.746638 Epoch 200, Training Loss 0.020141926605988035\n",
      "2022-03-26 21:19:18.764642 Epoch 200, Training Loss 0.02062959736570373\n",
      "2022-03-26 21:19:18.782646 Epoch 200, Training Loss 0.021128304207416446\n",
      "2022-03-26 21:19:18.800650 Epoch 200, Training Loss 0.021586636638702333\n",
      "2022-03-26 21:19:18.818654 Epoch 200, Training Loss 0.02203659759000744\n",
      "2022-03-26 21:19:18.837653 Epoch 200, Training Loss 0.022872291829275047\n",
      "2022-03-26 21:19:18.855657 Epoch 200, Training Loss 0.02360072568097078\n",
      "2022-03-26 21:19:18.873667 Epoch 200, Training Loss 0.02448409860548766\n",
      "2022-03-26 21:19:18.891671 Epoch 200, Training Loss 0.025314950539023065\n",
      "2022-03-26 21:19:18.909675 Epoch 200, Training Loss 0.02595248745987787\n",
      "2022-03-26 21:19:18.928679 Epoch 200, Training Loss 0.026641575157489925\n",
      "2022-03-26 21:19:18.946684 Epoch 200, Training Loss 0.027386137477272306\n",
      "2022-03-26 21:19:18.964687 Epoch 200, Training Loss 0.027950527913430157\n",
      "2022-03-26 21:19:18.982692 Epoch 200, Training Loss 0.02867515987295019\n",
      "2022-03-26 21:19:19.000697 Epoch 200, Training Loss 0.02949360756160658\n",
      "2022-03-26 21:19:19.020694 Epoch 200, Training Loss 0.03037003624012403\n",
      "2022-03-26 21:19:19.038698 Epoch 200, Training Loss 0.031169662809432924\n",
      "2022-03-26 21:19:19.056702 Epoch 200, Training Loss 0.03183049767675912\n",
      "2022-03-26 21:19:19.073706 Epoch 200, Training Loss 0.03245610429350373\n",
      "2022-03-26 21:19:19.092716 Epoch 200, Training Loss 0.03345215957030616\n",
      "2022-03-26 21:19:19.110721 Epoch 200, Training Loss 0.03404646860364148\n",
      "2022-03-26 21:19:19.127719 Epoch 200, Training Loss 0.034900869249992666\n",
      "2022-03-26 21:19:19.145723 Epoch 200, Training Loss 0.03549572955007139\n",
      "2022-03-26 21:19:19.164733 Epoch 200, Training Loss 0.036230329281228886\n",
      "2022-03-26 21:19:19.182737 Epoch 200, Training Loss 0.036965035950131425\n",
      "2022-03-26 21:19:19.200741 Epoch 200, Training Loss 0.03781842979628717\n",
      "2022-03-26 21:19:19.218745 Epoch 200, Training Loss 0.0386653473157712\n",
      "2022-03-26 21:19:19.236743 Epoch 200, Training Loss 0.03925617187834152\n",
      "2022-03-26 21:19:19.255753 Epoch 200, Training Loss 0.039809082589490945\n",
      "2022-03-26 21:19:19.272751 Epoch 200, Training Loss 0.04044498343144536\n",
      "2022-03-26 21:19:19.291762 Epoch 200, Training Loss 0.041198274782856405\n",
      "2022-03-26 21:19:19.309766 Epoch 200, Training Loss 0.04197550181994963\n",
      "2022-03-26 21:19:19.327770 Epoch 200, Training Loss 0.04295226737208988\n",
      "2022-03-26 21:19:19.345768 Epoch 200, Training Loss 0.04391194559881449\n",
      "2022-03-26 21:19:19.363778 Epoch 200, Training Loss 0.04436463540624779\n",
      "2022-03-26 21:19:19.381782 Epoch 200, Training Loss 0.045463374013181235\n",
      "2022-03-26 21:19:19.400780 Epoch 200, Training Loss 0.04652398561730104\n",
      "2022-03-26 21:19:19.418791 Epoch 200, Training Loss 0.04717123687572187\n",
      "2022-03-26 21:19:19.436789 Epoch 200, Training Loss 0.04768458027821368\n",
      "2022-03-26 21:19:19.454793 Epoch 200, Training Loss 0.04842467263074177\n",
      "2022-03-26 21:19:19.473803 Epoch 200, Training Loss 0.049269101846858364\n",
      "2022-03-26 21:19:19.491807 Epoch 200, Training Loss 0.05017187737900278\n",
      "2022-03-26 21:19:19.509811 Epoch 200, Training Loss 0.050942148355876696\n",
      "2022-03-26 21:19:19.528815 Epoch 200, Training Loss 0.051569159149818716\n",
      "2022-03-26 21:19:19.546814 Epoch 200, Training Loss 0.05251841136561635\n",
      "2022-03-26 21:19:19.565824 Epoch 200, Training Loss 0.053247419235956335\n",
      "2022-03-26 21:19:19.583822 Epoch 200, Training Loss 0.05398240304359085\n",
      "2022-03-26 21:19:19.601833 Epoch 200, Training Loss 0.05477313510597209\n",
      "2022-03-26 21:19:19.619830 Epoch 200, Training Loss 0.05549315685201484\n",
      "2022-03-26 21:19:19.638834 Epoch 200, Training Loss 0.05608456652335194\n",
      "2022-03-26 21:19:19.656844 Epoch 200, Training Loss 0.05674557021969115\n",
      "2022-03-26 21:19:19.675849 Epoch 200, Training Loss 0.05754335217006371\n",
      "2022-03-26 21:19:19.693853 Epoch 200, Training Loss 0.058349382275205745\n",
      "2022-03-26 21:19:19.712857 Epoch 200, Training Loss 0.05902237976755937\n",
      "2022-03-26 21:19:19.729855 Epoch 200, Training Loss 0.0599981773920986\n",
      "2022-03-26 21:19:19.748865 Epoch 200, Training Loss 0.06073400031422715\n",
      "2022-03-26 21:19:19.766869 Epoch 200, Training Loss 0.06139528831405103\n",
      "2022-03-26 21:19:19.784861 Epoch 200, Training Loss 0.06208684335431784\n",
      "2022-03-26 21:19:19.803872 Epoch 200, Training Loss 0.06307850054005527\n",
      "2022-03-26 21:19:19.821876 Epoch 200, Training Loss 0.06385249299618899\n",
      "2022-03-26 21:19:19.839886 Epoch 200, Training Loss 0.06463154559702519\n",
      "2022-03-26 21:19:19.857884 Epoch 200, Training Loss 0.06520393936682845\n",
      "2022-03-26 21:19:19.875894 Epoch 200, Training Loss 0.06584274422024827\n",
      "2022-03-26 21:19:19.894898 Epoch 200, Training Loss 0.06651783244841544\n",
      "2022-03-26 21:19:19.913903 Epoch 200, Training Loss 0.06717620767138498\n",
      "2022-03-26 21:19:19.932907 Epoch 200, Training Loss 0.06770872722958665\n",
      "2022-03-26 21:19:19.950911 Epoch 200, Training Loss 0.06875396998184721\n",
      "2022-03-26 21:19:19.968915 Epoch 200, Training Loss 0.06950767269677213\n",
      "2022-03-26 21:19:19.986919 Epoch 200, Training Loss 0.07048602737581638\n",
      "2022-03-26 21:19:20.005923 Epoch 200, Training Loss 0.07102170742838584\n",
      "2022-03-26 21:19:20.023928 Epoch 200, Training Loss 0.07174466992430675\n",
      "2022-03-26 21:19:20.041926 Epoch 200, Training Loss 0.0721965868149877\n",
      "2022-03-26 21:19:20.059936 Epoch 200, Training Loss 0.07287974407910691\n",
      "2022-03-26 21:19:20.077940 Epoch 200, Training Loss 0.07344345207257039\n",
      "2022-03-26 21:19:20.095944 Epoch 200, Training Loss 0.07420715838289627\n",
      "2022-03-26 21:19:20.114948 Epoch 200, Training Loss 0.07509752082855195\n",
      "2022-03-26 21:19:20.133953 Epoch 200, Training Loss 0.07583845885056059\n",
      "2022-03-26 21:19:20.152951 Epoch 200, Training Loss 0.0763336904061115\n",
      "2022-03-26 21:19:20.169955 Epoch 200, Training Loss 0.07682697486389628\n",
      "2022-03-26 21:19:20.188959 Epoch 200, Training Loss 0.07769019806476506\n",
      "2022-03-26 21:19:20.205963 Epoch 200, Training Loss 0.07859301917693194\n",
      "2022-03-26 21:19:20.224973 Epoch 200, Training Loss 0.07937997739638208\n",
      "2022-03-26 21:19:20.242977 Epoch 200, Training Loss 0.07996044435616954\n",
      "2022-03-26 21:19:20.260981 Epoch 200, Training Loss 0.0810843753006757\n",
      "2022-03-26 21:19:20.278979 Epoch 200, Training Loss 0.08164956788425251\n",
      "2022-03-26 21:19:20.296990 Epoch 200, Training Loss 0.08229956248074846\n",
      "2022-03-26 21:19:20.314994 Epoch 200, Training Loss 0.08309060018843092\n",
      "2022-03-26 21:19:20.332992 Epoch 200, Training Loss 0.08366949520909878\n",
      "2022-03-26 21:19:20.351996 Epoch 200, Training Loss 0.08472031320604827\n",
      "2022-03-26 21:19:20.371000 Epoch 200, Training Loss 0.08552855363739725\n",
      "2022-03-26 21:19:20.390005 Epoch 200, Training Loss 0.086136340828198\n",
      "2022-03-26 21:19:20.408009 Epoch 200, Training Loss 0.086840214288753\n",
      "2022-03-26 21:19:20.427013 Epoch 200, Training Loss 0.08745308902562426\n",
      "2022-03-26 21:19:20.445023 Epoch 200, Training Loss 0.08823832869529724\n",
      "2022-03-26 21:19:20.464027 Epoch 200, Training Loss 0.08877928574066943\n",
      "2022-03-26 21:19:20.482032 Epoch 200, Training Loss 0.08956515346951496\n",
      "2022-03-26 21:19:20.500037 Epoch 200, Training Loss 0.09068255625722353\n",
      "2022-03-26 21:19:20.519040 Epoch 200, Training Loss 0.09146006927465844\n",
      "2022-03-26 21:19:20.537038 Epoch 200, Training Loss 0.09211617654851635\n",
      "2022-03-26 21:19:20.556048 Epoch 200, Training Loss 0.09280369097314527\n",
      "2022-03-26 21:19:20.575053 Epoch 200, Training Loss 0.09375989368504575\n",
      "2022-03-26 21:19:20.594057 Epoch 200, Training Loss 0.09442030918567687\n",
      "2022-03-26 21:19:20.612061 Epoch 200, Training Loss 0.09490710969471261\n",
      "2022-03-26 21:19:20.631054 Epoch 200, Training Loss 0.0955552975539966\n",
      "2022-03-26 21:19:20.650070 Epoch 200, Training Loss 0.09614108990677787\n",
      "2022-03-26 21:19:20.668305 Epoch 200, Training Loss 0.0967950825114994\n",
      "2022-03-26 21:19:20.686072 Epoch 200, Training Loss 0.09765951487871692\n",
      "2022-03-26 21:19:20.705076 Epoch 200, Training Loss 0.09836181605715885\n",
      "2022-03-26 21:19:20.723074 Epoch 200, Training Loss 0.0989499211768665\n",
      "2022-03-26 21:19:20.740084 Epoch 200, Training Loss 0.09953895592323654\n",
      "2022-03-26 21:19:20.759094 Epoch 200, Training Loss 0.1005052374604413\n",
      "2022-03-26 21:19:20.778103 Epoch 200, Training Loss 0.10107237908541394\n",
      "2022-03-26 21:19:20.796103 Epoch 200, Training Loss 0.10146577157023008\n",
      "2022-03-26 21:19:20.815101 Epoch 200, Training Loss 0.1024128931867497\n",
      "2022-03-26 21:19:20.833111 Epoch 200, Training Loss 0.10307678831812671\n",
      "2022-03-26 21:19:20.853109 Epoch 200, Training Loss 0.10360024926607567\n",
      "2022-03-26 21:19:20.870113 Epoch 200, Training Loss 0.1045529759296066\n",
      "2022-03-26 21:19:20.889124 Epoch 200, Training Loss 0.10537202827765814\n",
      "2022-03-26 21:19:20.907128 Epoch 200, Training Loss 0.10622597281890148\n",
      "2022-03-26 21:19:20.925126 Epoch 200, Training Loss 0.1071237728876226\n",
      "2022-03-26 21:19:20.944136 Epoch 200, Training Loss 0.1079849446825969\n",
      "2022-03-26 21:19:20.964135 Epoch 200, Training Loss 0.10870948350033187\n",
      "2022-03-26 21:19:20.982145 Epoch 200, Training Loss 0.10955164796860932\n",
      "2022-03-26 21:19:21.000149 Epoch 200, Training Loss 0.11070855552583095\n",
      "2022-03-26 21:19:21.019147 Epoch 200, Training Loss 0.11142536807243171\n",
      "2022-03-26 21:19:21.038151 Epoch 200, Training Loss 0.11205203213807567\n",
      "2022-03-26 21:19:21.056162 Epoch 200, Training Loss 0.11297137204490965\n",
      "2022-03-26 21:19:21.075166 Epoch 200, Training Loss 0.11362098004964306\n",
      "2022-03-26 21:19:21.093170 Epoch 200, Training Loss 0.11426048072250299\n",
      "2022-03-26 21:19:21.112174 Epoch 200, Training Loss 0.11491515329274375\n",
      "2022-03-26 21:19:21.130178 Epoch 200, Training Loss 0.11555606061997621\n",
      "2022-03-26 21:19:21.149183 Epoch 200, Training Loss 0.11638798345537747\n",
      "2022-03-26 21:19:21.168187 Epoch 200, Training Loss 0.11716069509763546\n",
      "2022-03-26 21:19:21.186179 Epoch 200, Training Loss 0.11773059069348113\n",
      "2022-03-26 21:19:21.204189 Epoch 200, Training Loss 0.11884855873444501\n",
      "2022-03-26 21:19:21.223200 Epoch 200, Training Loss 0.11945183430333882\n",
      "2022-03-26 21:19:21.241204 Epoch 200, Training Loss 0.12081988956159948\n",
      "2022-03-26 21:19:21.260208 Epoch 200, Training Loss 0.12137818961497158\n",
      "2022-03-26 21:19:21.277206 Epoch 200, Training Loss 0.12209412051588678\n",
      "2022-03-26 21:19:21.296217 Epoch 200, Training Loss 0.12275291960257703\n",
      "2022-03-26 21:19:21.315220 Epoch 200, Training Loss 0.12345203338071818\n",
      "2022-03-26 21:19:21.333225 Epoch 200, Training Loss 0.12404746419328558\n",
      "2022-03-26 21:19:21.352230 Epoch 200, Training Loss 0.12470843480980914\n",
      "2022-03-26 21:19:21.371227 Epoch 200, Training Loss 0.125456947423613\n",
      "2022-03-26 21:19:21.389237 Epoch 200, Training Loss 0.12616384349515675\n",
      "2022-03-26 21:19:21.415237 Epoch 200, Training Loss 0.12680254876613617\n",
      "2022-03-26 21:19:21.441243 Epoch 200, Training Loss 0.12744846577991914\n",
      "2022-03-26 21:19:21.468256 Epoch 200, Training Loss 0.12808946495318352\n",
      "2022-03-26 21:19:21.493261 Epoch 200, Training Loss 0.12908983066716156\n",
      "2022-03-26 21:19:21.520261 Epoch 200, Training Loss 0.12968718746434088\n",
      "2022-03-26 21:19:21.545272 Epoch 200, Training Loss 0.13026050655433283\n",
      "2022-03-26 21:19:21.571273 Epoch 200, Training Loss 0.13076728258443915\n",
      "2022-03-26 21:19:21.598285 Epoch 200, Training Loss 0.1313933441248696\n",
      "2022-03-26 21:19:21.612276 Epoch 200, Training Loss 0.13234141705286168\n",
      "2022-03-26 21:19:21.626279 Epoch 200, Training Loss 0.1333139406140808\n",
      "2022-03-26 21:19:21.641282 Epoch 200, Training Loss 0.13397086405997996\n",
      "2022-03-26 21:19:21.655285 Epoch 200, Training Loss 0.13480810588583006\n",
      "2022-03-26 21:19:21.669289 Epoch 200, Training Loss 0.13533922751693775\n",
      "2022-03-26 21:19:21.683292 Epoch 200, Training Loss 0.13596203164828707\n",
      "2022-03-26 21:19:21.698295 Epoch 200, Training Loss 0.13663380465391653\n",
      "2022-03-26 21:19:21.712299 Epoch 200, Training Loss 0.1375058640528213\n",
      "2022-03-26 21:19:21.727301 Epoch 200, Training Loss 0.1379810099863945\n",
      "2022-03-26 21:19:21.741304 Epoch 200, Training Loss 0.13895279794092982\n",
      "2022-03-26 21:19:21.756308 Epoch 200, Training Loss 0.1397637913903922\n",
      "2022-03-26 21:19:21.770311 Epoch 200, Training Loss 0.14057951837854313\n",
      "2022-03-26 21:19:21.784315 Epoch 200, Training Loss 0.14118949634492245\n",
      "2022-03-26 21:19:21.798319 Epoch 200, Training Loss 0.14190937174708032\n",
      "2022-03-26 21:19:21.813321 Epoch 200, Training Loss 0.14235491071210798\n",
      "2022-03-26 21:19:21.828325 Epoch 200, Training Loss 0.14298998482544403\n",
      "2022-03-26 21:19:21.842328 Epoch 200, Training Loss 0.14379819461604212\n",
      "2022-03-26 21:19:21.856331 Epoch 200, Training Loss 0.1445311226731981\n",
      "2022-03-26 21:19:21.870334 Epoch 200, Training Loss 0.14536620546942172\n",
      "2022-03-26 21:19:21.884337 Epoch 200, Training Loss 0.14623161270033064\n",
      "2022-03-26 21:19:21.898340 Epoch 200, Training Loss 0.1471407688258554\n",
      "2022-03-26 21:19:21.912343 Epoch 200, Training Loss 0.14795082338783136\n",
      "2022-03-26 21:19:21.927353 Epoch 200, Training Loss 0.14864992565663573\n",
      "2022-03-26 21:19:21.941351 Epoch 200, Training Loss 0.1494808905493573\n",
      "2022-03-26 21:19:21.956360 Epoch 200, Training Loss 0.15034730198895535\n",
      "2022-03-26 21:19:21.970357 Epoch 200, Training Loss 0.1511159139444761\n",
      "2022-03-26 21:19:21.984359 Epoch 200, Training Loss 0.15194768586274607\n",
      "2022-03-26 21:19:21.998363 Epoch 200, Training Loss 0.1528557845012611\n",
      "2022-03-26 21:19:22.013368 Epoch 200, Training Loss 0.15365384625809272\n",
      "2022-03-26 21:19:22.028370 Epoch 200, Training Loss 0.15427966686465855\n",
      "2022-03-26 21:19:22.042373 Epoch 200, Training Loss 0.15510688184777185\n",
      "2022-03-26 21:19:22.056376 Epoch 200, Training Loss 0.15571788692718272\n",
      "2022-03-26 21:19:22.070380 Epoch 200, Training Loss 0.156545280495568\n",
      "2022-03-26 21:19:22.084378 Epoch 200, Training Loss 0.15729978886406745\n",
      "2022-03-26 21:19:22.098387 Epoch 200, Training Loss 0.15784251362161564\n",
      "2022-03-26 21:19:22.112390 Epoch 200, Training Loss 0.15890335304962705\n",
      "2022-03-26 21:19:22.127392 Epoch 200, Training Loss 0.15948479624507983\n",
      "2022-03-26 21:19:22.141395 Epoch 200, Training Loss 0.16007244030532936\n",
      "2022-03-26 21:19:22.155399 Epoch 200, Training Loss 0.16084334978362178\n",
      "2022-03-26 21:19:22.169402 Epoch 200, Training Loss 0.16178365772032677\n",
      "2022-03-26 21:19:22.184398 Epoch 200, Training Loss 0.16257850189343134\n",
      "2022-03-26 21:19:22.198409 Epoch 200, Training Loss 0.16344870684092003\n",
      "2022-03-26 21:19:22.213412 Epoch 200, Training Loss 0.16381045832963245\n",
      "2022-03-26 21:19:22.227415 Epoch 200, Training Loss 0.1645561003928904\n",
      "2022-03-26 21:19:22.242418 Epoch 200, Training Loss 0.1653165420150513\n",
      "2022-03-26 21:19:22.256422 Epoch 200, Training Loss 0.16608024085574138\n",
      "2022-03-26 21:19:22.271425 Epoch 200, Training Loss 0.16660476885640713\n",
      "2022-03-26 21:19:22.285429 Epoch 200, Training Loss 0.16711490523174902\n",
      "2022-03-26 21:19:22.299431 Epoch 200, Training Loss 0.16796310295534256\n",
      "2022-03-26 21:19:22.313435 Epoch 200, Training Loss 0.16882808091085585\n",
      "2022-03-26 21:19:22.328438 Epoch 200, Training Loss 0.16951364476967345\n",
      "2022-03-26 21:19:22.342441 Epoch 200, Training Loss 0.1703476015564121\n",
      "2022-03-26 21:19:22.356444 Epoch 200, Training Loss 0.17099090320679844\n",
      "2022-03-26 21:19:22.370449 Epoch 200, Training Loss 0.1715945887672322\n",
      "2022-03-26 21:19:22.385451 Epoch 200, Training Loss 0.17219386598490694\n",
      "2022-03-26 21:19:22.399454 Epoch 200, Training Loss 0.17297081291065802\n",
      "2022-03-26 21:19:22.414457 Epoch 200, Training Loss 0.17371422696448957\n",
      "2022-03-26 21:19:22.428460 Epoch 200, Training Loss 0.17482045075625105\n",
      "2022-03-26 21:19:22.442464 Epoch 200, Training Loss 0.1756794499543012\n",
      "2022-03-26 21:19:22.457467 Epoch 200, Training Loss 0.17667558411960407\n",
      "2022-03-26 21:19:22.471470 Epoch 200, Training Loss 0.17750553039791028\n",
      "2022-03-26 21:19:22.485474 Epoch 200, Training Loss 0.1783500722683299\n",
      "2022-03-26 21:19:22.499483 Epoch 200, Training Loss 0.17902602133391154\n",
      "2022-03-26 21:19:22.514483 Epoch 200, Training Loss 0.1797090253180555\n",
      "2022-03-26 21:19:22.528484 Epoch 200, Training Loss 0.18046426936945953\n",
      "2022-03-26 21:19:22.542487 Epoch 200, Training Loss 0.18113055234522465\n",
      "2022-03-26 21:19:22.556491 Epoch 200, Training Loss 0.1818008702200697\n",
      "2022-03-26 21:19:22.570493 Epoch 200, Training Loss 0.18238516022329745\n",
      "2022-03-26 21:19:22.585499 Epoch 200, Training Loss 0.1829000697340197\n",
      "2022-03-26 21:19:22.599499 Epoch 200, Training Loss 0.18371967712174292\n",
      "2022-03-26 21:19:22.614502 Epoch 200, Training Loss 0.18431658444502164\n",
      "2022-03-26 21:19:22.628506 Epoch 200, Training Loss 0.18502473404340428\n",
      "2022-03-26 21:19:22.642510 Epoch 200, Training Loss 0.18571643824772457\n",
      "2022-03-26 21:19:22.656512 Epoch 200, Training Loss 0.18656056867840956\n",
      "2022-03-26 21:19:22.670515 Epoch 200, Training Loss 0.18754468625768675\n",
      "2022-03-26 21:19:22.684518 Epoch 200, Training Loss 0.18829524692367106\n",
      "2022-03-26 21:19:22.698523 Epoch 200, Training Loss 0.18885318146032445\n",
      "2022-03-26 21:19:22.712525 Epoch 200, Training Loss 0.18969750221428053\n",
      "2022-03-26 21:19:22.727528 Epoch 200, Training Loss 0.19034555013222462\n",
      "2022-03-26 21:19:22.741532 Epoch 200, Training Loss 0.19105379081443144\n",
      "2022-03-26 21:19:22.755534 Epoch 200, Training Loss 0.19186137666177872\n",
      "2022-03-26 21:19:22.769538 Epoch 200, Training Loss 0.19236623699707753\n",
      "2022-03-26 21:19:22.783542 Epoch 200, Training Loss 0.19328665832424408\n",
      "2022-03-26 21:19:22.797546 Epoch 200, Training Loss 0.19425244244468182\n",
      "2022-03-26 21:19:22.812549 Epoch 200, Training Loss 0.1948512052102467\n",
      "2022-03-26 21:19:22.827551 Epoch 200, Training Loss 0.19575973365770277\n",
      "2022-03-26 21:19:22.841555 Epoch 200, Training Loss 0.1965834110060616\n",
      "2022-03-26 21:19:22.855558 Epoch 200, Training Loss 0.1972543003080446\n",
      "2022-03-26 21:19:22.869560 Epoch 200, Training Loss 0.19797590439734253\n",
      "2022-03-26 21:19:22.883564 Epoch 200, Training Loss 0.19869293463047202\n",
      "2022-03-26 21:19:22.898567 Epoch 200, Training Loss 0.19922672379809572\n",
      "2022-03-26 21:19:22.912570 Epoch 200, Training Loss 0.2001482246400755\n",
      "2022-03-26 21:19:22.926574 Epoch 200, Training Loss 0.20116699850925093\n",
      "2022-03-26 21:19:22.940576 Epoch 200, Training Loss 0.20183491154247538\n",
      "2022-03-26 21:19:22.954580 Epoch 200, Training Loss 0.20262760438425156\n",
      "2022-03-26 21:19:22.968583 Epoch 200, Training Loss 0.2032686440307466\n",
      "2022-03-26 21:19:22.982586 Epoch 200, Training Loss 0.20404006662728535\n",
      "2022-03-26 21:19:22.996590 Epoch 200, Training Loss 0.20503450952984792\n",
      "2022-03-26 21:19:23.011593 Epoch 200, Training Loss 0.2056811578819514\n",
      "2022-03-26 21:19:23.026596 Epoch 200, Training Loss 0.20646998716894624\n",
      "2022-03-26 21:19:23.041600 Epoch 200, Training Loss 0.20720368978160117\n",
      "2022-03-26 21:19:23.055604 Epoch 200, Training Loss 0.20803114928095542\n",
      "2022-03-26 21:19:23.069614 Epoch 200, Training Loss 0.20876338395773603\n",
      "2022-03-26 21:19:23.084609 Epoch 200, Training Loss 0.20959680605574946\n",
      "2022-03-26 21:19:23.098613 Epoch 200, Training Loss 0.21034386658759982\n",
      "2022-03-26 21:19:23.112616 Epoch 200, Training Loss 0.21122721672210548\n",
      "2022-03-26 21:19:23.127620 Epoch 200, Training Loss 0.21201417059697153\n",
      "2022-03-26 21:19:23.141624 Epoch 200, Training Loss 0.21254420234724078\n",
      "2022-03-26 21:19:23.155627 Epoch 200, Training Loss 0.21340834065471465\n",
      "2022-03-26 21:19:23.169630 Epoch 200, Training Loss 0.21402777765717956\n",
      "2022-03-26 21:19:23.184634 Epoch 200, Training Loss 0.21470909815310213\n",
      "2022-03-26 21:19:23.198636 Epoch 200, Training Loss 0.21544921520116078\n",
      "2022-03-26 21:19:23.212640 Epoch 200, Training Loss 0.21634147683982655\n",
      "2022-03-26 21:19:23.227644 Epoch 200, Training Loss 0.21720612010992396\n",
      "2022-03-26 21:19:23.241646 Epoch 200, Training Loss 0.2180333132176753\n",
      "2022-03-26 21:19:23.255649 Epoch 200, Training Loss 0.2189027398748471\n",
      "2022-03-26 21:19:23.269652 Epoch 200, Training Loss 0.21960406908598704\n",
      "2022-03-26 21:19:23.284655 Epoch 200, Training Loss 0.22035837707007327\n",
      "2022-03-26 21:19:23.298660 Epoch 200, Training Loss 0.2213902951353956\n",
      "2022-03-26 21:19:23.312661 Epoch 200, Training Loss 0.22230395636595118\n",
      "2022-03-26 21:19:23.327667 Epoch 200, Training Loss 0.2229189495448871\n",
      "2022-03-26 21:19:23.340669 Epoch 200, Training Loss 0.22367990451395664\n",
      "2022-03-26 21:19:23.354672 Epoch 200, Training Loss 0.22427211320766097\n",
      "2022-03-26 21:19:23.368669 Epoch 200, Training Loss 0.2247997823045077\n",
      "2022-03-26 21:19:23.383679 Epoch 200, Training Loss 0.225573535427413\n",
      "2022-03-26 21:19:23.397683 Epoch 200, Training Loss 0.22621323991462092\n",
      "2022-03-26 21:19:23.416694 Epoch 200, Training Loss 0.22695328318097097\n",
      "2022-03-26 21:19:23.435691 Epoch 200, Training Loss 0.22766783136083646\n",
      "2022-03-26 21:19:23.454688 Epoch 200, Training Loss 0.22862685530844246\n",
      "2022-03-26 21:19:23.473706 Epoch 200, Training Loss 0.22914423646829318\n",
      "2022-03-26 21:19:23.492710 Epoch 200, Training Loss 0.229753886189912\n",
      "2022-03-26 21:19:23.511712 Epoch 200, Training Loss 0.23048830131435638\n",
      "2022-03-26 21:19:23.530719 Epoch 200, Training Loss 0.2314816275063683\n",
      "2022-03-26 21:19:23.549723 Epoch 200, Training Loss 0.23257976038681577\n",
      "2022-03-26 21:19:23.568721 Epoch 200, Training Loss 0.2332831211102283\n",
      "2022-03-26 21:19:23.586732 Epoch 200, Training Loss 0.23403129195008437\n",
      "2022-03-26 21:19:23.605736 Epoch 200, Training Loss 0.23479467577031812\n",
      "2022-03-26 21:19:23.624740 Epoch 200, Training Loss 0.23554526783926102\n",
      "2022-03-26 21:19:23.643745 Epoch 200, Training Loss 0.23672879550158216\n",
      "2022-03-26 21:19:23.662749 Epoch 200, Training Loss 0.23758253127412723\n",
      "2022-03-26 21:19:23.681752 Epoch 200, Training Loss 0.23844137078965716\n",
      "2022-03-26 21:19:23.700755 Epoch 200, Training Loss 0.2389755888515726\n",
      "2022-03-26 21:19:23.718747 Epoch 200, Training Loss 0.239554738792617\n",
      "2022-03-26 21:19:23.737765 Epoch 200, Training Loss 0.24027221331663448\n",
      "2022-03-26 21:19:23.756776 Epoch 200, Training Loss 0.24076436761090214\n",
      "2022-03-26 21:19:23.775777 Epoch 200, Training Loss 0.24150166380435914\n",
      "2022-03-26 21:19:23.794770 Epoch 200, Training Loss 0.24242352463705155\n",
      "2022-03-26 21:19:23.813787 Epoch 200, Training Loss 0.24317105789013835\n",
      "2022-03-26 21:19:23.832791 Epoch 200, Training Loss 0.24382918859686692\n",
      "2022-03-26 21:19:23.852789 Epoch 200, Training Loss 0.24470497069456387\n",
      "2022-03-26 21:19:23.872800 Epoch 200, Training Loss 0.24525922338675965\n",
      "2022-03-26 21:19:23.890798 Epoch 200, Training Loss 0.24601655832641875\n",
      "2022-03-26 21:19:23.909803 Epoch 200, Training Loss 0.2467261218201474\n",
      "2022-03-26 21:19:23.928813 Epoch 200, Training Loss 0.24750143876465994\n",
      "2022-03-26 21:19:23.947817 Epoch 200, Training Loss 0.24810099979038433\n",
      "2022-03-26 21:19:23.967816 Epoch 200, Training Loss 0.24886765546353576\n",
      "2022-03-26 21:19:23.986820 Epoch 200, Training Loss 0.24952207124599105\n",
      "2022-03-26 21:19:24.005824 Epoch 200, Training Loss 0.25044227625860277\n",
      "2022-03-26 21:19:24.024835 Epoch 200, Training Loss 0.2510267518213033\n",
      "2022-03-26 21:19:24.042839 Epoch 200, Training Loss 0.2516472365164086\n",
      "2022-03-26 21:19:24.062837 Epoch 200, Training Loss 0.2521769492446309\n",
      "2022-03-26 21:19:24.082848 Epoch 200, Training Loss 0.2529468387365341\n",
      "2022-03-26 21:19:24.101846 Epoch 200, Training Loss 0.25386328667478486\n",
      "2022-03-26 21:19:24.120850 Epoch 200, Training Loss 0.2545483303268242\n",
      "2022-03-26 21:19:24.139861 Epoch 200, Training Loss 0.25502080777112174\n",
      "2022-03-26 21:19:24.158865 Epoch 200, Training Loss 0.25592231819086975\n",
      "2022-03-26 21:19:24.177869 Epoch 200, Training Loss 0.25661491913258877\n",
      "2022-03-26 21:19:24.196874 Epoch 200, Training Loss 0.25736855332503844\n",
      "2022-03-26 21:19:24.215878 Epoch 200, Training Loss 0.2580757173888214\n",
      "2022-03-26 21:19:24.234870 Epoch 200, Training Loss 0.25881789460816346\n",
      "2022-03-26 21:19:24.253881 Epoch 200, Training Loss 0.2595391662986687\n",
      "2022-03-26 21:19:24.272891 Epoch 200, Training Loss 0.2602868338527582\n",
      "2022-03-26 21:19:24.292895 Epoch 200, Training Loss 0.26098659733677154\n",
      "2022-03-26 21:19:24.311900 Epoch 200, Training Loss 0.2616362337718534\n",
      "2022-03-26 21:19:24.331904 Epoch 200, Training Loss 0.2623308727046108\n",
      "2022-03-26 21:19:24.350893 Epoch 200, Training Loss 0.2632553962338001\n",
      "2022-03-26 21:19:24.370907 Epoch 200, Training Loss 0.2640600769263704\n",
      "2022-03-26 21:19:24.389917 Epoch 200, Training Loss 0.26483397791757607\n",
      "2022-03-26 21:19:24.408922 Epoch 200, Training Loss 0.265577389837226\n",
      "2022-03-26 21:19:24.427920 Epoch 200, Training Loss 0.266491046875639\n",
      "2022-03-26 21:19:24.447931 Epoch 200, Training Loss 0.2671304048251008\n",
      "2022-03-26 21:19:24.467935 Epoch 200, Training Loss 0.2679090844776929\n",
      "2022-03-26 21:19:24.487933 Epoch 200, Training Loss 0.2688128684868898\n",
      "2022-03-26 21:19:24.506944 Epoch 200, Training Loss 0.2695961932620734\n",
      "2022-03-26 21:19:24.525942 Epoch 200, Training Loss 0.270350787683826\n",
      "2022-03-26 21:19:24.544953 Epoch 200, Training Loss 0.2713154166784433\n",
      "2022-03-26 21:19:24.564957 Epoch 200, Training Loss 0.2718245438526354\n",
      "2022-03-26 21:19:24.583961 Epoch 200, Training Loss 0.27250254310457905\n",
      "2022-03-26 21:19:24.603949 Epoch 200, Training Loss 0.2734444090701125\n",
      "2022-03-26 21:19:24.622970 Epoch 200, Training Loss 0.27418902047607296\n",
      "2022-03-26 21:19:24.641974 Epoch 200, Training Loss 0.27497554324624485\n",
      "2022-03-26 21:19:24.660979 Epoch 200, Training Loss 0.2756947004200552\n",
      "2022-03-26 21:19:24.681978 Epoch 200, Training Loss 0.27644659433980734\n",
      "2022-03-26 21:19:24.701044 Epoch 200, Training Loss 0.2770345077642699\n",
      "2022-03-26 21:19:24.719986 Epoch 200, Training Loss 0.27756162616602903\n",
      "2022-03-26 21:19:24.738990 Epoch 200, Training Loss 0.27856904169177765\n",
      "2022-03-26 21:19:24.759001 Epoch 200, Training Loss 0.27919175683537406\n",
      "2022-03-26 21:19:24.777999 Epoch 200, Training Loss 0.2799536755399021\n",
      "2022-03-26 21:19:24.798010 Epoch 200, Training Loss 0.2807177632589779\n",
      "2022-03-26 21:19:24.817014 Epoch 200, Training Loss 0.28158940641623936\n",
      "2022-03-26 21:19:24.837013 Epoch 200, Training Loss 0.2824230696767797\n",
      "2022-03-26 21:19:24.856017 Epoch 200, Training Loss 0.2831002289758009\n",
      "2022-03-26 21:19:24.876027 Epoch 200, Training Loss 0.28402974664250297\n",
      "2022-03-26 21:19:24.896032 Epoch 200, Training Loss 0.2847115743876723\n",
      "2022-03-26 21:19:24.916037 Epoch 200, Training Loss 0.28546563030966104\n",
      "2022-03-26 21:19:24.935027 Epoch 200, Training Loss 0.2863906689388368\n",
      "2022-03-26 21:19:24.954028 Epoch 200, Training Loss 0.286989623628309\n",
      "2022-03-26 21:19:24.978031 Epoch 200, Training Loss 0.2875811683247461\n",
      "2022-03-26 21:19:24.999050 Epoch 200, Training Loss 0.2882295266136794\n",
      "2022-03-26 21:19:25.019045 Epoch 200, Training Loss 0.28898061167858446\n",
      "2022-03-26 21:19:25.045048 Epoch 200, Training Loss 0.28957105060215194\n",
      "2022-03-26 21:19:25.070054 Epoch 200, Training Loss 0.29009753881055683\n",
      "2022-03-26 21:19:25.092059 Epoch 200, Training Loss 0.29071897672265384\n",
      "2022-03-26 21:19:25.112063 Epoch 200, Training Loss 0.29137686001675206\n",
      "2022-03-26 21:19:25.133073 Epoch 200, Training Loss 0.292358578699629\n",
      "2022-03-26 21:19:25.153070 Epoch 200, Training Loss 0.2933427421638118\n",
      "2022-03-26 21:19:25.173076 Epoch 200, Training Loss 0.2940470225670758\n",
      "2022-03-26 21:19:25.193099 Epoch 200, Training Loss 0.29459104986142015\n",
      "2022-03-26 21:19:25.212104 Epoch 200, Training Loss 0.29520361841944476\n",
      "2022-03-26 21:19:25.231108 Epoch 200, Training Loss 0.2959415941210964\n",
      "2022-03-26 21:19:25.250112 Epoch 200, Training Loss 0.29695611692907864\n",
      "2022-03-26 21:19:25.270111 Epoch 200, Training Loss 0.29762912303437966\n",
      "2022-03-26 21:19:25.289121 Epoch 200, Training Loss 0.29829946850114464\n",
      "2022-03-26 21:19:25.309126 Epoch 200, Training Loss 0.2989588657684643\n",
      "2022-03-26 21:19:25.329130 Epoch 200, Training Loss 0.29968680106008144\n",
      "2022-03-26 21:19:25.348129 Epoch 200, Training Loss 0.30035088079817157\n",
      "2022-03-26 21:19:25.367133 Epoch 200, Training Loss 0.30113980719043165\n",
      "2022-03-26 21:19:25.387137 Epoch 200, Training Loss 0.30184379254308197\n",
      "2022-03-26 21:19:25.406148 Epoch 200, Training Loss 0.30268615785309727\n",
      "2022-03-26 21:19:25.425152 Epoch 200, Training Loss 0.30331452739665576\n",
      "2022-03-26 21:19:25.444156 Epoch 200, Training Loss 0.30400604356432814\n",
      "2022-03-26 21:19:25.463161 Epoch 200, Training Loss 0.3047705671900069\n",
      "2022-03-26 21:19:25.483165 Epoch 200, Training Loss 0.30563566420236815\n",
      "2022-03-26 21:19:25.502169 Epoch 200, Training Loss 0.3064337401362636\n",
      "2022-03-26 21:19:25.522174 Epoch 200, Training Loss 0.3069740308977454\n",
      "2022-03-26 21:19:25.541172 Epoch 200, Training Loss 0.3079724798879355\n",
      "2022-03-26 21:19:25.560183 Epoch 200, Training Loss 0.3088503842006254\n",
      "2022-03-26 21:19:25.579187 Epoch 200, Training Loss 0.30948428028379865\n",
      "2022-03-26 21:19:25.599191 Epoch 200, Training Loss 0.3103898256788473\n",
      "2022-03-26 21:19:25.618190 Epoch 200, Training Loss 0.31123366006804853\n",
      "2022-03-26 21:19:25.638194 Epoch 200, Training Loss 0.31205653496410535\n",
      "2022-03-26 21:19:25.656204 Epoch 200, Training Loss 0.31309623013981774\n",
      "2022-03-26 21:19:25.675209 Epoch 200, Training Loss 0.31362477021144175\n",
      "2022-03-26 21:19:25.694213 Epoch 200, Training Loss 0.31463232118150464\n",
      "2022-03-26 21:19:25.714218 Epoch 200, Training Loss 0.3152811649205435\n",
      "2022-03-26 21:19:25.734222 Epoch 200, Training Loss 0.3159132817822039\n",
      "2022-03-26 21:19:25.753220 Epoch 200, Training Loss 0.3166897828164308\n",
      "2022-03-26 21:19:25.772231 Epoch 200, Training Loss 0.31765637320020923\n",
      "2022-03-26 21:19:25.791235 Epoch 200, Training Loss 0.31831963783334893\n",
      "2022-03-26 21:19:25.811234 Epoch 200, Training Loss 0.31909771785711694\n",
      "2022-03-26 21:19:25.830244 Epoch 200, Training Loss 0.3199317174799302\n",
      "2022-03-26 21:19:25.849248 Epoch 200, Training Loss 0.320726557537113\n",
      "2022-03-26 21:19:25.870247 Epoch 200, Training Loss 0.321607314488467\n",
      "2022-03-26 21:19:25.889257 Epoch 200, Training Loss 0.32232468108386947\n",
      "2022-03-26 21:19:25.908262 Epoch 200, Training Loss 0.32306418928039043\n",
      "2022-03-26 21:19:25.927260 Epoch 200, Training Loss 0.32422396251002844\n",
      "2022-03-26 21:19:25.947270 Epoch 200, Training Loss 0.32501190778849376\n",
      "2022-03-26 21:19:25.967544 Epoch 200, Training Loss 0.3255045258099466\n",
      "2022-03-26 21:19:25.987273 Epoch 200, Training Loss 0.32614231246816533\n",
      "2022-03-26 21:19:26.006284 Epoch 200, Training Loss 0.3268015438790821\n",
      "2022-03-26 21:19:26.025288 Epoch 200, Training Loss 0.32780275145150206\n",
      "2022-03-26 21:19:26.045293 Epoch 200, Training Loss 0.3287390555109819\n",
      "2022-03-26 21:19:26.064297 Epoch 200, Training Loss 0.3295554727544565\n",
      "2022-03-26 21:19:26.084303 Epoch 200, Training Loss 0.330303453423483\n",
      "2022-03-26 21:19:26.103300 Epoch 200, Training Loss 0.3307670211166982\n",
      "2022-03-26 21:19:26.122310 Epoch 200, Training Loss 0.33140770465974\n",
      "2022-03-26 21:19:26.142314 Epoch 200, Training Loss 0.3321926749651999\n",
      "2022-03-26 21:19:26.162319 Epoch 200, Training Loss 0.33303458356034116\n",
      "2022-03-26 21:19:26.181323 Epoch 200, Training Loss 0.33392608802184426\n",
      "2022-03-26 21:19:26.200328 Epoch 200, Training Loss 0.3346911794160638\n",
      "2022-03-26 21:19:26.219326 Epoch 200, Training Loss 0.3358804034378827\n",
      "2022-03-26 21:19:26.239337 Epoch 200, Training Loss 0.33650059068141996\n",
      "2022-03-26 21:19:26.258341 Epoch 200, Training Loss 0.3371885503115861\n",
      "2022-03-26 21:19:26.276345 Epoch 200, Training Loss 0.33823577640459057\n",
      "2022-03-26 21:19:26.296343 Epoch 200, Training Loss 0.33901593733169233\n",
      "2022-03-26 21:19:26.316354 Epoch 200, Training Loss 0.33993416228105344\n",
      "2022-03-26 21:19:26.335358 Epoch 200, Training Loss 0.3406317953944511\n",
      "2022-03-26 21:19:26.355363 Epoch 200, Training Loss 0.34126663211818853\n",
      "2022-03-26 21:19:26.375367 Epoch 200, Training Loss 0.3418489493753599\n",
      "2022-03-26 21:19:26.395366 Epoch 200, Training Loss 0.34239178819729543\n",
      "2022-03-26 21:19:26.414370 Epoch 200, Training Loss 0.34324645698832734\n",
      "2022-03-26 21:19:26.433375 Epoch 200, Training Loss 0.34407920537092496\n",
      "2022-03-26 21:19:26.454385 Epoch 200, Training Loss 0.3448350291575312\n",
      "2022-03-26 21:19:26.473390 Epoch 200, Training Loss 0.34557794770011513\n",
      "2022-03-26 21:19:26.493394 Epoch 200, Training Loss 0.3464930463020149\n",
      "2022-03-26 21:19:26.512398 Epoch 200, Training Loss 0.34725054077175266\n",
      "2022-03-26 21:19:26.532403 Epoch 200, Training Loss 0.34795202173845236\n",
      "2022-03-26 21:19:26.551392 Epoch 200, Training Loss 0.34885299655482593\n",
      "2022-03-26 21:19:26.571412 Epoch 200, Training Loss 0.34950350236404887\n",
      "2022-03-26 21:19:26.590416 Epoch 200, Training Loss 0.3502288060572446\n",
      "2022-03-26 21:19:26.610421 Epoch 200, Training Loss 0.35108759945920665\n",
      "2022-03-26 21:19:26.629425 Epoch 200, Training Loss 0.3518800194306142\n",
      "2022-03-26 21:19:26.649429 Epoch 200, Training Loss 0.3524112941511452\n",
      "2022-03-26 21:19:26.668434 Epoch 200, Training Loss 0.3530737572466321\n",
      "2022-03-26 21:19:26.687439 Epoch 200, Training Loss 0.3535944994758157\n",
      "2022-03-26 21:19:26.706442 Epoch 200, Training Loss 0.35438796954081797\n",
      "2022-03-26 21:19:26.726447 Epoch 200, Training Loss 0.35516961341928643\n",
      "2022-03-26 21:19:26.745451 Epoch 200, Training Loss 0.3558719903611771\n",
      "2022-03-26 21:19:26.764444 Epoch 200, Training Loss 0.3569388862156197\n",
      "2022-03-26 21:19:26.784455 Epoch 200, Training Loss 0.35783336930872534\n",
      "2022-03-26 21:19:26.803459 Epoch 200, Training Loss 0.3585277071694279\n",
      "2022-03-26 21:19:26.822469 Epoch 200, Training Loss 0.3592285770741875\n",
      "2022-03-26 21:19:26.841473 Epoch 200, Training Loss 0.35983720589476775\n",
      "2022-03-26 21:19:26.861472 Epoch 200, Training Loss 0.36057236089425926\n",
      "2022-03-26 21:19:26.880482 Epoch 200, Training Loss 0.36105935199333883\n",
      "2022-03-26 21:19:26.900472 Epoch 200, Training Loss 0.3619225268702373\n",
      "2022-03-26 21:19:26.919485 Epoch 200, Training Loss 0.36309181862627454\n",
      "2022-03-26 21:19:26.939495 Epoch 200, Training Loss 0.36377087826161736\n",
      "2022-03-26 21:19:26.958500 Epoch 200, Training Loss 0.3643410344562872\n",
      "2022-03-26 21:19:26.978504 Epoch 200, Training Loss 0.3650111776331197\n",
      "2022-03-26 21:19:26.998508 Epoch 200, Training Loss 0.36547555742056476\n",
      "2022-03-26 21:19:27.018501 Epoch 200, Training Loss 0.3662062677580987\n",
      "2022-03-26 21:19:27.037511 Epoch 200, Training Loss 0.3669284420550022\n",
      "2022-03-26 21:19:27.056516 Epoch 200, Training Loss 0.36765191850759793\n",
      "2022-03-26 21:19:27.076527 Epoch 200, Training Loss 0.368354225707481\n",
      "2022-03-26 21:19:27.095530 Epoch 200, Training Loss 0.3689993385921049\n",
      "2022-03-26 21:19:27.115529 Epoch 200, Training Loss 0.3696620292828211\n",
      "2022-03-26 21:19:27.134527 Epoch 200, Training Loss 0.37030952658189836\n",
      "2022-03-26 21:19:27.154544 Epoch 200, Training Loss 0.3711821625147329\n",
      "2022-03-26 21:19:27.173548 Epoch 200, Training Loss 0.37204557619131434\n",
      "2022-03-26 21:19:27.194547 Epoch 200, Training Loss 0.37237499437063853\n",
      "2022-03-26 21:19:27.212557 Epoch 200, Training Loss 0.37328533412855297\n",
      "2022-03-26 21:19:27.232562 Epoch 200, Training Loss 0.3741347372836774\n",
      "2022-03-26 21:19:27.253561 Epoch 200, Training Loss 0.3747794188349448\n",
      "2022-03-26 21:19:27.273571 Epoch 200, Training Loss 0.37541682854332886\n",
      "2022-03-26 21:19:27.292575 Epoch 200, Training Loss 0.3763906827668095\n",
      "2022-03-26 21:19:27.312580 Epoch 200, Training Loss 0.3771602021306372\n",
      "2022-03-26 21:19:27.331584 Epoch 200, Training Loss 0.3777845070871246\n",
      "2022-03-26 21:19:27.350588 Epoch 200, Training Loss 0.37842082257008613\n",
      "2022-03-26 21:19:27.369587 Epoch 200, Training Loss 0.3792443413792364\n",
      "2022-03-26 21:19:27.389597 Epoch 200, Training Loss 0.3799413406406827\n",
      "2022-03-26 21:19:27.408602 Epoch 200, Training Loss 0.38072437638669365\n",
      "2022-03-26 21:19:27.428606 Epoch 200, Training Loss 0.3811530615286449\n",
      "2022-03-26 21:19:27.448597 Epoch 200, Training Loss 0.3819833044582011\n",
      "2022-03-26 21:19:27.468603 Epoch 200, Training Loss 0.3827636055553051\n",
      "2022-03-26 21:19:27.487614 Epoch 200, Training Loss 0.38349508130184523\n",
      "2022-03-26 21:19:27.506624 Epoch 200, Training Loss 0.384161560164998\n",
      "2022-03-26 21:19:27.526628 Epoch 200, Training Loss 0.384809374008947\n",
      "2022-03-26 21:19:27.545633 Epoch 200, Training Loss 0.3856231299278986\n",
      "2022-03-26 21:19:27.565637 Epoch 200, Training Loss 0.38652676797431446\n",
      "2022-03-26 21:19:27.585642 Epoch 200, Training Loss 0.38710187474632507\n",
      "2022-03-26 21:19:27.604646 Epoch 200, Training Loss 0.3876449336938541\n",
      "2022-03-26 21:19:27.623638 Epoch 200, Training Loss 0.3885259844762895\n",
      "2022-03-26 21:19:27.644655 Epoch 200, Training Loss 0.3894200802916456\n",
      "2022-03-26 21:19:27.663660 Epoch 200, Training Loss 0.39021950967781377\n",
      "2022-03-26 21:19:27.683958 Epoch 200, Training Loss 0.39088502221400173\n",
      "2022-03-26 21:19:27.702663 Epoch 200, Training Loss 0.3916196188201075\n",
      "2022-03-26 21:19:27.721667 Epoch 200, Training Loss 0.3925898617795666\n",
      "2022-03-26 21:19:27.741677 Epoch 200, Training Loss 0.3930723953734883\n",
      "2022-03-26 21:19:27.760681 Epoch 200, Training Loss 0.39368170694164606\n",
      "2022-03-26 21:19:27.780686 Epoch 200, Training Loss 0.3943647579540072\n",
      "2022-03-26 21:19:27.799690 Epoch 200, Training Loss 0.39518256386375183\n",
      "2022-03-26 21:19:27.819689 Epoch 200, Training Loss 0.3958711010187178\n",
      "2022-03-26 21:19:27.839699 Epoch 200, Training Loss 0.3966135138364704\n",
      "2022-03-26 21:19:27.861699 Epoch 200, Training Loss 0.39742330680875215\n",
      "2022-03-26 21:19:27.888710 Epoch 200, Training Loss 0.39810740921046117\n",
      "2022-03-26 21:19:27.914716 Epoch 200, Training Loss 0.3986738558163119\n",
      "2022-03-26 21:19:27.940722 Epoch 200, Training Loss 0.3994186255328186\n",
      "2022-03-26 21:19:27.967729 Epoch 200, Training Loss 0.40022943803416494\n",
      "2022-03-26 21:19:27.994734 Epoch 200, Training Loss 0.4006102251655915\n",
      "2022-03-26 21:19:28.020722 Epoch 200, Training Loss 0.40128419721675346\n",
      "2022-03-26 21:19:28.048741 Epoch 200, Training Loss 0.4019793705333529\n",
      "2022-03-26 21:19:28.074753 Epoch 200, Training Loss 0.4026323096907657\n",
      "2022-03-26 21:19:28.100758 Epoch 200, Training Loss 0.4037369147819631\n",
      "2022-03-26 21:19:28.126758 Epoch 200, Training Loss 0.4045801857472076\n",
      "2022-03-26 21:19:28.152758 Epoch 200, Training Loss 0.4054087914545518\n",
      "2022-03-26 21:19:28.178776 Epoch 200, Training Loss 0.4061863372469192\n",
      "2022-03-26 21:19:28.204782 Epoch 200, Training Loss 0.40656622786960944\n",
      "2022-03-26 21:19:28.229788 Epoch 200, Training Loss 0.4072171368867235\n",
      "2022-03-26 21:19:28.256788 Epoch 200, Training Loss 0.407840750673238\n",
      "2022-03-26 21:19:28.275798 Epoch 200, Training Loss 0.40823861689823665\n",
      "2022-03-26 21:19:28.293802 Epoch 200, Training Loss 0.40899642135785974\n",
      "2022-03-26 21:19:28.312807 Epoch 200, Training Loss 0.4098425590626114\n",
      "2022-03-26 21:19:28.330811 Epoch 200, Training Loss 0.4104483073858349\n",
      "2022-03-26 21:19:28.349815 Epoch 200, Training Loss 0.4114887867208637\n",
      "2022-03-26 21:19:28.367805 Epoch 200, Training Loss 0.41235672638696785\n",
      "2022-03-26 21:19:28.385817 Epoch 200, Training Loss 0.41308932387462965\n",
      "2022-03-26 21:19:28.403827 Epoch 200, Training Loss 0.41391713581884\n",
      "2022-03-26 21:19:28.421831 Epoch 200, Training Loss 0.4146324177760907\n",
      "2022-03-26 21:19:28.439835 Epoch 200, Training Loss 0.4155171731167742\n",
      "2022-03-26 21:19:28.457840 Epoch 200, Training Loss 0.41641651109203964\n",
      "2022-03-26 21:19:28.476844 Epoch 200, Training Loss 0.4173238173393947\n",
      "2022-03-26 21:19:28.495848 Epoch 200, Training Loss 0.4182357695477698\n",
      "2022-03-26 21:19:28.514852 Epoch 200, Training Loss 0.4191627922798971\n",
      "2022-03-26 21:19:28.532857 Epoch 200, Training Loss 0.41992859992072407\n",
      "2022-03-26 21:19:28.551855 Epoch 200, Training Loss 0.42043748021583116\n",
      "2022-03-26 21:19:28.569859 Epoch 200, Training Loss 0.42107098269493076\n",
      "2022-03-26 21:19:28.588855 Epoch 200, Training Loss 0.4220604330606168\n",
      "2022-03-26 21:19:28.606873 Epoch 200, Training Loss 0.42304527069754005\n",
      "2022-03-26 21:19:28.625878 Epoch 200, Training Loss 0.4238289172768288\n",
      "2022-03-26 21:19:28.643876 Epoch 200, Training Loss 0.42474020293454073\n",
      "2022-03-26 21:19:28.662880 Epoch 200, Training Loss 0.42552297609998746\n",
      "2022-03-26 21:19:28.682890 Epoch 200, Training Loss 0.42623975518566876\n",
      "2022-03-26 21:19:28.700889 Epoch 200, Training Loss 0.42701775883622184\n",
      "2022-03-26 21:19:28.718893 Epoch 200, Training Loss 0.42801746318254935\n",
      "2022-03-26 21:19:28.736897 Epoch 200, Training Loss 0.4286556728279499\n",
      "2022-03-26 21:19:28.755907 Epoch 200, Training Loss 0.4293791434115461\n",
      "2022-03-26 21:19:28.773911 Epoch 200, Training Loss 0.42993471136940714\n",
      "2022-03-26 21:19:28.792915 Epoch 200, Training Loss 0.4305667384429966\n",
      "2022-03-26 21:19:28.810907 Epoch 200, Training Loss 0.4314520015664723\n",
      "2022-03-26 21:19:28.828924 Epoch 200, Training Loss 0.4324779409505522\n",
      "2022-03-26 21:19:28.846928 Epoch 200, Training Loss 0.432921481102019\n",
      "2022-03-26 21:19:28.864932 Epoch 200, Training Loss 0.43375152608622675\n",
      "2022-03-26 21:19:28.883936 Epoch 200, Training Loss 0.4345652472485057\n",
      "2022-03-26 21:19:28.901940 Epoch 200, Training Loss 0.43532026728705675\n",
      "2022-03-26 21:19:28.919938 Epoch 200, Training Loss 0.43604022691316924\n",
      "2022-03-26 21:19:28.938943 Epoch 200, Training Loss 0.4369605252962283\n",
      "2022-03-26 21:19:28.956947 Epoch 200, Training Loss 0.4375667738563874\n",
      "2022-03-26 21:19:28.974951 Epoch 200, Training Loss 0.43819296634410654\n",
      "2022-03-26 21:19:28.992961 Epoch 200, Training Loss 0.4389087576085649\n",
      "2022-03-26 21:19:29.011965 Epoch 200, Training Loss 0.4397085238143306\n",
      "2022-03-26 21:19:29.030969 Epoch 200, Training Loss 0.44051183222809714\n",
      "2022-03-26 21:19:29.048973 Epoch 200, Training Loss 0.4416346798467514\n",
      "2022-03-26 21:19:29.065971 Epoch 200, Training Loss 0.44236915274654204\n",
      "2022-03-26 21:19:29.084982 Epoch 200, Training Loss 0.4435444499373131\n",
      "2022-03-26 21:19:29.103986 Epoch 200, Training Loss 0.4443207681941254\n",
      "2022-03-26 21:19:29.121990 Epoch 200, Training Loss 0.44513401572051864\n",
      "2022-03-26 21:19:29.138994 Epoch 200, Training Loss 0.44583601018656854\n",
      "2022-03-26 21:19:29.156992 Epoch 200, Training Loss 0.44651385067064137\n",
      "2022-03-26 21:19:29.175996 Epoch 200, Training Loss 0.4471002437955583\n",
      "2022-03-26 21:19:29.194006 Epoch 200, Training Loss 0.4478627365187306\n",
      "2022-03-26 21:19:29.212010 Epoch 200, Training Loss 0.44868764574722864\n",
      "2022-03-26 21:19:29.230008 Epoch 200, Training Loss 0.44946512195003\n",
      "2022-03-26 21:19:29.249019 Epoch 200, Training Loss 0.4503207647663248\n",
      "2022-03-26 21:19:29.268018 Epoch 200, Training Loss 0.45097130655175277\n",
      "2022-03-26 21:19:29.286021 Epoch 200, Training Loss 0.45194990864342743\n",
      "2022-03-26 21:19:29.305032 Epoch 200, Training Loss 0.45255717829517694\n",
      "2022-03-26 21:19:29.324036 Epoch 200, Training Loss 0.4531878955909968\n",
      "2022-03-26 21:19:29.342040 Epoch 200, Training Loss 0.45387549019987933\n",
      "2022-03-26 21:19:29.360044 Epoch 200, Training Loss 0.45479083766260414\n",
      "2022-03-26 21:19:29.379048 Epoch 200, Training Loss 0.45545011290046566\n",
      "2022-03-26 21:19:29.397052 Epoch 200, Training Loss 0.45616814185438864\n",
      "2022-03-26 21:19:29.416051 Epoch 200, Training Loss 0.45697572526267116\n",
      "2022-03-26 21:19:29.434061 Epoch 200, Training Loss 0.45775873173990517\n",
      "2022-03-26 21:19:29.452052 Epoch 200, Training Loss 0.45865246017113365\n",
      "2022-03-26 21:19:29.470063 Epoch 200, Training Loss 0.4593774303984459\n",
      "2022-03-26 21:19:29.488073 Epoch 200, Training Loss 0.46012944406103296\n",
      "2022-03-26 21:19:29.507078 Epoch 200, Training Loss 0.4608230120157037\n",
      "2022-03-26 21:19:29.526082 Epoch 200, Training Loss 0.4617353285593755\n",
      "2022-03-26 21:19:29.544086 Epoch 200, Training Loss 0.4628901155022404\n",
      "2022-03-26 21:19:29.563090 Epoch 200, Training Loss 0.46361425065475964\n",
      "2022-03-26 21:19:29.582094 Epoch 200, Training Loss 0.46427245781092386\n",
      "2022-03-26 21:19:29.600337 Epoch 200, Training Loss 0.4650556508003903\n",
      "2022-03-26 21:19:29.618102 Epoch 200, Training Loss 0.46559014024636936\n",
      "2022-03-26 21:19:29.637101 Epoch 200, Training Loss 0.46624670789369843\n",
      "2022-03-26 21:19:29.655105 Epoch 200, Training Loss 0.4668508255878068\n",
      "2022-03-26 21:19:29.673115 Epoch 200, Training Loss 0.4675903245616142\n",
      "2022-03-26 21:19:29.692119 Epoch 200, Training Loss 0.46825097245938335\n",
      "2022-03-26 21:19:29.711124 Epoch 200, Training Loss 0.46899626474551226\n",
      "2022-03-26 21:19:29.729122 Epoch 200, Training Loss 0.47011338650722945\n",
      "2022-03-26 21:19:29.747132 Epoch 200, Training Loss 0.47083229657329256\n",
      "2022-03-26 21:19:29.766136 Epoch 200, Training Loss 0.47160000828525905\n",
      "2022-03-26 21:19:29.784140 Epoch 200, Training Loss 0.47237492072612736\n",
      "2022-03-26 21:19:29.802131 Epoch 200, Training Loss 0.473240259434561\n",
      "2022-03-26 21:19:29.821149 Epoch 200, Training Loss 0.47402990740888257\n",
      "2022-03-26 21:19:29.839153 Epoch 200, Training Loss 0.4745998517860232\n",
      "2022-03-26 21:19:29.857157 Epoch 200, Training Loss 0.47540821721944054\n",
      "2022-03-26 21:19:29.875155 Epoch 200, Training Loss 0.47622723328640393\n",
      "2022-03-26 21:19:29.893165 Epoch 200, Training Loss 0.4768517020413333\n",
      "2022-03-26 21:19:29.912169 Epoch 200, Training Loss 0.47771959620363574\n",
      "2022-03-26 21:19:29.931173 Epoch 200, Training Loss 0.4787230964206978\n",
      "2022-03-26 21:19:29.949177 Epoch 200, Training Loss 0.4795544811374391\n",
      "2022-03-26 21:19:29.969176 Epoch 200, Training Loss 0.4806572182099228\n",
      "2022-03-26 21:19:29.987180 Epoch 200, Training Loss 0.48155487102010974\n",
      "2022-03-26 21:19:30.006190 Epoch 200, Training Loss 0.4822789033508057\n",
      "2022-03-26 21:19:30.024195 Epoch 200, Training Loss 0.48319663416089303\n",
      "2022-03-26 21:19:30.043199 Epoch 200, Training Loss 0.48421387972734165\n",
      "2022-03-26 21:19:30.061203 Epoch 200, Training Loss 0.48487708041125244\n",
      "2022-03-26 21:19:30.079201 Epoch 200, Training Loss 0.48564025637743724\n",
      "2022-03-26 21:19:30.104213 Epoch 200, Training Loss 0.486360069583444\n",
      "2022-03-26 21:19:30.131219 Epoch 200, Training Loss 0.4871651688042809\n",
      "2022-03-26 21:19:30.157225 Epoch 200, Training Loss 0.4878508700129321\n",
      "2022-03-26 21:19:30.184231 Epoch 200, Training Loss 0.4888714618237732\n",
      "2022-03-26 21:19:30.209237 Epoch 200, Training Loss 0.4896928396676203\n",
      "2022-03-26 21:19:30.235232 Epoch 200, Training Loss 0.49036858728169785\n",
      "2022-03-26 21:19:30.261248 Epoch 200, Training Loss 0.4913365766215507\n",
      "2022-03-26 21:19:30.287248 Epoch 200, Training Loss 0.49226889837428434\n",
      "2022-03-26 21:19:30.305252 Epoch 200, Training Loss 0.49296207142912823\n",
      "2022-03-26 21:19:30.323262 Epoch 200, Training Loss 0.49386711003225475\n",
      "2022-03-26 21:19:30.342267 Epoch 200, Training Loss 0.49475180622561815\n",
      "2022-03-26 21:19:30.360265 Epoch 200, Training Loss 0.49561857796081193\n",
      "2022-03-26 21:19:30.379275 Epoch 200, Training Loss 0.49643225361928917\n",
      "2022-03-26 21:19:30.397273 Epoch 200, Training Loss 0.4971523869525441\n",
      "2022-03-26 21:19:30.415283 Epoch 200, Training Loss 0.4978459018575566\n",
      "2022-03-26 21:19:30.434288 Epoch 200, Training Loss 0.49858059930374554\n",
      "2022-03-26 21:19:30.452286 Epoch 200, Training Loss 0.49922174695507643\n",
      "2022-03-26 21:19:30.470290 Epoch 200, Training Loss 0.4999230343210118\n",
      "2022-03-26 21:19:30.489300 Epoch 200, Training Loss 0.5006922238013324\n",
      "2022-03-26 21:19:30.507298 Epoch 200, Training Loss 0.5014500587492647\n",
      "2022-03-26 21:19:30.525308 Epoch 200, Training Loss 0.5021574761708985\n",
      "2022-03-26 21:19:30.543296 Epoch 200, Training Loss 0.502862820844821\n",
      "2022-03-26 21:19:30.561316 Epoch 200, Training Loss 0.5036071528254262\n",
      "2022-03-26 21:19:30.580321 Epoch 200, Training Loss 0.5045653470336934\n",
      "2022-03-26 21:19:30.597324 Epoch 200, Training Loss 0.5052867923551203\n",
      "2022-03-26 21:19:30.616329 Epoch 200, Training Loss 0.5060021348316651\n",
      "2022-03-26 21:19:30.634333 Epoch 200, Training Loss 0.5067627534384618\n",
      "2022-03-26 21:19:30.652331 Epoch 200, Training Loss 0.5075723241509684\n",
      "2022-03-26 21:19:30.670335 Epoch 200, Training Loss 0.508576320260382\n",
      "2022-03-26 21:19:30.688345 Epoch 200, Training Loss 0.5090737571877897\n",
      "2022-03-26 21:19:30.706343 Epoch 200, Training Loss 0.5097811167578563\n",
      "2022-03-26 21:19:30.724353 Epoch 200, Training Loss 0.5104692951416421\n",
      "2022-03-26 21:19:30.742357 Epoch 200, Training Loss 0.5111465780707576\n",
      "2022-03-26 21:19:30.761362 Epoch 200, Training Loss 0.5116718278821472\n",
      "2022-03-26 21:19:30.779366 Epoch 200, Training Loss 0.5124478565000207\n",
      "2022-03-26 21:19:30.797364 Epoch 200, Training Loss 0.5133061049234532\n",
      "2022-03-26 21:19:30.815374 Epoch 200, Training Loss 0.5143022775040258\n",
      "2022-03-26 21:19:30.834378 Epoch 200, Training Loss 0.5151581829771057\n",
      "2022-03-26 21:19:30.851376 Epoch 200, Training Loss 0.5160915538325639\n",
      "2022-03-26 21:19:30.870380 Epoch 200, Training Loss 0.5167969828828827\n",
      "2022-03-26 21:19:30.889391 Epoch 200, Training Loss 0.5176434039764697\n",
      "2022-03-26 21:19:30.907395 Epoch 200, Training Loss 0.5185026011198682\n",
      "2022-03-26 21:19:30.926399 Epoch 200, Training Loss 0.5193560769795762\n",
      "2022-03-26 21:19:30.944403 Epoch 200, Training Loss 0.5205785907290476\n",
      "2022-03-26 21:19:30.963401 Epoch 200, Training Loss 0.5215361139658466\n",
      "2022-03-26 21:19:30.982412 Epoch 200, Training Loss 0.5225105066128704\n",
      "2022-03-26 21:19:31.001416 Epoch 200, Training Loss 0.5231450522494743\n",
      "2022-03-26 21:19:31.019420 Epoch 200, Training Loss 0.5239810144809811\n",
      "2022-03-26 21:19:31.037418 Epoch 200, Training Loss 0.524884685149888\n",
      "2022-03-26 21:19:31.055428 Epoch 200, Training Loss 0.525660792961145\n",
      "2022-03-26 21:19:31.073433 Epoch 200, Training Loss 0.5263257317835718\n",
      "2022-03-26 21:19:31.092437 Epoch 200, Training Loss 0.5271553220346455\n",
      "2022-03-26 21:19:31.110441 Epoch 200, Training Loss 0.5279789093662711\n",
      "2022-03-26 21:19:31.128439 Epoch 200, Training Loss 0.5286379913844721\n",
      "2022-03-26 21:19:31.147443 Epoch 200, Training Loss 0.5294918298263989\n",
      "2022-03-26 21:19:31.166453 Epoch 200, Training Loss 0.5303496683345121\n",
      "2022-03-26 21:19:31.184457 Epoch 200, Training Loss 0.5310203689138603\n",
      "2022-03-26 21:19:31.202462 Epoch 200, Training Loss 0.5318383313810734\n",
      "2022-03-26 21:19:31.221466 Epoch 200, Training Loss 0.5325854373405047\n",
      "2022-03-26 21:19:31.239470 Epoch 200, Training Loss 0.5332619815378847\n",
      "2022-03-26 21:19:31.258474 Epoch 200, Training Loss 0.534244205945593\n",
      "2022-03-26 21:19:31.276478 Epoch 200, Training Loss 0.535053709583819\n",
      "2022-03-26 21:19:31.294482 Epoch 200, Training Loss 0.5358522395648615\n",
      "2022-03-26 21:19:31.312481 Epoch 200, Training Loss 0.5367354237667435\n",
      "2022-03-26 21:19:31.330491 Epoch 200, Training Loss 0.537803825057681\n",
      "2022-03-26 21:19:31.349495 Epoch 200, Training Loss 0.5384788690015788\n",
      "2022-03-26 21:19:31.368493 Epoch 200, Training Loss 0.5389355894016183\n",
      "2022-03-26 21:19:31.386497 Epoch 200, Training Loss 0.5396420988051788\n",
      "2022-03-26 21:19:31.404507 Epoch 200, Training Loss 0.5406110256605441\n",
      "2022-03-26 21:19:31.423512 Epoch 200, Training Loss 0.5413305877191027\n",
      "2022-03-26 21:19:31.441517 Epoch 200, Training Loss 0.5417974075240553\n",
      "2022-03-26 21:19:31.460520 Epoch 200, Training Loss 0.5425280632112947\n",
      "2022-03-26 21:19:31.477524 Epoch 200, Training Loss 0.5433453409873006\n",
      "2022-03-26 21:19:31.495522 Epoch 200, Training Loss 0.5439304385496222\n",
      "2022-03-26 21:19:31.514532 Epoch 200, Training Loss 0.544788389254714\n",
      "2022-03-26 21:19:31.532536 Epoch 200, Training Loss 0.5453840963294744\n",
      "2022-03-26 21:19:31.551541 Epoch 200, Training Loss 0.5461173512975274\n",
      "2022-03-26 21:19:31.570545 Epoch 200, Training Loss 0.5467728668695215\n",
      "2022-03-26 21:19:31.588549 Epoch 200, Training Loss 0.5478691318074761\n",
      "2022-03-26 21:19:31.607548 Epoch 200, Training Loss 0.5485653812851747\n",
      "2022-03-26 21:19:31.625557 Epoch 200, Training Loss 0.54915916599581\n",
      "2022-03-26 21:19:31.642562 Epoch 200, Training Loss 0.5502409080562689\n",
      "2022-03-26 21:19:31.660560 Epoch 200, Training Loss 0.5508586632854798\n",
      "2022-03-26 21:19:31.679570 Epoch 200, Training Loss 0.5513839190039793\n",
      "2022-03-26 21:19:31.697574 Epoch 200, Training Loss 0.5519987099692035\n",
      "2022-03-26 21:19:31.716563 Epoch 200, Training Loss 0.5525955877569325\n",
      "2022-03-26 21:19:31.734570 Epoch 200, Training Loss 0.5533406280571848\n",
      "2022-03-26 21:19:31.752580 Epoch 200, Training Loss 0.5538763497262964\n",
      "2022-03-26 21:19:31.770584 Epoch 200, Training Loss 0.5545793400929712\n",
      "2022-03-26 21:19:31.789595 Epoch 200, Training Loss 0.5553903759974043\n",
      "2022-03-26 21:19:31.807599 Epoch 200, Training Loss 0.5561700766272557\n",
      "2022-03-26 21:19:31.825603 Epoch 200, Training Loss 0.5568966045022925\n",
      "2022-03-26 21:19:31.843607 Epoch 200, Training Loss 0.557456062234881\n",
      "2022-03-26 21:19:31.862605 Epoch 200, Training Loss 0.5581450132686464\n",
      "2022-03-26 21:19:31.880615 Epoch 200, Training Loss 0.558893936255094\n",
      "2022-03-26 21:19:31.898619 Epoch 200, Training Loss 0.5597121585589235\n",
      "2022-03-26 21:19:31.915617 Epoch 200, Training Loss 0.5603880660079629\n",
      "2022-03-26 21:19:31.933627 Epoch 200, Training Loss 0.5611520637103053\n",
      "2022-03-26 21:19:31.951626 Epoch 200, Training Loss 0.5618593376082228\n",
      "2022-03-26 21:19:31.969629 Epoch 200, Training Loss 0.5625608779509049\n",
      "2022-03-26 21:19:31.987640 Epoch 200, Training Loss 0.5631744964500828\n",
      "2022-03-26 21:19:32.006638 Epoch 200, Training Loss 0.5640983993135145\n",
      "2022-03-26 21:19:32.025648 Epoch 200, Training Loss 0.5649582284033451\n",
      "2022-03-26 21:19:32.043652 Epoch 200, Training Loss 0.5657287240028381\n",
      "2022-03-26 21:19:32.061656 Epoch 200, Training Loss 0.5664801015268506\n",
      "2022-03-26 21:19:32.079661 Epoch 200, Training Loss 0.5672664788677869\n",
      "2022-03-26 21:19:32.097664 Epoch 200, Training Loss 0.5679247631593738\n",
      "2022-03-26 21:19:32.116652 Epoch 200, Training Loss 0.5687943214498212\n",
      "2022-03-26 21:19:32.134661 Epoch 200, Training Loss 0.569575936669279\n",
      "2022-03-26 21:19:32.152671 Epoch 200, Training Loss 0.5702927906799804\n",
      "2022-03-26 21:19:32.170681 Epoch 200, Training Loss 0.5710250986049242\n",
      "2022-03-26 21:19:32.188685 Epoch 200, Training Loss 0.5719751174492604\n",
      "2022-03-26 21:19:32.207684 Epoch 200, Training Loss 0.5725915160249261\n",
      "2022-03-26 21:19:32.226688 Epoch 200, Training Loss 0.5733135796492667\n",
      "2022-03-26 21:19:32.244698 Epoch 200, Training Loss 0.5740161995829829\n",
      "2022-03-26 21:19:32.263702 Epoch 200, Training Loss 0.5749533977121344\n",
      "2022-03-26 21:19:32.281706 Epoch 200, Training Loss 0.5758811407305701\n",
      "2022-03-26 21:19:32.300694 Epoch 200, Training Loss 0.576499278168849\n",
      "2022-03-26 21:19:32.318709 Epoch 200, Training Loss 0.5772322336654834\n",
      "2022-03-26 21:19:32.336713 Epoch 200, Training Loss 0.5781060116141653\n",
      "2022-03-26 21:19:32.354723 Epoch 200, Training Loss 0.5787516780140455\n",
      "2022-03-26 21:19:32.372721 Epoch 200, Training Loss 0.5794556414151131\n",
      "2022-03-26 21:19:32.390731 Epoch 200, Training Loss 0.580245026580208\n",
      "2022-03-26 21:19:32.409735 Epoch 200, Training Loss 0.5810911774711536\n",
      "2022-03-26 21:19:32.428740 Epoch 200, Training Loss 0.5819060960617821\n",
      "2022-03-26 21:19:32.447738 Epoch 200, Training Loss 0.5826795762380981\n",
      "2022-03-26 21:19:32.465748 Epoch 200, Training Loss 0.5835059139201099\n",
      "2022-03-26 21:19:32.484746 Epoch 200, Training Loss 0.5842236467563283\n",
      "2022-03-26 21:19:32.502755 Epoch 200, Training Loss 0.5848808178435201\n",
      "2022-03-26 21:19:32.520761 Epoch 200, Training Loss 0.5854634642601013\n",
      "2022-03-26 21:19:32.538764 Epoch 200, Training Loss 0.5862359832924651\n",
      "2022-03-26 21:19:32.557763 Epoch 200, Training Loss 0.5869515968100799\n",
      "2022-03-26 21:19:32.575773 Epoch 200, Training Loss 0.5877641362454885\n",
      "2022-03-26 21:19:32.583764 Epoch 200, Training Loss 0.588225375165415\n",
      "2022-03-26 21:30:50.412982 Epoch 250, Training Loss 0.0008926032601719927\n",
      "2022-03-26 21:30:50.431211 Epoch 250, Training Loss 0.0014686710618036177\n",
      "2022-03-26 21:30:50.450212 Epoch 250, Training Loss 0.00214265607049703\n",
      "2022-03-26 21:30:50.469923 Epoch 250, Training Loss 0.003132488226036891\n",
      "2022-03-26 21:30:50.488407 Epoch 250, Training Loss 0.0036531671157578375\n",
      "2022-03-26 21:30:50.508306 Epoch 250, Training Loss 0.004315368926433651\n",
      "2022-03-26 21:30:50.527531 Epoch 250, Training Loss 0.005086200735758027\n",
      "2022-03-26 21:30:50.547357 Epoch 250, Training Loss 0.005719675775379171\n",
      "2022-03-26 21:30:50.566214 Epoch 250, Training Loss 0.006287993524995301\n",
      "2022-03-26 21:30:50.584369 Epoch 250, Training Loss 0.007055927801620015\n",
      "2022-03-26 21:30:50.604217 Epoch 250, Training Loss 0.007806588240596645\n",
      "2022-03-26 21:30:50.625025 Epoch 250, Training Loss 0.008364817827863767\n",
      "2022-03-26 21:30:50.644890 Epoch 250, Training Loss 0.009381364373599781\n",
      "2022-03-26 21:30:50.664049 Epoch 250, Training Loss 0.009899326411964338\n",
      "2022-03-26 21:30:50.682715 Epoch 250, Training Loss 0.010708609314830711\n",
      "2022-03-26 21:30:50.701531 Epoch 250, Training Loss 0.01138139552320056\n",
      "2022-03-26 21:30:50.720909 Epoch 250, Training Loss 0.01205981863886499\n",
      "2022-03-26 21:30:50.740102 Epoch 250, Training Loss 0.013046909819173691\n",
      "2022-03-26 21:30:50.760270 Epoch 250, Training Loss 0.013755716288181218\n",
      "2022-03-26 21:30:50.779958 Epoch 250, Training Loss 0.014288868097697987\n",
      "2022-03-26 21:30:50.798459 Epoch 250, Training Loss 0.015035677558320868\n",
      "2022-03-26 21:30:50.817468 Epoch 250, Training Loss 0.015470922869794509\n",
      "2022-03-26 21:30:50.837360 Epoch 250, Training Loss 0.01634130495436051\n",
      "2022-03-26 21:30:50.857281 Epoch 250, Training Loss 0.017093428527302754\n",
      "2022-03-26 21:30:50.876557 Epoch 250, Training Loss 0.01799289790718147\n",
      "2022-03-26 21:30:50.896427 Epoch 250, Training Loss 0.018618498350043432\n",
      "2022-03-26 21:30:50.915003 Epoch 250, Training Loss 0.019563884495774193\n",
      "2022-03-26 21:30:50.934245 Epoch 250, Training Loss 0.020222480377882643\n",
      "2022-03-26 21:30:50.953688 Epoch 250, Training Loss 0.020868574376301387\n",
      "2022-03-26 21:30:50.972673 Epoch 250, Training Loss 0.021569909608882408\n",
      "2022-03-26 21:30:50.992564 Epoch 250, Training Loss 0.022100765801146817\n",
      "2022-03-26 21:30:51.011571 Epoch 250, Training Loss 0.022788389907468617\n",
      "2022-03-26 21:30:51.030567 Epoch 250, Training Loss 0.023311223215459254\n",
      "2022-03-26 21:30:51.049827 Epoch 250, Training Loss 0.02392102034805376\n",
      "2022-03-26 21:30:51.068883 Epoch 250, Training Loss 0.024511755351215372\n",
      "2022-03-26 21:30:51.087737 Epoch 250, Training Loss 0.02524595149337788\n",
      "2022-03-26 21:30:51.106794 Epoch 250, Training Loss 0.02588435508253629\n",
      "2022-03-26 21:30:51.126020 Epoch 250, Training Loss 0.026709616451007327\n",
      "2022-03-26 21:30:51.144532 Epoch 250, Training Loss 0.027434932949292994\n",
      "2022-03-26 21:30:51.164534 Epoch 250, Training Loss 0.028190935549833585\n",
      "2022-03-26 21:30:51.182569 Epoch 250, Training Loss 0.02920376221694605\n",
      "2022-03-26 21:30:51.201544 Epoch 250, Training Loss 0.0298813168731187\n",
      "2022-03-26 21:30:51.220571 Epoch 250, Training Loss 0.030674915979890263\n",
      "2022-03-26 21:30:51.239337 Epoch 250, Training Loss 0.031188582710902708\n",
      "2022-03-26 21:30:51.259358 Epoch 250, Training Loss 0.031951757838658966\n",
      "2022-03-26 21:30:51.279824 Epoch 250, Training Loss 0.03269102174760131\n",
      "2022-03-26 21:30:51.299547 Epoch 250, Training Loss 0.03318280377961181\n",
      "2022-03-26 21:30:51.319377 Epoch 250, Training Loss 0.03398915302113194\n",
      "2022-03-26 21:30:51.339062 Epoch 250, Training Loss 0.034829602217125465\n",
      "2022-03-26 21:30:51.359002 Epoch 250, Training Loss 0.035694653573243515\n",
      "2022-03-26 21:30:51.379038 Epoch 250, Training Loss 0.036171324570160694\n",
      "2022-03-26 21:30:51.398351 Epoch 250, Training Loss 0.03668936820286314\n",
      "2022-03-26 21:30:51.417855 Epoch 250, Training Loss 0.037218891331911697\n",
      "2022-03-26 21:30:51.438559 Epoch 250, Training Loss 0.0377506059225258\n",
      "2022-03-26 21:30:51.462233 Epoch 250, Training Loss 0.03854515474013356\n",
      "2022-03-26 21:30:51.484941 Epoch 250, Training Loss 0.03908993593414726\n",
      "2022-03-26 21:30:51.504402 Epoch 250, Training Loss 0.03945566683321658\n",
      "2022-03-26 21:30:51.524409 Epoch 250, Training Loss 0.04018078241354364\n",
      "2022-03-26 21:30:51.544292 Epoch 250, Training Loss 0.04096309581528539\n",
      "2022-03-26 21:30:51.564120 Epoch 250, Training Loss 0.04182621863339563\n",
      "2022-03-26 21:30:51.582900 Epoch 250, Training Loss 0.042531161395180254\n",
      "2022-03-26 21:30:51.602790 Epoch 250, Training Loss 0.043143988532178545\n",
      "2022-03-26 21:30:51.622673 Epoch 250, Training Loss 0.0439072349263579\n",
      "2022-03-26 21:30:51.642696 Epoch 250, Training Loss 0.0448175072288879\n",
      "2022-03-26 21:30:51.661752 Epoch 250, Training Loss 0.04565303797459663\n",
      "2022-03-26 21:30:51.683463 Epoch 250, Training Loss 0.046273185278448606\n",
      "2022-03-26 21:30:51.703488 Epoch 250, Training Loss 0.04707235002609165\n",
      "2022-03-26 21:30:51.723510 Epoch 250, Training Loss 0.04807509580993896\n",
      "2022-03-26 21:30:51.743364 Epoch 250, Training Loss 0.0486094758791082\n",
      "2022-03-26 21:30:51.762663 Epoch 250, Training Loss 0.04914548238525\n",
      "2022-03-26 21:30:51.782553 Epoch 250, Training Loss 0.04987172283175047\n",
      "2022-03-26 21:30:51.802572 Epoch 250, Training Loss 0.05047538373476404\n",
      "2022-03-26 21:30:51.822593 Epoch 250, Training Loss 0.051377343475971074\n",
      "2022-03-26 21:30:51.842503 Epoch 250, Training Loss 0.052033749840143695\n",
      "2022-03-26 21:30:51.861345 Epoch 250, Training Loss 0.052698798572925656\n",
      "2022-03-26 21:30:51.881356 Epoch 250, Training Loss 0.053450518251989806\n",
      "2022-03-26 21:30:51.901298 Epoch 250, Training Loss 0.05405622663552804\n",
      "2022-03-26 21:30:51.921312 Epoch 250, Training Loss 0.05448865151161428\n",
      "2022-03-26 21:30:51.941169 Epoch 250, Training Loss 0.055013818814016666\n",
      "2022-03-26 21:30:51.961328 Epoch 250, Training Loss 0.05582527964926132\n",
      "2022-03-26 21:30:51.981340 Epoch 250, Training Loss 0.05658449449807482\n",
      "2022-03-26 21:30:51.999352 Epoch 250, Training Loss 0.057455745880561106\n",
      "2022-03-26 21:30:52.018498 Epoch 250, Training Loss 0.05850346382621609\n",
      "2022-03-26 21:30:52.037470 Epoch 250, Training Loss 0.05915499076513988\n",
      "2022-03-26 21:30:52.056484 Epoch 250, Training Loss 0.0597606779974135\n",
      "2022-03-26 21:30:52.076531 Epoch 250, Training Loss 0.0605652785057302\n",
      "2022-03-26 21:30:52.095542 Epoch 250, Training Loss 0.06128349176148319\n",
      "2022-03-26 21:30:52.115565 Epoch 250, Training Loss 0.062070312478658185\n",
      "2022-03-26 21:30:52.134465 Epoch 250, Training Loss 0.06262234017214811\n",
      "2022-03-26 21:30:52.153862 Epoch 250, Training Loss 0.06351751420656433\n",
      "2022-03-26 21:30:52.172893 Epoch 250, Training Loss 0.06422307580480795\n",
      "2022-03-26 21:30:52.191619 Epoch 250, Training Loss 0.06487745180001954\n",
      "2022-03-26 21:30:52.211083 Epoch 250, Training Loss 0.06565537374190357\n",
      "2022-03-26 21:30:52.230150 Epoch 250, Training Loss 0.066243537284834\n",
      "2022-03-26 21:30:52.249807 Epoch 250, Training Loss 0.06695877434804921\n",
      "2022-03-26 21:30:52.268855 Epoch 250, Training Loss 0.06751256453259216\n",
      "2022-03-26 21:30:52.286964 Epoch 250, Training Loss 0.068499628425864\n",
      "2022-03-26 21:30:52.307526 Epoch 250, Training Loss 0.06907331535730825\n",
      "2022-03-26 21:30:52.326961 Epoch 250, Training Loss 0.06990018460299353\n",
      "2022-03-26 21:30:52.345279 Epoch 250, Training Loss 0.07040048678360326\n",
      "2022-03-26 21:30:52.364255 Epoch 250, Training Loss 0.07105666722940363\n",
      "2022-03-26 21:30:52.384145 Epoch 250, Training Loss 0.07209146697350476\n",
      "2022-03-26 21:30:52.403185 Epoch 250, Training Loss 0.07277649698202568\n",
      "2022-03-26 21:30:52.422802 Epoch 250, Training Loss 0.07370043323015618\n",
      "2022-03-26 21:30:52.442614 Epoch 250, Training Loss 0.0745351694505233\n",
      "2022-03-26 21:30:52.461778 Epoch 250, Training Loss 0.07508547158192491\n",
      "2022-03-26 21:30:52.480817 Epoch 250, Training Loss 0.07599971955999389\n",
      "2022-03-26 21:30:52.499800 Epoch 250, Training Loss 0.07656151627945473\n",
      "2022-03-26 21:30:52.519812 Epoch 250, Training Loss 0.07716420837832838\n",
      "2022-03-26 21:30:52.539863 Epoch 250, Training Loss 0.07794200684255956\n",
      "2022-03-26 21:30:52.558908 Epoch 250, Training Loss 0.07904564050000037\n",
      "2022-03-26 21:30:52.577965 Epoch 250, Training Loss 0.07978274286403071\n",
      "2022-03-26 21:30:52.596419 Epoch 250, Training Loss 0.08048444487097318\n",
      "2022-03-26 21:30:52.615600 Epoch 250, Training Loss 0.08108620993469072\n",
      "2022-03-26 21:30:52.634752 Epoch 250, Training Loss 0.08182177728856616\n",
      "2022-03-26 21:30:52.653745 Epoch 250, Training Loss 0.082852904487144\n",
      "2022-03-26 21:30:52.672669 Epoch 250, Training Loss 0.08334121965538815\n",
      "2022-03-26 21:30:52.692690 Epoch 250, Training Loss 0.08403597909318822\n",
      "2022-03-26 21:30:52.712773 Epoch 250, Training Loss 0.08455845797458268\n",
      "2022-03-26 21:30:52.731946 Epoch 250, Training Loss 0.08541763072733378\n",
      "2022-03-26 21:30:52.751909 Epoch 250, Training Loss 0.08602130333023608\n",
      "2022-03-26 21:30:52.770951 Epoch 250, Training Loss 0.08682090573755981\n",
      "2022-03-26 21:30:52.790819 Epoch 250, Training Loss 0.08746067615573669\n",
      "2022-03-26 21:30:52.810879 Epoch 250, Training Loss 0.08812045917639037\n",
      "2022-03-26 21:30:52.829435 Epoch 250, Training Loss 0.08879503588694745\n",
      "2022-03-26 21:30:52.848918 Epoch 250, Training Loss 0.08970694914650734\n",
      "2022-03-26 21:30:52.867775 Epoch 250, Training Loss 0.09042200312742492\n",
      "2022-03-26 21:30:52.887318 Epoch 250, Training Loss 0.09109922622323341\n",
      "2022-03-26 21:30:52.906595 Epoch 250, Training Loss 0.09196100374469367\n",
      "2022-03-26 21:30:52.926513 Epoch 250, Training Loss 0.09270005202506815\n",
      "2022-03-26 21:30:52.945524 Epoch 250, Training Loss 0.0932590640566843\n",
      "2022-03-26 21:30:52.965432 Epoch 250, Training Loss 0.09414848395625648\n",
      "2022-03-26 21:30:52.985012 Epoch 250, Training Loss 0.09493083356286558\n",
      "2022-03-26 21:30:53.004830 Epoch 250, Training Loss 0.09567996890038785\n",
      "2022-03-26 21:30:53.023805 Epoch 250, Training Loss 0.09633508736215284\n",
      "2022-03-26 21:30:53.043968 Epoch 250, Training Loss 0.09703463742800077\n",
      "2022-03-26 21:30:53.062915 Epoch 250, Training Loss 0.09784323990802325\n",
      "2022-03-26 21:30:53.082030 Epoch 250, Training Loss 0.09848803106476278\n",
      "2022-03-26 21:30:53.101048 Epoch 250, Training Loss 0.09927982968442581\n",
      "2022-03-26 21:30:53.120196 Epoch 250, Training Loss 0.0997023511192073\n",
      "2022-03-26 21:30:53.138742 Epoch 250, Training Loss 0.1004796462976719\n",
      "2022-03-26 21:30:53.159120 Epoch 250, Training Loss 0.10117085232301747\n",
      "2022-03-26 21:30:53.178249 Epoch 250, Training Loss 0.1016871448977829\n",
      "2022-03-26 21:30:53.196799 Epoch 250, Training Loss 0.10246474046231535\n",
      "2022-03-26 21:30:53.216021 Epoch 250, Training Loss 0.10330722604871101\n",
      "2022-03-26 21:30:53.235560 Epoch 250, Training Loss 0.1040020713111019\n",
      "2022-03-26 21:30:53.254608 Epoch 250, Training Loss 0.10471440474395557\n",
      "2022-03-26 21:30:53.274613 Epoch 250, Training Loss 0.10518501458875358\n",
      "2022-03-26 21:30:53.293641 Epoch 250, Training Loss 0.10588553624079965\n",
      "2022-03-26 21:30:53.313652 Epoch 250, Training Loss 0.10647726642048877\n",
      "2022-03-26 21:30:53.332634 Epoch 250, Training Loss 0.10743785586655902\n",
      "2022-03-26 21:30:53.352636 Epoch 250, Training Loss 0.10834986409720253\n",
      "2022-03-26 21:30:53.371993 Epoch 250, Training Loss 0.10916360408601249\n",
      "2022-03-26 21:30:53.391025 Epoch 250, Training Loss 0.10988153151386534\n",
      "2022-03-26 21:30:53.411048 Epoch 250, Training Loss 0.11044576135285371\n",
      "2022-03-26 21:30:53.431890 Epoch 250, Training Loss 0.11117437508557458\n",
      "2022-03-26 21:30:53.450850 Epoch 250, Training Loss 0.11177027911481345\n",
      "2022-03-26 21:30:53.469948 Epoch 250, Training Loss 0.11256661119363497\n",
      "2022-03-26 21:30:53.489313 Epoch 250, Training Loss 0.11313536408764627\n",
      "2022-03-26 21:30:53.508589 Epoch 250, Training Loss 0.1137339212095646\n",
      "2022-03-26 21:30:53.527711 Epoch 250, Training Loss 0.1144777593557792\n",
      "2022-03-26 21:30:53.548697 Epoch 250, Training Loss 0.11522814555241324\n",
      "2022-03-26 21:30:53.567711 Epoch 250, Training Loss 0.1160081557148253\n",
      "2022-03-26 21:30:53.586601 Epoch 250, Training Loss 0.116520415181699\n",
      "2022-03-26 21:30:53.605608 Epoch 250, Training Loss 0.11702326565142483\n",
      "2022-03-26 21:30:53.625481 Epoch 250, Training Loss 0.11764023401548186\n",
      "2022-03-26 21:30:53.644483 Epoch 250, Training Loss 0.11808562877080629\n",
      "2022-03-26 21:30:53.663513 Epoch 250, Training Loss 0.11878805091161557\n",
      "2022-03-26 21:30:53.683543 Epoch 250, Training Loss 0.1195211230260332\n",
      "2022-03-26 21:30:53.702541 Epoch 250, Training Loss 0.12017257107645654\n",
      "2022-03-26 21:30:53.721547 Epoch 250, Training Loss 0.12084955651589367\n",
      "2022-03-26 21:30:53.741576 Epoch 250, Training Loss 0.12127485939913699\n",
      "2022-03-26 21:30:53.760564 Epoch 250, Training Loss 0.12229066363076115\n",
      "2022-03-26 21:30:53.780336 Epoch 250, Training Loss 0.12286663154506927\n",
      "2022-03-26 21:30:53.800361 Epoch 250, Training Loss 0.1234063542712375\n",
      "2022-03-26 21:30:53.819808 Epoch 250, Training Loss 0.12398986590792761\n",
      "2022-03-26 21:30:53.839490 Epoch 250, Training Loss 0.12452622665011365\n",
      "2022-03-26 21:30:53.859505 Epoch 250, Training Loss 0.1254194688690288\n",
      "2022-03-26 21:30:53.879440 Epoch 250, Training Loss 0.12639875565191058\n",
      "2022-03-26 21:30:53.899302 Epoch 250, Training Loss 0.12701682704488945\n",
      "2022-03-26 21:30:53.917777 Epoch 250, Training Loss 0.12770726103002153\n",
      "2022-03-26 21:30:53.937493 Epoch 250, Training Loss 0.1283205181665128\n",
      "2022-03-26 21:30:53.956495 Epoch 250, Training Loss 0.1291056690389848\n",
      "2022-03-26 21:30:53.977016 Epoch 250, Training Loss 0.12977825333852597\n",
      "2022-03-26 21:30:53.996798 Epoch 250, Training Loss 0.1303910394306378\n",
      "2022-03-26 21:30:54.014947 Epoch 250, Training Loss 0.1311187377518705\n",
      "2022-03-26 21:30:54.034382 Epoch 250, Training Loss 0.13195660634114004\n",
      "2022-03-26 21:30:54.054237 Epoch 250, Training Loss 0.13263861099472435\n",
      "2022-03-26 21:30:54.073344 Epoch 250, Training Loss 0.13327037106694467\n",
      "2022-03-26 21:30:54.093271 Epoch 250, Training Loss 0.13401222427177917\n",
      "2022-03-26 21:30:54.112366 Epoch 250, Training Loss 0.1348074140298702\n",
      "2022-03-26 21:30:54.131372 Epoch 250, Training Loss 0.13557104846400678\n",
      "2022-03-26 21:30:54.150390 Epoch 250, Training Loss 0.1363513481129161\n",
      "2022-03-26 21:30:54.170412 Epoch 250, Training Loss 0.13728070701174724\n",
      "2022-03-26 21:30:54.190261 Epoch 250, Training Loss 0.13802305100214146\n",
      "2022-03-26 21:30:54.210286 Epoch 250, Training Loss 0.13859331714527687\n",
      "2022-03-26 21:30:54.229253 Epoch 250, Training Loss 0.1391721521039753\n",
      "2022-03-26 21:30:54.248162 Epoch 250, Training Loss 0.13976353666056757\n",
      "2022-03-26 21:30:54.268024 Epoch 250, Training Loss 0.14051731269987647\n",
      "2022-03-26 21:30:54.287558 Epoch 250, Training Loss 0.1415911740659143\n",
      "2022-03-26 21:30:54.305879 Epoch 250, Training Loss 0.14233637442979055\n",
      "2022-03-26 21:30:54.326818 Epoch 250, Training Loss 0.14290789768214116\n",
      "2022-03-26 21:30:54.346396 Epoch 250, Training Loss 0.14372431713601816\n",
      "2022-03-26 21:30:54.365436 Epoch 250, Training Loss 0.14440915895544965\n",
      "2022-03-26 21:30:54.385356 Epoch 250, Training Loss 0.14517424013608557\n",
      "2022-03-26 21:30:54.404744 Epoch 250, Training Loss 0.14593065211839992\n",
      "2022-03-26 21:30:54.423771 Epoch 250, Training Loss 0.14648914169472502\n",
      "2022-03-26 21:30:54.443602 Epoch 250, Training Loss 0.14717764211127826\n",
      "2022-03-26 21:30:54.463007 Epoch 250, Training Loss 0.14779659579781926\n",
      "2022-03-26 21:30:54.481963 Epoch 250, Training Loss 0.14847128097053683\n",
      "2022-03-26 21:30:54.502044 Epoch 250, Training Loss 0.14899972405122675\n",
      "2022-03-26 21:30:54.521121 Epoch 250, Training Loss 0.14965725733953364\n",
      "2022-03-26 21:30:54.540181 Epoch 250, Training Loss 0.15037757192578766\n",
      "2022-03-26 21:30:54.567284 Epoch 250, Training Loss 0.15117581059103427\n",
      "2022-03-26 21:30:54.595739 Epoch 250, Training Loss 0.15169849888900358\n",
      "2022-03-26 21:30:54.621831 Epoch 250, Training Loss 0.15221133675721601\n",
      "2022-03-26 21:30:54.648738 Epoch 250, Training Loss 0.15297217952930714\n",
      "2022-03-26 21:30:54.674867 Epoch 250, Training Loss 0.1535343268262151\n",
      "2022-03-26 21:30:54.701432 Epoch 250, Training Loss 0.15429363923761852\n",
      "2022-03-26 21:30:54.727781 Epoch 250, Training Loss 0.15473873482640746\n",
      "2022-03-26 21:30:54.754586 Epoch 250, Training Loss 0.15555501586336004\n",
      "2022-03-26 21:30:54.780561 Epoch 250, Training Loss 0.15648115123324383\n",
      "2022-03-26 21:30:54.806475 Epoch 250, Training Loss 0.15689613481464287\n",
      "2022-03-26 21:30:54.832909 Epoch 250, Training Loss 0.1574553495935162\n",
      "2022-03-26 21:30:54.859067 Epoch 250, Training Loss 0.15827646264639658\n",
      "2022-03-26 21:30:54.885109 Epoch 250, Training Loss 0.15881545997946464\n",
      "2022-03-26 21:30:54.911205 Epoch 250, Training Loss 0.1595517960960603\n",
      "2022-03-26 21:30:54.937143 Epoch 250, Training Loss 0.16017354277851026\n",
      "2022-03-26 21:30:54.956197 Epoch 250, Training Loss 0.1608852790216046\n",
      "2022-03-26 21:30:54.974260 Epoch 250, Training Loss 0.1618948019755161\n",
      "2022-03-26 21:30:54.992895 Epoch 250, Training Loss 0.16263998839099084\n",
      "2022-03-26 21:30:55.012013 Epoch 250, Training Loss 0.16324733651202658\n",
      "2022-03-26 21:30:55.030075 Epoch 250, Training Loss 0.1640100312202483\n",
      "2022-03-26 21:30:55.047593 Epoch 250, Training Loss 0.16471775680246864\n",
      "2022-03-26 21:30:55.065821 Epoch 250, Training Loss 0.1654045329526867\n",
      "2022-03-26 21:30:55.083835 Epoch 250, Training Loss 0.16611406984536545\n",
      "2022-03-26 21:30:55.101962 Epoch 250, Training Loss 0.16679626947168805\n",
      "2022-03-26 21:30:55.119993 Epoch 250, Training Loss 0.16739350858399327\n",
      "2022-03-26 21:30:55.138493 Epoch 250, Training Loss 0.16819684138840726\n",
      "2022-03-26 21:30:55.157331 Epoch 250, Training Loss 0.16901959936179772\n",
      "2022-03-26 21:30:55.175713 Epoch 250, Training Loss 0.1698717723798264\n",
      "2022-03-26 21:30:55.194743 Epoch 250, Training Loss 0.17060795277738205\n",
      "2022-03-26 21:30:55.212751 Epoch 250, Training Loss 0.17121330353305164\n",
      "2022-03-26 21:30:55.231305 Epoch 250, Training Loss 0.17185981156271132\n",
      "2022-03-26 21:30:55.249338 Epoch 250, Training Loss 0.17268406917981785\n",
      "2022-03-26 21:30:55.267585 Epoch 250, Training Loss 0.1732029296324381\n",
      "2022-03-26 21:30:55.286599 Epoch 250, Training Loss 0.1739335535356151\n",
      "2022-03-26 21:30:55.305712 Epoch 250, Training Loss 0.17490161254125483\n",
      "2022-03-26 21:30:55.324134 Epoch 250, Training Loss 0.17577217679346918\n",
      "2022-03-26 21:30:55.342966 Epoch 250, Training Loss 0.1765765609110103\n",
      "2022-03-26 21:30:55.360928 Epoch 250, Training Loss 0.17733206624722542\n",
      "2022-03-26 21:30:55.380064 Epoch 250, Training Loss 0.17807167932353057\n",
      "2022-03-26 21:30:55.397395 Epoch 250, Training Loss 0.17860054626794117\n",
      "2022-03-26 21:30:55.416315 Epoch 250, Training Loss 0.17908771964900025\n",
      "2022-03-26 21:30:55.434368 Epoch 250, Training Loss 0.17955481781221716\n",
      "2022-03-26 21:30:55.452416 Epoch 250, Training Loss 0.18011278729609517\n",
      "2022-03-26 21:30:55.470420 Epoch 250, Training Loss 0.18060312620209307\n",
      "2022-03-26 21:30:55.490388 Epoch 250, Training Loss 0.18110355704336825\n",
      "2022-03-26 21:30:55.508263 Epoch 250, Training Loss 0.18156878535857285\n",
      "2022-03-26 21:30:55.527198 Epoch 250, Training Loss 0.18214265198049034\n",
      "2022-03-26 21:30:55.546049 Epoch 250, Training Loss 0.18301914902904148\n",
      "2022-03-26 21:30:55.564173 Epoch 250, Training Loss 0.18384947755452616\n",
      "2022-03-26 21:30:55.582112 Epoch 250, Training Loss 0.1844144230303557\n",
      "2022-03-26 21:30:55.600136 Epoch 250, Training Loss 0.18497767621446448\n",
      "2022-03-26 21:30:55.620282 Epoch 250, Training Loss 0.18557141801280438\n",
      "2022-03-26 21:30:55.638135 Epoch 250, Training Loss 0.18622624607342284\n",
      "2022-03-26 21:30:55.657147 Epoch 250, Training Loss 0.18699514690567465\n",
      "2022-03-26 21:30:55.676184 Epoch 250, Training Loss 0.18780458575624334\n",
      "2022-03-26 21:30:55.694145 Epoch 250, Training Loss 0.18839332892004487\n",
      "2022-03-26 21:30:55.713113 Epoch 250, Training Loss 0.18888960176569117\n",
      "2022-03-26 21:30:55.731152 Epoch 250, Training Loss 0.1897918208099692\n",
      "2022-03-26 21:30:55.749088 Epoch 250, Training Loss 0.19057194492243745\n",
      "2022-03-26 21:30:55.767143 Epoch 250, Training Loss 0.19130821099214237\n",
      "2022-03-26 21:30:55.785486 Epoch 250, Training Loss 0.19193897394420545\n",
      "2022-03-26 21:30:55.804519 Epoch 250, Training Loss 0.19275318322431706\n",
      "2022-03-26 21:30:55.823534 Epoch 250, Training Loss 0.1934867538225925\n",
      "2022-03-26 21:30:55.842543 Epoch 250, Training Loss 0.19408268601540715\n",
      "2022-03-26 21:30:55.861553 Epoch 250, Training Loss 0.19481974889707687\n",
      "2022-03-26 21:30:55.880686 Epoch 250, Training Loss 0.19542851544859463\n",
      "2022-03-26 21:30:55.899614 Epoch 250, Training Loss 0.19622002069450095\n",
      "2022-03-26 21:30:55.917696 Epoch 250, Training Loss 0.1970340898808311\n",
      "2022-03-26 21:30:55.935551 Epoch 250, Training Loss 0.19763116454681778\n",
      "2022-03-26 21:30:55.954414 Epoch 250, Training Loss 0.1985946337661475\n",
      "2022-03-26 21:30:55.972775 Epoch 250, Training Loss 0.19952666245000747\n",
      "2022-03-26 21:30:55.990804 Epoch 250, Training Loss 0.200345042149734\n",
      "2022-03-26 21:30:56.010028 Epoch 250, Training Loss 0.20125298133439115\n",
      "2022-03-26 21:30:56.030096 Epoch 250, Training Loss 0.20201956353071707\n",
      "2022-03-26 21:30:56.049052 Epoch 250, Training Loss 0.20263443662382452\n",
      "2022-03-26 21:30:56.066748 Epoch 250, Training Loss 0.2033271195028749\n",
      "2022-03-26 21:30:56.085143 Epoch 250, Training Loss 0.20389871352621355\n",
      "2022-03-26 21:30:56.103152 Epoch 250, Training Loss 0.20467608065708823\n",
      "2022-03-26 21:30:56.122198 Epoch 250, Training Loss 0.20537901500149455\n",
      "2022-03-26 21:30:56.139301 Epoch 250, Training Loss 0.2059694222172203\n",
      "2022-03-26 21:30:56.158311 Epoch 250, Training Loss 0.2063708575942632\n",
      "2022-03-26 21:30:56.177314 Epoch 250, Training Loss 0.20702625128924085\n",
      "2022-03-26 21:30:56.196074 Epoch 250, Training Loss 0.2076267074136173\n",
      "2022-03-26 21:30:56.214014 Epoch 250, Training Loss 0.2082957431026127\n",
      "2022-03-26 21:30:56.233102 Epoch 250, Training Loss 0.2091617883776155\n",
      "2022-03-26 21:30:56.251138 Epoch 250, Training Loss 0.21007884257589765\n",
      "2022-03-26 21:30:56.269166 Epoch 250, Training Loss 0.2108530513465862\n",
      "2022-03-26 21:30:56.287719 Epoch 250, Training Loss 0.21180392241538942\n",
      "2022-03-26 21:30:56.306727 Epoch 250, Training Loss 0.21240979661722012\n",
      "2022-03-26 21:30:56.325350 Epoch 250, Training Loss 0.21312013154139603\n",
      "2022-03-26 21:30:56.343240 Epoch 250, Training Loss 0.21391876060944384\n",
      "2022-03-26 21:30:56.361307 Epoch 250, Training Loss 0.21478622084688348\n",
      "2022-03-26 21:30:56.380131 Epoch 250, Training Loss 0.21544285610203853\n",
      "2022-03-26 21:30:56.397082 Epoch 250, Training Loss 0.21630443453483875\n",
      "2022-03-26 21:30:56.416193 Epoch 250, Training Loss 0.2173597943752318\n",
      "2022-03-26 21:30:56.434534 Epoch 250, Training Loss 0.21810540305379103\n",
      "2022-03-26 21:30:56.452478 Epoch 250, Training Loss 0.21866016185192197\n",
      "2022-03-26 21:30:56.471512 Epoch 250, Training Loss 0.219214354748921\n",
      "2022-03-26 21:30:56.489578 Epoch 250, Training Loss 0.22001125109012779\n",
      "2022-03-26 21:30:56.508945 Epoch 250, Training Loss 0.22057008590844587\n",
      "2022-03-26 21:30:56.527202 Epoch 250, Training Loss 0.22133602617341844\n",
      "2022-03-26 21:30:56.547098 Epoch 250, Training Loss 0.22200484745337834\n",
      "2022-03-26 21:30:56.565136 Epoch 250, Training Loss 0.2228372022319023\n",
      "2022-03-26 21:30:56.583377 Epoch 250, Training Loss 0.2236001453436244\n",
      "2022-03-26 21:30:56.601378 Epoch 250, Training Loss 0.22416260186820994\n",
      "2022-03-26 21:30:56.618810 Epoch 250, Training Loss 0.22488919811328048\n",
      "2022-03-26 21:30:56.637751 Epoch 250, Training Loss 0.22559328186694924\n",
      "2022-03-26 21:30:56.656901 Epoch 250, Training Loss 0.22644133522839802\n",
      "2022-03-26 21:30:56.675926 Epoch 250, Training Loss 0.22727394969109685\n",
      "2022-03-26 21:30:56.694974 Epoch 250, Training Loss 0.2279452248988554\n",
      "2022-03-26 21:30:56.713039 Epoch 250, Training Loss 0.22850970199803258\n",
      "2022-03-26 21:30:56.730991 Epoch 250, Training Loss 0.2292067412753849\n",
      "2022-03-26 21:30:56.749851 Epoch 250, Training Loss 0.22979829572808103\n",
      "2022-03-26 21:30:56.767861 Epoch 250, Training Loss 0.23061061496167537\n",
      "2022-03-26 21:30:56.795555 Epoch 250, Training Loss 0.2314156046532609\n",
      "2022-03-26 21:30:56.827509 Epoch 250, Training Loss 0.2319156773712324\n",
      "2022-03-26 21:30:56.862288 Epoch 250, Training Loss 0.2323828283173349\n",
      "2022-03-26 21:30:56.888104 Epoch 250, Training Loss 0.23297142201219984\n",
      "2022-03-26 21:30:56.917110 Epoch 250, Training Loss 0.23358753914265987\n",
      "2022-03-26 21:30:56.944095 Epoch 250, Training Loss 0.23429175963639604\n",
      "2022-03-26 21:30:56.969981 Epoch 250, Training Loss 0.23503859165836782\n",
      "2022-03-26 21:30:56.988919 Epoch 250, Training Loss 0.235840250197274\n",
      "2022-03-26 21:30:57.005917 Epoch 250, Training Loss 0.23666799011285347\n",
      "2022-03-26 21:30:57.024523 Epoch 250, Training Loss 0.23740127587409884\n",
      "2022-03-26 21:30:57.043531 Epoch 250, Training Loss 0.2379198355595474\n",
      "2022-03-26 21:30:57.062826 Epoch 250, Training Loss 0.23864164780777739\n",
      "2022-03-26 21:30:57.080754 Epoch 250, Training Loss 0.2393198943199099\n",
      "2022-03-26 21:30:57.098760 Epoch 250, Training Loss 0.23980497841335013\n",
      "2022-03-26 21:30:57.116754 Epoch 250, Training Loss 0.2404790428440894\n",
      "2022-03-26 21:30:57.135137 Epoch 250, Training Loss 0.24112044331972557\n",
      "2022-03-26 21:30:57.153151 Epoch 250, Training Loss 0.24190156318037712\n",
      "2022-03-26 21:30:57.171203 Epoch 250, Training Loss 0.2426256033618127\n",
      "2022-03-26 21:30:57.190095 Epoch 250, Training Loss 0.24330845330377368\n",
      "2022-03-26 21:30:57.208807 Epoch 250, Training Loss 0.24396128422768829\n",
      "2022-03-26 21:30:57.226863 Epoch 250, Training Loss 0.2447229616934686\n",
      "2022-03-26 21:30:57.245883 Epoch 250, Training Loss 0.24553059395926688\n",
      "2022-03-26 21:30:57.264017 Epoch 250, Training Loss 0.2460613078473474\n",
      "2022-03-26 21:30:57.281961 Epoch 250, Training Loss 0.2469004022190943\n",
      "2022-03-26 21:30:57.299997 Epoch 250, Training Loss 0.24766407400140983\n",
      "2022-03-26 21:30:57.318082 Epoch 250, Training Loss 0.24855497639502405\n",
      "2022-03-26 21:30:57.336085 Epoch 250, Training Loss 0.24934535494545843\n",
      "2022-03-26 21:30:57.355130 Epoch 250, Training Loss 0.24984145819988396\n",
      "2022-03-26 21:30:57.373059 Epoch 250, Training Loss 0.2507145458170215\n",
      "2022-03-26 21:30:57.391064 Epoch 250, Training Loss 0.2514481413394899\n",
      "2022-03-26 21:30:57.409138 Epoch 250, Training Loss 0.2523097098635895\n",
      "2022-03-26 21:30:57.427327 Epoch 250, Training Loss 0.25314176189320164\n",
      "2022-03-26 21:30:57.446014 Epoch 250, Training Loss 0.25378681067615516\n",
      "2022-03-26 21:30:57.465075 Epoch 250, Training Loss 0.25460712577375916\n",
      "2022-03-26 21:30:57.483150 Epoch 250, Training Loss 0.2554946175926482\n",
      "2022-03-26 21:30:57.501191 Epoch 250, Training Loss 0.256258821014858\n",
      "2022-03-26 21:30:57.519332 Epoch 250, Training Loss 0.2570028371365784\n",
      "2022-03-26 21:30:57.537287 Epoch 250, Training Loss 0.2576686819953382\n",
      "2022-03-26 21:30:57.555341 Epoch 250, Training Loss 0.2582920280563862\n",
      "2022-03-26 21:30:57.573701 Epoch 250, Training Loss 0.25911871620151394\n",
      "2022-03-26 21:30:57.592544 Epoch 250, Training Loss 0.25998647758722915\n",
      "2022-03-26 21:30:57.610897 Epoch 250, Training Loss 0.26080585402600903\n",
      "2022-03-26 21:30:57.629914 Epoch 250, Training Loss 0.26137283802642236\n",
      "2022-03-26 21:30:57.647933 Epoch 250, Training Loss 0.2620026864435362\n",
      "2022-03-26 21:30:57.665995 Epoch 250, Training Loss 0.2626803449886229\n",
      "2022-03-26 21:30:57.685049 Epoch 250, Training Loss 0.26364605738531294\n",
      "2022-03-26 21:30:57.703417 Epoch 250, Training Loss 0.26473530513398785\n",
      "2022-03-26 21:30:57.721451 Epoch 250, Training Loss 0.26546187813172256\n",
      "2022-03-26 21:30:57.739284 Epoch 250, Training Loss 0.2661783988289821\n",
      "2022-03-26 21:30:57.758296 Epoch 250, Training Loss 0.2668606563830924\n",
      "2022-03-26 21:30:57.776321 Epoch 250, Training Loss 0.267713583148349\n",
      "2022-03-26 21:30:57.794160 Epoch 250, Training Loss 0.26855824514270743\n",
      "2022-03-26 21:30:57.813126 Epoch 250, Training Loss 0.2693803646146794\n",
      "2022-03-26 21:30:57.831191 Epoch 250, Training Loss 0.2702017723370696\n",
      "2022-03-26 21:30:57.849196 Epoch 250, Training Loss 0.27080379441723496\n",
      "2022-03-26 21:30:57.868258 Epoch 250, Training Loss 0.2716564118023724\n",
      "2022-03-26 21:30:57.886148 Epoch 250, Training Loss 0.27237993810335387\n",
      "2022-03-26 21:30:57.904294 Epoch 250, Training Loss 0.27339994659661637\n",
      "2022-03-26 21:30:57.923444 Epoch 250, Training Loss 0.2739546707524058\n",
      "2022-03-26 21:30:57.942600 Epoch 250, Training Loss 0.27473743667687905\n",
      "2022-03-26 21:30:57.961606 Epoch 250, Training Loss 0.2753946238466541\n",
      "2022-03-26 21:30:57.980152 Epoch 250, Training Loss 0.2761645722572151\n",
      "2022-03-26 21:30:57.997429 Epoch 250, Training Loss 0.27680978895453234\n",
      "2022-03-26 21:30:58.015595 Epoch 250, Training Loss 0.2774967692239815\n",
      "2022-03-26 21:30:58.033930 Epoch 250, Training Loss 0.2781047441846574\n",
      "2022-03-26 21:30:58.051881 Epoch 250, Training Loss 0.2788276678079839\n",
      "2022-03-26 21:30:58.069906 Epoch 250, Training Loss 0.27970348946426227\n",
      "2022-03-26 21:30:58.088866 Epoch 250, Training Loss 0.2805040299206439\n",
      "2022-03-26 21:30:58.107818 Epoch 250, Training Loss 0.28139532896716274\n",
      "2022-03-26 21:30:58.125917 Epoch 250, Training Loss 0.2820874084825711\n",
      "2022-03-26 21:30:58.143948 Epoch 250, Training Loss 0.2827941715488653\n",
      "2022-03-26 21:30:58.162064 Epoch 250, Training Loss 0.2834094580634476\n",
      "2022-03-26 21:30:58.180103 Epoch 250, Training Loss 0.283925975641936\n",
      "2022-03-26 21:30:58.198118 Epoch 250, Training Loss 0.284394821128272\n",
      "2022-03-26 21:30:58.216195 Epoch 250, Training Loss 0.28515911814959155\n",
      "2022-03-26 21:30:58.234195 Epoch 250, Training Loss 0.2858777579367923\n",
      "2022-03-26 21:30:58.252185 Epoch 250, Training Loss 0.2866744535887028\n",
      "2022-03-26 21:30:58.270190 Epoch 250, Training Loss 0.28744615439106436\n",
      "2022-03-26 21:30:58.289285 Epoch 250, Training Loss 0.2883935193042926\n",
      "2022-03-26 21:30:58.307311 Epoch 250, Training Loss 0.28906262290599705\n",
      "2022-03-26 21:30:58.325389 Epoch 250, Training Loss 0.2899136994882008\n",
      "2022-03-26 21:30:58.344173 Epoch 250, Training Loss 0.2904818550019008\n",
      "2022-03-26 21:30:58.362416 Epoch 250, Training Loss 0.2915773888301971\n",
      "2022-03-26 21:30:58.380626 Epoch 250, Training Loss 0.2920594085055544\n",
      "2022-03-26 21:30:58.398279 Epoch 250, Training Loss 0.2927348423949288\n",
      "2022-03-26 21:30:58.416147 Epoch 250, Training Loss 0.29348708959796543\n",
      "2022-03-26 21:30:58.435140 Epoch 250, Training Loss 0.29399550502257576\n",
      "2022-03-26 21:30:58.453070 Epoch 250, Training Loss 0.29472072182408987\n",
      "2022-03-26 21:30:58.471109 Epoch 250, Training Loss 0.2955195905302492\n",
      "2022-03-26 21:30:58.490193 Epoch 250, Training Loss 0.2961679617004931\n",
      "2022-03-26 21:30:58.508161 Epoch 250, Training Loss 0.2968384009950301\n",
      "2022-03-26 21:30:58.527159 Epoch 250, Training Loss 0.2974944804483058\n",
      "2022-03-26 21:30:58.546155 Epoch 250, Training Loss 0.298094835237164\n",
      "2022-03-26 21:30:58.565142 Epoch 250, Training Loss 0.2988682042073716\n",
      "2022-03-26 21:30:58.583193 Epoch 250, Training Loss 0.2995152916673504\n",
      "2022-03-26 21:30:58.601280 Epoch 250, Training Loss 0.29996520635264606\n",
      "2022-03-26 21:30:58.620279 Epoch 250, Training Loss 0.3007652853684657\n",
      "2022-03-26 21:30:58.638093 Epoch 250, Training Loss 0.30148795601504536\n",
      "2022-03-26 21:30:58.656025 Epoch 250, Training Loss 0.30216371582444673\n",
      "2022-03-26 21:30:58.675095 Epoch 250, Training Loss 0.30288147503305274\n",
      "2022-03-26 21:30:58.692907 Epoch 250, Training Loss 0.3037490637787163\n",
      "2022-03-26 21:30:58.711918 Epoch 250, Training Loss 0.30455582964298367\n",
      "2022-03-26 21:30:58.729951 Epoch 250, Training Loss 0.3054263366533972\n",
      "2022-03-26 21:30:58.747996 Epoch 250, Training Loss 0.3061509092369348\n",
      "2022-03-26 21:30:58.765851 Epoch 250, Training Loss 0.30721379984217834\n",
      "2022-03-26 21:30:58.783804 Epoch 250, Training Loss 0.3079994299146525\n",
      "2022-03-26 21:30:58.801859 Epoch 250, Training Loss 0.3086359382361707\n",
      "2022-03-26 21:30:58.821197 Epoch 250, Training Loss 0.30939823697747476\n",
      "2022-03-26 21:30:58.840087 Epoch 250, Training Loss 0.31002941666661626\n",
      "2022-03-26 21:30:58.858033 Epoch 250, Training Loss 0.3107378647455474\n",
      "2022-03-26 21:30:58.877414 Epoch 250, Training Loss 0.3114682635687806\n",
      "2022-03-26 21:30:58.896262 Epoch 250, Training Loss 0.3122163120742954\n",
      "2022-03-26 21:30:58.914875 Epoch 250, Training Loss 0.31294271311796534\n",
      "2022-03-26 21:30:58.933362 Epoch 250, Training Loss 0.31360138758369116\n",
      "2022-03-26 21:30:58.951293 Epoch 250, Training Loss 0.3144820032979521\n",
      "2022-03-26 21:30:58.969565 Epoch 250, Training Loss 0.3151581583120634\n",
      "2022-03-26 21:30:58.988443 Epoch 250, Training Loss 0.31570777624769286\n",
      "2022-03-26 21:30:59.006459 Epoch 250, Training Loss 0.3162580258248712\n",
      "2022-03-26 21:30:59.025496 Epoch 250, Training Loss 0.3169770648564829\n",
      "2022-03-26 21:30:59.043523 Epoch 250, Training Loss 0.31763614352096986\n",
      "2022-03-26 21:30:59.062716 Epoch 250, Training Loss 0.31831903690877167\n",
      "2022-03-26 21:30:59.080949 Epoch 250, Training Loss 0.3190438697862503\n",
      "2022-03-26 21:30:59.099886 Epoch 250, Training Loss 0.31968041938131725\n",
      "2022-03-26 21:30:59.118987 Epoch 250, Training Loss 0.32045301082341565\n",
      "2022-03-26 21:30:59.137971 Epoch 250, Training Loss 0.3211993902464352\n",
      "2022-03-26 21:30:59.155953 Epoch 250, Training Loss 0.3218658466244598\n",
      "2022-03-26 21:30:59.173963 Epoch 250, Training Loss 0.3227232467106846\n",
      "2022-03-26 21:30:59.192860 Epoch 250, Training Loss 0.3232853443116483\n",
      "2022-03-26 21:30:59.219917 Epoch 250, Training Loss 0.3239269662086311\n",
      "2022-03-26 21:30:59.246150 Epoch 250, Training Loss 0.3246576014687033\n",
      "2022-03-26 21:30:59.271895 Epoch 250, Training Loss 0.3251638256985208\n",
      "2022-03-26 21:30:59.297929 Epoch 250, Training Loss 0.325990984888028\n",
      "2022-03-26 21:30:59.324353 Epoch 250, Training Loss 0.3266072217232126\n",
      "2022-03-26 21:30:59.350295 Epoch 250, Training Loss 0.32723533920467357\n",
      "2022-03-26 21:30:59.376905 Epoch 250, Training Loss 0.32792842955997836\n",
      "2022-03-26 21:30:59.402795 Epoch 250, Training Loss 0.3285832147845222\n",
      "2022-03-26 21:30:59.421838 Epoch 250, Training Loss 0.3291729096027896\n",
      "2022-03-26 21:30:59.439858 Epoch 250, Training Loss 0.3298918496236167\n",
      "2022-03-26 21:30:59.457983 Epoch 250, Training Loss 0.33047386051138955\n",
      "2022-03-26 21:30:59.477011 Epoch 250, Training Loss 0.3314517460515737\n",
      "2022-03-26 21:30:59.496062 Epoch 250, Training Loss 0.3321910590466941\n",
      "2022-03-26 21:30:59.513435 Epoch 250, Training Loss 0.3327595768758403\n",
      "2022-03-26 21:30:59.531837 Epoch 250, Training Loss 0.3332668353453317\n",
      "2022-03-26 21:30:59.549292 Epoch 250, Training Loss 0.3340437091753611\n",
      "2022-03-26 21:30:59.568486 Epoch 250, Training Loss 0.33504796687446897\n",
      "2022-03-26 21:30:59.586742 Epoch 250, Training Loss 0.3358293679135535\n",
      "2022-03-26 21:30:59.604978 Epoch 250, Training Loss 0.3366632603699594\n",
      "2022-03-26 21:30:59.622942 Epoch 250, Training Loss 0.33768053779669127\n",
      "2022-03-26 21:30:59.641977 Epoch 250, Training Loss 0.3384391918130543\n",
      "2022-03-26 21:30:59.660043 Epoch 250, Training Loss 0.33915141968013685\n",
      "2022-03-26 21:30:59.679059 Epoch 250, Training Loss 0.33971232217748454\n",
      "2022-03-26 21:30:59.696910 Epoch 250, Training Loss 0.34027272645774703\n",
      "2022-03-26 21:30:59.715867 Epoch 250, Training Loss 0.3410705946900351\n",
      "2022-03-26 21:30:59.734918 Epoch 250, Training Loss 0.34176674881554625\n",
      "2022-03-26 21:30:59.752964 Epoch 250, Training Loss 0.34248493333606767\n",
      "2022-03-26 21:30:59.771753 Epoch 250, Training Loss 0.3429356192231483\n",
      "2022-03-26 21:30:59.790741 Epoch 250, Training Loss 0.3437426541467457\n",
      "2022-03-26 21:30:59.808739 Epoch 250, Training Loss 0.34463139613875954\n",
      "2022-03-26 21:30:59.827760 Epoch 250, Training Loss 0.34546426578860756\n",
      "2022-03-26 21:30:59.845659 Epoch 250, Training Loss 0.3461704081891443\n",
      "2022-03-26 21:30:59.863752 Epoch 250, Training Loss 0.34680779285900426\n",
      "2022-03-26 21:30:59.882014 Epoch 250, Training Loss 0.3472622981690385\n",
      "2022-03-26 21:30:59.900056 Epoch 250, Training Loss 0.34794390731302977\n",
      "2022-03-26 21:30:59.918089 Epoch 250, Training Loss 0.34897419196717877\n",
      "2022-03-26 21:30:59.935729 Epoch 250, Training Loss 0.3498699866673526\n",
      "2022-03-26 21:30:59.954807 Epoch 250, Training Loss 0.35047006999592645\n",
      "2022-03-26 21:30:59.973751 Epoch 250, Training Loss 0.35116387522586473\n",
      "2022-03-26 21:30:59.992282 Epoch 250, Training Loss 0.3520587453680575\n",
      "2022-03-26 21:31:00.011349 Epoch 250, Training Loss 0.3527261291242317\n",
      "2022-03-26 21:31:00.029367 Epoch 250, Training Loss 0.35332958125854697\n",
      "2022-03-26 21:31:00.047911 Epoch 250, Training Loss 0.35409243138092555\n",
      "2022-03-26 21:31:00.065972 Epoch 250, Training Loss 0.35462623891775563\n",
      "2022-03-26 21:31:00.083959 Epoch 250, Training Loss 0.3552343879285676\n",
      "2022-03-26 21:31:00.101005 Epoch 250, Training Loss 0.35571554158349783\n",
      "2022-03-26 21:31:00.119164 Epoch 250, Training Loss 0.3563052599539842\n",
      "2022-03-26 21:31:00.138090 Epoch 250, Training Loss 0.3568459311333459\n",
      "2022-03-26 21:31:00.156148 Epoch 250, Training Loss 0.35781552484425744\n",
      "2022-03-26 21:31:00.174139 Epoch 250, Training Loss 0.3586569100313479\n",
      "2022-03-26 21:31:00.193165 Epoch 250, Training Loss 0.35945180504370833\n",
      "2022-03-26 21:31:00.212105 Epoch 250, Training Loss 0.3600559059311362\n",
      "2022-03-26 21:31:00.230126 Epoch 250, Training Loss 0.3608167993138208\n",
      "2022-03-26 21:31:00.247758 Epoch 250, Training Loss 0.3616404634759859\n",
      "2022-03-26 21:31:00.266666 Epoch 250, Training Loss 0.3624079373029187\n",
      "2022-03-26 21:31:00.283758 Epoch 250, Training Loss 0.36294258547865826\n",
      "2022-03-26 21:31:00.302967 Epoch 250, Training Loss 0.3637175667468849\n",
      "2022-03-26 21:31:00.321027 Epoch 250, Training Loss 0.36441760851294186\n",
      "2022-03-26 21:31:00.339044 Epoch 250, Training Loss 0.3652669247001638\n",
      "2022-03-26 21:31:00.358000 Epoch 250, Training Loss 0.36594451556120383\n",
      "2022-03-26 21:31:00.376281 Epoch 250, Training Loss 0.3667099366102682\n",
      "2022-03-26 21:31:00.394300 Epoch 250, Training Loss 0.3674703792995199\n",
      "2022-03-26 21:31:00.413162 Epoch 250, Training Loss 0.3681993944108334\n",
      "2022-03-26 21:31:00.432018 Epoch 250, Training Loss 0.3686498392497182\n",
      "2022-03-26 21:31:00.450060 Epoch 250, Training Loss 0.36917790187441785\n",
      "2022-03-26 21:31:00.467323 Epoch 250, Training Loss 0.3698153681767261\n",
      "2022-03-26 21:31:00.485285 Epoch 250, Training Loss 0.37060377763970126\n",
      "2022-03-26 21:31:00.503265 Epoch 250, Training Loss 0.3713401480556449\n",
      "2022-03-26 21:31:00.522293 Epoch 250, Training Loss 0.3721599805233119\n",
      "2022-03-26 21:31:00.540322 Epoch 250, Training Loss 0.373047845976432\n",
      "2022-03-26 21:31:00.558339 Epoch 250, Training Loss 0.3737626335657466\n",
      "2022-03-26 21:31:00.576555 Epoch 250, Training Loss 0.3744628747253467\n",
      "2022-03-26 21:31:00.595377 Epoch 250, Training Loss 0.3751074285305979\n",
      "2022-03-26 21:31:00.613281 Epoch 250, Training Loss 0.3756405010705104\n",
      "2022-03-26 21:31:00.632309 Epoch 250, Training Loss 0.3763360670765343\n",
      "2022-03-26 21:31:00.650343 Epoch 250, Training Loss 0.3772116907112434\n",
      "2022-03-26 21:31:00.668601 Epoch 250, Training Loss 0.3779414119318013\n",
      "2022-03-26 21:31:00.686622 Epoch 250, Training Loss 0.37861787320098\n",
      "2022-03-26 21:31:00.704697 Epoch 250, Training Loss 0.3793898615843195\n",
      "2022-03-26 21:31:00.722908 Epoch 250, Training Loss 0.38045149104064685\n",
      "2022-03-26 21:31:00.741961 Epoch 250, Training Loss 0.3812106612240872\n",
      "2022-03-26 21:31:00.760115 Epoch 250, Training Loss 0.3817065717542873\n",
      "2022-03-26 21:31:00.779159 Epoch 250, Training Loss 0.38215207893525244\n",
      "2022-03-26 21:31:00.796143 Epoch 250, Training Loss 0.3826505978927588\n",
      "2022-03-26 21:31:00.815154 Epoch 250, Training Loss 0.3834583903746227\n",
      "2022-03-26 21:31:00.833202 Epoch 250, Training Loss 0.38418502831245627\n",
      "2022-03-26 21:31:00.852181 Epoch 250, Training Loss 0.384971828602464\n",
      "2022-03-26 21:31:00.870319 Epoch 250, Training Loss 0.38575086733111946\n",
      "2022-03-26 21:31:00.889298 Epoch 250, Training Loss 0.3865893605496267\n",
      "2022-03-26 21:31:00.907299 Epoch 250, Training Loss 0.387251685990397\n",
      "2022-03-26 21:31:00.926312 Epoch 250, Training Loss 0.38779638478975464\n",
      "2022-03-26 21:31:00.946347 Epoch 250, Training Loss 0.38827780365486586\n",
      "2022-03-26 21:31:00.964395 Epoch 250, Training Loss 0.3890344225765799\n",
      "2022-03-26 21:31:00.982430 Epoch 250, Training Loss 0.38974473707358853\n",
      "2022-03-26 21:31:00.999434 Epoch 250, Training Loss 0.3902966642318784\n",
      "2022-03-26 21:31:01.018442 Epoch 250, Training Loss 0.39093860793296636\n",
      "2022-03-26 21:31:01.036457 Epoch 250, Training Loss 0.3917842269553553\n",
      "2022-03-26 21:31:01.055486 Epoch 250, Training Loss 0.39225868690196813\n",
      "2022-03-26 21:31:01.073518 Epoch 250, Training Loss 0.3932180652761703\n",
      "2022-03-26 21:31:01.091541 Epoch 250, Training Loss 0.39399657419423006\n",
      "2022-03-26 21:31:01.109927 Epoch 250, Training Loss 0.3951502858906451\n",
      "2022-03-26 21:31:01.128945 Epoch 250, Training Loss 0.3955856025066522\n",
      "2022-03-26 21:31:01.147890 Epoch 250, Training Loss 0.39621723738625225\n",
      "2022-03-26 21:31:01.165921 Epoch 250, Training Loss 0.3967094864610516\n",
      "2022-03-26 21:31:01.183722 Epoch 250, Training Loss 0.3975016179368319\n",
      "2022-03-26 21:31:01.202764 Epoch 250, Training Loss 0.39811898013362496\n",
      "2022-03-26 21:31:01.221067 Epoch 250, Training Loss 0.3990579749388463\n",
      "2022-03-26 21:31:01.240055 Epoch 250, Training Loss 0.39965700192372206\n",
      "2022-03-26 21:31:01.259077 Epoch 250, Training Loss 0.4003273204083333\n",
      "2022-03-26 21:31:01.277109 Epoch 250, Training Loss 0.4011098889210035\n",
      "2022-03-26 21:31:01.296142 Epoch 250, Training Loss 0.40198773644921726\n",
      "2022-03-26 21:31:01.314194 Epoch 250, Training Loss 0.402643398822421\n",
      "2022-03-26 21:31:01.332181 Epoch 250, Training Loss 0.4030666811310727\n",
      "2022-03-26 21:31:01.350212 Epoch 250, Training Loss 0.40361362790969935\n",
      "2022-03-26 21:31:01.368264 Epoch 250, Training Loss 0.4043203082764545\n",
      "2022-03-26 21:31:01.386167 Epoch 250, Training Loss 0.40509847404859256\n",
      "2022-03-26 21:31:01.405190 Epoch 250, Training Loss 0.4056814937564113\n",
      "2022-03-26 21:31:01.423143 Epoch 250, Training Loss 0.4064792472764354\n",
      "2022-03-26 21:31:01.442049 Epoch 250, Training Loss 0.40723748078279176\n",
      "2022-03-26 21:31:01.461078 Epoch 250, Training Loss 0.40800877182227574\n",
      "2022-03-26 21:31:01.480058 Epoch 250, Training Loss 0.40851786271537965\n",
      "2022-03-26 21:31:01.499106 Epoch 250, Training Loss 0.40907034167395834\n",
      "2022-03-26 21:31:01.517056 Epoch 250, Training Loss 0.409607222165598\n",
      "2022-03-26 21:31:01.535399 Epoch 250, Training Loss 0.41006080307009274\n",
      "2022-03-26 21:31:01.554278 Epoch 250, Training Loss 0.41112663023307194\n",
      "2022-03-26 21:31:01.572307 Epoch 250, Training Loss 0.4118999361686999\n",
      "2022-03-26 21:31:01.591193 Epoch 250, Training Loss 0.4126128340163804\n",
      "2022-03-26 21:31:01.609179 Epoch 250, Training Loss 0.41336122841176476\n",
      "2022-03-26 21:31:01.628167 Epoch 250, Training Loss 0.41399786661348076\n",
      "2022-03-26 21:31:01.647209 Epoch 250, Training Loss 0.41502912353981486\n",
      "2022-03-26 21:31:01.666184 Epoch 250, Training Loss 0.4155846204599151\n",
      "2022-03-26 21:31:01.684145 Epoch 250, Training Loss 0.4161465769762273\n",
      "2022-03-26 21:31:01.702116 Epoch 250, Training Loss 0.4168473259186196\n",
      "2022-03-26 21:31:01.720178 Epoch 250, Training Loss 0.41752047676716925\n",
      "2022-03-26 21:31:01.739162 Epoch 250, Training Loss 0.41834914733839157\n",
      "2022-03-26 21:31:01.757160 Epoch 250, Training Loss 0.4188887725019699\n",
      "2022-03-26 21:31:01.775096 Epoch 250, Training Loss 0.4194299424700725\n",
      "2022-03-26 21:31:01.794192 Epoch 250, Training Loss 0.4205328441794266\n",
      "2022-03-26 21:31:01.812853 Epoch 250, Training Loss 0.42111941223101845\n",
      "2022-03-26 21:31:01.831282 Epoch 250, Training Loss 0.42178290606002367\n",
      "2022-03-26 21:31:01.849303 Epoch 250, Training Loss 0.4224738889109448\n",
      "2022-03-26 21:31:01.867186 Epoch 250, Training Loss 0.42324404189806153\n",
      "2022-03-26 21:31:01.886323 Epoch 250, Training Loss 0.42385217215856325\n",
      "2022-03-26 21:31:01.904257 Epoch 250, Training Loss 0.4245685487985611\n",
      "2022-03-26 21:31:01.923300 Epoch 250, Training Loss 0.42540384219278154\n",
      "2022-03-26 21:31:01.941315 Epoch 250, Training Loss 0.42648236415422786\n",
      "2022-03-26 21:31:01.960340 Epoch 250, Training Loss 0.42722320690026977\n",
      "2022-03-26 21:31:01.978350 Epoch 250, Training Loss 0.4278635407424034\n",
      "2022-03-26 21:31:01.997348 Epoch 250, Training Loss 0.4286684023830897\n",
      "2022-03-26 21:31:02.015387 Epoch 250, Training Loss 0.4294354954110387\n",
      "2022-03-26 21:31:02.033260 Epoch 250, Training Loss 0.4300259314763272\n",
      "2022-03-26 21:31:02.052270 Epoch 250, Training Loss 0.4305965464819423\n",
      "2022-03-26 21:31:02.071309 Epoch 250, Training Loss 0.4313846948125478\n",
      "2022-03-26 21:31:02.089330 Epoch 250, Training Loss 0.4319667690397833\n",
      "2022-03-26 21:31:02.108249 Epoch 250, Training Loss 0.43273607170795236\n",
      "2022-03-26 21:31:02.126298 Epoch 250, Training Loss 0.4333907223266104\n",
      "2022-03-26 21:31:02.146265 Epoch 250, Training Loss 0.43390294798957113\n",
      "2022-03-26 21:31:02.163252 Epoch 250, Training Loss 0.43437342242816523\n",
      "2022-03-26 21:31:02.182304 Epoch 250, Training Loss 0.43513585158321255\n",
      "2022-03-26 21:31:02.200275 Epoch 250, Training Loss 0.4359121702973495\n",
      "2022-03-26 21:31:02.218694 Epoch 250, Training Loss 0.43640421921639794\n",
      "2022-03-26 21:31:02.235941 Epoch 250, Training Loss 0.4370305307228547\n",
      "2022-03-26 21:31:02.262314 Epoch 250, Training Loss 0.43781218511978987\n",
      "2022-03-26 21:31:02.288382 Epoch 250, Training Loss 0.43867491021790467\n",
      "2022-03-26 21:31:02.314658 Epoch 250, Training Loss 0.43929047509075125\n",
      "2022-03-26 21:31:02.339963 Epoch 250, Training Loss 0.440092307641683\n",
      "2022-03-26 21:31:02.365983 Epoch 250, Training Loss 0.4408603549537147\n",
      "2022-03-26 21:31:02.392287 Epoch 250, Training Loss 0.44149369733107974\n",
      "2022-03-26 21:31:02.418358 Epoch 250, Training Loss 0.44229134299870954\n",
      "2022-03-26 21:31:02.444558 Epoch 250, Training Loss 0.4430831499264368\n",
      "2022-03-26 21:31:02.461566 Epoch 250, Training Loss 0.4437571089438465\n",
      "2022-03-26 21:31:02.479564 Epoch 250, Training Loss 0.444386001819235\n",
      "2022-03-26 21:31:02.498486 Epoch 250, Training Loss 0.44511971784674603\n",
      "2022-03-26 21:31:02.516381 Epoch 250, Training Loss 0.44616729653704806\n",
      "2022-03-26 21:31:02.534790 Epoch 250, Training Loss 0.4468589142307906\n",
      "2022-03-26 21:31:02.553840 Epoch 250, Training Loss 0.447425402796177\n",
      "2022-03-26 21:31:02.572934 Epoch 250, Training Loss 0.44808364608098783\n",
      "2022-03-26 21:31:02.591299 Epoch 250, Training Loss 0.44897618341019085\n",
      "2022-03-26 21:31:02.609312 Epoch 250, Training Loss 0.44982615532472614\n",
      "2022-03-26 21:31:02.629163 Epoch 250, Training Loss 0.45037297637718715\n",
      "2022-03-26 21:31:02.647142 Epoch 250, Training Loss 0.451142813200536\n",
      "2022-03-26 21:31:02.664996 Epoch 250, Training Loss 0.4519611420610067\n",
      "2022-03-26 21:31:02.683008 Epoch 250, Training Loss 0.45259075480348926\n",
      "2022-03-26 21:31:02.702144 Epoch 250, Training Loss 0.4535249290258988\n",
      "2022-03-26 21:31:02.721117 Epoch 250, Training Loss 0.4540584874351311\n",
      "2022-03-26 21:31:02.739113 Epoch 250, Training Loss 0.4546831270008136\n",
      "2022-03-26 21:31:02.757181 Epoch 250, Training Loss 0.45539875439060923\n",
      "2022-03-26 21:31:02.775261 Epoch 250, Training Loss 0.45594290184700276\n",
      "2022-03-26 21:31:02.794282 Epoch 250, Training Loss 0.45658701974565113\n",
      "2022-03-26 21:31:02.812247 Epoch 250, Training Loss 0.45733743437263363\n",
      "2022-03-26 21:31:02.830276 Epoch 250, Training Loss 0.4581039617662235\n",
      "2022-03-26 21:31:02.849314 Epoch 250, Training Loss 0.45873981112104545\n",
      "2022-03-26 21:31:02.867040 Epoch 250, Training Loss 0.459594290351014\n",
      "2022-03-26 21:31:02.884999 Epoch 250, Training Loss 0.4604780894258748\n",
      "2022-03-26 21:31:02.904010 Epoch 250, Training Loss 0.4612218408328493\n",
      "2022-03-26 21:31:02.922050 Epoch 250, Training Loss 0.46208342307668815\n",
      "2022-03-26 21:31:02.940919 Epoch 250, Training Loss 0.4627983140213715\n",
      "2022-03-26 21:31:02.958983 Epoch 250, Training Loss 0.46344661849843877\n",
      "2022-03-26 21:31:02.977971 Epoch 250, Training Loss 0.46407153745136603\n",
      "2022-03-26 21:31:02.995981 Epoch 250, Training Loss 0.46488595138425415\n",
      "2022-03-26 21:31:03.013690 Epoch 250, Training Loss 0.4655693923420918\n",
      "2022-03-26 21:31:03.031717 Epoch 250, Training Loss 0.46606439508288106\n",
      "2022-03-26 21:31:03.049831 Epoch 250, Training Loss 0.466782259933479\n",
      "2022-03-26 21:31:03.067883 Epoch 250, Training Loss 0.4675909225516917\n",
      "2022-03-26 21:31:03.086181 Epoch 250, Training Loss 0.468423179996288\n",
      "2022-03-26 21:31:03.104762 Epoch 250, Training Loss 0.46896002630291084\n",
      "2022-03-26 21:31:03.122771 Epoch 250, Training Loss 0.469490021772092\n",
      "2022-03-26 21:31:03.141800 Epoch 250, Training Loss 0.47012136537400656\n",
      "2022-03-26 21:31:03.159290 Epoch 250, Training Loss 0.4711433452413515\n",
      "2022-03-26 21:31:03.177726 Epoch 250, Training Loss 0.4717417969118299\n",
      "2022-03-26 21:31:03.195742 Epoch 250, Training Loss 0.47242455561752517\n",
      "2022-03-26 21:31:03.213595 Epoch 250, Training Loss 0.47310056908966025\n",
      "2022-03-26 21:31:03.231882 Epoch 250, Training Loss 0.4736262665075414\n",
      "2022-03-26 21:31:03.249894 Epoch 250, Training Loss 0.4743069764750693\n",
      "2022-03-26 21:31:03.268047 Epoch 250, Training Loss 0.474791820930398\n",
      "2022-03-26 21:31:03.287052 Epoch 250, Training Loss 0.47564880462253795\n",
      "2022-03-26 21:31:03.305543 Epoch 250, Training Loss 0.4763911917538899\n",
      "2022-03-26 21:31:03.323411 Epoch 250, Training Loss 0.47723925327096145\n",
      "2022-03-26 21:31:03.342431 Epoch 250, Training Loss 0.4777915325311139\n",
      "2022-03-26 21:31:03.359453 Epoch 250, Training Loss 0.4786747010315166\n",
      "2022-03-26 21:31:03.378758 Epoch 250, Training Loss 0.4792092317510444\n",
      "2022-03-26 21:31:03.395844 Epoch 250, Training Loss 0.48009322694195505\n",
      "2022-03-26 21:31:03.414893 Epoch 250, Training Loss 0.4810867325576675\n",
      "2022-03-26 21:31:03.432633 Epoch 250, Training Loss 0.4818578089594536\n",
      "2022-03-26 21:31:03.452525 Epoch 250, Training Loss 0.4827697658935166\n",
      "2022-03-26 21:31:03.470543 Epoch 250, Training Loss 0.4835922806464193\n",
      "2022-03-26 21:31:03.488595 Epoch 250, Training Loss 0.4843894007718167\n",
      "2022-03-26 21:31:03.508611 Epoch 250, Training Loss 0.4852567926392226\n",
      "2022-03-26 21:31:03.526490 Epoch 250, Training Loss 0.48578822174493\n",
      "2022-03-26 21:31:03.545546 Epoch 250, Training Loss 0.48673762487785893\n",
      "2022-03-26 21:31:03.563105 Epoch 250, Training Loss 0.48722284506349\n",
      "2022-03-26 21:31:03.581158 Epoch 250, Training Loss 0.48775852282943627\n",
      "2022-03-26 21:31:03.599170 Epoch 250, Training Loss 0.48840696023553226\n",
      "2022-03-26 21:31:03.617156 Epoch 250, Training Loss 0.4890829685246548\n",
      "2022-03-26 21:31:03.635446 Epoch 250, Training Loss 0.48979485073053014\n",
      "2022-03-26 21:31:03.653931 Epoch 250, Training Loss 0.49052787711248375\n",
      "2022-03-26 21:31:03.672001 Epoch 250, Training Loss 0.491352825289797\n",
      "2022-03-26 21:31:03.689916 Epoch 250, Training Loss 0.4920167208784986\n",
      "2022-03-26 21:31:03.707996 Epoch 250, Training Loss 0.49287678945399915\n",
      "2022-03-26 21:31:03.726906 Epoch 250, Training Loss 0.493820103156902\n",
      "2022-03-26 21:31:03.745817 Epoch 250, Training Loss 0.494250945880285\n",
      "2022-03-26 21:31:03.763450 Epoch 250, Training Loss 0.49504349115864393\n",
      "2022-03-26 21:31:03.781823 Epoch 250, Training Loss 0.4956429484478958\n",
      "2022-03-26 21:31:03.798845 Epoch 250, Training Loss 0.4961603987780983\n",
      "2022-03-26 21:31:03.817938 Epoch 250, Training Loss 0.49700886022556773\n",
      "2022-03-26 21:31:03.835976 Epoch 250, Training Loss 0.49764204615979546\n",
      "2022-03-26 21:31:03.853531 Epoch 250, Training Loss 0.49837222752516225\n",
      "2022-03-26 21:31:03.871803 Epoch 250, Training Loss 0.49909326361725703\n",
      "2022-03-26 21:31:03.890844 Epoch 250, Training Loss 0.4998762971528656\n",
      "2022-03-26 21:31:03.908925 Epoch 250, Training Loss 0.5004576321529306\n",
      "2022-03-26 21:31:03.926837 Epoch 250, Training Loss 0.5011401118905953\n",
      "2022-03-26 21:31:03.946136 Epoch 250, Training Loss 0.5018613932611388\n",
      "2022-03-26 21:31:03.964705 Epoch 250, Training Loss 0.5027304262380161\n",
      "2022-03-26 21:31:03.983079 Epoch 250, Training Loss 0.5036007721177147\n",
      "2022-03-26 21:31:04.001039 Epoch 250, Training Loss 0.5042504725020255\n",
      "2022-03-26 21:31:04.019180 Epoch 250, Training Loss 0.5050490947102036\n",
      "2022-03-26 21:31:04.037066 Epoch 250, Training Loss 0.5057310885404382\n",
      "2022-03-26 21:31:04.055647 Epoch 250, Training Loss 0.5063292915787538\n",
      "2022-03-26 21:31:04.073770 Epoch 250, Training Loss 0.5072617729377868\n",
      "2022-03-26 21:31:04.091745 Epoch 250, Training Loss 0.5081463993129218\n",
      "2022-03-26 21:31:04.110624 Epoch 250, Training Loss 0.5086556797670891\n",
      "2022-03-26 21:31:04.129603 Epoch 250, Training Loss 0.5097719278100812\n",
      "2022-03-26 21:31:04.148650 Epoch 250, Training Loss 0.5104384865526044\n",
      "2022-03-26 21:31:04.165863 Epoch 250, Training Loss 0.51146015406722\n",
      "2022-03-26 21:31:04.183743 Epoch 250, Training Loss 0.5122081159859362\n",
      "2022-03-26 21:31:04.202803 Epoch 250, Training Loss 0.5128750227525106\n",
      "2022-03-26 21:31:04.220869 Epoch 250, Training Loss 0.5135086049966495\n",
      "2022-03-26 21:31:04.238869 Epoch 250, Training Loss 0.5142466742211901\n",
      "2022-03-26 21:31:04.256892 Epoch 250, Training Loss 0.5150527079849292\n",
      "2022-03-26 21:31:04.276024 Epoch 250, Training Loss 0.5159519125738412\n",
      "2022-03-26 21:31:04.295102 Epoch 250, Training Loss 0.5165117203884417\n",
      "2022-03-26 21:31:04.312429 Epoch 250, Training Loss 0.5173222120003322\n",
      "2022-03-26 21:31:04.331572 Epoch 250, Training Loss 0.518058455889792\n",
      "2022-03-26 21:31:04.349565 Epoch 250, Training Loss 0.5187216365276395\n",
      "2022-03-26 21:31:04.368822 Epoch 250, Training Loss 0.5195558989596794\n",
      "2022-03-26 21:31:04.386810 Epoch 250, Training Loss 0.5202306970915831\n",
      "2022-03-26 21:31:04.405242 Epoch 250, Training Loss 0.5208648378815492\n",
      "2022-03-26 21:31:04.423795 Epoch 250, Training Loss 0.5214919309939265\n",
      "2022-03-26 21:31:04.442794 Epoch 250, Training Loss 0.5222253396992793\n",
      "2022-03-26 21:31:04.460767 Epoch 250, Training Loss 0.5232313340124877\n",
      "2022-03-26 21:31:04.478801 Epoch 250, Training Loss 0.5238630629104116\n",
      "2022-03-26 21:31:04.496861 Epoch 250, Training Loss 0.5244590802418302\n",
      "2022-03-26 21:31:04.515837 Epoch 250, Training Loss 0.5252199960334222\n",
      "2022-03-26 21:31:04.532918 Epoch 250, Training Loss 0.5259464797004104\n",
      "2022-03-26 21:31:04.551900 Epoch 250, Training Loss 0.5266217963622354\n",
      "2022-03-26 21:31:04.570957 Epoch 250, Training Loss 0.5273975576738568\n",
      "2022-03-26 21:31:04.590019 Epoch 250, Training Loss 0.5278710057897031\n",
      "2022-03-26 21:31:04.609098 Epoch 250, Training Loss 0.5286183461280125\n",
      "2022-03-26 21:31:04.627834 Epoch 250, Training Loss 0.5294561971865042\n",
      "2022-03-26 21:31:04.645970 Epoch 250, Training Loss 0.5300567873070002\n",
      "2022-03-26 21:31:04.664549 Epoch 250, Training Loss 0.5309143772591716\n",
      "2022-03-26 21:31:04.682540 Epoch 250, Training Loss 0.5315700858221639\n",
      "2022-03-26 21:31:04.701581 Epoch 250, Training Loss 0.5322534152690102\n",
      "2022-03-26 21:31:04.720599 Epoch 250, Training Loss 0.5330269715898787\n",
      "2022-03-26 21:31:04.738513 Epoch 250, Training Loss 0.5339612477194623\n",
      "2022-03-26 21:31:04.757528 Epoch 250, Training Loss 0.5346946997182144\n",
      "2022-03-26 21:31:04.776389 Epoch 250, Training Loss 0.5352901532826826\n",
      "2022-03-26 21:31:04.795405 Epoch 250, Training Loss 0.5360539086029658\n",
      "2022-03-26 21:31:04.814326 Epoch 250, Training Loss 0.5369066533530155\n",
      "2022-03-26 21:31:04.832494 Epoch 250, Training Loss 0.5377356803325741\n",
      "2022-03-26 21:31:04.850516 Epoch 250, Training Loss 0.5384564396670407\n",
      "2022-03-26 21:31:04.869428 Epoch 250, Training Loss 0.5392048481632682\n",
      "2022-03-26 21:31:04.888485 Epoch 250, Training Loss 0.5397404811876204\n",
      "2022-03-26 21:31:04.906510 Epoch 250, Training Loss 0.5406935147159849\n",
      "2022-03-26 21:31:04.925533 Epoch 250, Training Loss 0.5414536599155582\n",
      "2022-03-26 21:31:04.944591 Epoch 250, Training Loss 0.5424566996067076\n",
      "2022-03-26 21:31:04.963551 Epoch 250, Training Loss 0.5430381031292478\n",
      "2022-03-26 21:31:04.981951 Epoch 250, Training Loss 0.5437793821629966\n",
      "2022-03-26 21:31:04.999846 Epoch 250, Training Loss 0.5444199865888757\n",
      "2022-03-26 21:31:05.018923 Epoch 250, Training Loss 0.5451365829733632\n",
      "2022-03-26 21:31:05.037806 Epoch 250, Training Loss 0.545954282860012\n",
      "2022-03-26 21:31:05.055812 Epoch 250, Training Loss 0.5463844225610919\n",
      "2022-03-26 21:31:05.074569 Epoch 250, Training Loss 0.5470127207238961\n",
      "2022-03-26 21:31:05.093582 Epoch 250, Training Loss 0.5476419161195341\n",
      "2022-03-26 21:31:05.112667 Epoch 250, Training Loss 0.548583506775634\n",
      "2022-03-26 21:31:05.131444 Epoch 250, Training Loss 0.5492498646764195\n",
      "2022-03-26 21:31:05.149984 Epoch 250, Training Loss 0.5499343503923977\n",
      "2022-03-26 21:31:05.168273 Epoch 250, Training Loss 0.5509860511783444\n",
      "2022-03-26 21:31:05.188886 Epoch 250, Training Loss 0.5517685239577232\n",
      "2022-03-26 21:31:05.208350 Epoch 250, Training Loss 0.5523661978333197\n",
      "2022-03-26 21:31:05.227891 Epoch 250, Training Loss 0.5530006049767785\n",
      "2022-03-26 21:31:05.247455 Epoch 250, Training Loss 0.5536235935242889\n",
      "2022-03-26 21:31:05.273879 Epoch 250, Training Loss 0.5544552259585437\n",
      "2022-03-26 21:31:05.301782 Epoch 250, Training Loss 0.5552600373697403\n",
      "2022-03-26 21:31:05.328698 Epoch 250, Training Loss 0.5562837818242095\n",
      "2022-03-26 21:31:05.354776 Epoch 250, Training Loss 0.5571984061042367\n",
      "2022-03-26 21:31:05.363757 Epoch 250, Training Loss 0.5576607209565999\n",
      "2022-03-26 21:42:26.951410 Epoch 300, Training Loss 0.0010651815730287595\n",
      "2022-03-26 21:42:26.970414 Epoch 300, Training Loss 0.0015915986674520976\n",
      "2022-03-26 21:42:26.989837 Epoch 300, Training Loss 0.00218529172260743\n",
      "2022-03-26 21:42:27.007782 Epoch 300, Training Loss 0.0029259568742474023\n",
      "2022-03-26 21:42:27.026790 Epoch 300, Training Loss 0.0033602660802928993\n",
      "2022-03-26 21:42:27.045795 Epoch 300, Training Loss 0.004191777323517958\n",
      "2022-03-26 21:42:27.067802 Epoch 300, Training Loss 0.005099915596835144\n",
      "2022-03-26 21:42:27.087810 Epoch 300, Training Loss 0.00560440218357174\n",
      "2022-03-26 21:42:27.108818 Epoch 300, Training Loss 0.006454792702594377\n",
      "2022-03-26 21:42:27.126822 Epoch 300, Training Loss 0.0072234362897360726\n",
      "2022-03-26 21:42:27.145731 Epoch 300, Training Loss 0.007738297819481482\n",
      "2022-03-26 21:42:27.164822 Epoch 300, Training Loss 0.008320926819615961\n",
      "2022-03-26 21:42:27.183735 Epoch 300, Training Loss 0.009373106043357069\n",
      "2022-03-26 21:42:27.202765 Epoch 300, Training Loss 0.01008867672489732\n",
      "2022-03-26 21:42:27.221961 Epoch 300, Training Loss 0.010778010158282716\n",
      "2022-03-26 21:42:27.239978 Epoch 300, Training Loss 0.011321099441679543\n",
      "2022-03-26 21:42:27.258889 Epoch 300, Training Loss 0.012050286049733077\n",
      "2022-03-26 21:42:27.278897 Epoch 300, Training Loss 0.012898921356786547\n",
      "2022-03-26 21:42:27.296746 Epoch 300, Training Loss 0.013513683548668767\n",
      "2022-03-26 21:42:27.315792 Epoch 300, Training Loss 0.014062210269596266\n",
      "2022-03-26 21:42:27.334796 Epoch 300, Training Loss 0.014743784352031815\n",
      "2022-03-26 21:42:27.353823 Epoch 300, Training Loss 0.015084104464791925\n",
      "2022-03-26 21:42:27.373344 Epoch 300, Training Loss 0.015585384176820136\n",
      "2022-03-26 21:42:27.391953 Epoch 300, Training Loss 0.016203468870323944\n",
      "2022-03-26 21:42:27.409903 Epoch 300, Training Loss 0.01675070803183729\n",
      "2022-03-26 21:42:27.429364 Epoch 300, Training Loss 0.01731017178586682\n",
      "2022-03-26 21:42:27.447820 Epoch 300, Training Loss 0.017833895276269645\n",
      "2022-03-26 21:42:27.467872 Epoch 300, Training Loss 0.018570439780459684\n",
      "2022-03-26 21:42:27.487144 Epoch 300, Training Loss 0.019076450630222137\n",
      "2022-03-26 21:42:27.505151 Epoch 300, Training Loss 0.019963332378041106\n",
      "2022-03-26 21:42:27.525300 Epoch 300, Training Loss 0.020363726846092497\n",
      "2022-03-26 21:42:27.543151 Epoch 300, Training Loss 0.02114000493455726\n",
      "2022-03-26 21:42:27.561901 Epoch 300, Training Loss 0.02155617454930035\n",
      "2022-03-26 21:42:27.581769 Epoch 300, Training Loss 0.022173110748191015\n",
      "2022-03-26 21:42:27.600210 Epoch 300, Training Loss 0.02271108386461692\n",
      "2022-03-26 21:42:27.620173 Epoch 300, Training Loss 0.0234032085789439\n",
      "2022-03-26 21:42:27.638328 Epoch 300, Training Loss 0.023993069367945347\n",
      "2022-03-26 21:42:27.657885 Epoch 300, Training Loss 0.024962245312798052\n",
      "2022-03-26 21:42:27.676314 Epoch 300, Training Loss 0.025328735103997427\n",
      "2022-03-26 21:42:27.695175 Epoch 300, Training Loss 0.025914789694349478\n",
      "2022-03-26 21:42:27.713206 Epoch 300, Training Loss 0.026645292787600663\n",
      "2022-03-26 21:42:27.732223 Epoch 300, Training Loss 0.027089369380870438\n",
      "2022-03-26 21:42:27.750381 Epoch 300, Training Loss 0.027568949191161738\n",
      "2022-03-26 21:42:27.769880 Epoch 300, Training Loss 0.028077751779190415\n",
      "2022-03-26 21:42:27.788875 Epoch 300, Training Loss 0.028702336161032967\n",
      "2022-03-26 21:42:27.807908 Epoch 300, Training Loss 0.029269846503996787\n",
      "2022-03-26 21:42:27.827178 Epoch 300, Training Loss 0.029696065294163305\n",
      "2022-03-26 21:42:27.846231 Epoch 300, Training Loss 0.030315977685591754\n",
      "2022-03-26 21:42:27.864184 Epoch 300, Training Loss 0.0312388307603119\n",
      "2022-03-26 21:42:27.883025 Epoch 300, Training Loss 0.0320615417054852\n",
      "2022-03-26 21:42:27.904800 Epoch 300, Training Loss 0.0326564198031145\n",
      "2022-03-26 21:42:27.931793 Epoch 300, Training Loss 0.03328344103930247\n",
      "2022-03-26 21:42:27.958910 Epoch 300, Training Loss 0.034039916361079496\n",
      "2022-03-26 21:42:27.984745 Epoch 300, Training Loss 0.03458726360364948\n",
      "2022-03-26 21:42:28.010767 Epoch 300, Training Loss 0.03512101153583478\n",
      "2022-03-26 21:42:28.037654 Epoch 300, Training Loss 0.03587465068263471\n",
      "2022-03-26 21:42:28.064232 Epoch 300, Training Loss 0.03650544252237091\n",
      "2022-03-26 21:42:28.091254 Epoch 300, Training Loss 0.03725063229155967\n",
      "2022-03-26 21:42:28.106969 Epoch 300, Training Loss 0.03791354059258385\n",
      "2022-03-26 21:42:28.121047 Epoch 300, Training Loss 0.038446704025768566\n",
      "2022-03-26 21:42:28.136170 Epoch 300, Training Loss 0.038874048901640854\n",
      "2022-03-26 21:42:28.151422 Epoch 300, Training Loss 0.0394107894138302\n",
      "2022-03-26 21:42:28.165159 Epoch 300, Training Loss 0.03985808561067752\n",
      "2022-03-26 21:42:28.180168 Epoch 300, Training Loss 0.04057033977392689\n",
      "2022-03-26 21:42:28.194028 Epoch 300, Training Loss 0.041281372118179144\n",
      "2022-03-26 21:42:28.210056 Epoch 300, Training Loss 0.04191677706778202\n",
      "2022-03-26 21:42:28.224198 Epoch 300, Training Loss 0.04258302170449815\n",
      "2022-03-26 21:42:28.239228 Epoch 300, Training Loss 0.04312559188631794\n",
      "2022-03-26 21:42:28.253023 Epoch 300, Training Loss 0.04384618483083632\n",
      "2022-03-26 21:42:28.268047 Epoch 300, Training Loss 0.044722482333402806\n",
      "2022-03-26 21:42:28.282165 Epoch 300, Training Loss 0.04521297459559672\n",
      "2022-03-26 21:42:28.297127 Epoch 300, Training Loss 0.046088247690969114\n",
      "2022-03-26 21:42:28.311186 Epoch 300, Training Loss 0.046729564323754566\n",
      "2022-03-26 21:42:28.326251 Epoch 300, Training Loss 0.04727611970871001\n",
      "2022-03-26 21:42:28.341249 Epoch 300, Training Loss 0.04771215356219455\n",
      "2022-03-26 21:42:28.355927 Epoch 300, Training Loss 0.04833529104509622\n",
      "2022-03-26 21:42:28.370931 Epoch 300, Training Loss 0.04911306184118666\n",
      "2022-03-26 21:42:28.386876 Epoch 300, Training Loss 0.04973433721248451\n",
      "2022-03-26 21:42:28.400880 Epoch 300, Training Loss 0.05018486536067465\n",
      "2022-03-26 21:42:28.416908 Epoch 300, Training Loss 0.050967654532483773\n",
      "2022-03-26 21:42:28.430912 Epoch 300, Training Loss 0.05168475016303684\n",
      "2022-03-26 21:42:28.445880 Epoch 300, Training Loss 0.05235325611765732\n",
      "2022-03-26 21:42:28.459907 Epoch 300, Training Loss 0.05313505579138656\n",
      "2022-03-26 21:42:28.474990 Epoch 300, Training Loss 0.053798984078799975\n",
      "2022-03-26 21:42:28.489012 Epoch 300, Training Loss 0.05441161670038462\n",
      "2022-03-26 21:42:28.503934 Epoch 300, Training Loss 0.0552180631233908\n",
      "2022-03-26 21:42:28.518059 Epoch 300, Training Loss 0.055851691900311834\n",
      "2022-03-26 21:42:28.534048 Epoch 300, Training Loss 0.0564722276252249\n",
      "2022-03-26 21:42:28.548932 Epoch 300, Training Loss 0.0571771996557865\n",
      "2022-03-26 21:42:28.562866 Epoch 300, Training Loss 0.057671571784007274\n",
      "2022-03-26 21:42:28.577870 Epoch 300, Training Loss 0.058189151765745316\n",
      "2022-03-26 21:42:28.592884 Epoch 300, Training Loss 0.05872811403725763\n",
      "2022-03-26 21:42:28.606774 Epoch 300, Training Loss 0.05957291505830672\n",
      "2022-03-26 21:42:28.622468 Epoch 300, Training Loss 0.06022972097177335\n",
      "2022-03-26 21:42:28.636431 Epoch 300, Training Loss 0.06102655747967303\n",
      "2022-03-26 21:42:28.651169 Epoch 300, Training Loss 0.06138652688859369\n",
      "2022-03-26 21:42:28.665505 Epoch 300, Training Loss 0.06201836173339268\n",
      "2022-03-26 21:42:28.679721 Epoch 300, Training Loss 0.06259892282583525\n",
      "2022-03-26 21:42:28.694640 Epoch 300, Training Loss 0.06309412316897946\n",
      "2022-03-26 21:42:28.709736 Epoch 300, Training Loss 0.063685828653138\n",
      "2022-03-26 21:42:28.723703 Epoch 300, Training Loss 0.06424453183818046\n",
      "2022-03-26 21:42:28.738708 Epoch 300, Training Loss 0.06479216525164407\n",
      "2022-03-26 21:42:28.753712 Epoch 300, Training Loss 0.06544799188061444\n",
      "2022-03-26 21:42:28.767829 Epoch 300, Training Loss 0.06605156615871907\n",
      "2022-03-26 21:42:28.782844 Epoch 300, Training Loss 0.06677522919976803\n",
      "2022-03-26 21:42:28.797816 Epoch 300, Training Loss 0.06779099341548617\n",
      "2022-03-26 21:42:28.811829 Epoch 300, Training Loss 0.06827718499676345\n",
      "2022-03-26 21:42:28.826826 Epoch 300, Training Loss 0.06912577190362584\n",
      "2022-03-26 21:42:28.841849 Epoch 300, Training Loss 0.06965846856079443\n",
      "2022-03-26 21:42:28.856901 Epoch 300, Training Loss 0.07021168952860186\n",
      "2022-03-26 21:42:28.871074 Epoch 300, Training Loss 0.07093443814903269\n",
      "2022-03-26 21:42:28.886142 Epoch 300, Training Loss 0.07147964633181882\n",
      "2022-03-26 21:42:28.901190 Epoch 300, Training Loss 0.07216988930769284\n",
      "2022-03-26 21:42:28.915201 Epoch 300, Training Loss 0.07292797829946289\n",
      "2022-03-26 21:42:28.930195 Epoch 300, Training Loss 0.07358625809402417\n",
      "2022-03-26 21:42:28.944159 Epoch 300, Training Loss 0.07421058141971792\n",
      "2022-03-26 21:42:28.959778 Epoch 300, Training Loss 0.07507548208736703\n",
      "2022-03-26 21:42:28.974712 Epoch 300, Training Loss 0.07605954448280432\n",
      "2022-03-26 21:42:28.989702 Epoch 300, Training Loss 0.07666874240578898\n",
      "2022-03-26 21:42:29.004715 Epoch 300, Training Loss 0.07740960999980302\n",
      "2022-03-26 21:42:29.019938 Epoch 300, Training Loss 0.07803562405469168\n",
      "2022-03-26 21:42:29.033942 Epoch 300, Training Loss 0.07865477648689924\n",
      "2022-03-26 21:42:29.048956 Epoch 300, Training Loss 0.0791120702958168\n",
      "2022-03-26 21:42:29.064090 Epoch 300, Training Loss 0.07957255009495084\n",
      "2022-03-26 21:42:29.078143 Epoch 300, Training Loss 0.08040410470779595\n",
      "2022-03-26 21:42:29.093157 Epoch 300, Training Loss 0.08108550165315419\n",
      "2022-03-26 21:42:29.108111 Epoch 300, Training Loss 0.08157802832401012\n",
      "2022-03-26 21:42:29.123171 Epoch 300, Training Loss 0.0819249104355912\n",
      "2022-03-26 21:42:29.137193 Epoch 300, Training Loss 0.0826850663822935\n",
      "2022-03-26 21:42:29.153036 Epoch 300, Training Loss 0.08342760549786755\n",
      "2022-03-26 21:42:29.168066 Epoch 300, Training Loss 0.08425080044495176\n",
      "2022-03-26 21:42:29.183049 Epoch 300, Training Loss 0.08490218759497718\n",
      "2022-03-26 21:42:29.197070 Epoch 300, Training Loss 0.08561166686475125\n",
      "2022-03-26 21:42:29.212032 Epoch 300, Training Loss 0.08633528577397241\n",
      "2022-03-26 21:42:29.228096 Epoch 300, Training Loss 0.08695874300301837\n",
      "2022-03-26 21:42:29.243819 Epoch 300, Training Loss 0.08751665272981005\n",
      "2022-03-26 21:42:29.257840 Epoch 300, Training Loss 0.08814160914524742\n",
      "2022-03-26 21:42:29.272806 Epoch 300, Training Loss 0.08882157908528662\n",
      "2022-03-26 21:42:29.286770 Epoch 300, Training Loss 0.08928024780262461\n",
      "2022-03-26 21:42:29.301774 Epoch 300, Training Loss 0.09002979534208927\n",
      "2022-03-26 21:42:29.316710 Epoch 300, Training Loss 0.0907376675730776\n",
      "2022-03-26 21:42:29.330714 Epoch 300, Training Loss 0.09120920784485614\n",
      "2022-03-26 21:42:29.345910 Epoch 300, Training Loss 0.0916150326237959\n",
      "2022-03-26 21:42:29.359914 Epoch 300, Training Loss 0.09228921466318847\n",
      "2022-03-26 21:42:29.373946 Epoch 300, Training Loss 0.0930271734438284\n",
      "2022-03-26 21:42:29.389536 Epoch 300, Training Loss 0.09371003237984064\n",
      "2022-03-26 21:42:29.403403 Epoch 300, Training Loss 0.09459260673931492\n",
      "2022-03-26 21:42:29.418434 Epoch 300, Training Loss 0.09525681856800527\n",
      "2022-03-26 21:42:29.432914 Epoch 300, Training Loss 0.09603188848099135\n",
      "2022-03-26 21:42:29.447803 Epoch 300, Training Loss 0.09691202499525016\n",
      "2022-03-26 21:42:29.462416 Epoch 300, Training Loss 0.0973749113890826\n",
      "2022-03-26 21:42:29.477063 Epoch 300, Training Loss 0.09794889946880243\n",
      "2022-03-26 21:42:29.491075 Epoch 300, Training Loss 0.09852151077269289\n",
      "2022-03-26 21:42:29.506012 Epoch 300, Training Loss 0.09932296862230276\n",
      "2022-03-26 21:42:29.519866 Epoch 300, Training Loss 0.10013368028356597\n",
      "2022-03-26 21:42:29.533813 Epoch 300, Training Loss 0.10078603048306292\n",
      "2022-03-26 21:42:29.548737 Epoch 300, Training Loss 0.10145796568649809\n",
      "2022-03-26 21:42:29.563813 Epoch 300, Training Loss 0.10207774831205987\n",
      "2022-03-26 21:42:29.577772 Epoch 300, Training Loss 0.10292186067842157\n",
      "2022-03-26 21:42:29.592786 Epoch 300, Training Loss 0.10347281655539638\n",
      "2022-03-26 21:42:29.606973 Epoch 300, Training Loss 0.10417503747336394\n",
      "2022-03-26 21:42:29.621997 Epoch 300, Training Loss 0.10494521858594606\n",
      "2022-03-26 21:42:29.636922 Epoch 300, Training Loss 0.10560828832256824\n",
      "2022-03-26 21:42:29.650925 Epoch 300, Training Loss 0.10614961259962653\n",
      "2022-03-26 21:42:29.665700 Epoch 300, Training Loss 0.10703327706860155\n",
      "2022-03-26 21:42:29.679704 Epoch 300, Training Loss 0.10767495125303488\n",
      "2022-03-26 21:42:29.693868 Epoch 300, Training Loss 0.10840876331872037\n",
      "2022-03-26 21:42:29.708303 Epoch 300, Training Loss 0.10901111974130812\n",
      "2022-03-26 21:42:29.722976 Epoch 300, Training Loss 0.11015681979601341\n",
      "2022-03-26 21:42:29.736937 Epoch 300, Training Loss 0.11099557193648785\n",
      "2022-03-26 21:42:29.752865 Epoch 300, Training Loss 0.1118129234942024\n",
      "2022-03-26 21:42:29.767200 Epoch 300, Training Loss 0.11243238244825007\n",
      "2022-03-26 21:42:29.782079 Epoch 300, Training Loss 0.11318140017711903\n",
      "2022-03-26 21:42:29.795967 Epoch 300, Training Loss 0.11397785558115187\n",
      "2022-03-26 21:42:29.809988 Epoch 300, Training Loss 0.11451671259177615\n",
      "2022-03-26 21:42:29.825002 Epoch 300, Training Loss 0.11526270031624133\n",
      "2022-03-26 21:42:29.838983 Epoch 300, Training Loss 0.11624915406222233\n",
      "2022-03-26 21:42:29.853868 Epoch 300, Training Loss 0.11675649660322672\n",
      "2022-03-26 21:42:29.868075 Epoch 300, Training Loss 0.117481362560521\n",
      "2022-03-26 21:42:29.882992 Epoch 300, Training Loss 0.11831613384244387\n",
      "2022-03-26 21:42:29.897752 Epoch 300, Training Loss 0.1189812015541984\n",
      "2022-03-26 21:42:29.917610 Epoch 300, Training Loss 0.11958176495931337\n",
      "2022-03-26 21:42:29.936493 Epoch 300, Training Loss 0.12040816796252794\n",
      "2022-03-26 21:42:29.957282 Epoch 300, Training Loss 0.12121558696260233\n",
      "2022-03-26 21:42:29.977045 Epoch 300, Training Loss 0.12189895208077052\n",
      "2022-03-26 21:42:29.995870 Epoch 300, Training Loss 0.12237990256922934\n",
      "2022-03-26 21:42:30.014858 Epoch 300, Training Loss 0.12303857444344884\n",
      "2022-03-26 21:42:30.033891 Epoch 300, Training Loss 0.12391251390394957\n",
      "2022-03-26 21:42:30.053869 Epoch 300, Training Loss 0.1251673294836298\n",
      "2022-03-26 21:42:30.073440 Epoch 300, Training Loss 0.12581647597158047\n",
      "2022-03-26 21:42:30.091865 Epoch 300, Training Loss 0.12657337332777965\n",
      "2022-03-26 21:42:30.111930 Epoch 300, Training Loss 0.12713550499942908\n",
      "2022-03-26 21:42:30.130986 Epoch 300, Training Loss 0.1279038914939022\n",
      "2022-03-26 21:42:30.150286 Epoch 300, Training Loss 0.12845611610376012\n",
      "2022-03-26 21:42:30.169200 Epoch 300, Training Loss 0.12933618020828422\n",
      "2022-03-26 21:42:30.188955 Epoch 300, Training Loss 0.13006563625677162\n",
      "2022-03-26 21:42:30.206934 Epoch 300, Training Loss 0.13091707603096048\n",
      "2022-03-26 21:42:30.225985 Epoch 300, Training Loss 0.1313729302581314\n",
      "2022-03-26 21:42:30.244877 Epoch 300, Training Loss 0.13195543257934053\n",
      "2022-03-26 21:42:30.265322 Epoch 300, Training Loss 0.1324660272320823\n",
      "2022-03-26 21:42:30.285170 Epoch 300, Training Loss 0.13330788948499334\n",
      "2022-03-26 21:42:30.304026 Epoch 300, Training Loss 0.1339951384326686\n",
      "2022-03-26 21:42:30.323052 Epoch 300, Training Loss 0.13453856369723444\n",
      "2022-03-26 21:42:30.343004 Epoch 300, Training Loss 0.13485878366795953\n",
      "2022-03-26 21:42:30.362044 Epoch 300, Training Loss 0.135480740719744\n",
      "2022-03-26 21:42:30.381093 Epoch 300, Training Loss 0.13621093626217465\n",
      "2022-03-26 21:42:30.399701 Epoch 300, Training Loss 0.13717638158127474\n",
      "2022-03-26 21:42:30.418760 Epoch 300, Training Loss 0.13768857355465364\n",
      "2022-03-26 21:42:30.439038 Epoch 300, Training Loss 0.13839295830415643\n",
      "2022-03-26 21:42:30.458172 Epoch 300, Training Loss 0.13880481309902942\n",
      "2022-03-26 21:42:30.476956 Epoch 300, Training Loss 0.13951659492214621\n",
      "2022-03-26 21:42:30.496872 Epoch 300, Training Loss 0.14023957616837737\n",
      "2022-03-26 21:42:30.515895 Epoch 300, Training Loss 0.1406676899975218\n",
      "2022-03-26 21:42:30.534913 Epoch 300, Training Loss 0.1416355909212776\n",
      "2022-03-26 21:42:30.553939 Epoch 300, Training Loss 0.14252772732921268\n",
      "2022-03-26 21:42:30.573875 Epoch 300, Training Loss 0.14337947244381966\n",
      "2022-03-26 21:42:30.593320 Epoch 300, Training Loss 0.14398096654268785\n",
      "2022-03-26 21:42:30.613923 Epoch 300, Training Loss 0.14457000689128477\n",
      "2022-03-26 21:42:30.633590 Epoch 300, Training Loss 0.14509010059601815\n",
      "2022-03-26 21:42:30.653426 Epoch 300, Training Loss 0.14580626214099357\n",
      "2022-03-26 21:42:30.672884 Epoch 300, Training Loss 0.14637334332289292\n",
      "2022-03-26 21:42:30.692921 Epoch 300, Training Loss 0.14687579824491534\n",
      "2022-03-26 21:42:30.711945 Epoch 300, Training Loss 0.14761924004310842\n",
      "2022-03-26 21:42:30.731753 Epoch 300, Training Loss 0.14840848694372055\n",
      "2022-03-26 21:42:30.751035 Epoch 300, Training Loss 0.1491149234040009\n",
      "2022-03-26 21:42:30.769964 Epoch 300, Training Loss 0.14955798088742034\n",
      "2022-03-26 21:42:30.788824 Epoch 300, Training Loss 0.15030290944801877\n",
      "2022-03-26 21:42:30.808111 Epoch 300, Training Loss 0.15091254335382712\n",
      "2022-03-26 21:42:30.826779 Epoch 300, Training Loss 0.15163994753909538\n",
      "2022-03-26 21:42:30.845690 Epoch 300, Training Loss 0.15237673983701963\n",
      "2022-03-26 21:42:30.865401 Epoch 300, Training Loss 0.15326037968668488\n",
      "2022-03-26 21:42:30.885998 Epoch 300, Training Loss 0.15399674857821305\n",
      "2022-03-26 21:42:30.912001 Epoch 300, Training Loss 0.1545205174961968\n",
      "2022-03-26 21:42:30.940007 Epoch 300, Training Loss 0.15529169771067627\n",
      "2022-03-26 21:42:30.960617 Epoch 300, Training Loss 0.15603213496220386\n",
      "2022-03-26 21:42:30.980480 Epoch 300, Training Loss 0.15677236283526702\n",
      "2022-03-26 21:42:31.001329 Epoch 300, Training Loss 0.15776162219169498\n",
      "2022-03-26 21:42:31.023742 Epoch 300, Training Loss 0.1582989003652197\n",
      "2022-03-26 21:42:31.043625 Epoch 300, Training Loss 0.15900176572982613\n",
      "2022-03-26 21:42:31.065449 Epoch 300, Training Loss 0.15940023928194705\n",
      "2022-03-26 21:42:31.084906 Epoch 300, Training Loss 0.16023616812875507\n",
      "2022-03-26 21:42:31.104810 Epoch 300, Training Loss 0.1609950709297224\n",
      "2022-03-26 21:42:31.125685 Epoch 300, Training Loss 0.1618025514399609\n",
      "2022-03-26 21:42:31.147149 Epoch 300, Training Loss 0.1625449573216231\n",
      "2022-03-26 21:42:31.167619 Epoch 300, Training Loss 0.1633150293622785\n",
      "2022-03-26 21:42:31.187380 Epoch 300, Training Loss 0.16370732186700376\n",
      "2022-03-26 21:42:31.207752 Epoch 300, Training Loss 0.16455581197348398\n",
      "2022-03-26 21:42:31.227750 Epoch 300, Training Loss 0.16516169417849588\n",
      "2022-03-26 21:42:31.248681 Epoch 300, Training Loss 0.16565533268177296\n",
      "2022-03-26 21:42:31.268012 Epoch 300, Training Loss 0.16630670329189057\n",
      "2022-03-26 21:42:31.288045 Epoch 300, Training Loss 0.1667614846735659\n",
      "2022-03-26 21:42:31.307744 Epoch 300, Training Loss 0.16783375035771322\n",
      "2022-03-26 21:42:31.327657 Epoch 300, Training Loss 0.16844715509573213\n",
      "2022-03-26 21:42:31.347666 Epoch 300, Training Loss 0.1692254418302375\n",
      "2022-03-26 21:42:31.375645 Epoch 300, Training Loss 0.169784869608062\n",
      "2022-03-26 21:42:31.399660 Epoch 300, Training Loss 0.17062100219299725\n",
      "2022-03-26 21:42:31.420107 Epoch 300, Training Loss 0.17115800414243929\n",
      "2022-03-26 21:42:31.441147 Epoch 300, Training Loss 0.17200969171036234\n",
      "2022-03-26 21:42:31.461147 Epoch 300, Training Loss 0.17247790174411082\n",
      "2022-03-26 21:42:31.485153 Epoch 300, Training Loss 0.17282163559475824\n",
      "2022-03-26 21:42:31.511039 Epoch 300, Training Loss 0.1734995845409915\n",
      "2022-03-26 21:42:31.532891 Epoch 300, Training Loss 0.17432718329569874\n",
      "2022-03-26 21:42:31.554303 Epoch 300, Training Loss 0.17500660414128658\n",
      "2022-03-26 21:42:31.573773 Epoch 300, Training Loss 0.17562789639548573\n",
      "2022-03-26 21:42:31.593799 Epoch 300, Training Loss 0.17643204262799314\n",
      "2022-03-26 21:42:31.615700 Epoch 300, Training Loss 0.17708834876184879\n",
      "2022-03-26 21:42:31.636753 Epoch 300, Training Loss 0.17774813193494401\n",
      "2022-03-26 21:42:31.657039 Epoch 300, Training Loss 0.17857697720417892\n",
      "2022-03-26 21:42:31.676800 Epoch 300, Training Loss 0.1790201455126028\n",
      "2022-03-26 21:42:31.696825 Epoch 300, Training Loss 0.17969986262833676\n",
      "2022-03-26 21:42:31.715842 Epoch 300, Training Loss 0.1804364726823919\n",
      "2022-03-26 21:42:31.736766 Epoch 300, Training Loss 0.1811036665726196\n",
      "2022-03-26 21:42:31.756703 Epoch 300, Training Loss 0.18158556966830397\n",
      "2022-03-26 21:42:31.776714 Epoch 300, Training Loss 0.18230794716978926\n",
      "2022-03-26 21:42:31.797429 Epoch 300, Training Loss 0.1831702210409257\n",
      "2022-03-26 21:42:31.817439 Epoch 300, Training Loss 0.18378478829818004\n",
      "2022-03-26 21:42:31.837034 Epoch 300, Training Loss 0.1846023691279809\n",
      "2022-03-26 21:42:31.857298 Epoch 300, Training Loss 0.18533414358373188\n",
      "2022-03-26 21:42:31.877193 Epoch 300, Training Loss 0.18618570043302862\n",
      "2022-03-26 21:42:31.896223 Epoch 300, Training Loss 0.1871274163198593\n",
      "2022-03-26 21:42:31.916168 Epoch 300, Training Loss 0.1879364802404438\n",
      "2022-03-26 21:42:31.934825 Epoch 300, Training Loss 0.18853224947324496\n",
      "2022-03-26 21:42:31.955236 Epoch 300, Training Loss 0.1891607619688639\n",
      "2022-03-26 21:42:31.975164 Epoch 300, Training Loss 0.18956335578733088\n",
      "2022-03-26 21:42:31.994548 Epoch 300, Training Loss 0.19020986480786062\n",
      "2022-03-26 21:42:32.014446 Epoch 300, Training Loss 0.19082742803694341\n",
      "2022-03-26 21:42:32.033880 Epoch 300, Training Loss 0.1916226227493847\n",
      "2022-03-26 21:42:32.054573 Epoch 300, Training Loss 0.19220867745406792\n",
      "2022-03-26 21:42:32.075067 Epoch 300, Training Loss 0.19283067425498573\n",
      "2022-03-26 21:42:32.094205 Epoch 300, Training Loss 0.19331643351203645\n",
      "2022-03-26 21:42:32.114053 Epoch 300, Training Loss 0.19409485226092132\n",
      "2022-03-26 21:42:32.135165 Epoch 300, Training Loss 0.19511647084180048\n",
      "2022-03-26 21:42:32.155715 Epoch 300, Training Loss 0.19593154195019657\n",
      "2022-03-26 21:42:32.175718 Epoch 300, Training Loss 0.19635480276459014\n",
      "2022-03-26 21:42:32.194201 Epoch 300, Training Loss 0.1968210307533479\n",
      "2022-03-26 21:42:32.214159 Epoch 300, Training Loss 0.19803166130314703\n",
      "2022-03-26 21:42:32.233841 Epoch 300, Training Loss 0.19864549078142552\n",
      "2022-03-26 21:42:32.253861 Epoch 300, Training Loss 0.1995239816129665\n",
      "2022-03-26 21:42:32.272993 Epoch 300, Training Loss 0.20001230996740443\n",
      "2022-03-26 21:42:32.293933 Epoch 300, Training Loss 0.2005474163824335\n",
      "2022-03-26 21:42:32.313041 Epoch 300, Training Loss 0.20136775983416516\n",
      "2022-03-26 21:42:32.333271 Epoch 300, Training Loss 0.2020105033198281\n",
      "2022-03-26 21:42:32.353254 Epoch 300, Training Loss 0.2025385130854214\n",
      "2022-03-26 21:42:32.373703 Epoch 300, Training Loss 0.20350961139439927\n",
      "2022-03-26 21:42:32.392720 Epoch 300, Training Loss 0.2041925851188962\n",
      "2022-03-26 21:42:32.412862 Epoch 300, Training Loss 0.2045548399314856\n",
      "2022-03-26 21:42:32.432673 Epoch 300, Training Loss 0.20512535032408927\n",
      "2022-03-26 21:42:32.452695 Epoch 300, Training Loss 0.20604881568028188\n",
      "2022-03-26 21:42:32.472642 Epoch 300, Training Loss 0.20664543092555707\n",
      "2022-03-26 21:42:32.492733 Epoch 300, Training Loss 0.20717293694805916\n",
      "2022-03-26 21:42:32.511751 Epoch 300, Training Loss 0.2079997604612804\n",
      "2022-03-26 21:42:32.531839 Epoch 300, Training Loss 0.20857997211958745\n",
      "2022-03-26 21:42:32.551843 Epoch 300, Training Loss 0.20946578784367006\n",
      "2022-03-26 21:42:32.573900 Epoch 300, Training Loss 0.20991977946380216\n",
      "2022-03-26 21:42:32.592956 Epoch 300, Training Loss 0.21040921953633007\n",
      "2022-03-26 21:42:32.612962 Epoch 300, Training Loss 0.21096379631925422\n",
      "2022-03-26 21:42:32.632750 Epoch 300, Training Loss 0.211621010471183\n",
      "2022-03-26 21:42:32.652859 Epoch 300, Training Loss 0.2122343161603069\n",
      "2022-03-26 21:42:32.672846 Epoch 300, Training Loss 0.21294568532415667\n",
      "2022-03-26 21:42:32.692865 Epoch 300, Training Loss 0.21376552754808265\n",
      "2022-03-26 21:42:32.711883 Epoch 300, Training Loss 0.21448225152614475\n",
      "2022-03-26 21:42:32.732123 Epoch 300, Training Loss 0.21499735158880043\n",
      "2022-03-26 21:42:32.751070 Epoch 300, Training Loss 0.21552097008508794\n",
      "2022-03-26 21:42:32.771510 Epoch 300, Training Loss 0.21618926460328308\n",
      "2022-03-26 21:42:32.792383 Epoch 300, Training Loss 0.21675064111762035\n",
      "2022-03-26 21:42:32.812449 Epoch 300, Training Loss 0.21746057786447617\n",
      "2022-03-26 21:42:32.832663 Epoch 300, Training Loss 0.218199606860995\n",
      "2022-03-26 21:42:32.852747 Epoch 300, Training Loss 0.21881073968642203\n",
      "2022-03-26 21:42:32.873446 Epoch 300, Training Loss 0.21946641631291042\n",
      "2022-03-26 21:42:32.893290 Epoch 300, Training Loss 0.21994550747182362\n",
      "2022-03-26 21:42:32.912205 Epoch 300, Training Loss 0.22051114880520364\n",
      "2022-03-26 21:42:32.932086 Epoch 300, Training Loss 0.22111185897341776\n",
      "2022-03-26 21:42:32.952081 Epoch 300, Training Loss 0.22163100780733405\n",
      "2022-03-26 21:42:32.972087 Epoch 300, Training Loss 0.2222969352894122\n",
      "2022-03-26 21:42:32.992172 Epoch 300, Training Loss 0.22296042157256085\n",
      "2022-03-26 21:42:33.012227 Epoch 300, Training Loss 0.2236617118348856\n",
      "2022-03-26 21:42:33.032397 Epoch 300, Training Loss 0.22412424959490063\n",
      "2022-03-26 21:42:33.051185 Epoch 300, Training Loss 0.22460906116096566\n",
      "2022-03-26 21:42:33.070827 Epoch 300, Training Loss 0.22528329571647107\n",
      "2022-03-26 21:42:33.091910 Epoch 300, Training Loss 0.2261519764009339\n",
      "2022-03-26 21:42:33.111964 Epoch 300, Training Loss 0.22673727808248662\n",
      "2022-03-26 21:42:33.131853 Epoch 300, Training Loss 0.22731316318292447\n",
      "2022-03-26 21:42:33.150919 Epoch 300, Training Loss 0.22774494429835884\n",
      "2022-03-26 21:42:33.169950 Epoch 300, Training Loss 0.22851293307283652\n",
      "2022-03-26 21:42:33.190852 Epoch 300, Training Loss 0.22929213209377836\n",
      "2022-03-26 21:42:33.210906 Epoch 300, Training Loss 0.22990876539131563\n",
      "2022-03-26 21:42:33.230909 Epoch 300, Training Loss 0.23050507693491934\n",
      "2022-03-26 21:42:33.250956 Epoch 300, Training Loss 0.23125334975817013\n",
      "2022-03-26 21:42:33.270886 Epoch 300, Training Loss 0.23208168587263892\n",
      "2022-03-26 21:42:33.290883 Epoch 300, Training Loss 0.232730735026662\n",
      "2022-03-26 21:42:33.310166 Epoch 300, Training Loss 0.23368532932780284\n",
      "2022-03-26 21:42:33.331010 Epoch 300, Training Loss 0.23443471162063081\n",
      "2022-03-26 21:42:33.349838 Epoch 300, Training Loss 0.23501637581821597\n",
      "2022-03-26 21:42:33.369947 Epoch 300, Training Loss 0.23586870303086918\n",
      "2022-03-26 21:42:33.390965 Epoch 300, Training Loss 0.23627826590519732\n",
      "2022-03-26 21:42:33.410421 Epoch 300, Training Loss 0.23679465204096206\n",
      "2022-03-26 21:42:33.432076 Epoch 300, Training Loss 0.23732883591786066\n",
      "2022-03-26 21:42:33.453051 Epoch 300, Training Loss 0.23813220340272653\n",
      "2022-03-26 21:42:33.472709 Epoch 300, Training Loss 0.23896458112370328\n",
      "2022-03-26 21:42:33.492767 Epoch 300, Training Loss 0.239605211235983\n",
      "2022-03-26 21:42:33.511765 Epoch 300, Training Loss 0.2401749435288217\n",
      "2022-03-26 21:42:33.531718 Epoch 300, Training Loss 0.24070387259316262\n",
      "2022-03-26 21:42:33.551700 Epoch 300, Training Loss 0.2417307584105855\n",
      "2022-03-26 21:42:33.571784 Epoch 300, Training Loss 0.24250753475424577\n",
      "2022-03-26 21:42:33.592943 Epoch 300, Training Loss 0.2429954373013333\n",
      "2022-03-26 21:42:33.611985 Epoch 300, Training Loss 0.24353155547090807\n",
      "2022-03-26 21:42:33.632694 Epoch 300, Training Loss 0.24424980599861926\n",
      "2022-03-26 21:42:33.652687 Epoch 300, Training Loss 0.24519223393991474\n",
      "2022-03-26 21:42:33.674608 Epoch 300, Training Loss 0.2460010858905285\n",
      "2022-03-26 21:42:33.693676 Epoch 300, Training Loss 0.2467697815364584\n",
      "2022-03-26 21:42:33.713610 Epoch 300, Training Loss 0.247280903255848\n",
      "2022-03-26 21:42:33.733449 Epoch 300, Training Loss 0.24790191616091278\n",
      "2022-03-26 21:42:33.754236 Epoch 300, Training Loss 0.24844349996970438\n",
      "2022-03-26 21:42:33.773933 Epoch 300, Training Loss 0.24936721167143652\n",
      "2022-03-26 21:42:33.792988 Epoch 300, Training Loss 0.2497256743862196\n",
      "2022-03-26 21:42:33.813049 Epoch 300, Training Loss 0.2503478185600027\n",
      "2022-03-26 21:42:33.832689 Epoch 300, Training Loss 0.251104431688938\n",
      "2022-03-26 21:42:33.853741 Epoch 300, Training Loss 0.25161132235508743\n",
      "2022-03-26 21:42:33.874165 Epoch 300, Training Loss 0.25225486913148093\n",
      "2022-03-26 21:42:33.893237 Epoch 300, Training Loss 0.252981125508123\n",
      "2022-03-26 21:42:33.913193 Epoch 300, Training Loss 0.25385915771927064\n",
      "2022-03-26 21:42:33.932894 Epoch 300, Training Loss 0.25431825380648493\n",
      "2022-03-26 21:42:33.951923 Epoch 300, Training Loss 0.25500335851136374\n",
      "2022-03-26 21:42:33.979773 Epoch 300, Training Loss 0.255529159322724\n",
      "2022-03-26 21:42:34.007225 Epoch 300, Training Loss 0.25617171766812846\n",
      "2022-03-26 21:42:34.033953 Epoch 300, Training Loss 0.25664312436300163\n",
      "2022-03-26 21:42:34.061044 Epoch 300, Training Loss 0.2573486175149908\n",
      "2022-03-26 21:42:34.088774 Epoch 300, Training Loss 0.25803490615714236\n",
      "2022-03-26 21:42:34.114798 Epoch 300, Training Loss 0.2587615187134584\n",
      "2022-03-26 21:42:34.142729 Epoch 300, Training Loss 0.2593462034259611\n",
      "2022-03-26 21:42:34.168328 Epoch 300, Training Loss 0.26025099095786014\n",
      "2022-03-26 21:42:34.194885 Epoch 300, Training Loss 0.26114893416919366\n",
      "2022-03-26 21:42:34.220870 Epoch 300, Training Loss 0.2619792501944715\n",
      "2022-03-26 21:42:34.247918 Epoch 300, Training Loss 0.26258749322360736\n",
      "2022-03-26 21:42:34.273691 Epoch 300, Training Loss 0.2634765472634674\n",
      "2022-03-26 21:42:34.300674 Epoch 300, Training Loss 0.2641058916326069\n",
      "2022-03-26 21:42:34.328046 Epoch 300, Training Loss 0.26517454368988874\n",
      "2022-03-26 21:42:34.354070 Epoch 300, Training Loss 0.26586291521711425\n",
      "2022-03-26 21:42:34.375207 Epoch 300, Training Loss 0.26636609880973006\n",
      "2022-03-26 21:42:34.394053 Epoch 300, Training Loss 0.26691713742435436\n",
      "2022-03-26 21:42:34.412085 Epoch 300, Training Loss 0.2674375089919171\n",
      "2022-03-26 21:42:34.431674 Epoch 300, Training Loss 0.26809384759582217\n",
      "2022-03-26 21:42:34.449656 Epoch 300, Training Loss 0.26880128491107763\n",
      "2022-03-26 21:42:34.468730 Epoch 300, Training Loss 0.2697738176187896\n",
      "2022-03-26 21:42:34.487739 Epoch 300, Training Loss 0.27020909197037785\n",
      "2022-03-26 21:42:34.506658 Epoch 300, Training Loss 0.2710537707333065\n",
      "2022-03-26 21:42:34.525989 Epoch 300, Training Loss 0.27167679365638575\n",
      "2022-03-26 21:42:34.543930 Epoch 300, Training Loss 0.2724076400480002\n",
      "2022-03-26 21:42:34.563049 Epoch 300, Training Loss 0.2731252212811004\n",
      "2022-03-26 21:42:34.581840 Epoch 300, Training Loss 0.2739055879280695\n",
      "2022-03-26 21:42:34.600707 Epoch 300, Training Loss 0.2746150545451952\n",
      "2022-03-26 21:42:34.619672 Epoch 300, Training Loss 0.2752478724474187\n",
      "2022-03-26 21:42:34.637711 Epoch 300, Training Loss 0.2762071688080688\n",
      "2022-03-26 21:42:34.656693 Epoch 300, Training Loss 0.27688449365860973\n",
      "2022-03-26 21:42:34.675433 Epoch 300, Training Loss 0.2776092558794314\n",
      "2022-03-26 21:42:34.693478 Epoch 300, Training Loss 0.2782329024789888\n",
      "2022-03-26 21:42:34.712337 Epoch 300, Training Loss 0.27886663377285004\n",
      "2022-03-26 21:42:34.732004 Epoch 300, Training Loss 0.27943871130266457\n",
      "2022-03-26 21:42:34.751062 Epoch 300, Training Loss 0.2802894290374673\n",
      "2022-03-26 21:42:34.770422 Epoch 300, Training Loss 0.2810316053421601\n",
      "2022-03-26 21:42:34.789058 Epoch 300, Training Loss 0.2817253910595804\n",
      "2022-03-26 21:42:34.808158 Epoch 300, Training Loss 0.2824188628998559\n",
      "2022-03-26 21:42:34.825921 Epoch 300, Training Loss 0.2830898167608339\n",
      "2022-03-26 21:42:34.844882 Epoch 300, Training Loss 0.2836762915944199\n",
      "2022-03-26 21:42:34.862816 Epoch 300, Training Loss 0.28442964864813763\n",
      "2022-03-26 21:42:34.881830 Epoch 300, Training Loss 0.2850748827237912\n",
      "2022-03-26 21:42:34.900735 Epoch 300, Training Loss 0.28605209950290983\n",
      "2022-03-26 21:42:34.919751 Epoch 300, Training Loss 0.28668934125881973\n",
      "2022-03-26 21:42:34.939333 Epoch 300, Training Loss 0.28748988087677285\n",
      "2022-03-26 21:42:34.958275 Epoch 300, Training Loss 0.28816467390188477\n",
      "2022-03-26 21:42:34.976118 Epoch 300, Training Loss 0.2890198133561922\n",
      "2022-03-26 21:42:34.994998 Epoch 300, Training Loss 0.28942521575771635\n",
      "2022-03-26 21:42:35.012920 Epoch 300, Training Loss 0.29004871128770093\n",
      "2022-03-26 21:42:35.032499 Epoch 300, Training Loss 0.2906921693431142\n",
      "2022-03-26 21:42:35.050068 Epoch 300, Training Loss 0.29127824885765913\n",
      "2022-03-26 21:42:35.068534 Epoch 300, Training Loss 0.2919672396786682\n",
      "2022-03-26 21:42:35.087065 Epoch 300, Training Loss 0.29279850000310736\n",
      "2022-03-26 21:42:35.105008 Epoch 300, Training Loss 0.29322021486966504\n",
      "2022-03-26 21:42:35.124037 Epoch 300, Training Loss 0.2939013576187441\n",
      "2022-03-26 21:42:35.142108 Epoch 300, Training Loss 0.29461994016414406\n",
      "2022-03-26 21:42:35.160101 Epoch 300, Training Loss 0.29512353664468927\n",
      "2022-03-26 21:42:35.179725 Epoch 300, Training Loss 0.29592944235752916\n",
      "2022-03-26 21:42:35.197380 Epoch 300, Training Loss 0.29671251720479686\n",
      "2022-03-26 21:42:35.215428 Epoch 300, Training Loss 0.29735427660405483\n",
      "2022-03-26 21:42:35.234427 Epoch 300, Training Loss 0.2979142509991556\n",
      "2022-03-26 21:42:35.253477 Epoch 300, Training Loss 0.29856684731552974\n",
      "2022-03-26 21:42:35.271360 Epoch 300, Training Loss 0.29934160815327976\n",
      "2022-03-26 21:42:35.290167 Epoch 300, Training Loss 0.2999786281448496\n",
      "2022-03-26 21:42:35.307774 Epoch 300, Training Loss 0.30067430261303396\n",
      "2022-03-26 21:42:35.326805 Epoch 300, Training Loss 0.3014650498433491\n",
      "2022-03-26 21:42:35.345822 Epoch 300, Training Loss 0.30212339648352865\n",
      "2022-03-26 21:42:35.364723 Epoch 300, Training Loss 0.30262139470071137\n",
      "2022-03-26 21:42:35.382763 Epoch 300, Training Loss 0.30338013896247007\n",
      "2022-03-26 21:42:35.401763 Epoch 300, Training Loss 0.303930334194237\n",
      "2022-03-26 21:42:35.419484 Epoch 300, Training Loss 0.3045144927928515\n",
      "2022-03-26 21:42:35.438015 Epoch 300, Training Loss 0.30529020219812614\n",
      "2022-03-26 21:42:35.455945 Epoch 300, Training Loss 0.305983283102055\n",
      "2022-03-26 21:42:35.473828 Epoch 300, Training Loss 0.3067861159744165\n",
      "2022-03-26 21:42:35.491827 Epoch 300, Training Loss 0.3074182897348843\n",
      "2022-03-26 21:42:35.510830 Epoch 300, Training Loss 0.30798145309281166\n",
      "2022-03-26 21:42:35.528648 Epoch 300, Training Loss 0.3084690286527814\n",
      "2022-03-26 21:42:35.547494 Epoch 300, Training Loss 0.3091061843935486\n",
      "2022-03-26 21:42:35.566091 Epoch 300, Training Loss 0.3094828884543665\n",
      "2022-03-26 21:42:35.584652 Epoch 300, Training Loss 0.3102542737789471\n",
      "2022-03-26 21:42:35.602710 Epoch 300, Training Loss 0.31090997022283656\n",
      "2022-03-26 21:42:35.620517 Epoch 300, Training Loss 0.3115516652917618\n",
      "2022-03-26 21:42:35.638983 Epoch 300, Training Loss 0.3122951031264747\n",
      "2022-03-26 21:42:35.657401 Epoch 300, Training Loss 0.31309960744417537\n",
      "2022-03-26 21:42:35.675365 Epoch 300, Training Loss 0.31362902828494604\n",
      "2022-03-26 21:42:35.694633 Epoch 300, Training Loss 0.3146760966771704\n",
      "2022-03-26 21:42:35.712504 Epoch 300, Training Loss 0.3156058749426966\n",
      "2022-03-26 21:42:35.730250 Epoch 300, Training Loss 0.3161744707838044\n",
      "2022-03-26 21:42:35.749822 Epoch 300, Training Loss 0.3171376004395887\n",
      "2022-03-26 21:42:35.768709 Epoch 300, Training Loss 0.3176632315453971\n",
      "2022-03-26 21:42:35.786318 Epoch 300, Training Loss 0.3186634709615537\n",
      "2022-03-26 21:42:35.805162 Epoch 300, Training Loss 0.3194713188559198\n",
      "2022-03-26 21:42:35.822660 Epoch 300, Training Loss 0.3201687400755675\n",
      "2022-03-26 21:42:35.840595 Epoch 300, Training Loss 0.3210393434290386\n",
      "2022-03-26 21:42:35.859463 Epoch 300, Training Loss 0.3218474323334901\n",
      "2022-03-26 21:42:35.877126 Epoch 300, Training Loss 0.3225047155414396\n",
      "2022-03-26 21:42:35.895752 Epoch 300, Training Loss 0.3232506015111723\n",
      "2022-03-26 21:42:35.914768 Epoch 300, Training Loss 0.3236108091100098\n",
      "2022-03-26 21:42:35.932801 Epoch 300, Training Loss 0.32453249090010555\n",
      "2022-03-26 21:42:35.951829 Epoch 300, Training Loss 0.32511139308552606\n",
      "2022-03-26 21:42:35.970911 Epoch 300, Training Loss 0.325630168742536\n",
      "2022-03-26 21:42:35.988886 Epoch 300, Training Loss 0.32632550067456484\n",
      "2022-03-26 21:42:36.006821 Epoch 300, Training Loss 0.3271309669746462\n",
      "2022-03-26 21:42:36.025723 Epoch 300, Training Loss 0.3278043012103766\n",
      "2022-03-26 21:42:36.044052 Epoch 300, Training Loss 0.32815207022687665\n",
      "2022-03-26 21:42:36.061983 Epoch 300, Training Loss 0.3287415398127588\n",
      "2022-03-26 21:42:36.080987 Epoch 300, Training Loss 0.3296144323047165\n",
      "2022-03-26 21:42:36.099190 Epoch 300, Training Loss 0.3304036320246699\n",
      "2022-03-26 21:42:36.117274 Epoch 300, Training Loss 0.3309448042412853\n",
      "2022-03-26 21:42:36.135907 Epoch 300, Training Loss 0.3316169129994214\n",
      "2022-03-26 21:42:36.153812 Epoch 300, Training Loss 0.332497520786722\n",
      "2022-03-26 21:42:36.172840 Epoch 300, Training Loss 0.3332063473780137\n",
      "2022-03-26 21:42:36.196901 Epoch 300, Training Loss 0.3337981262246666\n",
      "2022-03-26 21:42:36.222782 Epoch 300, Training Loss 0.3344666641157911\n",
      "2022-03-26 21:42:36.247922 Epoch 300, Training Loss 0.33490320918200267\n",
      "2022-03-26 21:42:36.274883 Epoch 300, Training Loss 0.3355017997648405\n",
      "2022-03-26 21:42:36.299945 Epoch 300, Training Loss 0.33634764333362777\n",
      "2022-03-26 21:42:36.325942 Epoch 300, Training Loss 0.33674860332170714\n",
      "2022-03-26 21:42:36.351799 Epoch 300, Training Loss 0.33736815667518266\n",
      "2022-03-26 21:42:36.378662 Epoch 300, Training Loss 0.33811026026525764\n",
      "2022-03-26 21:42:36.392913 Epoch 300, Training Loss 0.3390183720137457\n",
      "2022-03-26 21:42:36.406919 Epoch 300, Training Loss 0.33967168785422047\n",
      "2022-03-26 21:42:36.421822 Epoch 300, Training Loss 0.3402951535437723\n",
      "2022-03-26 21:42:36.435708 Epoch 300, Training Loss 0.3410008207840078\n",
      "2022-03-26 21:42:36.449666 Epoch 300, Training Loss 0.34159075276321155\n",
      "2022-03-26 21:42:36.464649 Epoch 300, Training Loss 0.3424510096040223\n",
      "2022-03-26 21:42:36.480463 Epoch 300, Training Loss 0.34321182127803795\n",
      "2022-03-26 21:42:36.495418 Epoch 300, Training Loss 0.34401919934755704\n",
      "2022-03-26 21:42:36.510417 Epoch 300, Training Loss 0.3444930353890295\n",
      "2022-03-26 21:42:36.524259 Epoch 300, Training Loss 0.3451106465228683\n",
      "2022-03-26 21:42:36.539045 Epoch 300, Training Loss 0.34555784889194363\n",
      "2022-03-26 21:42:36.553900 Epoch 300, Training Loss 0.3460482645522603\n",
      "2022-03-26 21:42:36.568911 Epoch 300, Training Loss 0.3466771145153533\n",
      "2022-03-26 21:42:36.583759 Epoch 300, Training Loss 0.3473454757267252\n",
      "2022-03-26 21:42:36.597761 Epoch 300, Training Loss 0.3477628707047314\n",
      "2022-03-26 21:42:36.613483 Epoch 300, Training Loss 0.34829828421325637\n",
      "2022-03-26 21:42:36.627248 Epoch 300, Training Loss 0.349143063511385\n",
      "2022-03-26 21:42:36.640988 Epoch 300, Training Loss 0.34979578155233426\n",
      "2022-03-26 21:42:36.655461 Epoch 300, Training Loss 0.35053195882483823\n",
      "2022-03-26 21:42:36.670347 Epoch 300, Training Loss 0.3512300773121207\n",
      "2022-03-26 21:42:36.684986 Epoch 300, Training Loss 0.3520530769815835\n",
      "2022-03-26 21:42:36.699008 Epoch 300, Training Loss 0.35276200799533475\n",
      "2022-03-26 21:42:36.714008 Epoch 300, Training Loss 0.35327319069134305\n",
      "2022-03-26 21:42:36.727908 Epoch 300, Training Loss 0.3541214634161776\n",
      "2022-03-26 21:42:36.741929 Epoch 300, Training Loss 0.3549331302761727\n",
      "2022-03-26 21:42:36.755821 Epoch 300, Training Loss 0.35585609798693596\n",
      "2022-03-26 21:42:36.770825 Epoch 300, Training Loss 0.35654709432893394\n",
      "2022-03-26 21:42:36.785847 Epoch 300, Training Loss 0.3570706987624888\n",
      "2022-03-26 21:42:36.799801 Epoch 300, Training Loss 0.3579404391443638\n",
      "2022-03-26 21:42:36.814889 Epoch 300, Training Loss 0.35860402870666036\n",
      "2022-03-26 21:42:36.828818 Epoch 300, Training Loss 0.3596218459289092\n",
      "2022-03-26 21:42:36.842941 Epoch 300, Training Loss 0.3604663911530429\n",
      "2022-03-26 21:42:36.856937 Epoch 300, Training Loss 0.36121316943936943\n",
      "2022-03-26 21:42:36.870965 Epoch 300, Training Loss 0.36192332554961104\n",
      "2022-03-26 21:42:36.885877 Epoch 300, Training Loss 0.36239262199615274\n",
      "2022-03-26 21:42:36.900161 Epoch 300, Training Loss 0.36318739848521053\n",
      "2022-03-26 21:42:36.913883 Epoch 300, Training Loss 0.3638006979623414\n",
      "2022-03-26 21:42:36.928761 Epoch 300, Training Loss 0.36450982295796086\n",
      "2022-03-26 21:42:36.942783 Epoch 300, Training Loss 0.36509580037477984\n",
      "2022-03-26 21:42:36.956804 Epoch 300, Training Loss 0.3656013483357856\n",
      "2022-03-26 21:42:36.971169 Epoch 300, Training Loss 0.36620669413710494\n",
      "2022-03-26 21:42:36.986017 Epoch 300, Training Loss 0.36691787084350197\n",
      "2022-03-26 21:42:37.000441 Epoch 300, Training Loss 0.3675064897293325\n",
      "2022-03-26 21:42:37.015071 Epoch 300, Training Loss 0.368098273073011\n",
      "2022-03-26 21:42:37.028993 Epoch 300, Training Loss 0.3688328941459851\n",
      "2022-03-26 21:42:37.044052 Epoch 300, Training Loss 0.3697039570344988\n",
      "2022-03-26 21:42:37.057085 Epoch 300, Training Loss 0.3702938908048908\n",
      "2022-03-26 21:42:37.072099 Epoch 300, Training Loss 0.3711202501336022\n",
      "2022-03-26 21:42:37.085373 Epoch 300, Training Loss 0.3718369427087057\n",
      "2022-03-26 21:42:37.100421 Epoch 300, Training Loss 0.37244399974260795\n",
      "2022-03-26 21:42:37.115052 Epoch 300, Training Loss 0.37320253104352585\n",
      "2022-03-26 21:42:37.129098 Epoch 300, Training Loss 0.37387352667348767\n",
      "2022-03-26 21:42:37.143191 Epoch 300, Training Loss 0.37441963948252255\n",
      "2022-03-26 21:42:37.157206 Epoch 300, Training Loss 0.37493953432725824\n",
      "2022-03-26 21:42:37.171066 Epoch 300, Training Loss 0.3758484752815398\n",
      "2022-03-26 21:42:37.185118 Epoch 300, Training Loss 0.37665801169469837\n",
      "2022-03-26 21:42:37.200070 Epoch 300, Training Loss 0.37737345432533936\n",
      "2022-03-26 21:42:37.214414 Epoch 300, Training Loss 0.37839910269850663\n",
      "2022-03-26 21:42:37.228301 Epoch 300, Training Loss 0.3792842641434706\n",
      "2022-03-26 21:42:37.244399 Epoch 300, Training Loss 0.37995339015408247\n",
      "2022-03-26 21:42:37.258932 Epoch 300, Training Loss 0.3807898030028014\n",
      "2022-03-26 21:42:37.272887 Epoch 300, Training Loss 0.381376088465876\n",
      "2022-03-26 21:42:37.287407 Epoch 300, Training Loss 0.38208690868771594\n",
      "2022-03-26 21:42:37.301652 Epoch 300, Training Loss 0.38272549326310074\n",
      "2022-03-26 21:42:37.315759 Epoch 300, Training Loss 0.38349794034305434\n",
      "2022-03-26 21:42:37.329772 Epoch 300, Training Loss 0.3842172082656485\n",
      "2022-03-26 21:42:37.344642 Epoch 300, Training Loss 0.3848657527619311\n",
      "2022-03-26 21:42:37.358859 Epoch 300, Training Loss 0.3853773605030821\n",
      "2022-03-26 21:42:37.372876 Epoch 300, Training Loss 0.3862643592497882\n",
      "2022-03-26 21:42:37.388275 Epoch 300, Training Loss 0.3867875962229946\n",
      "2022-03-26 21:42:37.401899 Epoch 300, Training Loss 0.3873862352060235\n",
      "2022-03-26 21:42:37.416029 Epoch 300, Training Loss 0.38800882149840255\n",
      "2022-03-26 21:42:37.431047 Epoch 300, Training Loss 0.3886281460752268\n",
      "2022-03-26 21:42:37.445237 Epoch 300, Training Loss 0.3893038414780746\n",
      "2022-03-26 21:42:37.459194 Epoch 300, Training Loss 0.3900539380357698\n",
      "2022-03-26 21:42:37.474167 Epoch 300, Training Loss 0.39083694809538017\n",
      "2022-03-26 21:42:37.489169 Epoch 300, Training Loss 0.391403595276196\n",
      "2022-03-26 21:42:37.503112 Epoch 300, Training Loss 0.39198487055728504\n",
      "2022-03-26 21:42:37.517087 Epoch 300, Training Loss 0.3925934433937073\n",
      "2022-03-26 21:42:37.531655 Epoch 300, Training Loss 0.3932225190465103\n",
      "2022-03-26 21:42:37.545948 Epoch 300, Training Loss 0.39389364287981293\n",
      "2022-03-26 21:42:37.559952 Epoch 300, Training Loss 0.3947470766656539\n",
      "2022-03-26 21:42:37.573937 Epoch 300, Training Loss 0.39558892748544894\n",
      "2022-03-26 21:42:37.588958 Epoch 300, Training Loss 0.3964971658366416\n",
      "2022-03-26 21:42:37.602872 Epoch 300, Training Loss 0.39738782767749503\n",
      "2022-03-26 21:42:37.617174 Epoch 300, Training Loss 0.39815714452272793\n",
      "2022-03-26 21:42:37.632260 Epoch 300, Training Loss 0.39909210038916837\n",
      "2022-03-26 21:42:37.646388 Epoch 300, Training Loss 0.3998522133473545\n",
      "2022-03-26 21:42:37.660417 Epoch 300, Training Loss 0.40055752295972136\n",
      "2022-03-26 21:42:37.674816 Epoch 300, Training Loss 0.4012997182433867\n",
      "2022-03-26 21:42:37.688820 Epoch 300, Training Loss 0.40192454916132075\n",
      "2022-03-26 21:42:37.703307 Epoch 300, Training Loss 0.40251976846124204\n",
      "2022-03-26 21:42:37.717329 Epoch 300, Training Loss 0.4029906703840436\n",
      "2022-03-26 21:42:37.731195 Epoch 300, Training Loss 0.4036683818263471\n",
      "2022-03-26 21:42:37.745525 Epoch 300, Training Loss 0.4045281507779875\n",
      "2022-03-26 21:42:37.760555 Epoch 300, Training Loss 0.40529760177178153\n",
      "2022-03-26 21:42:37.774159 Epoch 300, Training Loss 0.40603064255946125\n",
      "2022-03-26 21:42:37.789211 Epoch 300, Training Loss 0.40687084685811\n",
      "2022-03-26 21:42:37.804045 Epoch 300, Training Loss 0.4076004951353878\n",
      "2022-03-26 21:42:37.819067 Epoch 300, Training Loss 0.4084155283620595\n",
      "2022-03-26 21:42:37.833044 Epoch 300, Training Loss 0.40902414712149776\n",
      "2022-03-26 21:42:37.847075 Epoch 300, Training Loss 0.40971921449122223\n",
      "2022-03-26 21:42:37.862099 Epoch 300, Training Loss 0.4100670070218308\n",
      "2022-03-26 21:42:37.875263 Epoch 300, Training Loss 0.41083590061310915\n",
      "2022-03-26 21:42:37.890284 Epoch 300, Training Loss 0.41139359192927477\n",
      "2022-03-26 21:42:37.904100 Epoch 300, Training Loss 0.41225902217885724\n",
      "2022-03-26 21:42:37.919128 Epoch 300, Training Loss 0.41314544751668525\n",
      "2022-03-26 21:42:37.933846 Epoch 300, Training Loss 0.41345240713080483\n",
      "2022-03-26 21:42:37.947849 Epoch 300, Training Loss 0.41399689011104274\n",
      "2022-03-26 21:42:37.962815 Epoch 300, Training Loss 0.41466524194725946\n",
      "2022-03-26 21:42:37.976820 Epoch 300, Training Loss 0.4155215345456472\n",
      "2022-03-26 21:42:37.991784 Epoch 300, Training Loss 0.4162350551932669\n",
      "2022-03-26 21:42:38.005754 Epoch 300, Training Loss 0.41691134420349774\n",
      "2022-03-26 21:42:38.019831 Epoch 300, Training Loss 0.41758355867984653\n",
      "2022-03-26 21:42:38.034812 Epoch 300, Training Loss 0.4182034587616201\n",
      "2022-03-26 21:42:38.048816 Epoch 300, Training Loss 0.4189615615493501\n",
      "2022-03-26 21:42:38.063042 Epoch 300, Training Loss 0.41961513067145484\n",
      "2022-03-26 21:42:38.078067 Epoch 300, Training Loss 0.42014347264528884\n",
      "2022-03-26 21:42:38.092064 Epoch 300, Training Loss 0.42094698319654633\n",
      "2022-03-26 21:42:38.105514 Epoch 300, Training Loss 0.4216834466780543\n",
      "2022-03-26 21:42:38.119403 Epoch 300, Training Loss 0.4223945033367333\n",
      "2022-03-26 21:42:38.134325 Epoch 300, Training Loss 0.4232349933108405\n",
      "2022-03-26 21:42:38.148317 Epoch 300, Training Loss 0.4238241241334954\n",
      "2022-03-26 21:42:38.163302 Epoch 300, Training Loss 0.4243406269251538\n",
      "2022-03-26 21:42:38.177324 Epoch 300, Training Loss 0.42506282622247094\n",
      "2022-03-26 21:42:38.195206 Epoch 300, Training Loss 0.42596762229109664\n",
      "2022-03-26 21:42:38.214178 Epoch 300, Training Loss 0.4265381114943253\n",
      "2022-03-26 21:42:38.234016 Epoch 300, Training Loss 0.4272639167964306\n",
      "2022-03-26 21:42:38.253204 Epoch 300, Training Loss 0.42786867348739255\n",
      "2022-03-26 21:42:38.272921 Epoch 300, Training Loss 0.42860661001156664\n",
      "2022-03-26 21:42:38.291953 Epoch 300, Training Loss 0.42931855205074904\n",
      "2022-03-26 21:42:38.311373 Epoch 300, Training Loss 0.4297800920046199\n",
      "2022-03-26 21:42:38.331839 Epoch 300, Training Loss 0.43056119296251966\n",
      "2022-03-26 21:42:38.350341 Epoch 300, Training Loss 0.43121124205686856\n",
      "2022-03-26 21:42:38.369292 Epoch 300, Training Loss 0.43196810907719996\n",
      "2022-03-26 21:42:38.389069 Epoch 300, Training Loss 0.4326357669994959\n",
      "2022-03-26 21:42:38.408044 Epoch 300, Training Loss 0.4331851590167531\n",
      "2022-03-26 21:42:38.427080 Epoch 300, Training Loss 0.43390383066423716\n",
      "2022-03-26 21:42:38.446009 Epoch 300, Training Loss 0.4346185831157753\n",
      "2022-03-26 21:42:38.464894 Epoch 300, Training Loss 0.4354736699777491\n",
      "2022-03-26 21:42:38.483898 Epoch 300, Training Loss 0.43604955187691447\n",
      "2022-03-26 21:42:38.503932 Epoch 300, Training Loss 0.43662940628845676\n",
      "2022-03-26 21:42:38.522192 Epoch 300, Training Loss 0.43752697571311766\n",
      "2022-03-26 21:42:38.541076 Epoch 300, Training Loss 0.43826062840116603\n",
      "2022-03-26 21:42:38.560089 Epoch 300, Training Loss 0.4387559044315382\n",
      "2022-03-26 21:42:38.579058 Epoch 300, Training Loss 0.43980736687512656\n",
      "2022-03-26 21:42:38.597082 Epoch 300, Training Loss 0.4406363771623358\n",
      "2022-03-26 21:42:38.618086 Epoch 300, Training Loss 0.44108488515514854\n",
      "2022-03-26 21:42:38.636284 Epoch 300, Training Loss 0.4420536882279779\n",
      "2022-03-26 21:42:38.656369 Epoch 300, Training Loss 0.44270342420738984\n",
      "2022-03-26 21:42:38.675496 Epoch 300, Training Loss 0.44341502035670266\n",
      "2022-03-26 21:42:38.693468 Epoch 300, Training Loss 0.4442315042171332\n",
      "2022-03-26 21:42:38.713170 Epoch 300, Training Loss 0.4451051483221371\n",
      "2022-03-26 21:42:38.732108 Epoch 300, Training Loss 0.4457153064744247\n",
      "2022-03-26 21:42:38.750717 Epoch 300, Training Loss 0.44684121954014233\n",
      "2022-03-26 21:42:38.770095 Epoch 300, Training Loss 0.4475206476267037\n",
      "2022-03-26 21:42:38.790025 Epoch 300, Training Loss 0.448177609030548\n",
      "2022-03-26 21:42:38.808250 Epoch 300, Training Loss 0.4489075719471783\n",
      "2022-03-26 21:42:38.828168 Epoch 300, Training Loss 0.4498171540706054\n",
      "2022-03-26 21:42:38.847063 Epoch 300, Training Loss 0.4506339980741901\n",
      "2022-03-26 21:42:38.867063 Epoch 300, Training Loss 0.45147038188279437\n",
      "2022-03-26 21:42:38.885077 Epoch 300, Training Loss 0.4522253328653248\n",
      "2022-03-26 21:42:38.904594 Epoch 300, Training Loss 0.4531863563124786\n",
      "2022-03-26 21:42:38.923659 Epoch 300, Training Loss 0.4538456759108302\n",
      "2022-03-26 21:42:38.943101 Epoch 300, Training Loss 0.4547278190131687\n",
      "2022-03-26 21:42:38.961792 Epoch 300, Training Loss 0.45545435008947804\n",
      "2022-03-26 21:42:38.980974 Epoch 300, Training Loss 0.45601768025656797\n",
      "2022-03-26 21:42:39.000959 Epoch 300, Training Loss 0.4566166169198273\n",
      "2022-03-26 21:42:39.019947 Epoch 300, Training Loss 0.45746997661907657\n",
      "2022-03-26 21:42:39.038940 Epoch 300, Training Loss 0.45823276774657656\n",
      "2022-03-26 21:42:39.058171 Epoch 300, Training Loss 0.45872019097933076\n",
      "2022-03-26 21:42:39.076455 Epoch 300, Training Loss 0.4592632174186999\n",
      "2022-03-26 21:42:39.096627 Epoch 300, Training Loss 0.4601295889948335\n",
      "2022-03-26 21:42:39.115608 Epoch 300, Training Loss 0.46077548946870867\n",
      "2022-03-26 21:42:39.134481 Epoch 300, Training Loss 0.4613440164062373\n",
      "2022-03-26 21:42:39.154237 Epoch 300, Training Loss 0.46218189360845424\n",
      "2022-03-26 21:42:39.173645 Epoch 300, Training Loss 0.46288191342292845\n",
      "2022-03-26 21:42:39.192100 Epoch 300, Training Loss 0.4637005476238173\n",
      "2022-03-26 21:42:39.211006 Epoch 300, Training Loss 0.46473366456568393\n",
      "2022-03-26 21:42:39.230029 Epoch 300, Training Loss 0.46533250400934684\n",
      "2022-03-26 21:42:39.250083 Epoch 300, Training Loss 0.46615116138135076\n",
      "2022-03-26 21:42:39.269225 Epoch 300, Training Loss 0.46659213243543035\n",
      "2022-03-26 21:42:39.289071 Epoch 300, Training Loss 0.4674073465339973\n",
      "2022-03-26 21:42:39.308189 Epoch 300, Training Loss 0.4680177758797965\n",
      "2022-03-26 21:42:39.327279 Epoch 300, Training Loss 0.46874671949602453\n",
      "2022-03-26 21:42:39.346076 Epoch 300, Training Loss 0.46929973527751007\n",
      "2022-03-26 21:42:39.366091 Epoch 300, Training Loss 0.4699325981499899\n",
      "2022-03-26 21:42:39.386065 Epoch 300, Training Loss 0.4705686533009\n",
      "2022-03-26 21:42:39.405067 Epoch 300, Training Loss 0.47135912514556094\n",
      "2022-03-26 21:42:39.424096 Epoch 300, Training Loss 0.4720282485646665\n",
      "2022-03-26 21:42:39.443210 Epoch 300, Training Loss 0.472497455108806\n",
      "2022-03-26 21:42:39.462982 Epoch 300, Training Loss 0.47298390324920647\n",
      "2022-03-26 21:42:39.482889 Epoch 300, Training Loss 0.4737608168664796\n",
      "2022-03-26 21:42:39.502935 Epoch 300, Training Loss 0.4744869935924135\n",
      "2022-03-26 21:42:39.521271 Epoch 300, Training Loss 0.4751545801720656\n",
      "2022-03-26 21:42:39.540441 Epoch 300, Training Loss 0.4759742098924754\n",
      "2022-03-26 21:42:39.559331 Epoch 300, Training Loss 0.47638483715179325\n",
      "2022-03-26 21:42:39.578864 Epoch 300, Training Loss 0.47716032970896766\n",
      "2022-03-26 21:42:39.597653 Epoch 300, Training Loss 0.47787034298147996\n",
      "2022-03-26 21:42:39.617854 Epoch 300, Training Loss 0.4784654096111922\n",
      "2022-03-26 21:42:39.637276 Epoch 300, Training Loss 0.4792832465427916\n",
      "2022-03-26 21:42:39.655835 Epoch 300, Training Loss 0.4798617984937585\n",
      "2022-03-26 21:42:39.675782 Epoch 300, Training Loss 0.4805470660062092\n",
      "2022-03-26 21:42:39.694723 Epoch 300, Training Loss 0.48114820522115664\n",
      "2022-03-26 21:42:39.713747 Epoch 300, Training Loss 0.48186418612289916\n",
      "2022-03-26 21:42:39.732716 Epoch 300, Training Loss 0.4827309038937854\n",
      "2022-03-26 21:42:39.751728 Epoch 300, Training Loss 0.483495576226193\n",
      "2022-03-26 21:42:39.772080 Epoch 300, Training Loss 0.4843101495367182\n",
      "2022-03-26 21:42:39.790956 Epoch 300, Training Loss 0.4849551108182239\n",
      "2022-03-26 21:42:39.811075 Epoch 300, Training Loss 0.485596704361079\n",
      "2022-03-26 21:42:39.829950 Epoch 300, Training Loss 0.4862751628431823\n",
      "2022-03-26 21:42:39.848322 Epoch 300, Training Loss 0.48693017398609834\n",
      "2022-03-26 21:42:39.868169 Epoch 300, Training Loss 0.48775911026293667\n",
      "2022-03-26 21:42:39.887244 Epoch 300, Training Loss 0.48853086266676177\n",
      "2022-03-26 21:42:39.906419 Epoch 300, Training Loss 0.48911326532930977\n",
      "2022-03-26 21:42:39.926241 Epoch 300, Training Loss 0.4898878289839191\n",
      "2022-03-26 21:42:39.945009 Epoch 300, Training Loss 0.49043071860700005\n",
      "2022-03-26 21:42:39.964708 Epoch 300, Training Loss 0.4910299830195849\n",
      "2022-03-26 21:42:39.982740 Epoch 300, Training Loss 0.4916118186757997\n",
      "2022-03-26 21:42:40.002774 Epoch 300, Training Loss 0.4922362929186248\n",
      "2022-03-26 21:42:40.022714 Epoch 300, Training Loss 0.4929966359873257\n",
      "2022-03-26 21:42:40.041725 Epoch 300, Training Loss 0.49364746321954994\n",
      "2022-03-26 21:42:40.061044 Epoch 300, Training Loss 0.4944311790835217\n",
      "2022-03-26 21:42:40.080059 Epoch 300, Training Loss 0.49528149376287484\n",
      "2022-03-26 21:42:40.099074 Epoch 300, Training Loss 0.49594765928242823\n",
      "2022-03-26 21:42:40.118088 Epoch 300, Training Loss 0.4965640192141618\n",
      "2022-03-26 21:42:40.137134 Epoch 300, Training Loss 0.49720791309995727\n",
      "2022-03-26 21:42:40.156164 Epoch 300, Training Loss 0.49792789513497704\n",
      "2022-03-26 21:42:40.174590 Epoch 300, Training Loss 0.4988824773170149\n",
      "2022-03-26 21:42:40.193689 Epoch 300, Training Loss 0.4995377995931279\n",
      "2022-03-26 21:42:40.212684 Epoch 300, Training Loss 0.5001605534187669\n",
      "2022-03-26 21:42:40.231767 Epoch 300, Training Loss 0.500665661166696\n",
      "2022-03-26 21:42:40.251093 Epoch 300, Training Loss 0.5014528908083201\n",
      "2022-03-26 21:42:40.271328 Epoch 300, Training Loss 0.5022481732508716\n",
      "2022-03-26 21:42:40.291224 Epoch 300, Training Loss 0.5028821447926104\n",
      "2022-03-26 21:42:40.309977 Epoch 300, Training Loss 0.5034410370432812\n",
      "2022-03-26 21:42:40.328986 Epoch 300, Training Loss 0.5040616303148782\n",
      "2022-03-26 21:42:40.348189 Epoch 300, Training Loss 0.504771660203519\n",
      "2022-03-26 21:42:40.367161 Epoch 300, Training Loss 0.5054202681917059\n",
      "2022-03-26 21:42:40.387137 Epoch 300, Training Loss 0.5061965628200785\n",
      "2022-03-26 21:42:40.406050 Epoch 300, Training Loss 0.5066795437918294\n",
      "2022-03-26 21:42:40.426075 Epoch 300, Training Loss 0.507560366826594\n",
      "2022-03-26 21:42:40.445066 Epoch 300, Training Loss 0.5082610799452228\n",
      "2022-03-26 21:42:40.463940 Epoch 300, Training Loss 0.508889604979159\n",
      "2022-03-26 21:42:40.482843 Epoch 300, Training Loss 0.5098203028483159\n",
      "2022-03-26 21:42:40.504068 Epoch 300, Training Loss 0.5103545805529865\n",
      "2022-03-26 21:42:40.522932 Epoch 300, Training Loss 0.5110663459886371\n",
      "2022-03-26 21:42:40.542218 Epoch 300, Training Loss 0.511451120076277\n",
      "2022-03-26 21:42:40.560604 Epoch 300, Training Loss 0.5119546570283983\n",
      "2022-03-26 21:42:40.580707 Epoch 300, Training Loss 0.5125469633228029\n",
      "2022-03-26 21:42:40.599738 Epoch 300, Training Loss 0.5131123458104365\n",
      "2022-03-26 21:42:40.618950 Epoch 300, Training Loss 0.5137210883143003\n",
      "2022-03-26 21:42:40.638391 Epoch 300, Training Loss 0.5141481580331807\n",
      "2022-03-26 21:42:40.657449 Epoch 300, Training Loss 0.5148424926926108\n",
      "2022-03-26 21:42:40.676349 Epoch 300, Training Loss 0.5153113818153396\n",
      "2022-03-26 21:42:40.697232 Epoch 300, Training Loss 0.516045033817401\n",
      "2022-03-26 21:42:40.716188 Epoch 300, Training Loss 0.5167697411592659\n",
      "2022-03-26 21:42:40.734855 Epoch 300, Training Loss 0.5176244197065568\n",
      "2022-03-26 21:42:40.753689 Epoch 300, Training Loss 0.518370867880714\n",
      "2022-03-26 21:42:40.772704 Epoch 300, Training Loss 0.5189440674565332\n",
      "2022-03-26 21:42:40.791705 Epoch 300, Training Loss 0.5194135943184728\n",
      "2022-03-26 21:42:40.810803 Epoch 300, Training Loss 0.5200114078305261\n",
      "2022-03-26 21:42:40.830720 Epoch 300, Training Loss 0.5205135102314717\n",
      "2022-03-26 21:42:40.849850 Epoch 300, Training Loss 0.5213546750643064\n",
      "2022-03-26 21:42:40.869310 Epoch 300, Training Loss 0.5221163294351924\n",
      "2022-03-26 21:42:40.888180 Epoch 300, Training Loss 0.5230432554431583\n",
      "2022-03-26 21:42:40.907431 Epoch 300, Training Loss 0.5238746599773006\n",
      "2022-03-26 21:42:40.927284 Epoch 300, Training Loss 0.5248152269884143\n",
      "2022-03-26 21:42:40.946238 Epoch 300, Training Loss 0.5253184739204929\n",
      "2022-03-26 21:42:40.966153 Epoch 300, Training Loss 0.5260510015137055\n",
      "2022-03-26 21:42:40.986010 Epoch 300, Training Loss 0.5269705889856114\n",
      "2022-03-26 21:42:41.005985 Epoch 300, Training Loss 0.52778335411073\n",
      "2022-03-26 21:42:41.023940 Epoch 300, Training Loss 0.5287516090037573\n",
      "2022-03-26 21:42:41.043442 Epoch 300, Training Loss 0.5293571857921303\n",
      "2022-03-26 21:42:41.062536 Epoch 300, Training Loss 0.5305497619273413\n",
      "2022-03-26 21:42:41.082082 Epoch 300, Training Loss 0.5313194328942872\n",
      "2022-03-26 21:42:41.102933 Epoch 300, Training Loss 0.5321378121366891\n",
      "2022-03-26 21:42:41.110936 Epoch 300, Training Loss 0.5324227448810092\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "N5eQDSSFayUo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Accuracy val: 0.61\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht3gDSXya60S"
   },
   "source": [
    "Problem 2 Part 2 Subsection A: Using Batch Normilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5FqJ82mFa4Dz"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, n_chans):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "    self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = torch.relu(out)\n",
    "    return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "22SUl8vIbdQE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:47:20.061424 Epoch 1, Training Loss 0.0029193165966921756\n",
      "2022-03-26 21:47:20.091432 Epoch 1, Training Loss 0.005855395970746989\n",
      "2022-03-26 21:47:20.122439 Epoch 1, Training Loss 0.008823782586685532\n",
      "2022-03-26 21:47:20.152445 Epoch 1, Training Loss 0.0117649425326101\n",
      "2022-03-26 21:47:20.184452 Epoch 1, Training Loss 0.014716754788937777\n",
      "2022-03-26 21:47:20.215459 Epoch 1, Training Loss 0.01767643729744055\n",
      "2022-03-26 21:47:20.246468 Epoch 1, Training Loss 0.020637740564468265\n",
      "2022-03-26 21:47:20.277476 Epoch 1, Training Loss 0.02355118755184476\n",
      "2022-03-26 21:47:20.304499 Epoch 1, Training Loss 0.026497203980565376\n",
      "2022-03-26 21:47:20.330506 Epoch 1, Training Loss 0.02941852396406481\n",
      "2022-03-26 21:47:20.357511 Epoch 1, Training Loss 0.032361017468640264\n",
      "2022-03-26 21:47:20.384499 Epoch 1, Training Loss 0.03529807978578846\n",
      "2022-03-26 21:47:20.411524 Epoch 1, Training Loss 0.038255017431800625\n",
      "2022-03-26 21:47:20.437524 Epoch 1, Training Loss 0.041236679877161675\n",
      "2022-03-26 21:47:20.463535 Epoch 1, Training Loss 0.044171040930101636\n",
      "2022-03-26 21:47:20.490542 Epoch 1, Training Loss 0.04712613891152775\n",
      "2022-03-26 21:47:20.517729 Epoch 1, Training Loss 0.05003998163715958\n",
      "2022-03-26 21:47:20.543554 Epoch 1, Training Loss 0.05296492576599121\n",
      "2022-03-26 21:47:20.569547 Epoch 1, Training Loss 0.05591063517743669\n",
      "2022-03-26 21:47:20.596560 Epoch 1, Training Loss 0.058867058485670164\n",
      "2022-03-26 21:47:20.622565 Epoch 1, Training Loss 0.061801065264455494\n",
      "2022-03-26 21:47:20.648571 Epoch 1, Training Loss 0.0647579172383184\n",
      "2022-03-26 21:47:20.674583 Epoch 1, Training Loss 0.06772553768304303\n",
      "2022-03-26 21:47:20.701591 Epoch 1, Training Loss 0.0706309055733254\n",
      "2022-03-26 21:47:20.727584 Epoch 1, Training Loss 0.07354203514430833\n",
      "2022-03-26 21:47:20.753589 Epoch 1, Training Loss 0.07648641587523243\n",
      "2022-03-26 21:47:20.779589 Epoch 1, Training Loss 0.07942428460816288\n",
      "2022-03-26 21:47:20.805613 Epoch 1, Training Loss 0.08236383964948336\n",
      "2022-03-26 21:47:20.830613 Epoch 1, Training Loss 0.08528157756151751\n",
      "2022-03-26 21:47:20.856663 Epoch 1, Training Loss 0.08821235318927814\n",
      "2022-03-26 21:47:20.882619 Epoch 1, Training Loss 0.09111521219658425\n",
      "2022-03-26 21:47:20.903623 Epoch 1, Training Loss 0.09403701084654045\n",
      "2022-03-26 21:47:20.921627 Epoch 1, Training Loss 0.09695131486029271\n",
      "2022-03-26 21:47:20.939640 Epoch 1, Training Loss 0.09989332695446355\n",
      "2022-03-26 21:47:20.957642 Epoch 1, Training Loss 0.10284112969322888\n",
      "2022-03-26 21:47:20.975639 Epoch 1, Training Loss 0.10578216128337109\n",
      "2022-03-26 21:47:20.993643 Epoch 1, Training Loss 0.10869523570360735\n",
      "2022-03-26 21:47:21.011648 Epoch 1, Training Loss 0.11160955130291716\n",
      "2022-03-26 21:47:21.030664 Epoch 1, Training Loss 0.11453101580100292\n",
      "2022-03-26 21:47:21.048656 Epoch 1, Training Loss 0.11746838879402337\n",
      "2022-03-26 21:47:21.066672 Epoch 1, Training Loss 0.12041027314217805\n",
      "2022-03-26 21:47:21.084676 Epoch 1, Training Loss 0.12329250132031452\n",
      "2022-03-26 21:47:21.102668 Epoch 1, Training Loss 0.12621774484434395\n",
      "2022-03-26 21:47:21.120666 Epoch 1, Training Loss 0.1291270981664243\n",
      "2022-03-26 21:47:21.138676 Epoch 1, Training Loss 0.1320944344601058\n",
      "2022-03-26 21:47:21.156693 Epoch 1, Training Loss 0.13503375352191194\n",
      "2022-03-26 21:47:21.173940 Epoch 1, Training Loss 0.13796155928345896\n",
      "2022-03-26 21:47:21.192138 Epoch 1, Training Loss 0.14088166034435068\n",
      "2022-03-26 21:47:21.210149 Epoch 1, Training Loss 0.14376928129464464\n",
      "2022-03-26 21:47:21.228146 Epoch 1, Training Loss 0.1466538488407574\n",
      "2022-03-26 21:47:21.247142 Epoch 1, Training Loss 0.1495738608757858\n",
      "2022-03-26 21:47:21.265160 Epoch 1, Training Loss 0.15246516389919973\n",
      "2022-03-26 21:47:21.283152 Epoch 1, Training Loss 0.15536731557772898\n",
      "2022-03-26 21:47:21.301162 Epoch 1, Training Loss 0.15825405602564896\n",
      "2022-03-26 21:47:21.319167 Epoch 1, Training Loss 0.161166950259977\n",
      "2022-03-26 21:47:21.338172 Epoch 1, Training Loss 0.1641276690661145\n",
      "2022-03-26 21:47:21.356161 Epoch 1, Training Loss 0.16705752577623137\n",
      "2022-03-26 21:47:21.374173 Epoch 1, Training Loss 0.16997756616538748\n",
      "2022-03-26 21:47:21.391189 Epoch 1, Training Loss 0.1728741403126046\n",
      "2022-03-26 21:47:21.409181 Epoch 1, Training Loss 0.1757791871609895\n",
      "2022-03-26 21:47:21.428197 Epoch 1, Training Loss 0.17866093027012428\n",
      "2022-03-26 21:47:21.447196 Epoch 1, Training Loss 0.18156371336154012\n",
      "2022-03-26 21:47:21.465193 Epoch 1, Training Loss 0.18448668428699075\n",
      "2022-03-26 21:47:21.483195 Epoch 1, Training Loss 0.1874276902669531\n",
      "2022-03-26 21:47:21.502209 Epoch 1, Training Loss 0.19031190323402813\n",
      "2022-03-26 21:47:21.520213 Epoch 1, Training Loss 0.19322214620497524\n",
      "2022-03-26 21:47:21.538216 Epoch 1, Training Loss 0.1961083832909079\n",
      "2022-03-26 21:47:21.556220 Epoch 1, Training Loss 0.19901630244291652\n",
      "2022-03-26 21:47:21.574221 Epoch 1, Training Loss 0.20188361116687356\n",
      "2022-03-26 21:47:21.593216 Epoch 1, Training Loss 0.2047639429721686\n",
      "2022-03-26 21:47:21.612233 Epoch 1, Training Loss 0.2076778988094281\n",
      "2022-03-26 21:47:21.630243 Epoch 1, Training Loss 0.2106073527689785\n",
      "2022-03-26 21:47:21.648248 Epoch 1, Training Loss 0.21352590014562584\n",
      "2022-03-26 21:47:21.666252 Epoch 1, Training Loss 0.2164008858258767\n",
      "2022-03-26 21:47:21.684236 Epoch 1, Training Loss 0.2192626871416331\n",
      "2022-03-26 21:47:21.702240 Epoch 1, Training Loss 0.2221332431754188\n",
      "2022-03-26 21:47:21.720251 Epoch 1, Training Loss 0.22498490743320007\n",
      "2022-03-26 21:47:21.738262 Epoch 1, Training Loss 0.22784530811602502\n",
      "2022-03-26 21:47:21.756254 Epoch 1, Training Loss 0.23071487206022454\n",
      "2022-03-26 21:47:21.775276 Epoch 1, Training Loss 0.23359495843462932\n",
      "2022-03-26 21:47:21.793280 Epoch 1, Training Loss 0.2364462574424646\n",
      "2022-03-26 21:47:21.811284 Epoch 1, Training Loss 0.23933214940073544\n",
      "2022-03-26 21:47:21.829289 Epoch 1, Training Loss 0.24224366281953308\n",
      "2022-03-26 21:47:21.847280 Epoch 1, Training Loss 0.24514983925977937\n",
      "2022-03-26 21:47:21.866291 Epoch 1, Training Loss 0.24803643366869757\n",
      "2022-03-26 21:47:21.883284 Epoch 1, Training Loss 0.2509001941632127\n",
      "2022-03-26 21:47:21.902305 Epoch 1, Training Loss 0.25369571084561554\n",
      "2022-03-26 21:47:21.920303 Epoch 1, Training Loss 0.25659727044117725\n",
      "2022-03-26 21:47:21.938313 Epoch 1, Training Loss 0.2595019938086\n",
      "2022-03-26 21:47:21.956311 Epoch 1, Training Loss 0.2623390965449536\n",
      "2022-03-26 21:47:21.975321 Epoch 1, Training Loss 0.2652443019325471\n",
      "2022-03-26 21:47:21.994320 Epoch 1, Training Loss 0.26815344854389006\n",
      "2022-03-26 21:47:22.012324 Epoch 1, Training Loss 0.27103887586032643\n",
      "2022-03-26 21:47:22.030334 Epoch 1, Training Loss 0.27388401531502415\n",
      "2022-03-26 21:47:22.048338 Epoch 1, Training Loss 0.27674912735629265\n",
      "2022-03-26 21:47:22.066342 Epoch 1, Training Loss 0.27962133463691263\n",
      "2022-03-26 21:47:22.085334 Epoch 1, Training Loss 0.2824741626334617\n",
      "2022-03-26 21:47:22.103338 Epoch 1, Training Loss 0.28534326254559295\n",
      "2022-03-26 21:47:22.121348 Epoch 1, Training Loss 0.2882106264533899\n",
      "2022-03-26 21:47:22.139352 Epoch 1, Training Loss 0.29104201598545476\n",
      "2022-03-26 21:47:22.157342 Epoch 1, Training Loss 0.29388684323986475\n",
      "2022-03-26 21:47:22.175361 Epoch 1, Training Loss 0.296721238004582\n",
      "2022-03-26 21:47:22.194365 Epoch 1, Training Loss 0.2995661341625711\n",
      "2022-03-26 21:47:22.212375 Epoch 1, Training Loss 0.3023962803813807\n",
      "2022-03-26 21:47:22.231380 Epoch 1, Training Loss 0.305224318333599\n",
      "2022-03-26 21:47:22.248365 Epoch 1, Training Loss 0.3080535703302954\n",
      "2022-03-26 21:47:22.266368 Epoch 1, Training Loss 0.31091840614748123\n",
      "2022-03-26 21:47:22.284373 Epoch 1, Training Loss 0.3137816788290468\n",
      "2022-03-26 21:47:22.302376 Epoch 1, Training Loss 0.3166004995555829\n",
      "2022-03-26 21:47:22.320394 Epoch 1, Training Loss 0.3194251142804275\n",
      "2022-03-26 21:47:22.339398 Epoch 1, Training Loss 0.32224088587114574\n",
      "2022-03-26 21:47:22.357398 Epoch 1, Training Loss 0.3250979932067949\n",
      "2022-03-26 21:47:22.375400 Epoch 1, Training Loss 0.32796351531582413\n",
      "2022-03-26 21:47:22.392410 Epoch 1, Training Loss 0.3307899892177728\n",
      "2022-03-26 21:47:22.411414 Epoch 1, Training Loss 0.33361351337579204\n",
      "2022-03-26 21:47:22.430425 Epoch 1, Training Loss 0.3364707728481049\n",
      "2022-03-26 21:47:22.448429 Epoch 1, Training Loss 0.3392686410938078\n",
      "2022-03-26 21:47:22.467433 Epoch 1, Training Loss 0.342103978252167\n",
      "2022-03-26 21:47:22.485432 Epoch 1, Training Loss 0.34491665009647376\n",
      "2022-03-26 21:47:22.503435 Epoch 1, Training Loss 0.3477541927791313\n",
      "2022-03-26 21:47:22.522446 Epoch 1, Training Loss 0.350599094425016\n",
      "2022-03-26 21:47:22.539443 Epoch 1, Training Loss 0.35341571206631867\n",
      "2022-03-26 21:47:22.557447 Epoch 1, Training Loss 0.3562099076902775\n",
      "2022-03-26 21:47:22.575457 Epoch 1, Training Loss 0.35899909530454277\n",
      "2022-03-26 21:47:22.593456 Epoch 1, Training Loss 0.36181914379529634\n",
      "2022-03-26 21:47:22.611460 Epoch 1, Training Loss 0.36461634221284284\n",
      "2022-03-26 21:47:22.629470 Epoch 1, Training Loss 0.3673934540175416\n",
      "2022-03-26 21:47:22.647474 Epoch 1, Training Loss 0.3702443527138751\n",
      "2022-03-26 21:47:22.665465 Epoch 1, Training Loss 0.37305497970727397\n",
      "2022-03-26 21:47:22.683482 Epoch 1, Training Loss 0.3758594687942349\n",
      "2022-03-26 21:47:22.701486 Epoch 1, Training Loss 0.378674907452615\n",
      "2022-03-26 21:47:22.719484 Epoch 1, Training Loss 0.38146072305986645\n",
      "2022-03-26 21:47:22.737488 Epoch 1, Training Loss 0.3843231710326641\n",
      "2022-03-26 21:47:22.755492 Epoch 1, Training Loss 0.3870661158086089\n",
      "2022-03-26 21:47:22.773502 Epoch 1, Training Loss 0.3897868153994041\n",
      "2022-03-26 21:47:22.791507 Epoch 1, Training Loss 0.3926023851575144\n",
      "2022-03-26 21:47:22.809511 Epoch 1, Training Loss 0.39535359043599394\n",
      "2022-03-26 21:47:22.827515 Epoch 1, Training Loss 0.3981088817576923\n",
      "2022-03-26 21:47:22.846513 Epoch 1, Training Loss 0.40093017813494747\n",
      "2022-03-26 21:47:22.864523 Epoch 1, Training Loss 0.40368048705713216\n",
      "2022-03-26 21:47:22.882527 Epoch 1, Training Loss 0.40642406355084665\n",
      "2022-03-26 21:47:22.900686 Epoch 1, Training Loss 0.40916865713456096\n",
      "2022-03-26 21:47:22.918535 Epoch 1, Training Loss 0.41195443859490594\n",
      "2022-03-26 21:47:22.936533 Epoch 1, Training Loss 0.41472295544031634\n",
      "2022-03-26 21:47:22.955544 Epoch 1, Training Loss 0.41748568834856037\n",
      "2022-03-26 21:47:22.972545 Epoch 1, Training Loss 0.42029848489005245\n",
      "2022-03-26 21:47:22.991532 Epoch 1, Training Loss 0.42308958534084623\n",
      "2022-03-26 21:47:23.009550 Epoch 1, Training Loss 0.42581670729400556\n",
      "2022-03-26 21:47:23.027560 Epoch 1, Training Loss 0.42859268828731056\n",
      "2022-03-26 21:47:23.046564 Epoch 1, Training Loss 0.43140799858990836\n",
      "2022-03-26 21:47:23.064551 Epoch 1, Training Loss 0.4342404829571619\n",
      "2022-03-26 21:47:23.082552 Epoch 1, Training Loss 0.43713520402493683\n",
      "2022-03-26 21:47:23.100557 Epoch 1, Training Loss 0.4398791933303599\n",
      "2022-03-26 21:47:23.118561 Epoch 1, Training Loss 0.44263938046477336\n",
      "2022-03-26 21:47:23.136565 Epoch 1, Training Loss 0.44538367282399133\n",
      "2022-03-26 21:47:23.153569 Epoch 1, Training Loss 0.4482754987219106\n",
      "2022-03-26 21:47:23.172573 Epoch 1, Training Loss 0.4510531931582009\n",
      "2022-03-26 21:47:23.190577 Epoch 1, Training Loss 0.45378160293754716\n",
      "2022-03-26 21:47:23.208601 Epoch 1, Training Loss 0.4565695905319565\n",
      "2022-03-26 21:47:23.226605 Epoch 1, Training Loss 0.45927654843196236\n",
      "2022-03-26 21:47:23.245597 Epoch 1, Training Loss 0.4620468186600434\n",
      "2022-03-26 21:47:23.263594 Epoch 1, Training Loss 0.4647751488649022\n",
      "2022-03-26 21:47:23.282618 Epoch 1, Training Loss 0.4675588757180802\n",
      "2022-03-26 21:47:23.300622 Epoch 1, Training Loss 0.47031940462644145\n",
      "2022-03-26 21:47:23.318626 Epoch 1, Training Loss 0.4731538317087666\n",
      "2022-03-26 21:47:23.336611 Epoch 1, Training Loss 0.4759433351819168\n",
      "2022-03-26 21:47:23.354622 Epoch 1, Training Loss 0.47876553858637505\n",
      "2022-03-26 21:47:23.372632 Epoch 1, Training Loss 0.4815509413819179\n",
      "2022-03-26 21:47:23.391626 Epoch 1, Training Loss 0.4843343145707074\n",
      "2022-03-26 21:47:23.409627 Epoch 1, Training Loss 0.4870277445029725\n",
      "2022-03-26 21:47:23.429632 Epoch 1, Training Loss 0.4897990168817818\n",
      "2022-03-26 21:47:23.448638 Epoch 1, Training Loss 0.4927238384476098\n",
      "2022-03-26 21:47:23.467644 Epoch 1, Training Loss 0.49543342017151815\n",
      "2022-03-26 21:47:23.485652 Epoch 1, Training Loss 0.4981886800902579\n",
      "2022-03-26 21:47:23.504650 Epoch 1, Training Loss 0.5009217759227509\n",
      "2022-03-26 21:47:23.522653 Epoch 1, Training Loss 0.503672086064468\n",
      "2022-03-26 21:47:23.541658 Epoch 1, Training Loss 0.5063810220459843\n",
      "2022-03-26 21:47:23.559661 Epoch 1, Training Loss 0.5092191016277694\n",
      "2022-03-26 21:47:23.578665 Epoch 1, Training Loss 0.5119831028496823\n",
      "2022-03-26 21:47:23.597669 Epoch 1, Training Loss 0.5148038885477558\n",
      "2022-03-26 21:47:23.616674 Epoch 1, Training Loss 0.5175470555834758\n",
      "2022-03-26 21:47:23.634821 Epoch 1, Training Loss 0.5202117182714555\n",
      "2022-03-26 21:47:23.654817 Epoch 1, Training Loss 0.5228954593238928\n",
      "2022-03-26 21:47:23.672822 Epoch 1, Training Loss 0.5256961931657913\n",
      "2022-03-26 21:47:23.691826 Epoch 1, Training Loss 0.5284239648248229\n",
      "2022-03-26 21:47:23.710830 Epoch 1, Training Loss 0.5311708776542293\n",
      "2022-03-26 21:47:23.728834 Epoch 1, Training Loss 0.533946971759162\n",
      "2022-03-26 21:47:23.746838 Epoch 1, Training Loss 0.5367811110318469\n",
      "2022-03-26 21:47:23.765842 Epoch 1, Training Loss 0.5395516361421941\n",
      "2022-03-26 21:47:23.784847 Epoch 1, Training Loss 0.5423162087150242\n",
      "2022-03-26 21:47:23.802850 Epoch 1, Training Loss 0.54499588415141\n",
      "2022-03-26 21:47:23.822858 Epoch 1, Training Loss 0.5477342697055748\n",
      "2022-03-26 21:47:23.850864 Epoch 1, Training Loss 0.5505020039160843\n",
      "2022-03-26 21:47:23.870865 Epoch 1, Training Loss 0.5532239587105754\n",
      "2022-03-26 21:47:23.890870 Epoch 1, Training Loss 0.5559617241325281\n",
      "2022-03-26 21:47:23.910874 Epoch 1, Training Loss 0.5587841083326608\n",
      "2022-03-26 21:47:23.929887 Epoch 1, Training Loss 0.5613669199711832\n",
      "2022-03-26 21:47:23.947897 Epoch 1, Training Loss 0.5640698290237075\n",
      "2022-03-26 21:47:23.967897 Epoch 1, Training Loss 0.5667849242534784\n",
      "2022-03-26 21:47:23.986906 Epoch 1, Training Loss 0.5695374210167419\n",
      "2022-03-26 21:47:24.005910 Epoch 1, Training Loss 0.5722660247017356\n",
      "2022-03-26 21:47:24.023909 Epoch 1, Training Loss 0.5749971311720435\n",
      "2022-03-26 21:47:24.044919 Epoch 1, Training Loss 0.5779222027420083\n",
      "2022-03-26 21:47:24.062929 Epoch 1, Training Loss 0.5806409480321743\n",
      "2022-03-26 21:47:24.081927 Epoch 1, Training Loss 0.5834293618531483\n",
      "2022-03-26 21:47:24.099931 Epoch 1, Training Loss 0.5860941660068834\n",
      "2022-03-26 21:47:24.118937 Epoch 1, Training Loss 0.5888717714173105\n",
      "2022-03-26 21:47:24.141941 Epoch 1, Training Loss 0.5914762053648224\n",
      "2022-03-26 21:47:24.167953 Epoch 1, Training Loss 0.5942789053977908\n",
      "2022-03-26 21:47:24.193959 Epoch 1, Training Loss 0.5969312895289467\n",
      "2022-03-26 21:47:24.219966 Epoch 1, Training Loss 0.5996110978943613\n",
      "2022-03-26 21:47:24.244970 Epoch 1, Training Loss 0.6023468581002082\n",
      "2022-03-26 21:47:24.270976 Epoch 1, Training Loss 0.6050420252563399\n",
      "2022-03-26 21:47:24.297982 Epoch 1, Training Loss 0.6078026913620932\n",
      "2022-03-26 21:47:24.323988 Epoch 1, Training Loss 0.6105525249715351\n",
      "2022-03-26 21:47:24.342986 Epoch 1, Training Loss 0.613338283260765\n",
      "2022-03-26 21:47:24.361979 Epoch 1, Training Loss 0.6160606377569916\n",
      "2022-03-26 21:47:24.379994 Epoch 1, Training Loss 0.618763214182061\n",
      "2022-03-26 21:47:24.399005 Epoch 1, Training Loss 0.6214546800574379\n",
      "2022-03-26 21:47:24.417011 Epoch 1, Training Loss 0.6241660358960671\n",
      "2022-03-26 21:47:24.435013 Epoch 1, Training Loss 0.6269592284546484\n",
      "2022-03-26 21:47:24.453011 Epoch 1, Training Loss 0.6297125532803938\n",
      "2022-03-26 21:47:24.471015 Epoch 1, Training Loss 0.6323727491261709\n",
      "2022-03-26 21:47:24.489025 Epoch 1, Training Loss 0.6351393281346391\n",
      "2022-03-26 21:47:24.507023 Epoch 1, Training Loss 0.6378590624655605\n",
      "2022-03-26 21:47:24.525028 Epoch 1, Training Loss 0.640522201652722\n",
      "2022-03-26 21:47:24.543031 Epoch 1, Training Loss 0.6433149746921666\n",
      "2022-03-26 21:47:24.561036 Epoch 1, Training Loss 0.6460691208729659\n",
      "2022-03-26 21:47:24.579046 Epoch 1, Training Loss 0.6487912537191834\n",
      "2022-03-26 21:47:24.597044 Epoch 1, Training Loss 0.6514913794939475\n",
      "2022-03-26 21:47:24.615054 Epoch 1, Training Loss 0.6541711954814394\n",
      "2022-03-26 21:47:24.633058 Epoch 1, Training Loss 0.6568050408912132\n",
      "2022-03-26 21:47:24.651062 Epoch 1, Training Loss 0.6595525698893515\n",
      "2022-03-26 21:47:24.669060 Epoch 1, Training Loss 0.6623138146632163\n",
      "2022-03-26 21:47:24.686064 Epoch 1, Training Loss 0.6649859884510869\n",
      "2022-03-26 21:47:24.704074 Epoch 1, Training Loss 0.6677570111306427\n",
      "2022-03-26 21:47:24.723072 Epoch 1, Training Loss 0.6703446176655762\n",
      "2022-03-26 21:47:24.741083 Epoch 1, Training Loss 0.6731475192262694\n",
      "2022-03-26 21:47:24.759087 Epoch 1, Training Loss 0.6759442048304526\n",
      "2022-03-26 21:47:24.777084 Epoch 1, Training Loss 0.6785780115200736\n",
      "2022-03-26 21:47:24.795095 Epoch 1, Training Loss 0.6813505689811219\n",
      "2022-03-26 21:47:24.813099 Epoch 1, Training Loss 0.6840399930544216\n",
      "2022-03-26 21:47:24.831103 Epoch 1, Training Loss 0.6868018015571262\n",
      "2022-03-26 21:47:24.849112 Epoch 1, Training Loss 0.6895835722803765\n",
      "2022-03-26 21:47:24.868105 Epoch 1, Training Loss 0.6923243612279673\n",
      "2022-03-26 21:47:24.885109 Epoch 1, Training Loss 0.6950315727907068\n",
      "2022-03-26 21:47:24.904119 Epoch 1, Training Loss 0.6976237132421235\n",
      "2022-03-26 21:47:24.921117 Epoch 1, Training Loss 0.7003662799630324\n",
      "2022-03-26 21:47:24.940122 Epoch 1, Training Loss 0.7031320043841897\n",
      "2022-03-26 21:47:24.957125 Epoch 1, Training Loss 0.7058597470793273\n",
      "2022-03-26 21:47:24.976136 Epoch 1, Training Loss 0.708570732484998\n",
      "2022-03-26 21:47:24.993133 Epoch 1, Training Loss 0.7112037269660579\n",
      "2022-03-26 21:47:25.012144 Epoch 1, Training Loss 0.7138279277040526\n",
      "2022-03-26 21:47:25.030148 Epoch 1, Training Loss 0.71642851158786\n",
      "2022-03-26 21:47:25.048146 Epoch 1, Training Loss 0.7191233165428766\n",
      "2022-03-26 21:47:25.066150 Epoch 1, Training Loss 0.7217820666330245\n",
      "2022-03-26 21:47:25.084154 Epoch 1, Training Loss 0.7244311269286954\n",
      "2022-03-26 21:47:25.102158 Epoch 1, Training Loss 0.7271398943098609\n",
      "2022-03-26 21:47:25.120162 Epoch 1, Training Loss 0.7298527928569433\n",
      "2022-03-26 21:47:25.138173 Epoch 1, Training Loss 0.7325845478135912\n",
      "2022-03-26 21:47:25.157171 Epoch 1, Training Loss 0.7353187296396632\n",
      "2022-03-26 21:47:25.174181 Epoch 1, Training Loss 0.7381551631576265\n",
      "2022-03-26 21:47:25.193179 Epoch 1, Training Loss 0.7409007284037598\n",
      "2022-03-26 21:47:25.211189 Epoch 1, Training Loss 0.743561237669357\n",
      "2022-03-26 21:47:25.229193 Epoch 1, Training Loss 0.7462986674150238\n",
      "2022-03-26 21:47:25.248198 Epoch 1, Training Loss 0.7489793709172007\n",
      "2022-03-26 21:47:25.266202 Epoch 1, Training Loss 0.7516655379244129\n",
      "2022-03-26 21:47:25.284193 Epoch 1, Training Loss 0.754328939615918\n",
      "2022-03-26 21:47:25.302204 Epoch 1, Training Loss 0.757061311655947\n",
      "2022-03-26 21:47:25.321214 Epoch 1, Training Loss 0.7598308361399814\n",
      "2022-03-26 21:47:25.340218 Epoch 1, Training Loss 0.7625096005856838\n",
      "2022-03-26 21:47:25.359223 Epoch 1, Training Loss 0.7651603078598257\n",
      "2022-03-26 21:47:25.377227 Epoch 1, Training Loss 0.7677540108370964\n",
      "2022-03-26 21:47:25.395218 Epoch 1, Training Loss 0.7704263318835012\n",
      "2022-03-26 21:47:25.413235 Epoch 1, Training Loss 0.7730343277801943\n",
      "2022-03-26 21:47:25.431239 Epoch 1, Training Loss 0.7756522396946197\n",
      "2022-03-26 21:47:25.449243 Epoch 1, Training Loss 0.7783359017823358\n",
      "2022-03-26 21:47:25.467241 Epoch 1, Training Loss 0.7810103280465012\n",
      "2022-03-26 21:47:25.485252 Epoch 1, Training Loss 0.78366790738557\n",
      "2022-03-26 21:47:25.502249 Epoch 1, Training Loss 0.7863946976259236\n",
      "2022-03-26 21:47:25.521259 Epoch 1, Training Loss 0.7890722949791442\n",
      "2022-03-26 21:47:25.539263 Epoch 1, Training Loss 0.7916907471464113\n",
      "2022-03-26 21:47:25.558268 Epoch 1, Training Loss 0.7943683941955761\n",
      "2022-03-26 21:47:25.576272 Epoch 1, Training Loss 0.7970377023872512\n",
      "2022-03-26 21:47:25.595276 Epoch 1, Training Loss 0.7996766545888409\n",
      "2022-03-26 21:47:25.613280 Epoch 1, Training Loss 0.8023069880502608\n",
      "2022-03-26 21:47:25.632285 Epoch 1, Training Loss 0.804943616737795\n",
      "2022-03-26 21:47:25.650272 Epoch 1, Training Loss 0.8075554858693077\n",
      "2022-03-26 21:47:25.668293 Epoch 1, Training Loss 0.8103554956138591\n",
      "2022-03-26 21:47:25.685291 Epoch 1, Training Loss 0.8131050834875277\n",
      "2022-03-26 21:47:25.703294 Epoch 1, Training Loss 0.8156901314435407\n",
      "2022-03-26 21:47:25.721305 Epoch 1, Training Loss 0.8184117328785264\n",
      "2022-03-26 21:47:25.739309 Epoch 1, Training Loss 0.8210859091385551\n",
      "2022-03-26 21:47:25.757313 Epoch 1, Training Loss 0.8236607218642369\n",
      "2022-03-26 21:47:25.776303 Epoch 1, Training Loss 0.8263306233584119\n",
      "2022-03-26 21:47:25.794315 Epoch 1, Training Loss 0.8290222706392293\n",
      "2022-03-26 21:47:25.812326 Epoch 1, Training Loss 0.8315348306580272\n",
      "2022-03-26 21:47:25.830330 Epoch 1, Training Loss 0.8342826657587915\n",
      "2022-03-26 21:47:25.848129 Epoch 1, Training Loss 0.8368412218130458\n",
      "2022-03-26 21:47:25.866135 Epoch 1, Training Loss 0.839455457142247\n",
      "2022-03-26 21:47:25.884137 Epoch 1, Training Loss 0.8421350597115733\n",
      "2022-03-26 21:47:25.901135 Epoch 1, Training Loss 0.844837369363936\n",
      "2022-03-26 21:47:25.919139 Epoch 1, Training Loss 0.847378889770459\n",
      "2022-03-26 21:47:25.938149 Epoch 1, Training Loss 0.8500571191463324\n",
      "2022-03-26 21:47:25.955147 Epoch 1, Training Loss 0.8527164110137374\n",
      "2022-03-26 21:47:25.974151 Epoch 1, Training Loss 0.8552168205266109\n",
      "2022-03-26 21:47:25.992161 Epoch 1, Training Loss 0.8577915042867441\n",
      "2022-03-26 21:47:26.010153 Epoch 1, Training Loss 0.8604437777453371\n",
      "2022-03-26 21:47:26.029170 Epoch 1, Training Loss 0.8629222067115861\n",
      "2022-03-26 21:47:26.047168 Epoch 1, Training Loss 0.8656075484002642\n",
      "2022-03-26 21:47:26.065172 Epoch 1, Training Loss 0.8681596203533279\n",
      "2022-03-26 21:47:26.083182 Epoch 1, Training Loss 0.8707458594875872\n",
      "2022-03-26 21:47:26.101186 Epoch 1, Training Loss 0.8734357286902035\n",
      "2022-03-26 21:47:26.119184 Epoch 1, Training Loss 0.8760984712244605\n",
      "2022-03-26 21:47:26.137188 Epoch 1, Training Loss 0.8786942949685295\n",
      "2022-03-26 21:47:26.154192 Epoch 1, Training Loss 0.8813811473529357\n",
      "2022-03-26 21:47:26.172202 Epoch 1, Training Loss 0.8841150974678567\n",
      "2022-03-26 21:47:26.191207 Epoch 1, Training Loss 0.8866793447748169\n",
      "2022-03-26 21:47:26.210211 Epoch 1, Training Loss 0.8891843904924515\n",
      "2022-03-26 21:47:26.227209 Epoch 1, Training Loss 0.8917159555513231\n",
      "2022-03-26 21:47:26.245213 Epoch 1, Training Loss 0.8943281006020354\n",
      "2022-03-26 21:47:26.264223 Epoch 1, Training Loss 0.8970106873670807\n",
      "2022-03-26 21:47:26.283221 Epoch 1, Training Loss 0.8996265827847258\n",
      "2022-03-26 21:47:26.300225 Epoch 1, Training Loss 0.9021481399036124\n",
      "2022-03-26 21:47:26.318235 Epoch 1, Training Loss 0.9047575163109528\n",
      "2022-03-26 21:47:26.336233 Epoch 1, Training Loss 0.9074928719369347\n",
      "2022-03-26 21:47:26.354231 Epoch 1, Training Loss 0.9100283942259181\n",
      "2022-03-26 21:47:26.372248 Epoch 1, Training Loss 0.9126666107446032\n",
      "2022-03-26 21:47:26.391252 Epoch 1, Training Loss 0.9152436113113638\n",
      "2022-03-26 21:47:26.409250 Epoch 1, Training Loss 0.917762160148767\n",
      "2022-03-26 21:47:26.428261 Epoch 1, Training Loss 0.9204170223697067\n",
      "2022-03-26 21:47:26.446885 Epoch 1, Training Loss 0.9230574819133105\n",
      "2022-03-26 21:47:26.464889 Epoch 1, Training Loss 0.9255668909653373\n",
      "2022-03-26 21:47:26.483894 Epoch 1, Training Loss 0.9281937768087363\n",
      "2022-03-26 21:47:26.501885 Epoch 1, Training Loss 0.9307800625901088\n",
      "2022-03-26 21:47:26.519896 Epoch 1, Training Loss 0.9334817850376333\n",
      "2022-03-26 21:47:26.537906 Epoch 1, Training Loss 0.936106637920565\n",
      "2022-03-26 21:47:26.555910 Epoch 1, Training Loss 0.9386957575902914\n",
      "2022-03-26 21:47:26.573914 Epoch 1, Training Loss 0.9412891980632186\n",
      "2022-03-26 21:47:26.591918 Epoch 1, Training Loss 0.9438976607359278\n",
      "2022-03-26 21:47:26.609922 Epoch 1, Training Loss 0.9467097499486431\n",
      "2022-03-26 21:47:26.628927 Epoch 1, Training Loss 0.9492371449690036\n",
      "2022-03-26 21:47:26.647911 Epoch 1, Training Loss 0.9518036471913233\n",
      "2022-03-26 21:47:26.665923 Epoch 1, Training Loss 0.9544708852267936\n",
      "2022-03-26 21:47:26.684927 Epoch 1, Training Loss 0.9571761897457834\n",
      "2022-03-26 21:47:26.702937 Epoch 1, Training Loss 0.9598267360416519\n",
      "2022-03-26 21:47:26.720948 Epoch 1, Training Loss 0.962377809350143\n",
      "2022-03-26 21:47:26.737951 Epoch 1, Training Loss 0.9650544737610975\n",
      "2022-03-26 21:47:26.757521 Epoch 1, Training Loss 0.9676373500348358\n",
      "2022-03-26 21:47:26.775243 Epoch 1, Training Loss 0.97030542619393\n",
      "2022-03-26 21:47:26.794242 Epoch 1, Training Loss 0.9729851333381575\n",
      "2022-03-26 21:47:26.812252 Epoch 1, Training Loss 0.975626430395619\n",
      "2022-03-26 21:47:26.831256 Epoch 1, Training Loss 0.978195754310969\n",
      "2022-03-26 21:47:26.849260 Epoch 1, Training Loss 0.9807736555023876\n",
      "2022-03-26 21:47:26.867617 Epoch 1, Training Loss 0.9832539959331913\n",
      "2022-03-26 21:47:26.885621 Epoch 1, Training Loss 0.9858999287380892\n",
      "2022-03-26 21:47:26.903625 Epoch 1, Training Loss 0.9886254845067973\n",
      "2022-03-26 21:47:26.921629 Epoch 1, Training Loss 0.9912401792948203\n",
      "2022-03-26 21:47:26.939640 Epoch 1, Training Loss 0.9937725300374238\n",
      "2022-03-26 21:47:26.957644 Epoch 1, Training Loss 0.9965047517700878\n",
      "2022-03-26 21:47:26.975647 Epoch 1, Training Loss 0.9990835173050766\n",
      "2022-03-26 21:47:26.993645 Epoch 1, Training Loss 1.0017173953373413\n",
      "2022-03-26 21:47:27.012656 Epoch 1, Training Loss 1.0043324254967672\n",
      "2022-03-26 21:47:27.030660 Epoch 1, Training Loss 1.0068984676504988\n",
      "2022-03-26 21:47:27.049664 Epoch 1, Training Loss 1.009474641679193\n",
      "2022-03-26 21:47:27.068662 Epoch 1, Training Loss 1.0120287549770093\n",
      "2022-03-26 21:47:27.086667 Epoch 1, Training Loss 1.0146912536047914\n",
      "2022-03-26 21:47:27.104677 Epoch 1, Training Loss 1.0172092896288314\n",
      "2022-03-26 21:47:27.122681 Epoch 1, Training Loss 1.0197869024008437\n",
      "2022-03-26 21:47:27.140685 Epoch 1, Training Loss 1.0224861174898074\n",
      "2022-03-26 21:47:27.158683 Epoch 1, Training Loss 1.0251741756868484\n",
      "2022-03-26 21:47:27.176693 Epoch 1, Training Loss 1.0277331658946278\n",
      "2022-03-26 21:47:27.195698 Epoch 1, Training Loss 1.030328804269776\n",
      "2022-03-26 21:47:27.214703 Epoch 1, Training Loss 1.0328590816549024\n",
      "2022-03-26 21:47:27.232694 Epoch 1, Training Loss 1.0354621603970637\n",
      "2022-03-26 21:47:27.250710 Epoch 1, Training Loss 1.03810273549136\n",
      "2022-03-26 21:47:27.269708 Epoch 1, Training Loss 1.0406386512319754\n",
      "2022-03-26 21:47:27.287673 Epoch 1, Training Loss 1.0432484989885784\n",
      "2022-03-26 21:47:27.306684 Epoch 1, Training Loss 1.0457285779821293\n",
      "2022-03-26 21:47:27.324688 Epoch 1, Training Loss 1.048395233081125\n",
      "2022-03-26 21:47:27.342685 Epoch 1, Training Loss 1.0509880326897896\n",
      "2022-03-26 21:47:27.360696 Epoch 1, Training Loss 1.0536346139810275\n",
      "2022-03-26 21:47:27.378083 Epoch 1, Training Loss 1.0561101538750826\n",
      "2022-03-26 21:47:27.397087 Epoch 1, Training Loss 1.0586489661575278\n",
      "2022-03-26 21:47:27.415091 Epoch 1, Training Loss 1.061191162184986\n",
      "2022-03-26 21:47:27.433087 Epoch 1, Training Loss 1.0637566040239066\n",
      "2022-03-26 21:47:27.453080 Epoch 1, Training Loss 1.0663125795476578\n",
      "2022-03-26 21:47:27.471098 Epoch 1, Training Loss 1.0689783962181463\n",
      "2022-03-26 21:47:27.489102 Epoch 1, Training Loss 1.0716496355393355\n",
      "2022-03-26 21:47:27.507106 Epoch 1, Training Loss 1.0742063764720926\n",
      "2022-03-26 21:47:27.526117 Epoch 1, Training Loss 1.0767757662421906\n",
      "2022-03-26 21:47:27.543743 Epoch 1, Training Loss 1.0792941647722287\n",
      "2022-03-26 21:47:27.561747 Epoch 1, Training Loss 1.0818096313939984\n",
      "2022-03-26 21:47:27.579751 Epoch 1, Training Loss 1.0844913288150602\n",
      "2022-03-26 21:47:27.597749 Epoch 1, Training Loss 1.0870086831205032\n",
      "2022-03-26 21:47:27.616760 Epoch 1, Training Loss 1.089539666614874\n",
      "2022-03-26 21:47:27.635757 Epoch 1, Training Loss 1.0922479410000774\n",
      "2022-03-26 21:47:27.654768 Epoch 1, Training Loss 1.0947008076531197\n",
      "2022-03-26 21:47:27.672772 Epoch 1, Training Loss 1.0973798952749014\n",
      "2022-03-26 21:47:27.690776 Epoch 1, Training Loss 1.0999137506155712\n",
      "2022-03-26 21:47:27.709395 Epoch 1, Training Loss 1.1023719728450336\n",
      "2022-03-26 21:47:27.727393 Epoch 1, Training Loss 1.1050691534491146\n",
      "2022-03-26 21:47:27.746403 Epoch 1, Training Loss 1.107625429892479\n",
      "2022-03-26 21:47:27.764407 Epoch 1, Training Loss 1.110260985086641\n",
      "2022-03-26 21:47:27.782411 Epoch 1, Training Loss 1.1127315544716232\n",
      "2022-03-26 21:47:27.800416 Epoch 1, Training Loss 1.1154552678317975\n",
      "2022-03-26 21:47:27.818407 Epoch 1, Training Loss 1.1180739818936418\n",
      "2022-03-26 21:47:27.837418 Epoch 1, Training Loss 1.1205678752926\n",
      "2022-03-26 21:47:27.856422 Epoch 1, Training Loss 1.1229931497207992\n",
      "2022-03-26 21:47:27.875432 Epoch 1, Training Loss 1.125533532456059\n",
      "2022-03-26 21:47:27.894437 Epoch 1, Training Loss 1.1280820558747977\n",
      "2022-03-26 21:47:27.912441 Epoch 1, Training Loss 1.1308122614155645\n",
      "2022-03-26 21:47:27.931439 Epoch 1, Training Loss 1.13329790437313\n",
      "2022-03-26 21:47:27.948449 Epoch 1, Training Loss 1.1358907372140519\n",
      "2022-03-26 21:47:27.967455 Epoch 1, Training Loss 1.1384783483222318\n",
      "2022-03-26 21:47:27.992459 Epoch 1, Training Loss 1.1410527037232734\n",
      "2022-03-26 21:47:28.018459 Epoch 1, Training Loss 1.1435635591406956\n",
      "2022-03-26 21:47:28.044471 Epoch 1, Training Loss 1.1460755648820296\n",
      "2022-03-26 21:47:28.071471 Epoch 1, Training Loss 1.1485351407924271\n",
      "2022-03-26 21:47:28.096483 Epoch 1, Training Loss 1.1512668228820158\n",
      "2022-03-26 21:47:28.122488 Epoch 1, Training Loss 1.1537533318600082\n",
      "2022-03-26 21:47:28.149495 Epoch 1, Training Loss 1.1563064771540024\n",
      "2022-03-26 21:47:28.174500 Epoch 1, Training Loss 1.1590038248340186\n",
      "2022-03-26 21:47:28.192504 Epoch 1, Training Loss 1.161585719993962\n",
      "2022-03-26 21:47:28.211509 Epoch 1, Training Loss 1.164088315823499\n",
      "2022-03-26 21:47:28.229507 Epoch 1, Training Loss 1.1666209452292498\n",
      "2022-03-26 21:47:28.247517 Epoch 1, Training Loss 1.1693324065574295\n",
      "2022-03-26 21:47:28.265521 Epoch 1, Training Loss 1.1719898730897538\n",
      "2022-03-26 21:47:28.283519 Epoch 1, Training Loss 1.1747000886656134\n",
      "2022-03-26 21:47:28.301516 Epoch 1, Training Loss 1.1771934288541983\n",
      "2022-03-26 21:47:28.320527 Epoch 1, Training Loss 1.179763864373307\n",
      "2022-03-26 21:47:28.338537 Epoch 1, Training Loss 1.1823252799260953\n",
      "2022-03-26 21:47:28.356535 Epoch 1, Training Loss 1.184839221979956\n",
      "2022-03-26 21:47:28.374539 Epoch 1, Training Loss 1.1874734058099634\n",
      "2022-03-26 21:47:28.392543 Epoch 1, Training Loss 1.189958416439993\n",
      "2022-03-26 21:47:28.410554 Epoch 1, Training Loss 1.1924914830481\n",
      "2022-03-26 21:47:28.429558 Epoch 1, Training Loss 1.1950983243525182\n",
      "2022-03-26 21:47:28.447549 Epoch 1, Training Loss 1.1975684550107288\n",
      "2022-03-26 21:47:28.465566 Epoch 1, Training Loss 1.2001561435592143\n",
      "2022-03-26 21:47:28.483564 Epoch 1, Training Loss 1.2027615836209349\n",
      "2022-03-26 21:47:28.502568 Epoch 1, Training Loss 1.2052410354699625\n",
      "2022-03-26 21:47:28.520572 Epoch 1, Training Loss 1.2077300553126713\n",
      "2022-03-26 21:47:28.587588 Epoch 1, Training Loss 1.2103042003443785\n",
      "2022-03-26 21:47:28.605598 Epoch 1, Training Loss 1.212890088558197\n",
      "2022-03-26 21:47:28.623596 Epoch 1, Training Loss 1.2153706488097111\n",
      "2022-03-26 21:47:28.641600 Epoch 1, Training Loss 1.2178870846548349\n",
      "2022-03-26 21:47:28.660610 Epoch 1, Training Loss 1.2203710251452062\n",
      "2022-03-26 21:47:28.679608 Epoch 1, Training Loss 1.2227607268811491\n",
      "2022-03-26 21:47:28.696620 Epoch 1, Training Loss 1.2253698685285075\n",
      "2022-03-26 21:47:28.715617 Epoch 1, Training Loss 1.2278184512692034\n",
      "2022-03-26 21:47:28.732627 Epoch 1, Training Loss 1.2303050073516337\n",
      "2022-03-26 21:47:28.751618 Epoch 1, Training Loss 1.232755137069146\n",
      "2022-03-26 21:47:28.769623 Epoch 1, Training Loss 1.2352100993361315\n",
      "2022-03-26 21:47:28.788640 Epoch 1, Training Loss 1.2376696811917494\n",
      "2022-03-26 21:47:28.805637 Epoch 1, Training Loss 1.2402003232170553\n",
      "2022-03-26 21:47:28.823633 Epoch 1, Training Loss 1.242887991773503\n",
      "2022-03-26 21:47:28.841645 Epoch 1, Training Loss 1.2454342287214821\n",
      "2022-03-26 21:47:28.860649 Epoch 1, Training Loss 1.2479777915398482\n",
      "2022-03-26 21:47:28.878659 Epoch 1, Training Loss 1.2505170053533277\n",
      "2022-03-26 21:47:28.897652 Epoch 1, Training Loss 1.2530937772577682\n",
      "2022-03-26 21:47:28.917658 Epoch 1, Training Loss 1.2556484843154088\n",
      "2022-03-26 21:47:28.935666 Epoch 1, Training Loss 1.2582512206738563\n",
      "2022-03-26 21:47:28.953671 Epoch 1, Training Loss 1.2608126468975525\n",
      "2022-03-26 21:47:28.971967 Epoch 1, Training Loss 1.2633760868740813\n",
      "2022-03-26 21:47:28.989972 Epoch 1, Training Loss 1.2658985508677294\n",
      "2022-03-26 21:47:29.009978 Epoch 1, Training Loss 1.2684054502745723\n",
      "2022-03-26 21:47:29.030982 Epoch 1, Training Loss 1.2711511856454718\n",
      "2022-03-26 21:47:29.051985 Epoch 1, Training Loss 1.2738238506000061\n",
      "2022-03-26 21:47:29.070990 Epoch 1, Training Loss 1.2764047100720808\n",
      "2022-03-26 21:47:29.088994 Epoch 1, Training Loss 1.2788220290332803\n",
      "2022-03-26 21:47:29.108011 Epoch 1, Training Loss 1.2813766370039157\n",
      "2022-03-26 21:47:29.126021 Epoch 1, Training Loss 1.2839803785619224\n",
      "2022-03-26 21:47:29.144025 Epoch 1, Training Loss 1.2864575427206582\n",
      "2022-03-26 21:47:29.164029 Epoch 1, Training Loss 1.2889887865851908\n",
      "2022-03-26 21:47:29.181021 Epoch 1, Training Loss 1.2916143373455233\n",
      "2022-03-26 21:47:29.200032 Epoch 1, Training Loss 1.294151839850199\n",
      "2022-03-26 21:47:29.218036 Epoch 1, Training Loss 1.2966244690253605\n",
      "2022-03-26 21:47:29.236039 Epoch 1, Training Loss 1.2992189345152483\n",
      "2022-03-26 21:47:29.254050 Epoch 1, Training Loss 1.3017320651227555\n",
      "2022-03-26 21:47:29.272041 Epoch 1, Training Loss 1.3042891870069382\n",
      "2022-03-26 21:47:29.290058 Epoch 1, Training Loss 1.306779587512736\n",
      "2022-03-26 21:47:29.309056 Epoch 1, Training Loss 1.3091318316166969\n",
      "2022-03-26 21:47:29.327052 Epoch 1, Training Loss 1.31163024140136\n",
      "2022-03-26 21:47:29.345070 Epoch 1, Training Loss 1.3141948374945793\n",
      "2022-03-26 21:47:29.363075 Epoch 1, Training Loss 1.3167048558554686\n",
      "2022-03-26 21:47:29.381078 Epoch 1, Training Loss 1.319151038099128\n",
      "2022-03-26 21:47:29.399077 Epoch 1, Training Loss 1.3215934103712097\n",
      "2022-03-26 21:47:29.417066 Epoch 1, Training Loss 1.3240603575925998\n",
      "2022-03-26 21:47:29.436085 Epoch 1, Training Loss 1.326684240947294\n",
      "2022-03-26 21:47:29.454095 Epoch 1, Training Loss 1.3291672340134526\n",
      "2022-03-26 21:47:29.471099 Epoch 1, Training Loss 1.33177427928466\n",
      "2022-03-26 21:47:29.489098 Epoch 1, Training Loss 1.334180050188928\n",
      "2022-03-26 21:47:29.509610 Epoch 1, Training Loss 1.3367635578755528\n",
      "2022-03-26 21:47:29.527615 Epoch 1, Training Loss 1.3392534778855951\n",
      "2022-03-26 21:47:29.545612 Epoch 1, Training Loss 1.3416406240914485\n",
      "2022-03-26 21:47:29.563622 Epoch 1, Training Loss 1.344202416174857\n",
      "2022-03-26 21:47:29.581620 Epoch 1, Training Loss 1.346547159544952\n",
      "2022-03-26 21:47:29.600610 Epoch 1, Training Loss 1.3491109059289899\n",
      "2022-03-26 21:47:29.617622 Epoch 1, Training Loss 1.351554777006359\n",
      "2022-03-26 21:47:29.636618 Epoch 1, Training Loss 1.3542041677953032\n",
      "2022-03-26 21:47:29.653636 Epoch 1, Training Loss 1.356664212463457\n",
      "2022-03-26 21:47:29.671641 Epoch 1, Training Loss 1.359104994465323\n",
      "2022-03-26 21:47:29.689651 Epoch 1, Training Loss 1.361701027359194\n",
      "2022-03-26 21:47:29.707655 Epoch 1, Training Loss 1.3641709616726927\n",
      "2022-03-26 21:47:29.727640 Epoch 1, Training Loss 1.3668585780941311\n",
      "2022-03-26 21:47:29.746644 Epoch 1, Training Loss 1.3693881545530255\n",
      "2022-03-26 21:47:29.766650 Epoch 1, Training Loss 1.3720250628183566\n",
      "2022-03-26 21:47:29.785660 Epoch 1, Training Loss 1.3745489077799766\n",
      "2022-03-26 21:47:29.802671 Epoch 1, Training Loss 1.3771259360910986\n",
      "2022-03-26 21:47:29.821669 Epoch 1, Training Loss 1.3795768679560299\n",
      "2022-03-26 21:47:29.839679 Epoch 1, Training Loss 1.3822034766606968\n",
      "2022-03-26 21:47:29.857683 Epoch 1, Training Loss 1.3848907253931246\n",
      "2022-03-26 21:47:29.876693 Epoch 1, Training Loss 1.3872958999460616\n",
      "2022-03-26 21:47:29.894697 Epoch 1, Training Loss 1.3898291717404905\n",
      "2022-03-26 21:47:29.912695 Epoch 1, Training Loss 1.3922528382152548\n",
      "2022-03-26 21:47:29.931706 Epoch 1, Training Loss 1.394955690864407\n",
      "2022-03-26 21:47:29.949710 Epoch 1, Training Loss 1.3974651854361415\n",
      "2022-03-26 21:47:29.967708 Epoch 1, Training Loss 1.3999609787140965\n",
      "2022-03-26 21:47:29.985712 Epoch 1, Training Loss 1.402344449554258\n",
      "2022-03-26 21:47:30.002722 Epoch 1, Training Loss 1.4048947916006493\n",
      "2022-03-26 21:47:30.020720 Epoch 1, Training Loss 1.4073921268248497\n",
      "2022-03-26 21:47:30.038724 Epoch 1, Training Loss 1.4098499706943932\n",
      "2022-03-26 21:47:30.056728 Epoch 1, Training Loss 1.4123268289029445\n",
      "2022-03-26 21:47:30.074738 Epoch 1, Training Loss 1.4147535768311348\n",
      "2022-03-26 21:47:30.093742 Epoch 1, Training Loss 1.4171668591401767\n",
      "2022-03-26 21:47:30.111740 Epoch 1, Training Loss 1.4196149629095327\n",
      "2022-03-26 21:47:30.129751 Epoch 1, Training Loss 1.4221237027431692\n",
      "2022-03-26 21:47:30.149737 Epoch 1, Training Loss 1.42450183569013\n",
      "2022-03-26 21:47:30.167747 Epoch 1, Training Loss 1.4269305705414403\n",
      "2022-03-26 21:47:30.185763 Epoch 1, Training Loss 1.4294313212184955\n",
      "2022-03-26 21:47:30.203767 Epoch 1, Training Loss 1.432004417147478\n",
      "2022-03-26 21:47:30.221756 Epoch 1, Training Loss 1.4344187412420502\n",
      "2022-03-26 21:47:30.239776 Epoch 1, Training Loss 1.4370018876422093\n",
      "2022-03-26 21:47:30.257773 Epoch 1, Training Loss 1.4395045090819258\n",
      "2022-03-26 21:47:30.275783 Epoch 1, Training Loss 1.442072271386071\n",
      "2022-03-26 21:47:30.293781 Epoch 1, Training Loss 1.4445462385406884\n",
      "2022-03-26 21:47:30.311792 Epoch 1, Training Loss 1.4469032267780255\n",
      "2022-03-26 21:47:30.330796 Epoch 1, Training Loss 1.4495404966347052\n",
      "2022-03-26 21:47:30.348800 Epoch 1, Training Loss 1.4519486848045797\n",
      "2022-03-26 21:47:30.366793 Epoch 1, Training Loss 1.4543024290858022\n",
      "2022-03-26 21:47:30.384802 Epoch 1, Training Loss 1.4567197655777797\n",
      "2022-03-26 21:47:30.403813 Epoch 1, Training Loss 1.459065049658041\n",
      "2022-03-26 21:47:30.421817 Epoch 1, Training Loss 1.4615456971061198\n",
      "2022-03-26 21:47:30.439815 Epoch 1, Training Loss 1.463935021396793\n",
      "2022-03-26 21:47:30.457825 Epoch 1, Training Loss 1.4665301896422112\n",
      "2022-03-26 21:47:30.475823 Epoch 1, Training Loss 1.4690690558889639\n",
      "2022-03-26 21:47:30.493827 Epoch 1, Training Loss 1.4717602421865439\n",
      "2022-03-26 21:47:30.512837 Epoch 1, Training Loss 1.4743874563890345\n",
      "2022-03-26 21:47:30.530821 Epoch 1, Training Loss 1.4769537826938093\n",
      "2022-03-26 21:47:30.548827 Epoch 1, Training Loss 1.479580817320158\n",
      "2022-03-26 21:47:30.566850 Epoch 1, Training Loss 1.4821783099942805\n",
      "2022-03-26 21:47:30.585854 Epoch 1, Training Loss 1.484668588851724\n",
      "2022-03-26 21:47:30.603858 Epoch 1, Training Loss 1.4871881518827375\n",
      "2022-03-26 21:47:30.621845 Epoch 1, Training Loss 1.4898740938862267\n",
      "2022-03-26 21:47:30.641848 Epoch 1, Training Loss 1.4924583488413135\n",
      "2022-03-26 21:47:30.659851 Epoch 1, Training Loss 1.4950237498258996\n",
      "2022-03-26 21:47:30.677854 Epoch 1, Training Loss 1.4973993229744074\n",
      "2022-03-26 21:47:30.695879 Epoch 1, Training Loss 1.4999551768498043\n",
      "2022-03-26 21:47:30.714883 Epoch 1, Training Loss 1.5023619575268776\n",
      "2022-03-26 21:47:30.733869 Epoch 1, Training Loss 1.504838068466967\n",
      "2022-03-26 21:47:30.752886 Epoch 1, Training Loss 1.5074058152220744\n",
      "2022-03-26 21:47:30.770896 Epoch 1, Training Loss 1.5099551500871664\n",
      "2022-03-26 21:47:30.789894 Epoch 1, Training Loss 1.5124572992629712\n",
      "2022-03-26 21:47:30.808905 Epoch 1, Training Loss 1.5150176693716317\n",
      "2022-03-26 21:47:30.826902 Epoch 1, Training Loss 1.5174402394867919\n",
      "2022-03-26 21:47:30.845907 Epoch 1, Training Loss 1.5198156758952324\n",
      "2022-03-26 21:47:30.864911 Epoch 1, Training Loss 1.522258321952332\n",
      "2022-03-26 21:47:30.883910 Epoch 1, Training Loss 1.5247969561830506\n",
      "2022-03-26 21:47:30.901920 Epoch 1, Training Loss 1.5274814832241028\n",
      "2022-03-26 21:47:30.919930 Epoch 1, Training Loss 1.5297724679302986\n",
      "2022-03-26 21:47:30.937928 Epoch 1, Training Loss 1.5321606458605403\n",
      "2022-03-26 21:47:30.955938 Epoch 1, Training Loss 1.5345894176027048\n",
      "2022-03-26 21:47:30.973943 Epoch 1, Training Loss 1.5371435920295813\n",
      "2022-03-26 21:47:30.993940 Epoch 1, Training Loss 1.5395229699666544\n",
      "2022-03-26 21:47:31.010944 Epoch 1, Training Loss 1.5422475632194363\n",
      "2022-03-26 21:47:31.029948 Epoch 1, Training Loss 1.5446860770435284\n",
      "2022-03-26 21:47:31.047959 Epoch 1, Training Loss 1.547154638163574\n",
      "2022-03-26 21:47:31.066943 Epoch 1, Training Loss 1.5496445967413275\n",
      "2022-03-26 21:47:31.084954 Epoch 1, Training Loss 1.5520325104903687\n",
      "2022-03-26 21:47:31.103965 Epoch 1, Training Loss 1.5545119342901517\n",
      "2022-03-26 21:47:31.121975 Epoch 1, Training Loss 1.5570393020234754\n",
      "2022-03-26 21:47:31.139973 Epoch 1, Training Loss 1.5595594449421328\n",
      "2022-03-26 21:47:31.157984 Epoch 1, Training Loss 1.5619936990920844\n",
      "2022-03-26 21:47:31.176968 Epoch 1, Training Loss 1.5646413932066134\n",
      "2022-03-26 21:47:31.198973 Epoch 1, Training Loss 1.567232000705836\n",
      "2022-03-26 21:47:31.217977 Epoch 1, Training Loss 1.5698198203540519\n",
      "2022-03-26 21:47:31.237981 Epoch 1, Training Loss 1.5723780477443314\n",
      "2022-03-26 21:47:31.256986 Epoch 1, Training Loss 1.5750018835372632\n",
      "2022-03-26 21:47:31.276991 Epoch 1, Training Loss 1.5773861092679642\n",
      "2022-03-26 21:47:31.296015 Epoch 1, Training Loss 1.5799940871765545\n",
      "2022-03-26 21:47:31.314000 Epoch 1, Training Loss 1.5823359254680935\n",
      "2022-03-26 21:47:31.334005 Epoch 1, Training Loss 1.5848703263970592\n",
      "2022-03-26 21:47:31.352009 Epoch 1, Training Loss 1.587346704872063\n",
      "2022-03-26 21:47:31.371026 Epoch 1, Training Loss 1.5897508073036017\n",
      "2022-03-26 21:47:31.389036 Epoch 1, Training Loss 1.5923712037103561\n",
      "2022-03-26 21:47:31.408034 Epoch 1, Training Loss 1.5948770223066324\n",
      "2022-03-26 21:47:31.427045 Epoch 1, Training Loss 1.5975586968614621\n",
      "2022-03-26 21:47:31.446043 Epoch 1, Training Loss 1.5999853946363833\n",
      "2022-03-26 21:47:31.464053 Epoch 1, Training Loss 1.6025495126729121\n",
      "2022-03-26 21:47:31.483051 Epoch 1, Training Loss 1.6051495154495434\n",
      "2022-03-26 21:47:31.501055 Epoch 1, Training Loss 1.6076460662095442\n",
      "2022-03-26 21:47:31.520065 Epoch 1, Training Loss 1.6101964369149464\n",
      "2022-03-26 21:47:31.538070 Epoch 1, Training Loss 1.612731011017509\n",
      "2022-03-26 21:47:31.556074 Epoch 1, Training Loss 1.6151400653602521\n",
      "2022-03-26 21:47:31.574078 Epoch 1, Training Loss 1.6176875425726556\n",
      "2022-03-26 21:47:31.592076 Epoch 1, Training Loss 1.6201525655244013\n",
      "2022-03-26 21:47:31.611080 Epoch 1, Training Loss 1.6226220518121939\n",
      "2022-03-26 21:47:31.637093 Epoch 1, Training Loss 1.625193715400403\n",
      "2022-03-26 21:47:31.663098 Epoch 1, Training Loss 1.6277625484539724\n",
      "2022-03-26 21:47:31.689091 Epoch 1, Training Loss 1.6302754243316553\n",
      "2022-03-26 21:47:31.715095 Epoch 1, Training Loss 1.6327834402203865\n",
      "2022-03-26 21:47:31.741116 Epoch 1, Training Loss 1.6352209922907603\n",
      "2022-03-26 21:47:31.767122 Epoch 1, Training Loss 1.63780538352859\n",
      "2022-03-26 21:47:31.792121 Epoch 1, Training Loss 1.6402975941253135\n",
      "2022-03-26 21:47:31.819127 Epoch 1, Training Loss 1.642743514321954\n",
      "2022-03-26 21:47:31.838138 Epoch 1, Training Loss 1.6451425939569693\n",
      "2022-03-26 21:47:31.856136 Epoch 1, Training Loss 1.6476396194199467\n",
      "2022-03-26 21:47:31.874146 Epoch 1, Training Loss 1.6499979392341946\n",
      "2022-03-26 21:47:31.892144 Epoch 1, Training Loss 1.6525106535238379\n",
      "2022-03-26 21:47:31.910154 Epoch 1, Training Loss 1.6549201165623677\n",
      "2022-03-26 21:47:31.928158 Epoch 1, Training Loss 1.657532718632837\n",
      "2022-03-26 21:47:31.946156 Epoch 1, Training Loss 1.660114667452205\n",
      "2022-03-26 21:47:31.963160 Epoch 1, Training Loss 1.6626410331872419\n",
      "2022-03-26 21:47:31.982164 Epoch 1, Training Loss 1.6649227310019685\n",
      "2022-03-26 21:47:32.000162 Epoch 1, Training Loss 1.6673486555933648\n",
      "2022-03-26 21:47:32.018174 Epoch 1, Training Loss 1.6697010964993626\n",
      "2022-03-26 21:47:32.036176 Epoch 1, Training Loss 1.6721004041869316\n",
      "2022-03-26 21:47:32.054187 Epoch 1, Training Loss 1.6745514378828161\n",
      "2022-03-26 21:47:32.072185 Epoch 1, Training Loss 1.6770083890546619\n",
      "2022-03-26 21:47:32.090195 Epoch 1, Training Loss 1.679341138476301\n",
      "2022-03-26 21:47:32.109193 Epoch 1, Training Loss 1.6818805457381032\n",
      "2022-03-26 21:47:32.127191 Epoch 1, Training Loss 1.6843679486333256\n",
      "2022-03-26 21:47:32.146208 Epoch 1, Training Loss 1.6867569537114\n",
      "2022-03-26 21:47:32.165212 Epoch 1, Training Loss 1.6894143452424832\n",
      "2022-03-26 21:47:32.183213 Epoch 1, Training Loss 1.6920115031549692\n",
      "2022-03-26 21:47:32.202214 Epoch 1, Training Loss 1.6942629933052356\n",
      "2022-03-26 21:47:32.220218 Epoch 1, Training Loss 1.6967809794808897\n",
      "2022-03-26 21:47:32.238257 Epoch 1, Training Loss 1.6991988731467205\n",
      "2022-03-26 21:47:32.257227 Epoch 1, Training Loss 1.701649082133837\n",
      "2022-03-26 21:47:32.276231 Epoch 1, Training Loss 1.7040860029437659\n",
      "2022-03-26 21:47:32.294225 Epoch 1, Training Loss 1.706508769403638\n",
      "2022-03-26 21:47:32.314246 Epoch 1, Training Loss 1.7090153527991545\n",
      "2022-03-26 21:47:32.332244 Epoch 1, Training Loss 1.711661349324619\n",
      "2022-03-26 21:47:32.351242 Epoch 1, Training Loss 1.714046262871579\n",
      "2022-03-26 21:47:32.369252 Epoch 1, Training Loss 1.71648671422773\n",
      "2022-03-26 21:47:32.386250 Epoch 1, Training Loss 1.7188693251451264\n",
      "2022-03-26 21:47:32.404266 Epoch 1, Training Loss 1.721163157154532\n",
      "2022-03-26 21:47:32.423264 Epoch 1, Training Loss 1.7237123769262563\n",
      "2022-03-26 21:47:32.440274 Epoch 1, Training Loss 1.726004539395842\n",
      "2022-03-26 21:47:32.459278 Epoch 1, Training Loss 1.7284311176565907\n",
      "2022-03-26 21:47:32.478277 Epoch 1, Training Loss 1.7308653018358724\n",
      "2022-03-26 21:47:32.497287 Epoch 1, Training Loss 1.7333892297256939\n",
      "2022-03-26 21:47:32.516278 Epoch 1, Training Loss 1.7360658032814864\n",
      "2022-03-26 21:47:32.534285 Epoch 1, Training Loss 1.7387408798612902\n",
      "2022-03-26 21:47:32.553294 Epoch 1, Training Loss 1.741187172929954\n",
      "2022-03-26 21:47:32.571304 Epoch 1, Training Loss 1.7435018563514475\n",
      "2022-03-26 21:47:32.589291 Epoch 1, Training Loss 1.7461983412123092\n",
      "2022-03-26 21:47:32.609293 Epoch 1, Training Loss 1.7486095850729881\n",
      "2022-03-26 21:47:32.627296 Epoch 1, Training Loss 1.7511654284299183\n",
      "2022-03-26 21:47:32.646315 Epoch 1, Training Loss 1.7535906525524072\n",
      "2022-03-26 21:47:32.664325 Epoch 1, Training Loss 1.756035985391768\n",
      "2022-03-26 21:47:32.683318 Epoch 1, Training Loss 1.7585199278638797\n",
      "2022-03-26 21:47:32.701322 Epoch 1, Training Loss 1.7610431813522982\n",
      "2022-03-26 21:47:32.720340 Epoch 1, Training Loss 1.7635113888079552\n",
      "2022-03-26 21:47:32.738343 Epoch 1, Training Loss 1.7658406741478865\n",
      "2022-03-26 21:47:32.756340 Epoch 1, Training Loss 1.7680888166817863\n",
      "2022-03-26 21:47:32.775350 Epoch 1, Training Loss 1.7706210857157207\n",
      "2022-03-26 21:47:32.793080 Epoch 1, Training Loss 1.7729250810030477\n",
      "2022-03-26 21:47:32.811073 Epoch 1, Training Loss 1.7754070313690264\n",
      "2022-03-26 21:47:32.829088 Epoch 1, Training Loss 1.7779378804099528\n",
      "2022-03-26 21:47:32.846092 Epoch 1, Training Loss 1.780423998375378\n",
      "2022-03-26 21:47:32.865102 Epoch 1, Training Loss 1.7829095090136808\n",
      "2022-03-26 21:47:32.884100 Epoch 1, Training Loss 1.7853143893544325\n",
      "2022-03-26 21:47:32.902104 Epoch 1, Training Loss 1.7877308191240902\n",
      "2022-03-26 21:47:32.920108 Epoch 1, Training Loss 1.7902499478491372\n",
      "2022-03-26 21:47:32.938101 Epoch 1, Training Loss 1.7926275396286069\n",
      "2022-03-26 21:47:32.956116 Epoch 1, Training Loss 1.7951096871015055\n",
      "2022-03-26 21:47:32.974115 Epoch 1, Training Loss 1.7975188786416407\n",
      "2022-03-26 21:47:32.992131 Epoch 1, Training Loss 1.7998859205514268\n",
      "2022-03-26 21:47:33.010122 Epoch 1, Training Loss 1.8022916867300067\n",
      "2022-03-26 21:47:33.028139 Epoch 1, Training Loss 1.8048256379564096\n",
      "2022-03-26 21:47:33.047137 Epoch 1, Training Loss 1.8071179281720116\n",
      "2022-03-26 21:47:33.065135 Epoch 1, Training Loss 1.809537024601646\n",
      "2022-03-26 21:47:33.084146 Epoch 1, Training Loss 1.811935121598451\n",
      "2022-03-26 21:47:33.103156 Epoch 1, Training Loss 1.8145406339174646\n",
      "2022-03-26 21:47:33.121160 Epoch 1, Training Loss 1.8169519838774602\n",
      "2022-03-26 21:47:33.139164 Epoch 1, Training Loss 1.8195790660655713\n",
      "2022-03-26 21:47:33.157159 Epoch 1, Training Loss 1.8220537051825267\n",
      "2022-03-26 21:47:33.175172 Epoch 1, Training Loss 1.8245661187049982\n",
      "2022-03-26 21:47:33.193177 Epoch 1, Training Loss 1.8271159179070418\n",
      "2022-03-26 21:47:33.211705 Epoch 1, Training Loss 1.8295236220750053\n",
      "2022-03-26 21:47:33.229715 Epoch 1, Training Loss 1.8319253996205147\n",
      "2022-03-26 21:47:33.247719 Epoch 1, Training Loss 1.8343148025710259\n",
      "2022-03-26 21:47:33.265723 Epoch 1, Training Loss 1.8367583995584942\n",
      "2022-03-26 21:47:33.284721 Epoch 1, Training Loss 1.8390453529479864\n",
      "2022-03-26 21:47:33.302725 Epoch 1, Training Loss 1.8414747288160007\n",
      "2022-03-26 21:47:33.322007 Epoch 1, Training Loss 1.8439910122195777\n",
      "2022-03-26 21:47:33.339647 Epoch 1, Training Loss 1.8464597279153516\n",
      "2022-03-26 21:47:33.357651 Epoch 1, Training Loss 1.8489128134744552\n",
      "2022-03-26 21:47:33.374648 Epoch 1, Training Loss 1.8513773911444427\n",
      "2022-03-26 21:47:33.392153 Epoch 1, Training Loss 1.8537876078539797\n",
      "2022-03-26 21:47:33.410157 Epoch 1, Training Loss 1.856177966003223\n",
      "2022-03-26 21:47:33.429168 Epoch 1, Training Loss 1.8586001635512428\n",
      "2022-03-26 21:47:33.448172 Epoch 1, Training Loss 1.8612275341587603\n",
      "2022-03-26 21:47:33.466170 Epoch 1, Training Loss 1.8636361342256942\n",
      "2022-03-26 21:47:33.484167 Epoch 1, Training Loss 1.8660723048707712\n",
      "2022-03-26 21:47:33.502171 Epoch 1, Training Loss 1.8683855532075437\n",
      "2022-03-26 21:47:33.520188 Epoch 1, Training Loss 1.870764157198884\n",
      "2022-03-26 21:47:33.538192 Epoch 1, Training Loss 1.8731939298722444\n",
      "2022-03-26 21:47:33.556190 Epoch 1, Training Loss 1.87558818320789\n",
      "2022-03-26 21:47:33.574200 Epoch 1, Training Loss 1.8780183616806478\n",
      "2022-03-26 21:47:33.592204 Epoch 1, Training Loss 1.8803392942909085\n",
      "2022-03-26 21:47:33.610209 Epoch 1, Training Loss 1.882888074421212\n",
      "2022-03-26 21:47:33.629213 Epoch 1, Training Loss 1.8853146290535208\n",
      "2022-03-26 21:47:33.646217 Epoch 1, Training Loss 1.8877193415561295\n",
      "2022-03-26 21:47:33.664215 Epoch 1, Training Loss 1.8901920727146861\n",
      "2022-03-26 21:47:33.682225 Epoch 1, Training Loss 1.8926561203454157\n",
      "2022-03-26 21:47:33.700229 Epoch 1, Training Loss 1.895120511731833\n",
      "2022-03-26 21:47:33.718227 Epoch 1, Training Loss 1.8975296607407768\n",
      "2022-03-26 21:47:33.737237 Epoch 1, Training Loss 1.9000297868648148\n",
      "2022-03-26 21:47:33.755241 Epoch 1, Training Loss 1.9022316632368375\n",
      "2022-03-26 21:47:33.773245 Epoch 1, Training Loss 1.9047898659315865\n",
      "2022-03-26 21:47:33.791243 Epoch 1, Training Loss 1.9071838072193859\n",
      "2022-03-26 21:47:33.810254 Epoch 1, Training Loss 1.9096302204119884\n",
      "2022-03-26 21:47:33.829258 Epoch 1, Training Loss 1.912051404071281\n",
      "2022-03-26 21:47:33.847262 Epoch 1, Training Loss 1.9145516567218028\n",
      "2022-03-26 21:47:33.865266 Epoch 1, Training Loss 1.916906366415341\n",
      "2022-03-26 21:47:33.884270 Epoch 1, Training Loss 1.9195527595937099\n",
      "2022-03-26 21:47:33.901268 Epoch 1, Training Loss 1.9221956582020616\n",
      "2022-03-26 21:47:33.920279 Epoch 1, Training Loss 1.9245319300905213\n",
      "2022-03-26 21:47:33.938283 Epoch 1, Training Loss 1.9270198191218364\n",
      "2022-03-26 21:47:33.956281 Epoch 1, Training Loss 1.9295877356968267\n",
      "2022-03-26 21:47:33.975291 Epoch 1, Training Loss 1.9319397926025683\n",
      "2022-03-26 21:47:33.994295 Epoch 1, Training Loss 1.934529620515721\n",
      "2022-03-26 21:47:34.012300 Epoch 1, Training Loss 1.9369736676630767\n",
      "2022-03-26 21:47:34.030304 Epoch 1, Training Loss 1.9394836354133722\n",
      "2022-03-26 21:47:34.048308 Epoch 1, Training Loss 1.9419940187193243\n",
      "2022-03-26 21:47:34.066306 Epoch 1, Training Loss 1.9446349304045558\n",
      "2022-03-26 21:47:34.084310 Epoch 1, Training Loss 1.9469194715590123\n",
      "2022-03-26 21:47:34.102314 Epoch 1, Training Loss 1.9492904482900029\n",
      "2022-03-26 21:47:34.120324 Epoch 1, Training Loss 1.9517129536175057\n",
      "2022-03-26 21:47:34.138316 Epoch 1, Training Loss 1.9540906496669934\n",
      "2022-03-26 21:47:34.156326 Epoch 1, Training Loss 1.9564970007637883\n",
      "2022-03-26 21:47:34.175337 Epoch 1, Training Loss 1.9589893965769911\n",
      "2022-03-26 21:47:34.194341 Epoch 1, Training Loss 1.9613080283869868\n",
      "2022-03-26 21:47:34.213345 Epoch 1, Training Loss 1.963697915644292\n",
      "2022-03-26 21:47:34.231349 Epoch 1, Training Loss 1.9660031303115513\n",
      "2022-03-26 21:47:34.250341 Epoch 1, Training Loss 1.9683843721514163\n",
      "2022-03-26 21:47:34.267351 Epoch 1, Training Loss 1.9707934080487322\n",
      "2022-03-26 21:47:34.286362 Epoch 1, Training Loss 1.9731544384261226\n",
      "2022-03-26 21:47:34.304366 Epoch 1, Training Loss 1.9754512736864407\n",
      "2022-03-26 21:47:34.322358 Epoch 1, Training Loss 1.9778006573772187\n",
      "2022-03-26 21:47:34.341375 Epoch 1, Training Loss 1.980334248231805\n",
      "2022-03-26 21:47:34.359379 Epoch 1, Training Loss 1.9828130700399198\n",
      "2022-03-26 21:47:34.377376 Epoch 1, Training Loss 1.9852036036493834\n",
      "2022-03-26 21:47:34.396388 Epoch 1, Training Loss 1.9876985777064662\n",
      "2022-03-26 21:47:34.415385 Epoch 1, Training Loss 1.9901156675480212\n",
      "2022-03-26 21:47:34.433389 Epoch 1, Training Loss 1.9925678552264143\n",
      "2022-03-26 21:47:34.452394 Epoch 1, Training Loss 1.9950899849157504\n",
      "2022-03-26 21:47:34.470403 Epoch 1, Training Loss 1.9975367110708486\n",
      "2022-03-26 21:47:34.488407 Epoch 1, Training Loss 2.000006228151834\n",
      "2022-03-26 21:47:34.506412 Epoch 1, Training Loss 2.002417829335498\n",
      "2022-03-26 21:47:34.524416 Epoch 1, Training Loss 2.0048080935807486\n",
      "2022-03-26 21:47:34.542697 Epoch 1, Training Loss 2.0073251994064703\n",
      "2022-03-26 21:47:34.561695 Epoch 1, Training Loss 2.0096796153451475\n",
      "2022-03-26 21:47:34.580693 Epoch 1, Training Loss 2.011998396852742\n",
      "2022-03-26 21:47:34.598704 Epoch 1, Training Loss 2.014421115445969\n",
      "2022-03-26 21:47:34.617701 Epoch 1, Training Loss 2.0166944760800627\n",
      "2022-03-26 21:47:34.636712 Epoch 1, Training Loss 2.0192080286457714\n",
      "2022-03-26 21:47:34.654723 Epoch 1, Training Loss 2.021610401780404\n",
      "2022-03-26 21:47:34.672727 Epoch 1, Training Loss 2.023989133365319\n",
      "2022-03-26 21:47:34.690730 Epoch 1, Training Loss 2.0265656601437523\n",
      "2022-03-26 21:47:34.708729 Epoch 1, Training Loss 2.028808015081889\n",
      "2022-03-26 21:47:34.727733 Epoch 1, Training Loss 2.0312472155027073\n",
      "2022-03-26 21:47:34.746743 Epoch 1, Training Loss 2.0336032800967128\n",
      "2022-03-26 21:47:34.764741 Epoch 1, Training Loss 2.0361429131244453\n",
      "2022-03-26 21:47:34.782751 Epoch 1, Training Loss 2.0387114287947146\n",
      "2022-03-26 21:47:34.801750 Epoch 1, Training Loss 2.041192463139439\n",
      "2022-03-26 21:47:34.819760 Epoch 1, Training Loss 2.043853121492869\n",
      "2022-03-26 21:47:34.837764 Epoch 1, Training Loss 2.0462123789750706\n",
      "2022-03-26 21:47:34.856769 Epoch 1, Training Loss 2.0487756756565454\n",
      "2022-03-26 21:47:34.863758 Epoch 1, Training Loss 2.0511138338567045\n",
      "2022-03-26 21:58:40.354715 Epoch 50, Training Loss 0.0007992341847675841\n",
      "2022-03-26 21:58:40.373422 Epoch 50, Training Loss 0.0018408681120713958\n",
      "2022-03-26 21:58:40.393429 Epoch 50, Training Loss 0.002864093274411643\n",
      "2022-03-26 21:58:40.413832 Epoch 50, Training Loss 0.0038226979315433354\n",
      "2022-03-26 21:58:40.433624 Epoch 50, Training Loss 0.004750325627948927\n",
      "2022-03-26 21:58:40.452624 Epoch 50, Training Loss 0.005787534939358606\n",
      "2022-03-26 21:58:40.472019 Epoch 50, Training Loss 0.006687080387569145\n",
      "2022-03-26 21:58:40.492008 Epoch 50, Training Loss 0.007698479820700253\n",
      "2022-03-26 21:58:40.511729 Epoch 50, Training Loss 0.00863756952078446\n",
      "2022-03-26 21:58:40.531737 Epoch 50, Training Loss 0.009598029848864622\n",
      "2022-03-26 21:58:40.551745 Epoch 50, Training Loss 0.010715637213128912\n",
      "2022-03-26 21:58:40.572768 Epoch 50, Training Loss 0.011561630708177376\n",
      "2022-03-26 21:58:40.591817 Epoch 50, Training Loss 0.012439455293938327\n",
      "2022-03-26 21:58:40.611149 Epoch 50, Training Loss 0.013362925452039674\n",
      "2022-03-26 21:58:40.631565 Epoch 50, Training Loss 0.014355662686135763\n",
      "2022-03-26 21:58:40.650585 Epoch 50, Training Loss 0.015530671686162728\n",
      "2022-03-26 21:58:40.669634 Epoch 50, Training Loss 0.016535350428822704\n",
      "2022-03-26 21:58:40.688040 Epoch 50, Training Loss 0.01769420062489522\n",
      "2022-03-26 21:58:40.708162 Epoch 50, Training Loss 0.01862472699731207\n",
      "2022-03-26 21:58:40.727215 Epoch 50, Training Loss 0.019764784047060915\n",
      "2022-03-26 21:58:40.746873 Epoch 50, Training Loss 0.020868694309688286\n",
      "2022-03-26 21:58:40.765950 Epoch 50, Training Loss 0.021757499260060927\n",
      "2022-03-26 21:58:40.785658 Epoch 50, Training Loss 0.022660421440973307\n",
      "2022-03-26 21:58:40.805097 Epoch 50, Training Loss 0.023684765219383532\n",
      "2022-03-26 21:58:40.824196 Epoch 50, Training Loss 0.024748182967495737\n",
      "2022-03-26 21:58:40.843237 Epoch 50, Training Loss 0.02570145590530942\n",
      "2022-03-26 21:58:40.863004 Epoch 50, Training Loss 0.026613979388380903\n",
      "2022-03-26 21:58:40.882317 Epoch 50, Training Loss 0.02761948070562709\n",
      "2022-03-26 21:58:40.901352 Epoch 50, Training Loss 0.028707638802125936\n",
      "2022-03-26 21:58:40.920350 Epoch 50, Training Loss 0.029679431634790757\n",
      "2022-03-26 21:58:40.940452 Epoch 50, Training Loss 0.030818531885171484\n",
      "2022-03-26 21:58:40.959441 Epoch 50, Training Loss 0.03174075209881034\n",
      "2022-03-26 21:58:40.979463 Epoch 50, Training Loss 0.032604044127037456\n",
      "2022-03-26 21:58:40.998473 Epoch 50, Training Loss 0.03374472946462119\n",
      "2022-03-26 21:58:41.018556 Epoch 50, Training Loss 0.03480371459365806\n",
      "2022-03-26 21:58:41.037041 Epoch 50, Training Loss 0.0359228616937652\n",
      "2022-03-26 21:58:41.057109 Epoch 50, Training Loss 0.036852944918605676\n",
      "2022-03-26 21:58:41.076188 Epoch 50, Training Loss 0.03799453400589926\n",
      "2022-03-26 21:58:41.095358 Epoch 50, Training Loss 0.03908377809597708\n",
      "2022-03-26 21:58:41.115472 Epoch 50, Training Loss 0.04009208296570937\n",
      "2022-03-26 21:58:41.135357 Epoch 50, Training Loss 0.04094063603054837\n",
      "2022-03-26 21:58:41.154380 Epoch 50, Training Loss 0.04196342391431179\n",
      "2022-03-26 21:58:41.173349 Epoch 50, Training Loss 0.042834188169835474\n",
      "2022-03-26 21:58:41.192993 Epoch 50, Training Loss 0.043790377085775975\n",
      "2022-03-26 21:58:41.213075 Epoch 50, Training Loss 0.04498914989364117\n",
      "2022-03-26 21:58:41.233188 Epoch 50, Training Loss 0.04594570886143638\n",
      "2022-03-26 21:58:41.252172 Epoch 50, Training Loss 0.04698997210053837\n",
      "2022-03-26 21:58:41.270691 Epoch 50, Training Loss 0.04804119673531378\n",
      "2022-03-26 21:58:41.290706 Epoch 50, Training Loss 0.049428729450001437\n",
      "2022-03-26 21:58:41.310047 Epoch 50, Training Loss 0.05049260650449397\n",
      "2022-03-26 21:58:41.329178 Epoch 50, Training Loss 0.05168929536019445\n",
      "2022-03-26 21:58:41.348433 Epoch 50, Training Loss 0.05283923588140541\n",
      "2022-03-26 21:58:41.368479 Epoch 50, Training Loss 0.053758384778981316\n",
      "2022-03-26 21:58:41.387502 Epoch 50, Training Loss 0.05515758003420232\n",
      "2022-03-26 21:58:41.407704 Epoch 50, Training Loss 0.05616789873298782\n",
      "2022-03-26 21:58:41.427709 Epoch 50, Training Loss 0.05719335038033898\n",
      "2022-03-26 21:58:41.447754 Epoch 50, Training Loss 0.058290337510121144\n",
      "2022-03-26 21:58:41.466794 Epoch 50, Training Loss 0.059204742274320946\n",
      "2022-03-26 21:58:41.485800 Epoch 50, Training Loss 0.06039727130509399\n",
      "2022-03-26 21:58:41.504789 Epoch 50, Training Loss 0.06146543753116637\n",
      "2022-03-26 21:58:41.524816 Epoch 50, Training Loss 0.06279592814347933\n",
      "2022-03-26 21:58:41.543730 Epoch 50, Training Loss 0.0637889433547359\n",
      "2022-03-26 21:58:41.563628 Epoch 50, Training Loss 0.06536826643797443\n",
      "2022-03-26 21:58:41.583646 Epoch 50, Training Loss 0.06629514793300872\n",
      "2022-03-26 21:58:41.602989 Epoch 50, Training Loss 0.0671780454685621\n",
      "2022-03-26 21:58:41.622014 Epoch 50, Training Loss 0.06814357142924043\n",
      "2022-03-26 21:58:41.642112 Epoch 50, Training Loss 0.06924304457576684\n",
      "2022-03-26 21:58:41.662161 Epoch 50, Training Loss 0.07022213920607896\n",
      "2022-03-26 21:58:41.681115 Epoch 50, Training Loss 0.07110372978403136\n",
      "2022-03-26 21:58:41.701543 Epoch 50, Training Loss 0.07207632987090694\n",
      "2022-03-26 21:58:41.721555 Epoch 50, Training Loss 0.07332405371739127\n",
      "2022-03-26 21:58:41.740569 Epoch 50, Training Loss 0.07440382143115754\n",
      "2022-03-26 21:58:41.760603 Epoch 50, Training Loss 0.07557537442887835\n",
      "2022-03-26 21:58:41.779626 Epoch 50, Training Loss 0.07655191695903574\n",
      "2022-03-26 21:58:41.798648 Epoch 50, Training Loss 0.07737844298257852\n",
      "2022-03-26 21:58:41.818582 Epoch 50, Training Loss 0.07843722826074762\n",
      "2022-03-26 21:58:41.838604 Epoch 50, Training Loss 0.07927099983100697\n",
      "2022-03-26 21:58:41.858924 Epoch 50, Training Loss 0.07998452276524985\n",
      "2022-03-26 21:58:41.878975 Epoch 50, Training Loss 0.08106245706453348\n",
      "2022-03-26 21:58:41.898377 Epoch 50, Training Loss 0.08197512170847725\n",
      "2022-03-26 21:58:41.924443 Epoch 50, Training Loss 0.0827519325039271\n",
      "2022-03-26 21:58:41.949988 Epoch 50, Training Loss 0.08362686496866328\n",
      "2022-03-26 21:58:41.976974 Epoch 50, Training Loss 0.08498409977349479\n",
      "2022-03-26 21:58:42.003107 Epoch 50, Training Loss 0.08619029191143983\n",
      "2022-03-26 21:58:42.030444 Epoch 50, Training Loss 0.08714849145516106\n",
      "2022-03-26 21:58:42.056360 Epoch 50, Training Loss 0.08829582484481889\n",
      "2022-03-26 21:58:42.083837 Epoch 50, Training Loss 0.08907155452481925\n",
      "2022-03-26 21:58:42.109800 Epoch 50, Training Loss 0.09007043683010599\n",
      "2022-03-26 21:58:42.136687 Epoch 50, Training Loss 0.09115887557149238\n",
      "2022-03-26 21:58:42.161743 Epoch 50, Training Loss 0.09230982921922298\n",
      "2022-03-26 21:58:42.187810 Epoch 50, Training Loss 0.0934573931004995\n",
      "2022-03-26 21:58:42.214030 Epoch 50, Training Loss 0.0944279512328565\n",
      "2022-03-26 21:58:42.239771 Epoch 50, Training Loss 0.09543451377193031\n",
      "2022-03-26 21:58:42.265983 Epoch 50, Training Loss 0.09630709566423655\n",
      "2022-03-26 21:58:42.292557 Epoch 50, Training Loss 0.09697518804494072\n",
      "2022-03-26 21:58:42.318654 Epoch 50, Training Loss 0.0980402765524052\n",
      "2022-03-26 21:58:42.336687 Epoch 50, Training Loss 0.0992209245939084\n",
      "2022-03-26 21:58:42.354593 Epoch 50, Training Loss 0.10043463370074397\n",
      "2022-03-26 21:58:42.373633 Epoch 50, Training Loss 0.10123671755156553\n",
      "2022-03-26 21:58:42.391910 Epoch 50, Training Loss 0.102328809402178\n",
      "2022-03-26 21:58:42.410786 Epoch 50, Training Loss 0.1033577809248434\n",
      "2022-03-26 21:58:42.428678 Epoch 50, Training Loss 0.10450227745353718\n",
      "2022-03-26 21:58:42.447712 Epoch 50, Training Loss 0.10534113188228948\n",
      "2022-03-26 21:58:42.467335 Epoch 50, Training Loss 0.10611694609112751\n",
      "2022-03-26 21:58:42.485129 Epoch 50, Training Loss 0.10719797442026456\n",
      "2022-03-26 21:58:42.503032 Epoch 50, Training Loss 0.10821795562649017\n",
      "2022-03-26 21:58:42.522051 Epoch 50, Training Loss 0.10933893583619686\n",
      "2022-03-26 21:58:42.540449 Epoch 50, Training Loss 0.1102555404081369\n",
      "2022-03-26 21:58:42.558691 Epoch 50, Training Loss 0.11141613583125727\n",
      "2022-03-26 21:58:42.576706 Epoch 50, Training Loss 0.11237668770048624\n",
      "2022-03-26 21:58:42.595733 Epoch 50, Training Loss 0.11340569176942186\n",
      "2022-03-26 21:58:42.614730 Epoch 50, Training Loss 0.11432818492965015\n",
      "2022-03-26 21:58:42.632778 Epoch 50, Training Loss 0.11547016937409521\n",
      "2022-03-26 21:58:42.650731 Epoch 50, Training Loss 0.11656271992132182\n",
      "2022-03-26 21:58:42.669735 Epoch 50, Training Loss 0.1177765554784204\n",
      "2022-03-26 21:58:42.688622 Epoch 50, Training Loss 0.11912588024383311\n",
      "2022-03-26 21:58:42.706632 Epoch 50, Training Loss 0.12025981394531172\n",
      "2022-03-26 21:58:42.724693 Epoch 50, Training Loss 0.12130639528679421\n",
      "2022-03-26 21:58:42.742710 Epoch 50, Training Loss 0.12241525928992444\n",
      "2022-03-26 21:58:42.761075 Epoch 50, Training Loss 0.12313535108285792\n",
      "2022-03-26 21:58:42.780135 Epoch 50, Training Loss 0.12439476788196417\n",
      "2022-03-26 21:58:42.798160 Epoch 50, Training Loss 0.12530621154533933\n",
      "2022-03-26 21:58:42.816464 Epoch 50, Training Loss 0.12631495841933638\n",
      "2022-03-26 21:58:42.834461 Epoch 50, Training Loss 0.1273736751750302\n",
      "2022-03-26 21:58:42.852699 Epoch 50, Training Loss 0.1285739675964541\n",
      "2022-03-26 21:58:42.871126 Epoch 50, Training Loss 0.12941591003361871\n",
      "2022-03-26 21:58:42.891162 Epoch 50, Training Loss 0.13037407825060207\n",
      "2022-03-26 21:58:42.909212 Epoch 50, Training Loss 0.13160152249324047\n",
      "2022-03-26 21:58:42.927487 Epoch 50, Training Loss 0.13249329395611267\n",
      "2022-03-26 21:58:42.946492 Epoch 50, Training Loss 0.13344086367455896\n",
      "2022-03-26 21:58:42.965434 Epoch 50, Training Loss 0.13460230126100428\n",
      "2022-03-26 21:58:42.983456 Epoch 50, Training Loss 0.13571436737504455\n",
      "2022-03-26 21:58:43.001062 Epoch 50, Training Loss 0.13657874112848736\n",
      "2022-03-26 21:58:43.018863 Epoch 50, Training Loss 0.1375700221647082\n",
      "2022-03-26 21:58:43.036910 Epoch 50, Training Loss 0.13852230690019515\n",
      "2022-03-26 21:58:43.056003 Epoch 50, Training Loss 0.13960230342872307\n",
      "2022-03-26 21:58:43.075095 Epoch 50, Training Loss 0.14101850064209356\n",
      "2022-03-26 21:58:43.094635 Epoch 50, Training Loss 0.1421321496329344\n",
      "2022-03-26 21:58:43.114655 Epoch 50, Training Loss 0.1431888950145458\n",
      "2022-03-26 21:58:43.132676 Epoch 50, Training Loss 0.14398931969157266\n",
      "2022-03-26 21:58:43.150685 Epoch 50, Training Loss 0.14531957844029303\n",
      "2022-03-26 21:58:43.169050 Epoch 50, Training Loss 0.1463271228553694\n",
      "2022-03-26 21:58:43.187471 Epoch 50, Training Loss 0.14749969179977845\n",
      "2022-03-26 21:58:43.205512 Epoch 50, Training Loss 0.14847914901230952\n",
      "2022-03-26 21:58:43.223554 Epoch 50, Training Loss 0.14939519450487687\n",
      "2022-03-26 21:58:43.241487 Epoch 50, Training Loss 0.15073233827605576\n",
      "2022-03-26 21:58:43.260342 Epoch 50, Training Loss 0.15196302411196483\n",
      "2022-03-26 21:58:43.279874 Epoch 50, Training Loss 0.15316093135672762\n",
      "2022-03-26 21:58:43.297762 Epoch 50, Training Loss 0.15433157587905064\n",
      "2022-03-26 21:58:43.317669 Epoch 50, Training Loss 0.15548417162712272\n",
      "2022-03-26 21:58:43.336686 Epoch 50, Training Loss 0.1566406811594658\n",
      "2022-03-26 21:58:43.354714 Epoch 50, Training Loss 0.15753316406703666\n",
      "2022-03-26 21:58:43.372731 Epoch 50, Training Loss 0.15846007909921125\n",
      "2022-03-26 21:58:43.390769 Epoch 50, Training Loss 0.15937788536786424\n",
      "2022-03-26 21:58:43.408637 Epoch 50, Training Loss 0.16044650998566767\n",
      "2022-03-26 21:58:43.427610 Epoch 50, Training Loss 0.16142100819846247\n",
      "2022-03-26 21:58:43.446615 Epoch 50, Training Loss 0.1623093011739004\n",
      "2022-03-26 21:58:43.465521 Epoch 50, Training Loss 0.16320573856763523\n",
      "2022-03-26 21:58:43.483974 Epoch 50, Training Loss 0.16432726596627395\n",
      "2022-03-26 21:58:43.501999 Epoch 50, Training Loss 0.1654440849028585\n",
      "2022-03-26 21:58:43.521922 Epoch 50, Training Loss 0.16652752226575865\n",
      "2022-03-26 21:58:43.541921 Epoch 50, Training Loss 0.16739066040424436\n",
      "2022-03-26 21:58:43.560715 Epoch 50, Training Loss 0.168282425998117\n",
      "2022-03-26 21:58:43.584465 Epoch 50, Training Loss 0.16946761260557053\n",
      "2022-03-26 21:58:43.603483 Epoch 50, Training Loss 0.170519201118318\n",
      "2022-03-26 21:58:43.621315 Epoch 50, Training Loss 0.17155171477276346\n",
      "2022-03-26 21:58:43.641370 Epoch 50, Training Loss 0.17254431069354573\n",
      "2022-03-26 21:58:43.659339 Epoch 50, Training Loss 0.17358389382472122\n",
      "2022-03-26 21:58:43.678374 Epoch 50, Training Loss 0.17452324198944794\n",
      "2022-03-26 21:58:43.697383 Epoch 50, Training Loss 0.17554510661098352\n",
      "2022-03-26 21:58:43.716322 Epoch 50, Training Loss 0.17661624521855504\n",
      "2022-03-26 21:58:43.734336 Epoch 50, Training Loss 0.1778411238699618\n",
      "2022-03-26 21:58:43.753478 Epoch 50, Training Loss 0.1787824157406302\n",
      "2022-03-26 21:58:43.771624 Epoch 50, Training Loss 0.1798726508532034\n",
      "2022-03-26 21:58:43.791668 Epoch 50, Training Loss 0.18103883623162195\n",
      "2022-03-26 21:58:43.811187 Epoch 50, Training Loss 0.18205176838828474\n",
      "2022-03-26 21:58:43.829097 Epoch 50, Training Loss 0.18332711334728524\n",
      "2022-03-26 21:58:43.847992 Epoch 50, Training Loss 0.18430669304659908\n",
      "2022-03-26 21:58:43.866028 Epoch 50, Training Loss 0.18515543026082656\n",
      "2022-03-26 21:58:43.884092 Epoch 50, Training Loss 0.18635047099474447\n",
      "2022-03-26 21:58:43.903150 Epoch 50, Training Loss 0.18745060992972626\n",
      "2022-03-26 21:58:43.921158 Epoch 50, Training Loss 0.18862867507788225\n",
      "2022-03-26 21:58:43.940139 Epoch 50, Training Loss 0.18957353888265313\n",
      "2022-03-26 21:58:43.958102 Epoch 50, Training Loss 0.19053955799173516\n",
      "2022-03-26 21:58:43.977105 Epoch 50, Training Loss 0.19182036012944664\n",
      "2022-03-26 21:58:43.996189 Epoch 50, Training Loss 0.19292320299636373\n",
      "2022-03-26 21:58:44.015596 Epoch 50, Training Loss 0.1939291520344327\n",
      "2022-03-26 21:58:44.033633 Epoch 50, Training Loss 0.19529084194346766\n",
      "2022-03-26 21:58:44.052664 Epoch 50, Training Loss 0.19638193514950744\n",
      "2022-03-26 21:58:44.072341 Epoch 50, Training Loss 0.19730275785526658\n",
      "2022-03-26 21:58:44.091793 Epoch 50, Training Loss 0.19827400784358343\n",
      "2022-03-26 21:58:44.111370 Epoch 50, Training Loss 0.19918098878067778\n",
      "2022-03-26 21:58:44.129722 Epoch 50, Training Loss 0.20010276615162334\n",
      "2022-03-26 21:58:44.155022 Epoch 50, Training Loss 0.20112944464854268\n",
      "2022-03-26 21:58:44.182623 Epoch 50, Training Loss 0.20259663005314216\n",
      "2022-03-26 21:58:44.207734 Epoch 50, Training Loss 0.20352026835426956\n",
      "2022-03-26 21:58:44.233768 Epoch 50, Training Loss 0.20490164425976745\n",
      "2022-03-26 21:58:44.260710 Epoch 50, Training Loss 0.20597400971690713\n",
      "2022-03-26 21:58:44.287444 Epoch 50, Training Loss 0.2070155498164389\n",
      "2022-03-26 21:58:44.314747 Epoch 50, Training Loss 0.2082549739066902\n",
      "2022-03-26 21:58:44.340722 Epoch 50, Training Loss 0.20905529812473775\n",
      "2022-03-26 21:58:44.359663 Epoch 50, Training Loss 0.21002091753208424\n",
      "2022-03-26 21:58:44.378603 Epoch 50, Training Loss 0.2107617180518177\n",
      "2022-03-26 21:58:44.397628 Epoch 50, Training Loss 0.21173796210142656\n",
      "2022-03-26 21:58:44.416558 Epoch 50, Training Loss 0.21283204140870468\n",
      "2022-03-26 21:58:44.434573 Epoch 50, Training Loss 0.21389036875246736\n",
      "2022-03-26 21:58:44.452615 Epoch 50, Training Loss 0.2153152574206252\n",
      "2022-03-26 21:58:44.471641 Epoch 50, Training Loss 0.21639809172476648\n",
      "2022-03-26 21:58:44.491662 Epoch 50, Training Loss 0.21719282248136026\n",
      "2022-03-26 21:58:44.510536 Epoch 50, Training Loss 0.2182437917765449\n",
      "2022-03-26 21:58:44.529152 Epoch 50, Training Loss 0.21943100715232322\n",
      "2022-03-26 21:58:44.547805 Epoch 50, Training Loss 0.22047719565193977\n",
      "2022-03-26 21:58:44.566892 Epoch 50, Training Loss 0.22147601454154306\n",
      "2022-03-26 21:58:44.584599 Epoch 50, Training Loss 0.2223034299090695\n",
      "2022-03-26 21:58:44.604591 Epoch 50, Training Loss 0.22358778042866445\n",
      "2022-03-26 21:58:44.623516 Epoch 50, Training Loss 0.2247806781393183\n",
      "2022-03-26 21:58:44.641535 Epoch 50, Training Loss 0.22566631825073905\n",
      "2022-03-26 21:58:44.660402 Epoch 50, Training Loss 0.22698136760145807\n",
      "2022-03-26 21:58:44.679315 Epoch 50, Training Loss 0.22805419488026357\n",
      "2022-03-26 21:58:44.698279 Epoch 50, Training Loss 0.22902790900996273\n",
      "2022-03-26 21:58:44.717259 Epoch 50, Training Loss 0.23003083284553663\n",
      "2022-03-26 21:58:44.735306 Epoch 50, Training Loss 0.23097666976092113\n",
      "2022-03-26 21:58:44.756400 Epoch 50, Training Loss 0.23199033782915082\n",
      "2022-03-26 21:58:44.775687 Epoch 50, Training Loss 0.23282821533625084\n",
      "2022-03-26 21:58:44.793706 Epoch 50, Training Loss 0.2338800446304214\n",
      "2022-03-26 21:58:44.812527 Epoch 50, Training Loss 0.23485148135963305\n",
      "2022-03-26 21:58:44.833265 Epoch 50, Training Loss 0.23623341962199687\n",
      "2022-03-26 21:58:44.855806 Epoch 50, Training Loss 0.23753072904503864\n",
      "2022-03-26 21:58:44.880847 Epoch 50, Training Loss 0.23850703757742178\n",
      "2022-03-26 21:58:44.899299 Epoch 50, Training Loss 0.23961047001202088\n",
      "2022-03-26 21:58:44.917646 Epoch 50, Training Loss 0.24049037687308952\n",
      "2022-03-26 21:58:44.936302 Epoch 50, Training Loss 0.24154845718532572\n",
      "2022-03-26 21:58:44.955313 Epoch 50, Training Loss 0.2426268693126376\n",
      "2022-03-26 21:58:44.974306 Epoch 50, Training Loss 0.24373717213530674\n",
      "2022-03-26 21:58:44.993322 Epoch 50, Training Loss 0.24464682590626086\n",
      "2022-03-26 21:58:45.010326 Epoch 50, Training Loss 0.24581260716213899\n",
      "2022-03-26 21:58:45.029320 Epoch 50, Training Loss 0.24693933396083315\n",
      "2022-03-26 21:58:45.047329 Epoch 50, Training Loss 0.24795314601010374\n",
      "2022-03-26 21:58:45.066333 Epoch 50, Training Loss 0.24899137256395482\n",
      "2022-03-26 21:58:45.085349 Epoch 50, Training Loss 0.25016612630061175\n",
      "2022-03-26 21:58:45.104353 Epoch 50, Training Loss 0.2512713662346306\n",
      "2022-03-26 21:58:45.122357 Epoch 50, Training Loss 0.25213614197643214\n",
      "2022-03-26 21:58:45.140361 Epoch 50, Training Loss 0.25297218713614034\n",
      "2022-03-26 21:58:45.158359 Epoch 50, Training Loss 0.2539443318801158\n",
      "2022-03-26 21:58:45.177364 Epoch 50, Training Loss 0.2549697511336383\n",
      "2022-03-26 21:58:45.195357 Epoch 50, Training Loss 0.25639383430066315\n",
      "2022-03-26 21:58:45.213358 Epoch 50, Training Loss 0.2575435683397991\n",
      "2022-03-26 21:58:45.230361 Epoch 50, Training Loss 0.25865188462045186\n",
      "2022-03-26 21:58:45.248365 Epoch 50, Training Loss 0.25963166981097074\n",
      "2022-03-26 21:58:45.266369 Epoch 50, Training Loss 0.2606923657152659\n",
      "2022-03-26 21:58:45.283373 Epoch 50, Training Loss 0.2617634057693774\n",
      "2022-03-26 21:58:45.301377 Epoch 50, Training Loss 0.2626621301674172\n",
      "2022-03-26 21:58:45.319381 Epoch 50, Training Loss 0.2634885854580823\n",
      "2022-03-26 21:58:45.337386 Epoch 50, Training Loss 0.2643108026450857\n",
      "2022-03-26 21:58:45.355390 Epoch 50, Training Loss 0.2653076439867239\n",
      "2022-03-26 21:58:45.372399 Epoch 50, Training Loss 0.26618058884235296\n",
      "2022-03-26 21:58:45.390404 Epoch 50, Training Loss 0.26719646876120506\n",
      "2022-03-26 21:58:45.408406 Epoch 50, Training Loss 0.26801367336526855\n",
      "2022-03-26 21:58:45.426412 Epoch 50, Training Loss 0.2691718992369864\n",
      "2022-03-26 21:58:45.443411 Epoch 50, Training Loss 0.27062331654531574\n",
      "2022-03-26 21:58:45.462420 Epoch 50, Training Loss 0.2718726045945111\n",
      "2022-03-26 21:58:45.480425 Epoch 50, Training Loss 0.2728268126087725\n",
      "2022-03-26 21:58:45.498428 Epoch 50, Training Loss 0.273971272246612\n",
      "2022-03-26 21:58:45.517435 Epoch 50, Training Loss 0.2747859201772743\n",
      "2022-03-26 21:58:45.536431 Epoch 50, Training Loss 0.2759427159948422\n",
      "2022-03-26 21:58:45.554435 Epoch 50, Training Loss 0.2770000963717166\n",
      "2022-03-26 21:58:45.572443 Epoch 50, Training Loss 0.2779418487682977\n",
      "2022-03-26 21:58:45.590446 Epoch 50, Training Loss 0.2789479610712632\n",
      "2022-03-26 21:58:45.610448 Epoch 50, Training Loss 0.27993482999179675\n",
      "2022-03-26 21:58:45.628460 Epoch 50, Training Loss 0.28095980983256075\n",
      "2022-03-26 21:58:45.647476 Epoch 50, Training Loss 0.28173739304932793\n",
      "2022-03-26 21:58:45.665474 Epoch 50, Training Loss 0.2827767967568029\n",
      "2022-03-26 21:58:45.684479 Epoch 50, Training Loss 0.2840545342096587\n",
      "2022-03-26 21:58:45.702483 Epoch 50, Training Loss 0.2852670173815754\n",
      "2022-03-26 21:58:45.722487 Epoch 50, Training Loss 0.2863825761601138\n",
      "2022-03-26 21:58:45.740497 Epoch 50, Training Loss 0.28748987489344213\n",
      "2022-03-26 21:58:45.759502 Epoch 50, Training Loss 0.28835238840269006\n",
      "2022-03-26 21:58:45.777506 Epoch 50, Training Loss 0.28939570970547474\n",
      "2022-03-26 21:58:45.796498 Epoch 50, Training Loss 0.2906547032506265\n",
      "2022-03-26 21:58:45.814508 Epoch 50, Training Loss 0.2917190374773177\n",
      "2022-03-26 21:58:45.832512 Epoch 50, Training Loss 0.2925961520665747\n",
      "2022-03-26 21:58:45.851523 Epoch 50, Training Loss 0.2937743190456839\n",
      "2022-03-26 21:58:45.869527 Epoch 50, Training Loss 0.2947242035890174\n",
      "2022-03-26 21:58:45.887525 Epoch 50, Training Loss 0.2958587416450081\n",
      "2022-03-26 21:58:45.905535 Epoch 50, Training Loss 0.2969162660791441\n",
      "2022-03-26 21:58:45.924539 Epoch 50, Training Loss 0.29801803225141654\n",
      "2022-03-26 21:58:45.942537 Epoch 50, Training Loss 0.29901054387202347\n",
      "2022-03-26 21:58:45.960547 Epoch 50, Training Loss 0.29990342831062844\n",
      "2022-03-26 21:58:45.978546 Epoch 50, Training Loss 0.3011444541804321\n",
      "2022-03-26 21:58:45.996550 Epoch 50, Training Loss 0.3021876940794308\n",
      "2022-03-26 21:58:46.014559 Epoch 50, Training Loss 0.30335860910927853\n",
      "2022-03-26 21:58:46.032619 Epoch 50, Training Loss 0.3044423813100361\n",
      "2022-03-26 21:58:46.050568 Epoch 50, Training Loss 0.305602860877581\n",
      "2022-03-26 21:58:46.068560 Epoch 50, Training Loss 0.30685473616470765\n",
      "2022-03-26 21:58:46.086576 Epoch 50, Training Loss 0.3078245153969816\n",
      "2022-03-26 21:58:46.104567 Epoch 50, Training Loss 0.30906774206539556\n",
      "2022-03-26 21:58:46.123578 Epoch 50, Training Loss 0.3100371756364622\n",
      "2022-03-26 21:58:46.141588 Epoch 50, Training Loss 0.31116232489380996\n",
      "2022-03-26 21:58:46.160587 Epoch 50, Training Loss 0.3120629839275194\n",
      "2022-03-26 21:58:46.177590 Epoch 50, Training Loss 0.312876174821878\n",
      "2022-03-26 21:58:46.197595 Epoch 50, Training Loss 0.31402473254581853\n",
      "2022-03-26 21:58:46.215599 Epoch 50, Training Loss 0.3149419433777899\n",
      "2022-03-26 21:58:46.233603 Epoch 50, Training Loss 0.31594755681579373\n",
      "2022-03-26 21:58:46.251601 Epoch 50, Training Loss 0.31678152656006386\n",
      "2022-03-26 21:58:46.269617 Epoch 50, Training Loss 0.31761833766232367\n",
      "2022-03-26 21:58:46.288616 Epoch 50, Training Loss 0.3188678844810447\n",
      "2022-03-26 21:58:46.306626 Epoch 50, Training Loss 0.3200028374829256\n",
      "2022-03-26 21:58:46.325630 Epoch 50, Training Loss 0.3209811638840629\n",
      "2022-03-26 21:58:46.344628 Epoch 50, Training Loss 0.3222421903134612\n",
      "2022-03-26 21:58:46.362638 Epoch 50, Training Loss 0.32310091557405185\n",
      "2022-03-26 21:58:46.380643 Epoch 50, Training Loss 0.3241152957729671\n",
      "2022-03-26 21:58:46.398641 Epoch 50, Training Loss 0.32509648731297547\n",
      "2022-03-26 21:58:46.416645 Epoch 50, Training Loss 0.32635647614898583\n",
      "2022-03-26 21:58:46.434655 Epoch 50, Training Loss 0.32769450591043436\n",
      "2022-03-26 21:58:46.453659 Epoch 50, Training Loss 0.32875432512339425\n",
      "2022-03-26 21:58:46.471657 Epoch 50, Training Loss 0.3301529866807601\n",
      "2022-03-26 21:58:46.489962 Epoch 50, Training Loss 0.3313322537542914\n",
      "2022-03-26 21:58:46.508665 Epoch 50, Training Loss 0.3323800458627589\n",
      "2022-03-26 21:58:46.527664 Epoch 50, Training Loss 0.3335963502869277\n",
      "2022-03-26 21:58:46.545662 Epoch 50, Training Loss 0.33449718767724684\n",
      "2022-03-26 21:58:46.563680 Epoch 50, Training Loss 0.3356189411466994\n",
      "2022-03-26 21:58:46.581683 Epoch 50, Training Loss 0.3367458197009533\n",
      "2022-03-26 21:58:46.647691 Epoch 50, Training Loss 0.3375989930404117\n",
      "2022-03-26 21:58:46.665702 Epoch 50, Training Loss 0.33857629129953704\n",
      "2022-03-26 21:58:46.683705 Epoch 50, Training Loss 0.33961860747898326\n",
      "2022-03-26 21:58:46.702703 Epoch 50, Training Loss 0.340678281110266\n",
      "2022-03-26 21:58:46.721720 Epoch 50, Training Loss 0.3418317737481783\n",
      "2022-03-26 21:58:46.740724 Epoch 50, Training Loss 0.34272304794672503\n",
      "2022-03-26 21:58:46.759722 Epoch 50, Training Loss 0.3437280557344637\n",
      "2022-03-26 21:58:46.785736 Epoch 50, Training Loss 0.3448380668602331\n",
      "2022-03-26 21:58:46.812728 Epoch 50, Training Loss 0.34604283648988476\n",
      "2022-03-26 21:58:46.837746 Epoch 50, Training Loss 0.34718056133641\n",
      "2022-03-26 21:58:46.863742 Epoch 50, Training Loss 0.3486222867160807\n",
      "2022-03-26 21:58:46.890753 Epoch 50, Training Loss 0.3494511649126897\n",
      "2022-03-26 21:58:46.915747 Epoch 50, Training Loss 0.3504857397292886\n",
      "2022-03-26 21:58:46.941764 Epoch 50, Training Loss 0.3517233885615073\n",
      "2022-03-26 21:58:46.967764 Epoch 50, Training Loss 0.3526937357909844\n",
      "2022-03-26 21:58:46.985780 Epoch 50, Training Loss 0.3537135799522595\n",
      "2022-03-26 21:58:47.004778 Epoch 50, Training Loss 0.3549855147938594\n",
      "2022-03-26 21:58:47.023783 Epoch 50, Training Loss 0.3557813558584589\n",
      "2022-03-26 21:58:47.042787 Epoch 50, Training Loss 0.3568451261276479\n",
      "2022-03-26 21:58:47.060791 Epoch 50, Training Loss 0.35778121128106666\n",
      "2022-03-26 21:58:47.078796 Epoch 50, Training Loss 0.3591016630077606\n",
      "2022-03-26 21:58:47.097786 Epoch 50, Training Loss 0.36035011979320164\n",
      "2022-03-26 21:58:47.115803 Epoch 50, Training Loss 0.36129326100849435\n",
      "2022-03-26 21:58:47.133802 Epoch 50, Training Loss 0.3621897964983645\n",
      "2022-03-26 21:58:47.150805 Epoch 50, Training Loss 0.3633343145975371\n",
      "2022-03-26 21:58:47.169821 Epoch 50, Training Loss 0.36442511465848254\n",
      "2022-03-26 21:58:47.188826 Epoch 50, Training Loss 0.3654620562063154\n",
      "2022-03-26 21:58:47.207831 Epoch 50, Training Loss 0.3668732140070337\n",
      "2022-03-26 21:58:47.226834 Epoch 50, Training Loss 0.36777825687852356\n",
      "2022-03-26 21:58:47.244838 Epoch 50, Training Loss 0.3688795770068303\n",
      "2022-03-26 21:58:47.264827 Epoch 50, Training Loss 0.3701572638490926\n",
      "2022-03-26 21:58:47.283835 Epoch 50, Training Loss 0.37112011522283334\n",
      "2022-03-26 21:58:47.301833 Epoch 50, Training Loss 0.37250871685764675\n",
      "2022-03-26 21:58:47.320836 Epoch 50, Training Loss 0.37339261524817524\n",
      "2022-03-26 21:58:47.338841 Epoch 50, Training Loss 0.37463547819105864\n",
      "2022-03-26 21:58:47.357864 Epoch 50, Training Loss 0.3755092678777397\n",
      "2022-03-26 21:58:47.376862 Epoch 50, Training Loss 0.3766930075100316\n",
      "2022-03-26 21:58:47.396861 Epoch 50, Training Loss 0.37765938660982623\n",
      "2022-03-26 21:58:47.414861 Epoch 50, Training Loss 0.3784081336025082\n",
      "2022-03-26 21:58:47.432870 Epoch 50, Training Loss 0.37942136934651133\n",
      "2022-03-26 21:58:47.450879 Epoch 50, Training Loss 0.38046830663900544\n",
      "2022-03-26 21:58:47.468889 Epoch 50, Training Loss 0.3815142378172911\n",
      "2022-03-26 21:58:47.486887 Epoch 50, Training Loss 0.3828251761243776\n",
      "2022-03-26 21:58:47.504897 Epoch 50, Training Loss 0.38381662781891007\n",
      "2022-03-26 21:58:47.522896 Epoch 50, Training Loss 0.3845581817809883\n",
      "2022-03-26 21:58:47.541900 Epoch 50, Training Loss 0.386042868999569\n",
      "2022-03-26 21:58:47.558897 Epoch 50, Training Loss 0.38713464697303673\n",
      "2022-03-26 21:58:47.578914 Epoch 50, Training Loss 0.3882005146092466\n",
      "2022-03-26 21:58:47.596907 Epoch 50, Training Loss 0.3895716252534286\n",
      "2022-03-26 21:58:47.615911 Epoch 50, Training Loss 0.39062201801468344\n",
      "2022-03-26 21:58:47.633921 Epoch 50, Training Loss 0.391407546713529\n",
      "2022-03-26 21:58:47.652916 Epoch 50, Training Loss 0.39275302492139286\n",
      "2022-03-26 21:58:47.671915 Epoch 50, Training Loss 0.3939125205549743\n",
      "2022-03-26 21:58:47.690939 Epoch 50, Training Loss 0.39513708540545706\n",
      "2022-03-26 21:58:47.708944 Epoch 50, Training Loss 0.3960746742422928\n",
      "2022-03-26 21:58:47.727936 Epoch 50, Training Loss 0.3976103915735279\n",
      "2022-03-26 21:58:47.745948 Epoch 50, Training Loss 0.3988353214452944\n",
      "2022-03-26 21:58:47.765951 Epoch 50, Training Loss 0.39988306827862247\n",
      "2022-03-26 21:58:47.783955 Epoch 50, Training Loss 0.40115686816632595\n",
      "2022-03-26 21:58:47.802959 Epoch 50, Training Loss 0.4021688293465568\n",
      "2022-03-26 21:58:47.821969 Epoch 50, Training Loss 0.40304703640815853\n",
      "2022-03-26 21:58:47.840968 Epoch 50, Training Loss 0.4041736151861108\n",
      "2022-03-26 21:58:47.859959 Epoch 50, Training Loss 0.4053302125247848\n",
      "2022-03-26 21:58:47.879972 Epoch 50, Training Loss 0.406395567500073\n",
      "2022-03-26 21:58:47.900388 Epoch 50, Training Loss 0.4074823126158751\n",
      "2022-03-26 21:58:47.921139 Epoch 50, Training Loss 0.4082256281162467\n",
      "2022-03-26 21:58:47.939144 Epoch 50, Training Loss 0.40936782940879196\n",
      "2022-03-26 21:58:47.958228 Epoch 50, Training Loss 0.41018618692827347\n",
      "2022-03-26 21:58:47.979238 Epoch 50, Training Loss 0.41105702443196035\n",
      "2022-03-26 21:58:47.998169 Epoch 50, Training Loss 0.4119903029078413\n",
      "2022-03-26 21:58:48.017075 Epoch 50, Training Loss 0.41304033163868253\n",
      "2022-03-26 21:58:48.036019 Epoch 50, Training Loss 0.414210330952159\n",
      "2022-03-26 21:58:48.054651 Epoch 50, Training Loss 0.4152984446119469\n",
      "2022-03-26 21:58:48.072539 Epoch 50, Training Loss 0.4163145009056686\n",
      "2022-03-26 21:58:48.091467 Epoch 50, Training Loss 0.4174544678624634\n",
      "2022-03-26 21:58:48.110332 Epoch 50, Training Loss 0.41838586589564447\n",
      "2022-03-26 21:58:48.128961 Epoch 50, Training Loss 0.4196858610338567\n",
      "2022-03-26 21:58:48.147317 Epoch 50, Training Loss 0.42086084503347004\n",
      "2022-03-26 21:58:48.166324 Epoch 50, Training Loss 0.42196292508288724\n",
      "2022-03-26 21:58:48.185232 Epoch 50, Training Loss 0.42288180598822395\n",
      "2022-03-26 21:58:48.204656 Epoch 50, Training Loss 0.4239566035740211\n",
      "2022-03-26 21:58:48.224641 Epoch 50, Training Loss 0.42489655266332504\n",
      "2022-03-26 21:58:48.242658 Epoch 50, Training Loss 0.425732490275522\n",
      "2022-03-26 21:58:48.261601 Epoch 50, Training Loss 0.4269469939076992\n",
      "2022-03-26 21:58:48.281632 Epoch 50, Training Loss 0.4279012557338266\n",
      "2022-03-26 21:58:48.299519 Epoch 50, Training Loss 0.42902483712986605\n",
      "2022-03-26 21:58:48.318454 Epoch 50, Training Loss 0.4298627157802777\n",
      "2022-03-26 21:58:48.338474 Epoch 50, Training Loss 0.43105180344313304\n",
      "2022-03-26 21:58:48.357301 Epoch 50, Training Loss 0.4322792349569023\n",
      "2022-03-26 21:58:48.375963 Epoch 50, Training Loss 0.4333354201920502\n",
      "2022-03-26 21:58:48.395989 Epoch 50, Training Loss 0.4345654817036046\n",
      "2022-03-26 21:58:48.416033 Epoch 50, Training Loss 0.435553883957436\n",
      "2022-03-26 21:58:48.433039 Epoch 50, Training Loss 0.4367601787647628\n",
      "2022-03-26 21:58:48.453153 Epoch 50, Training Loss 0.4380441099176626\n",
      "2022-03-26 21:58:48.471211 Epoch 50, Training Loss 0.4392048845358212\n",
      "2022-03-26 21:58:48.490215 Epoch 50, Training Loss 0.44044711736157116\n",
      "2022-03-26 21:58:48.509009 Epoch 50, Training Loss 0.44150836022613604\n",
      "2022-03-26 21:58:48.528028 Epoch 50, Training Loss 0.44262196340829213\n",
      "2022-03-26 21:58:48.546975 Epoch 50, Training Loss 0.44389216430351863\n",
      "2022-03-26 21:58:48.566002 Epoch 50, Training Loss 0.4448789999155742\n",
      "2022-03-26 21:58:48.584089 Epoch 50, Training Loss 0.44567013991153454\n",
      "2022-03-26 21:58:48.603113 Epoch 50, Training Loss 0.4465879008288274\n",
      "2022-03-26 21:58:48.623128 Epoch 50, Training Loss 0.447548316262872\n",
      "2022-03-26 21:58:48.642153 Epoch 50, Training Loss 0.4483666609772636\n",
      "2022-03-26 21:58:48.660115 Epoch 50, Training Loss 0.449586599150582\n",
      "2022-03-26 21:58:48.679133 Epoch 50, Training Loss 0.45063871762636676\n",
      "2022-03-26 21:58:48.697173 Epoch 50, Training Loss 0.45170174413324926\n",
      "2022-03-26 21:58:48.716161 Epoch 50, Training Loss 0.4527988816466173\n",
      "2022-03-26 21:58:48.734258 Epoch 50, Training Loss 0.4538570577683656\n",
      "2022-03-26 21:58:48.753250 Epoch 50, Training Loss 0.45481810758790703\n",
      "2022-03-26 21:58:48.772272 Epoch 50, Training Loss 0.45607205707093945\n",
      "2022-03-26 21:58:48.791313 Epoch 50, Training Loss 0.4576295689701119\n",
      "2022-03-26 21:58:48.810317 Epoch 50, Training Loss 0.4585115523137095\n",
      "2022-03-26 21:58:48.829321 Epoch 50, Training Loss 0.4593987522832573\n",
      "2022-03-26 21:58:48.848350 Epoch 50, Training Loss 0.46052261394308047\n",
      "2022-03-26 21:58:48.868377 Epoch 50, Training Loss 0.461590948617062\n",
      "2022-03-26 21:58:48.886422 Epoch 50, Training Loss 0.46257537115565345\n",
      "2022-03-26 21:58:48.906483 Epoch 50, Training Loss 0.4635610356355262\n",
      "2022-03-26 21:58:48.926497 Epoch 50, Training Loss 0.46462942305428295\n",
      "2022-03-26 21:58:48.946455 Epoch 50, Training Loss 0.46608142451861934\n",
      "2022-03-26 21:58:48.965468 Epoch 50, Training Loss 0.4673844863234274\n",
      "2022-03-26 21:58:48.983465 Epoch 50, Training Loss 0.468325674533844\n",
      "2022-03-26 21:58:49.002160 Epoch 50, Training Loss 0.46938898542043195\n",
      "2022-03-26 21:58:49.022130 Epoch 50, Training Loss 0.47062604155991694\n",
      "2022-03-26 21:58:49.040159 Epoch 50, Training Loss 0.4717705149174956\n",
      "2022-03-26 21:58:49.059254 Epoch 50, Training Loss 0.47259534068424686\n",
      "2022-03-26 21:58:49.079285 Epoch 50, Training Loss 0.47352471276927177\n",
      "2022-03-26 21:58:49.098325 Epoch 50, Training Loss 0.4745329642844627\n",
      "2022-03-26 21:58:49.116110 Epoch 50, Training Loss 0.4757577993375871\n",
      "2022-03-26 21:58:49.136253 Epoch 50, Training Loss 0.47688723517500836\n",
      "2022-03-26 21:58:49.155245 Epoch 50, Training Loss 0.4778916745082192\n",
      "2022-03-26 21:58:49.174282 Epoch 50, Training Loss 0.4788812020093279\n",
      "2022-03-26 21:58:49.192207 Epoch 50, Training Loss 0.4798607292687497\n",
      "2022-03-26 21:58:49.211339 Epoch 50, Training Loss 0.4811359186611517\n",
      "2022-03-26 21:58:49.231321 Epoch 50, Training Loss 0.4819584615395197\n",
      "2022-03-26 21:58:49.250330 Epoch 50, Training Loss 0.4830026445943681\n",
      "2022-03-26 21:58:49.269390 Epoch 50, Training Loss 0.483971313975961\n",
      "2022-03-26 21:58:49.288352 Epoch 50, Training Loss 0.485015476984746\n",
      "2022-03-26 21:58:49.306343 Epoch 50, Training Loss 0.4858580122670859\n",
      "2022-03-26 21:58:49.324277 Epoch 50, Training Loss 0.48675804179342813\n",
      "2022-03-26 21:58:49.344487 Epoch 50, Training Loss 0.48806591694007445\n",
      "2022-03-26 21:58:49.364474 Epoch 50, Training Loss 0.4890305284801347\n",
      "2022-03-26 21:58:49.383328 Epoch 50, Training Loss 0.4901561772122103\n",
      "2022-03-26 21:58:49.402278 Epoch 50, Training Loss 0.4914559994817085\n",
      "2022-03-26 21:58:49.421276 Epoch 50, Training Loss 0.49228664043614323\n",
      "2022-03-26 21:58:49.441925 Epoch 50, Training Loss 0.49330295488962433\n",
      "2022-03-26 21:58:49.460961 Epoch 50, Training Loss 0.49433170788733244\n",
      "2022-03-26 21:58:49.479959 Epoch 50, Training Loss 0.49564996064471467\n",
      "2022-03-26 21:58:49.497939 Epoch 50, Training Loss 0.4968415591722864\n",
      "2022-03-26 21:58:49.517326 Epoch 50, Training Loss 0.498071907502611\n",
      "2022-03-26 21:58:49.536471 Epoch 50, Training Loss 0.4990787997727504\n",
      "2022-03-26 21:58:49.554954 Epoch 50, Training Loss 0.49992613246678697\n",
      "2022-03-26 21:58:49.575049 Epoch 50, Training Loss 0.5011031723693203\n",
      "2022-03-26 21:58:49.594068 Epoch 50, Training Loss 0.5019605368604441\n",
      "2022-03-26 21:58:49.612591 Epoch 50, Training Loss 0.503184710850801\n",
      "2022-03-26 21:58:49.631457 Epoch 50, Training Loss 0.5043679801247004\n",
      "2022-03-26 21:58:49.650136 Epoch 50, Training Loss 0.5051206293923166\n",
      "2022-03-26 21:58:49.670197 Epoch 50, Training Loss 0.5062939012446976\n",
      "2022-03-26 21:58:49.688759 Epoch 50, Training Loss 0.5072293531559312\n",
      "2022-03-26 21:58:49.707788 Epoch 50, Training Loss 0.5081203021966588\n",
      "2022-03-26 21:58:49.726809 Epoch 50, Training Loss 0.5092263363511361\n",
      "2022-03-26 21:58:49.746816 Epoch 50, Training Loss 0.5101271673389103\n",
      "2022-03-26 21:58:49.766843 Epoch 50, Training Loss 0.5113191952943192\n",
      "2022-03-26 21:58:49.786748 Epoch 50, Training Loss 0.512523252183519\n",
      "2022-03-26 21:58:49.813600 Epoch 50, Training Loss 0.5134609224241408\n",
      "2022-03-26 21:58:49.840516 Epoch 50, Training Loss 0.5145742645501481\n",
      "2022-03-26 21:58:49.866455 Epoch 50, Training Loss 0.5154992809990788\n",
      "2022-03-26 21:58:49.893465 Epoch 50, Training Loss 0.5166556284860577\n",
      "2022-03-26 21:58:49.920110 Epoch 50, Training Loss 0.5177477243001504\n",
      "2022-03-26 21:58:49.946455 Epoch 50, Training Loss 0.5190691732994431\n",
      "2022-03-26 21:58:49.971330 Epoch 50, Training Loss 0.5199809599562984\n",
      "2022-03-26 21:58:49.999059 Epoch 50, Training Loss 0.5213872940491533\n",
      "2022-03-26 21:58:50.017072 Epoch 50, Training Loss 0.5225283606430454\n",
      "2022-03-26 21:58:50.035161 Epoch 50, Training Loss 0.5235598435639726\n",
      "2022-03-26 21:58:50.054040 Epoch 50, Training Loss 0.5247218669070612\n",
      "2022-03-26 21:58:50.074788 Epoch 50, Training Loss 0.5258786025864389\n",
      "2022-03-26 21:58:50.093023 Epoch 50, Training Loss 0.5270164935942501\n",
      "2022-03-26 21:58:50.112105 Epoch 50, Training Loss 0.5281368147229295\n",
      "2022-03-26 21:58:50.131975 Epoch 50, Training Loss 0.5292916499135439\n",
      "2022-03-26 21:58:50.150065 Epoch 50, Training Loss 0.5303930443571047\n",
      "2022-03-26 21:58:50.168714 Epoch 50, Training Loss 0.5314169061153441\n",
      "2022-03-26 21:58:50.187730 Epoch 50, Training Loss 0.5325786818933609\n",
      "2022-03-26 21:58:50.206682 Epoch 50, Training Loss 0.5333010039823439\n",
      "2022-03-26 21:58:50.225716 Epoch 50, Training Loss 0.5343359478599276\n",
      "2022-03-26 21:58:50.244763 Epoch 50, Training Loss 0.5355335823867632\n",
      "2022-03-26 21:58:50.264132 Epoch 50, Training Loss 0.5370622207898923\n",
      "2022-03-26 21:58:50.282938 Epoch 50, Training Loss 0.5381070903652464\n",
      "2022-03-26 21:58:50.300978 Epoch 50, Training Loss 0.539077126995072\n",
      "2022-03-26 21:58:50.319988 Epoch 50, Training Loss 0.5401919264622661\n",
      "2022-03-26 21:58:50.338069 Epoch 50, Training Loss 0.541449136136438\n",
      "2022-03-26 21:58:50.356123 Epoch 50, Training Loss 0.5427198894798299\n",
      "2022-03-26 21:58:50.376664 Epoch 50, Training Loss 0.5437302715942988\n",
      "2022-03-26 21:58:50.394698 Epoch 50, Training Loss 0.5448548296071074\n",
      "2022-03-26 21:58:50.413219 Epoch 50, Training Loss 0.5461000662935359\n",
      "2022-03-26 21:58:50.431114 Epoch 50, Training Loss 0.5471871784885826\n",
      "2022-03-26 21:58:50.449994 Epoch 50, Training Loss 0.5485743377214808\n",
      "2022-03-26 21:58:50.468140 Epoch 50, Training Loss 0.5495982169342772\n",
      "2022-03-26 21:58:50.487126 Epoch 50, Training Loss 0.5505503569265155\n",
      "2022-03-26 21:58:50.505946 Epoch 50, Training Loss 0.5515223998395379\n",
      "2022-03-26 21:58:50.524997 Epoch 50, Training Loss 0.5524507570449654\n",
      "2022-03-26 21:58:50.544090 Epoch 50, Training Loss 0.5538394903892752\n",
      "2022-03-26 21:58:50.562944 Epoch 50, Training Loss 0.5550014515362127\n",
      "2022-03-26 21:58:50.581987 Epoch 50, Training Loss 0.5557467637159635\n",
      "2022-03-26 21:58:50.600043 Epoch 50, Training Loss 0.5564523357564531\n",
      "2022-03-26 21:58:50.618109 Epoch 50, Training Loss 0.5576026089813398\n",
      "2022-03-26 21:58:50.637038 Epoch 50, Training Loss 0.5586095246512567\n",
      "2022-03-26 21:58:50.655013 Epoch 50, Training Loss 0.5597647828671634\n",
      "2022-03-26 21:58:50.673453 Epoch 50, Training Loss 0.5606448735727374\n",
      "2022-03-26 21:58:50.693501 Epoch 50, Training Loss 0.5619838013673377\n",
      "2022-03-26 21:58:50.711477 Epoch 50, Training Loss 0.5626991096207553\n",
      "2022-03-26 21:58:50.730324 Epoch 50, Training Loss 0.5638906238481517\n",
      "2022-03-26 21:58:50.749323 Epoch 50, Training Loss 0.5648914019165137\n",
      "2022-03-26 21:58:50.767279 Epoch 50, Training Loss 0.5657950958327564\n",
      "2022-03-26 21:58:50.785329 Epoch 50, Training Loss 0.5667620385851702\n",
      "2022-03-26 21:58:50.804342 Epoch 50, Training Loss 0.5678274542321939\n",
      "2022-03-26 21:58:50.823365 Epoch 50, Training Loss 0.5688464068390829\n",
      "2022-03-26 21:58:50.841451 Epoch 50, Training Loss 0.5698306312036636\n",
      "2022-03-26 21:58:50.860495 Epoch 50, Training Loss 0.5708611071719538\n",
      "2022-03-26 21:58:50.879332 Epoch 50, Training Loss 0.5720792553004097\n",
      "2022-03-26 21:58:50.898279 Epoch 50, Training Loss 0.573095501383857\n",
      "2022-03-26 21:58:50.916704 Epoch 50, Training Loss 0.5741330875307703\n",
      "2022-03-26 21:58:50.934719 Epoch 50, Training Loss 0.5751709488346753\n",
      "2022-03-26 21:58:50.953732 Epoch 50, Training Loss 0.5763751693698757\n",
      "2022-03-26 21:58:50.973697 Epoch 50, Training Loss 0.5774417110263844\n",
      "2022-03-26 21:58:50.991706 Epoch 50, Training Loss 0.5782071079134636\n",
      "2022-03-26 21:58:51.010741 Epoch 50, Training Loss 0.5794803343922891\n",
      "2022-03-26 21:58:51.029174 Epoch 50, Training Loss 0.580332311904034\n",
      "2022-03-26 21:58:51.048305 Epoch 50, Training Loss 0.5817325419324744\n",
      "2022-03-26 21:58:51.066206 Epoch 50, Training Loss 0.5829559167480225\n",
      "2022-03-26 21:58:51.084242 Epoch 50, Training Loss 0.5839437880479467\n",
      "2022-03-26 21:58:51.103798 Epoch 50, Training Loss 0.5847254741527236\n",
      "2022-03-26 21:58:51.123192 Epoch 50, Training Loss 0.5858850896053607\n",
      "2022-03-26 21:58:51.141104 Epoch 50, Training Loss 0.586776838506884\n",
      "2022-03-26 21:58:51.159089 Epoch 50, Training Loss 0.5879428888983129\n",
      "2022-03-26 21:58:51.177511 Epoch 50, Training Loss 0.588807739793797\n",
      "2022-03-26 21:58:51.196439 Epoch 50, Training Loss 0.5897381195936666\n",
      "2022-03-26 21:58:51.215480 Epoch 50, Training Loss 0.5908774158838764\n",
      "2022-03-26 21:58:51.233500 Epoch 50, Training Loss 0.591778342300059\n",
      "2022-03-26 21:58:51.251353 Epoch 50, Training Loss 0.5930464719719899\n",
      "2022-03-26 21:58:51.269779 Epoch 50, Training Loss 0.594012581860013\n",
      "2022-03-26 21:58:51.288812 Epoch 50, Training Loss 0.5949090119365537\n",
      "2022-03-26 21:58:51.307837 Epoch 50, Training Loss 0.5959721610826605\n",
      "2022-03-26 21:58:51.325735 Epoch 50, Training Loss 0.5968179394826865\n",
      "2022-03-26 21:58:51.343735 Epoch 50, Training Loss 0.5975457805654277\n",
      "2022-03-26 21:58:51.362784 Epoch 50, Training Loss 0.5986139333766439\n",
      "2022-03-26 21:58:51.381654 Epoch 50, Training Loss 0.5997634923366635\n",
      "2022-03-26 21:58:51.400665 Epoch 50, Training Loss 0.6009257273448397\n",
      "2022-03-26 21:58:51.418676 Epoch 50, Training Loss 0.6016148723604734\n",
      "2022-03-26 21:58:51.438705 Epoch 50, Training Loss 0.6027903020229486\n",
      "2022-03-26 21:58:51.457536 Epoch 50, Training Loss 0.6037902536294649\n",
      "2022-03-26 21:58:51.475545 Epoch 50, Training Loss 0.6045488890479592\n",
      "2022-03-26 21:58:51.493569 Epoch 50, Training Loss 0.6053468470683183\n",
      "2022-03-26 21:58:51.512569 Epoch 50, Training Loss 0.6064490792543992\n",
      "2022-03-26 21:58:51.531418 Epoch 50, Training Loss 0.6076614500768959\n",
      "2022-03-26 21:58:51.549354 Epoch 50, Training Loss 0.6087447337024962\n",
      "2022-03-26 21:58:51.569444 Epoch 50, Training Loss 0.6097093598769449\n",
      "2022-03-26 21:58:51.588476 Epoch 50, Training Loss 0.6107424613459945\n",
      "2022-03-26 21:58:51.606481 Epoch 50, Training Loss 0.6117358977532448\n",
      "2022-03-26 21:58:51.624506 Epoch 50, Training Loss 0.6128650930379053\n",
      "2022-03-26 21:58:51.644516 Epoch 50, Training Loss 0.613938099816632\n",
      "2022-03-26 21:58:51.662520 Epoch 50, Training Loss 0.6150222692800604\n",
      "2022-03-26 21:58:51.680463 Epoch 50, Training Loss 0.6161337362226013\n",
      "2022-03-26 21:58:51.699467 Epoch 50, Training Loss 0.617161715503239\n",
      "2022-03-26 21:58:51.719478 Epoch 50, Training Loss 0.6181516804353661\n",
      "2022-03-26 21:58:51.737482 Epoch 50, Training Loss 0.6189818302993579\n",
      "2022-03-26 21:58:51.755486 Epoch 50, Training Loss 0.6200099322954407\n",
      "2022-03-26 21:58:51.774430 Epoch 50, Training Loss 0.6211843078246202\n",
      "2022-03-26 21:58:51.792440 Epoch 50, Training Loss 0.6222667350336109\n",
      "2022-03-26 21:58:51.811249 Epoch 50, Training Loss 0.6233127197188795\n",
      "2022-03-26 21:58:51.829163 Epoch 50, Training Loss 0.624297082347943\n",
      "2022-03-26 21:58:51.847686 Epoch 50, Training Loss 0.6252919687791858\n",
      "2022-03-26 21:58:51.866476 Epoch 50, Training Loss 0.6262439912389917\n",
      "2022-03-26 21:58:51.884483 Epoch 50, Training Loss 0.6272216980414622\n",
      "2022-03-26 21:58:51.903439 Epoch 50, Training Loss 0.628228454333742\n",
      "2022-03-26 21:58:51.922451 Epoch 50, Training Loss 0.6295843153353542\n",
      "2022-03-26 21:58:51.940997 Epoch 50, Training Loss 0.6305491094241666\n",
      "2022-03-26 21:58:51.960053 Epoch 50, Training Loss 0.6313146496825206\n",
      "2022-03-26 21:58:51.979020 Epoch 50, Training Loss 0.6325580258198711\n",
      "2022-03-26 21:58:51.997277 Epoch 50, Training Loss 0.6336467883471028\n",
      "2022-03-26 21:58:52.016356 Epoch 50, Training Loss 0.6349791869178147\n",
      "2022-03-26 21:58:52.035334 Epoch 50, Training Loss 0.6358340261384959\n",
      "2022-03-26 21:58:52.054360 Epoch 50, Training Loss 0.6368870200098627\n",
      "2022-03-26 21:58:52.073378 Epoch 50, Training Loss 0.6381026841795353\n",
      "2022-03-26 21:58:52.091586 Epoch 50, Training Loss 0.6392420114915999\n",
      "2022-03-26 21:58:52.110530 Epoch 50, Training Loss 0.6401932965154233\n",
      "2022-03-26 21:58:52.129468 Epoch 50, Training Loss 0.641247818384634\n",
      "2022-03-26 21:58:52.147452 Epoch 50, Training Loss 0.6424959350729842\n",
      "2022-03-26 21:58:52.166508 Epoch 50, Training Loss 0.6438736555826329\n",
      "2022-03-26 21:58:52.183526 Epoch 50, Training Loss 0.645060161038128\n",
      "2022-03-26 21:58:52.203043 Epoch 50, Training Loss 0.6457562733946554\n",
      "2022-03-26 21:58:52.221076 Epoch 50, Training Loss 0.6469423108546021\n",
      "2022-03-26 21:58:52.239761 Epoch 50, Training Loss 0.6478462931902512\n",
      "2022-03-26 21:58:52.258804 Epoch 50, Training Loss 0.6491348297547197\n",
      "2022-03-26 21:58:52.276710 Epoch 50, Training Loss 0.6501472033655552\n",
      "2022-03-26 21:58:52.296638 Epoch 50, Training Loss 0.6512741299388963\n",
      "2022-03-26 21:58:52.314684 Epoch 50, Training Loss 0.6522244881943363\n",
      "2022-03-26 21:58:52.332672 Epoch 50, Training Loss 0.6532502806247653\n",
      "2022-03-26 21:58:52.351553 Epoch 50, Training Loss 0.6543688639960326\n",
      "2022-03-26 21:58:52.369972 Epoch 50, Training Loss 0.6556837147154162\n",
      "2022-03-26 21:58:52.388896 Epoch 50, Training Loss 0.6570196753877509\n",
      "2022-03-26 21:58:52.408348 Epoch 50, Training Loss 0.657977786134271\n",
      "2022-03-26 21:58:52.426376 Epoch 50, Training Loss 0.6588935449605098\n",
      "2022-03-26 21:58:52.445378 Epoch 50, Training Loss 0.6602243207909567\n",
      "2022-03-26 21:58:52.464402 Epoch 50, Training Loss 0.6612432462632504\n",
      "2022-03-26 21:58:52.482458 Epoch 50, Training Loss 0.6622346894210561\n",
      "2022-03-26 21:58:52.501469 Epoch 50, Training Loss 0.663357225068085\n",
      "2022-03-26 21:58:52.519471 Epoch 50, Training Loss 0.6645336093957467\n",
      "2022-03-26 21:58:52.538458 Epoch 50, Training Loss 0.6656796057968188\n",
      "2022-03-26 21:58:52.557462 Epoch 50, Training Loss 0.6669744924663583\n",
      "2022-03-26 21:58:52.575506 Epoch 50, Training Loss 0.6684914971404063\n",
      "2022-03-26 21:58:52.594509 Epoch 50, Training Loss 0.6694158301176623\n",
      "2022-03-26 21:58:52.613770 Epoch 50, Training Loss 0.6705002813692897\n",
      "2022-03-26 21:58:52.635876 Epoch 50, Training Loss 0.6717536338912252\n",
      "2022-03-26 21:58:52.662932 Epoch 50, Training Loss 0.6732748604339102\n",
      "2022-03-26 21:58:52.688069 Epoch 50, Training Loss 0.6746771068066892\n",
      "2022-03-26 21:58:52.714109 Epoch 50, Training Loss 0.676057976606252\n",
      "2022-03-26 21:58:52.740200 Epoch 50, Training Loss 0.6771112230732618\n",
      "2022-03-26 21:58:52.767030 Epoch 50, Training Loss 0.6781451881236737\n",
      "2022-03-26 21:58:52.793130 Epoch 50, Training Loss 0.6791555615489745\n",
      "2022-03-26 21:58:52.819645 Epoch 50, Training Loss 0.6804849893388236\n",
      "2022-03-26 21:58:52.833617 Epoch 50, Training Loss 0.6818689855620684\n",
      "2022-03-26 21:58:52.848674 Epoch 50, Training Loss 0.6825340656215882\n",
      "2022-03-26 21:58:52.862560 Epoch 50, Training Loss 0.6837305309979812\n",
      "2022-03-26 21:58:52.877509 Epoch 50, Training Loss 0.6845623361485084\n",
      "2022-03-26 21:58:52.891486 Epoch 50, Training Loss 0.6857701444717319\n",
      "2022-03-26 21:58:52.906512 Epoch 50, Training Loss 0.6866904696845033\n",
      "2022-03-26 21:58:52.920515 Epoch 50, Training Loss 0.6878532172011598\n",
      "2022-03-26 21:58:52.935549 Epoch 50, Training Loss 0.6892615228205385\n",
      "2022-03-26 21:58:52.950544 Epoch 50, Training Loss 0.6903195679949983\n",
      "2022-03-26 21:58:52.965571 Epoch 50, Training Loss 0.6914298443690591\n",
      "2022-03-26 21:58:52.979570 Epoch 50, Training Loss 0.6925359199114163\n",
      "2022-03-26 21:58:52.994577 Epoch 50, Training Loss 0.6934437241090838\n",
      "2022-03-26 21:58:53.008579 Epoch 50, Training Loss 0.6946260092203574\n",
      "2022-03-26 21:58:53.023417 Epoch 50, Training Loss 0.6954405031850576\n",
      "2022-03-26 21:58:53.038432 Epoch 50, Training Loss 0.696382744842783\n",
      "2022-03-26 21:58:53.053723 Epoch 50, Training Loss 0.697396730599196\n",
      "2022-03-26 21:58:53.068610 Epoch 50, Training Loss 0.6984475397545359\n",
      "2022-03-26 21:58:53.083612 Epoch 50, Training Loss 0.6995918509143088\n",
      "2022-03-26 21:58:53.097562 Epoch 50, Training Loss 0.7007300026733857\n",
      "2022-03-26 21:58:53.113560 Epoch 50, Training Loss 0.7016426231855016\n",
      "2022-03-26 21:58:53.128063 Epoch 50, Training Loss 0.7027666074845492\n",
      "2022-03-26 21:58:53.142636 Epoch 50, Training Loss 0.7039329475911377\n",
      "2022-03-26 21:58:53.157501 Epoch 50, Training Loss 0.70502667940791\n",
      "2022-03-26 21:58:53.173504 Epoch 50, Training Loss 0.7060513717439169\n",
      "2022-03-26 21:58:53.187516 Epoch 50, Training Loss 0.7072472465617577\n",
      "2022-03-26 21:58:53.204515 Epoch 50, Training Loss 0.7083743553027473\n",
      "2022-03-26 21:58:53.218520 Epoch 50, Training Loss 0.7093417434131398\n",
      "2022-03-26 21:58:53.232457 Epoch 50, Training Loss 0.7102929203559065\n",
      "2022-03-26 21:58:53.247867 Epoch 50, Training Loss 0.7113590678747963\n",
      "2022-03-26 21:58:53.262989 Epoch 50, Training Loss 0.7127499117723206\n",
      "2022-03-26 21:58:53.277504 Epoch 50, Training Loss 0.7136542995262634\n",
      "2022-03-26 21:58:53.291515 Epoch 50, Training Loss 0.7150063127507944\n",
      "2022-03-26 21:58:53.306443 Epoch 50, Training Loss 0.7159372033822872\n",
      "2022-03-26 21:58:53.321451 Epoch 50, Training Loss 0.717083756195005\n",
      "2022-03-26 21:58:53.336027 Epoch 50, Training Loss 0.7183993848236016\n",
      "2022-03-26 21:58:53.350349 Epoch 50, Training Loss 0.7192965297747755\n",
      "2022-03-26 21:58:53.367346 Epoch 50, Training Loss 0.7203942570082672\n",
      "2022-03-26 21:58:53.382446 Epoch 50, Training Loss 0.7215409209508725\n",
      "2022-03-26 21:58:53.397454 Epoch 50, Training Loss 0.723008771915265\n",
      "2022-03-26 21:58:53.412334 Epoch 50, Training Loss 0.723961496901939\n",
      "2022-03-26 21:58:53.426340 Epoch 50, Training Loss 0.7252419781501945\n",
      "2022-03-26 21:58:53.442227 Epoch 50, Training Loss 0.7261677623709755\n",
      "2022-03-26 21:58:53.457289 Epoch 50, Training Loss 0.7272290895357156\n",
      "2022-03-26 21:58:53.471273 Epoch 50, Training Loss 0.7280289559717983\n",
      "2022-03-26 21:58:53.486326 Epoch 50, Training Loss 0.7292147076038449\n",
      "2022-03-26 21:58:53.501042 Epoch 50, Training Loss 0.7301893045225412\n",
      "2022-03-26 21:58:53.515771 Epoch 50, Training Loss 0.7312214128349138\n",
      "2022-03-26 21:58:53.530795 Epoch 50, Training Loss 0.7322870557722838\n",
      "2022-03-26 21:58:53.545323 Epoch 50, Training Loss 0.7334912104527359\n",
      "2022-03-26 21:58:53.559832 Epoch 50, Training Loss 0.7345061495023615\n",
      "2022-03-26 21:58:53.576329 Epoch 50, Training Loss 0.7355436913650054\n",
      "2022-03-26 21:58:53.592934 Epoch 50, Training Loss 0.7366234139560739\n",
      "2022-03-26 21:58:53.607703 Epoch 50, Training Loss 0.7379497432190439\n",
      "2022-03-26 21:58:53.622706 Epoch 50, Training Loss 0.7392568634751507\n",
      "2022-03-26 21:58:53.637120 Epoch 50, Training Loss 0.7405104499186397\n",
      "2022-03-26 21:58:53.651422 Epoch 50, Training Loss 0.7412896865926435\n",
      "2022-03-26 21:58:53.666331 Epoch 50, Training Loss 0.7421464061798038\n",
      "2022-03-26 21:58:53.681362 Epoch 50, Training Loss 0.7431600551166193\n",
      "2022-03-26 21:58:53.695524 Epoch 50, Training Loss 0.7440118537382092\n",
      "2022-03-26 21:58:53.710475 Epoch 50, Training Loss 0.7452504930593778\n",
      "2022-03-26 21:58:53.725491 Epoch 50, Training Loss 0.7462370888808804\n",
      "2022-03-26 21:58:53.740459 Epoch 50, Training Loss 0.7471771736431609\n",
      "2022-03-26 21:58:53.754463 Epoch 50, Training Loss 0.7481212159404365\n",
      "2022-03-26 21:58:53.770252 Epoch 50, Training Loss 0.749103836772387\n",
      "2022-03-26 21:58:53.784264 Epoch 50, Training Loss 0.7504435386651617\n",
      "2022-03-26 21:58:53.799140 Epoch 50, Training Loss 0.7518488335639925\n",
      "2022-03-26 21:58:53.813143 Epoch 50, Training Loss 0.7529956276154579\n",
      "2022-03-26 21:58:53.827191 Epoch 50, Training Loss 0.7541942476768932\n",
      "2022-03-26 21:58:53.841733 Epoch 50, Training Loss 0.755257087442881\n",
      "2022-03-26 21:58:53.856740 Epoch 50, Training Loss 0.756244356446254\n",
      "2022-03-26 21:58:53.870751 Epoch 50, Training Loss 0.757205645522803\n",
      "2022-03-26 21:58:53.884754 Epoch 50, Training Loss 0.758821314481823\n",
      "2022-03-26 21:58:53.899604 Epoch 50, Training Loss 0.7599226155549365\n",
      "2022-03-26 21:58:53.913607 Epoch 50, Training Loss 0.7609232593222958\n",
      "2022-03-26 21:58:53.928635 Epoch 50, Training Loss 0.7622013327563205\n",
      "2022-03-26 21:58:53.943729 Epoch 50, Training Loss 0.7633638193692698\n",
      "2022-03-26 21:58:53.958759 Epoch 50, Training Loss 0.7647687706648542\n",
      "2022-03-26 21:58:53.973770 Epoch 50, Training Loss 0.7659354574997407\n",
      "2022-03-26 21:58:53.987623 Epoch 50, Training Loss 0.766851891794473\n",
      "2022-03-26 21:58:54.001619 Epoch 50, Training Loss 0.7678484615614957\n",
      "2022-03-26 21:58:54.015637 Epoch 50, Training Loss 0.7686530199959455\n",
      "2022-03-26 21:58:54.030670 Epoch 50, Training Loss 0.7697165065714161\n",
      "2022-03-26 21:58:54.044673 Epoch 50, Training Loss 0.7705597290602486\n",
      "2022-03-26 21:58:54.058650 Epoch 50, Training Loss 0.7716178916909201\n",
      "2022-03-26 21:58:54.073653 Epoch 50, Training Loss 0.7724853182387779\n",
      "2022-03-26 21:58:54.087876 Epoch 50, Training Loss 0.7734271750578186\n",
      "2022-03-26 21:58:54.102462 Epoch 50, Training Loss 0.774551214159602\n",
      "2022-03-26 21:58:54.117070 Epoch 50, Training Loss 0.7754992105619377\n",
      "2022-03-26 21:58:54.131117 Epoch 50, Training Loss 0.7766086596357243\n",
      "2022-03-26 21:58:54.145158 Epoch 50, Training Loss 0.7776333024282285\n",
      "2022-03-26 21:58:54.160214 Epoch 50, Training Loss 0.7786892699006268\n",
      "2022-03-26 21:58:54.175255 Epoch 50, Training Loss 0.7797265984975469\n",
      "2022-03-26 21:58:54.190226 Epoch 50, Training Loss 0.7809648958923262\n",
      "2022-03-26 21:58:54.204243 Epoch 50, Training Loss 0.7821740108682677\n",
      "2022-03-26 21:58:54.219081 Epoch 50, Training Loss 0.7831868042269021\n",
      "2022-03-26 21:58:54.233110 Epoch 50, Training Loss 0.7843335168745816\n",
      "2022-03-26 21:58:54.247031 Epoch 50, Training Loss 0.785551434435198\n",
      "2022-03-26 21:58:54.262112 Epoch 50, Training Loss 0.786602689863166\n",
      "2022-03-26 21:58:54.276751 Epoch 50, Training Loss 0.7877390828278973\n",
      "2022-03-26 21:58:54.290638 Epoch 50, Training Loss 0.7885465360510989\n",
      "2022-03-26 21:58:54.304641 Epoch 50, Training Loss 0.7895392800688439\n",
      "2022-03-26 21:58:54.319697 Epoch 50, Training Loss 0.7906589782451425\n",
      "2022-03-26 21:58:54.333701 Epoch 50, Training Loss 0.7916813226764464\n",
      "2022-03-26 21:58:54.348622 Epoch 50, Training Loss 0.7925099368442965\n",
      "2022-03-26 21:58:54.362627 Epoch 50, Training Loss 0.7937115091649468\n",
      "2022-03-26 21:58:54.377455 Epoch 50, Training Loss 0.794948834287541\n",
      "2022-03-26 21:58:54.391508 Epoch 50, Training Loss 0.7958601377046931\n",
      "2022-03-26 21:58:54.405527 Epoch 50, Training Loss 0.796815554656641\n",
      "2022-03-26 21:58:54.420085 Epoch 50, Training Loss 0.7980461084781705\n",
      "2022-03-26 21:58:54.434107 Epoch 50, Training Loss 0.7991516649570611\n",
      "2022-03-26 21:58:54.448115 Epoch 50, Training Loss 0.8002371725523868\n",
      "2022-03-26 21:58:54.463142 Epoch 50, Training Loss 0.8012055372795486\n",
      "2022-03-26 21:58:54.477112 Epoch 50, Training Loss 0.8024765798045547\n",
      "2022-03-26 21:58:54.492123 Epoch 50, Training Loss 0.8038002939327903\n",
      "2022-03-26 21:58:54.506748 Epoch 50, Training Loss 0.8050809023172959\n",
      "2022-03-26 21:58:54.520765 Epoch 50, Training Loss 0.806017322308572\n",
      "2022-03-26 21:58:54.534648 Epoch 50, Training Loss 0.8067252715987623\n",
      "2022-03-26 21:58:54.549653 Epoch 50, Training Loss 0.8079095841826075\n",
      "2022-03-26 21:58:54.563656 Epoch 50, Training Loss 0.8089431991510074\n",
      "2022-03-26 21:58:54.577681 Epoch 50, Training Loss 0.8099574487075172\n",
      "2022-03-26 21:58:54.592692 Epoch 50, Training Loss 0.8109936315537719\n",
      "2022-03-26 21:58:54.608720 Epoch 50, Training Loss 0.8120867391224103\n",
      "2022-03-26 21:58:54.623748 Epoch 50, Training Loss 0.8133032693125098\n",
      "2022-03-26 21:58:54.642624 Epoch 50, Training Loss 0.8146063637398088\n",
      "2022-03-26 21:58:54.662640 Epoch 50, Training Loss 0.8156946317466629\n",
      "2022-03-26 21:58:54.681681 Epoch 50, Training Loss 0.8169905201096059\n",
      "2022-03-26 21:58:54.701519 Epoch 50, Training Loss 0.8180230414623495\n",
      "2022-03-26 21:58:54.721541 Epoch 50, Training Loss 0.8191604474773797\n",
      "2022-03-26 21:58:54.747544 Epoch 50, Training Loss 0.8201386880539262\n",
      "2022-03-26 21:58:54.771112 Epoch 50, Training Loss 0.8210604785348449\n",
      "2022-03-26 21:58:54.791965 Epoch 50, Training Loss 0.8222890950529776\n",
      "2022-03-26 21:58:54.811450 Epoch 50, Training Loss 0.8233344660847998\n",
      "2022-03-26 21:58:54.831225 Epoch 50, Training Loss 0.8245366293451061\n",
      "2022-03-26 21:58:54.851226 Epoch 50, Training Loss 0.8253475955837523\n",
      "2022-03-26 21:58:54.870967 Epoch 50, Training Loss 0.8265066887716503\n",
      "2022-03-26 21:58:54.890354 Epoch 50, Training Loss 0.8277434106830441\n",
      "2022-03-26 21:58:54.910359 Epoch 50, Training Loss 0.8287763101670443\n",
      "2022-03-26 21:58:54.930371 Epoch 50, Training Loss 0.8297044325362691\n",
      "2022-03-26 21:58:54.950366 Epoch 50, Training Loss 0.830998424069046\n",
      "2022-03-26 21:58:54.958388 Epoch 50, Training Loss 0.8320620964707621\n",
      "2022-03-26 22:10:16.967916 Epoch 100, Training Loss 0.000769231096862832\n",
      "2022-03-26 22:10:16.986464 Epoch 100, Training Loss 0.00159021320245455\n",
      "2022-03-26 22:10:17.004460 Epoch 100, Training Loss 0.002280172484610087\n",
      "2022-03-26 22:10:17.022139 Epoch 100, Training Loss 0.003180182193551222\n",
      "2022-03-26 22:10:17.039972 Epoch 100, Training Loss 0.004326798879277066\n",
      "2022-03-26 22:10:17.057668 Epoch 100, Training Loss 0.005379601893827433\n",
      "2022-03-26 22:10:17.075672 Epoch 100, Training Loss 0.006263979179475009\n",
      "2022-03-26 22:10:17.093684 Epoch 100, Training Loss 0.0070698185040212955\n",
      "2022-03-26 22:10:17.111692 Epoch 100, Training Loss 0.00805149389349896\n",
      "2022-03-26 22:10:17.129698 Epoch 100, Training Loss 0.008938524576709093\n",
      "2022-03-26 22:10:17.147503 Epoch 100, Training Loss 0.009776800566012293\n",
      "2022-03-26 22:10:17.166520 Epoch 100, Training Loss 0.01050846518762886\n",
      "2022-03-26 22:10:17.184528 Epoch 100, Training Loss 0.011930600609011053\n",
      "2022-03-26 22:10:17.202540 Epoch 100, Training Loss 0.012890861314885756\n",
      "2022-03-26 22:10:17.220583 Epoch 100, Training Loss 0.013735924916499106\n",
      "2022-03-26 22:10:17.239605 Epoch 100, Training Loss 0.014678116192293289\n",
      "2022-03-26 22:10:17.257995 Epoch 100, Training Loss 0.015834422977379217\n",
      "2022-03-26 22:10:17.276072 Epoch 100, Training Loss 0.016603565963028033\n",
      "2022-03-26 22:10:17.294105 Epoch 100, Training Loss 0.017298974649375663\n",
      "2022-03-26 22:10:17.312080 Epoch 100, Training Loss 0.01813951889267358\n",
      "2022-03-26 22:10:17.330606 Epoch 100, Training Loss 0.019234830461194754\n",
      "2022-03-26 22:10:17.348631 Epoch 100, Training Loss 0.020000703804328314\n",
      "2022-03-26 22:10:17.367647 Epoch 100, Training Loss 0.020903896492765382\n",
      "2022-03-26 22:10:17.385863 Epoch 100, Training Loss 0.02184669265661703\n",
      "2022-03-26 22:10:17.404899 Epoch 100, Training Loss 0.02300126092208316\n",
      "2022-03-26 22:10:17.423432 Epoch 100, Training Loss 0.02363256031594923\n",
      "2022-03-26 22:10:17.441472 Epoch 100, Training Loss 0.024572935250714002\n",
      "2022-03-26 22:10:17.459482 Epoch 100, Training Loss 0.025539247047565782\n",
      "2022-03-26 22:10:17.477529 Epoch 100, Training Loss 0.026306526511526474\n",
      "2022-03-26 22:10:17.495539 Epoch 100, Training Loss 0.02713324720292445\n",
      "2022-03-26 22:10:17.514012 Epoch 100, Training Loss 0.02799683572995998\n",
      "2022-03-26 22:10:17.532276 Epoch 100, Training Loss 0.028854364171967177\n",
      "2022-03-26 22:10:17.551391 Epoch 100, Training Loss 0.029841533554789357\n",
      "2022-03-26 22:10:17.570371 Epoch 100, Training Loss 0.03094562689971436\n",
      "2022-03-26 22:10:17.589234 Epoch 100, Training Loss 0.031664289858030235\n",
      "2022-03-26 22:10:17.607397 Epoch 100, Training Loss 0.03232315952515663\n",
      "2022-03-26 22:10:17.625426 Epoch 100, Training Loss 0.033249993915752986\n",
      "2022-03-26 22:10:17.644438 Epoch 100, Training Loss 0.03427669444047581\n",
      "2022-03-26 22:10:17.662433 Epoch 100, Training Loss 0.03533155480614099\n",
      "2022-03-26 22:10:17.680516 Epoch 100, Training Loss 0.03629351355840483\n",
      "2022-03-26 22:10:17.698484 Epoch 100, Training Loss 0.037213112234764395\n",
      "2022-03-26 22:10:17.717350 Epoch 100, Training Loss 0.03802846101543787\n",
      "2022-03-26 22:10:17.735378 Epoch 100, Training Loss 0.03879980258929455\n",
      "2022-03-26 22:10:17.753237 Epoch 100, Training Loss 0.03973039855127749\n",
      "2022-03-26 22:10:17.772371 Epoch 100, Training Loss 0.0403862668730109\n",
      "2022-03-26 22:10:17.790598 Epoch 100, Training Loss 0.041380147723590624\n",
      "2022-03-26 22:10:17.808620 Epoch 100, Training Loss 0.042406089318073006\n",
      "2022-03-26 22:10:17.826531 Epoch 100, Training Loss 0.043448755884414436\n",
      "2022-03-26 22:10:17.844537 Epoch 100, Training Loss 0.04432191995098768\n",
      "2022-03-26 22:10:17.862470 Epoch 100, Training Loss 0.045109339992103675\n",
      "2022-03-26 22:10:17.880697 Epoch 100, Training Loss 0.04594248342696968\n",
      "2022-03-26 22:10:17.899458 Epoch 100, Training Loss 0.04702141233112501\n",
      "2022-03-26 22:10:17.917348 Epoch 100, Training Loss 0.048013214793656486\n",
      "2022-03-26 22:10:17.936379 Epoch 100, Training Loss 0.04894374726373521\n",
      "2022-03-26 22:10:17.954237 Epoch 100, Training Loss 0.050016579954215636\n",
      "2022-03-26 22:10:17.971576 Epoch 100, Training Loss 0.05055139459612425\n",
      "2022-03-26 22:10:17.990580 Epoch 100, Training Loss 0.05134570819642538\n",
      "2022-03-26 22:10:18.008609 Epoch 100, Training Loss 0.0523027319584966\n",
      "2022-03-26 22:10:18.026662 Epoch 100, Training Loss 0.05327981779032656\n",
      "2022-03-26 22:10:18.044628 Epoch 100, Training Loss 0.05417552826654576\n",
      "2022-03-26 22:10:18.062068 Epoch 100, Training Loss 0.05490824160978312\n",
      "2022-03-26 22:10:18.081108 Epoch 100, Training Loss 0.05583944810016076\n",
      "2022-03-26 22:10:18.099224 Epoch 100, Training Loss 0.05651011803875799\n",
      "2022-03-26 22:10:18.118177 Epoch 100, Training Loss 0.05755987443277598\n",
      "2022-03-26 22:10:18.136212 Epoch 100, Training Loss 0.05866703101436196\n",
      "2022-03-26 22:10:18.154270 Epoch 100, Training Loss 0.059618906596737443\n",
      "2022-03-26 22:10:18.173231 Epoch 100, Training Loss 0.06046930275609731\n",
      "2022-03-26 22:10:18.191291 Epoch 100, Training Loss 0.0614215467134705\n",
      "2022-03-26 22:10:18.210268 Epoch 100, Training Loss 0.06236827868939666\n",
      "2022-03-26 22:10:18.228361 Epoch 100, Training Loss 0.06303979963292856\n",
      "2022-03-26 22:10:18.247368 Epoch 100, Training Loss 0.06385655552529923\n",
      "2022-03-26 22:10:18.265374 Epoch 100, Training Loss 0.06466012507143533\n",
      "2022-03-26 22:10:18.284379 Epoch 100, Training Loss 0.06579069271112037\n",
      "2022-03-26 22:10:18.302830 Epoch 100, Training Loss 0.066904071041995\n",
      "2022-03-26 22:10:18.320886 Epoch 100, Training Loss 0.06762121286233673\n",
      "2022-03-26 22:10:18.340803 Epoch 100, Training Loss 0.06856346191347712\n",
      "2022-03-26 22:10:18.358897 Epoch 100, Training Loss 0.06937185310951584\n",
      "2022-03-26 22:10:18.377877 Epoch 100, Training Loss 0.07032600273866482\n",
      "2022-03-26 22:10:18.396827 Epoch 100, Training Loss 0.07105788840052417\n",
      "2022-03-26 22:10:18.415886 Epoch 100, Training Loss 0.07175646421244687\n",
      "2022-03-26 22:10:18.434723 Epoch 100, Training Loss 0.07252591642577325\n",
      "2022-03-26 22:10:18.452780 Epoch 100, Training Loss 0.07341708818360058\n",
      "2022-03-26 22:10:18.470678 Epoch 100, Training Loss 0.07454199406801892\n",
      "2022-03-26 22:10:18.489949 Epoch 100, Training Loss 0.0752318730897001\n",
      "2022-03-26 22:10:18.508888 Epoch 100, Training Loss 0.07597110254685287\n",
      "2022-03-26 22:10:18.526868 Epoch 100, Training Loss 0.0770370538734719\n",
      "2022-03-26 22:10:18.544816 Epoch 100, Training Loss 0.0779101330301036\n",
      "2022-03-26 22:10:18.562885 Epoch 100, Training Loss 0.07856632193640979\n",
      "2022-03-26 22:10:18.580834 Epoch 100, Training Loss 0.07949375092525922\n",
      "2022-03-26 22:10:18.600631 Epoch 100, Training Loss 0.08040602592860951\n",
      "2022-03-26 22:10:18.619160 Epoch 100, Training Loss 0.0816513951629629\n",
      "2022-03-26 22:10:18.637006 Epoch 100, Training Loss 0.08240437820134565\n",
      "2022-03-26 22:10:18.655043 Epoch 100, Training Loss 0.0834945730693505\n",
      "2022-03-26 22:10:18.673958 Epoch 100, Training Loss 0.08457388749817753\n",
      "2022-03-26 22:10:18.691930 Epoch 100, Training Loss 0.08560944373345436\n",
      "2022-03-26 22:10:18.710038 Epoch 100, Training Loss 0.0863439251699716\n",
      "2022-03-26 22:10:18.728127 Epoch 100, Training Loss 0.08729895270998826\n",
      "2022-03-26 22:10:18.746160 Epoch 100, Training Loss 0.08808074384699087\n",
      "2022-03-26 22:10:18.765146 Epoch 100, Training Loss 0.08888928999986186\n",
      "2022-03-26 22:10:18.783586 Epoch 100, Training Loss 0.08966761850335105\n",
      "2022-03-26 22:10:18.801685 Epoch 100, Training Loss 0.09036198631881753\n",
      "2022-03-26 22:10:18.820689 Epoch 100, Training Loss 0.09114057641200092\n",
      "2022-03-26 22:10:18.839718 Epoch 100, Training Loss 0.09194801820208655\n",
      "2022-03-26 22:10:18.857639 Epoch 100, Training Loss 0.09287305141958739\n",
      "2022-03-26 22:10:18.875637 Epoch 100, Training Loss 0.09383906938535783\n",
      "2022-03-26 22:10:18.893678 Epoch 100, Training Loss 0.09475151321772114\n",
      "2022-03-26 22:10:18.912676 Epoch 100, Training Loss 0.09572451597894244\n",
      "2022-03-26 22:10:18.937994 Epoch 100, Training Loss 0.09630535920257763\n",
      "2022-03-26 22:10:18.963893 Epoch 100, Training Loss 0.09713192447981871\n",
      "2022-03-26 22:10:18.989677 Epoch 100, Training Loss 0.09801932369046809\n",
      "2022-03-26 22:10:19.015694 Epoch 100, Training Loss 0.0989196583285661\n",
      "2022-03-26 22:10:19.042571 Epoch 100, Training Loss 0.0999924080908451\n",
      "2022-03-26 22:10:19.068627 Epoch 100, Training Loss 0.10103777500674548\n",
      "2022-03-26 22:10:19.094644 Epoch 100, Training Loss 0.10203524760882873\n",
      "2022-03-26 22:10:19.120603 Epoch 100, Training Loss 0.10302989783189485\n",
      "2022-03-26 22:10:19.136604 Epoch 100, Training Loss 0.10401290914286739\n",
      "2022-03-26 22:10:19.150152 Epoch 100, Training Loss 0.10480663652920053\n",
      "2022-03-26 22:10:19.165173 Epoch 100, Training Loss 0.10575714288160319\n",
      "2022-03-26 22:10:19.179260 Epoch 100, Training Loss 0.10668827574271375\n",
      "2022-03-26 22:10:19.193288 Epoch 100, Training Loss 0.10761650123864488\n",
      "2022-03-26 22:10:19.207177 Epoch 100, Training Loss 0.10872216313086508\n",
      "2022-03-26 22:10:19.222200 Epoch 100, Training Loss 0.10953209886465536\n",
      "2022-03-26 22:10:19.236269 Epoch 100, Training Loss 0.11020684470910855\n",
      "2022-03-26 22:10:19.251247 Epoch 100, Training Loss 0.11125950816342288\n",
      "2022-03-26 22:10:19.265276 Epoch 100, Training Loss 0.11237225927355345\n",
      "2022-03-26 22:10:19.280321 Epoch 100, Training Loss 0.11340737541008483\n",
      "2022-03-26 22:10:19.294342 Epoch 100, Training Loss 0.11433770604755568\n",
      "2022-03-26 22:10:19.307654 Epoch 100, Training Loss 0.11515537879960921\n",
      "2022-03-26 22:10:19.322658 Epoch 100, Training Loss 0.11579362171537735\n",
      "2022-03-26 22:10:19.336697 Epoch 100, Training Loss 0.11669925179170526\n",
      "2022-03-26 22:10:19.350701 Epoch 100, Training Loss 0.11738329984800285\n",
      "2022-03-26 22:10:19.364715 Epoch 100, Training Loss 0.11820181041879727\n",
      "2022-03-26 22:10:19.379728 Epoch 100, Training Loss 0.1191061506871982\n",
      "2022-03-26 22:10:19.393769 Epoch 100, Training Loss 0.12019871137178767\n",
      "2022-03-26 22:10:19.407808 Epoch 100, Training Loss 0.12095657734157485\n",
      "2022-03-26 22:10:19.421831 Epoch 100, Training Loss 0.12168257426270439\n",
      "2022-03-26 22:10:19.435820 Epoch 100, Training Loss 0.12226850552784513\n",
      "2022-03-26 22:10:19.450664 Epoch 100, Training Loss 0.12328320509180084\n",
      "2022-03-26 22:10:19.464680 Epoch 100, Training Loss 0.12404967753021308\n",
      "2022-03-26 22:10:19.479694 Epoch 100, Training Loss 0.12487898797482785\n",
      "2022-03-26 22:10:19.492947 Epoch 100, Training Loss 0.1258997272728654\n",
      "2022-03-26 22:10:19.508131 Epoch 100, Training Loss 0.1269351806100982\n",
      "2022-03-26 22:10:19.522175 Epoch 100, Training Loss 0.12785753802112912\n",
      "2022-03-26 22:10:19.536323 Epoch 100, Training Loss 0.12870807335962114\n",
      "2022-03-26 22:10:19.550246 Epoch 100, Training Loss 0.12969008640712484\n",
      "2022-03-26 22:10:19.565362 Epoch 100, Training Loss 0.13063157187855762\n",
      "2022-03-26 22:10:19.579371 Epoch 100, Training Loss 0.13162381261053596\n",
      "2022-03-26 22:10:19.593235 Epoch 100, Training Loss 0.13253388010784792\n",
      "2022-03-26 22:10:19.606241 Epoch 100, Training Loss 0.1333669684350948\n",
      "2022-03-26 22:10:19.621133 Epoch 100, Training Loss 0.13437130433671615\n",
      "2022-03-26 22:10:19.634059 Epoch 100, Training Loss 0.1353759871190771\n",
      "2022-03-26 22:10:19.649008 Epoch 100, Training Loss 0.13651912280208314\n",
      "2022-03-26 22:10:19.662993 Epoch 100, Training Loss 0.13726907934221771\n",
      "2022-03-26 22:10:19.676883 Epoch 100, Training Loss 0.13806891887236739\n",
      "2022-03-26 22:10:19.690775 Epoch 100, Training Loss 0.13892221668034868\n",
      "2022-03-26 22:10:19.703797 Epoch 100, Training Loss 0.1399620178791568\n",
      "2022-03-26 22:10:19.718820 Epoch 100, Training Loss 0.14075653285474118\n",
      "2022-03-26 22:10:19.733836 Epoch 100, Training Loss 0.1415487796525516\n",
      "2022-03-26 22:10:19.747838 Epoch 100, Training Loss 0.14256790467082997\n",
      "2022-03-26 22:10:19.761846 Epoch 100, Training Loss 0.1432615694258829\n",
      "2022-03-26 22:10:19.774853 Epoch 100, Training Loss 0.14398434022655876\n",
      "2022-03-26 22:10:19.788773 Epoch 100, Training Loss 0.14497863731878188\n",
      "2022-03-26 22:10:19.802776 Epoch 100, Training Loss 0.14576957593945897\n",
      "2022-03-26 22:10:19.816633 Epoch 100, Training Loss 0.14624776239590268\n",
      "2022-03-26 22:10:19.830634 Epoch 100, Training Loss 0.14710396756906338\n",
      "2022-03-26 22:10:19.844637 Epoch 100, Training Loss 0.14793739401166092\n",
      "2022-03-26 22:10:19.858650 Epoch 100, Training Loss 0.14879411664765205\n",
      "2022-03-26 22:10:19.873652 Epoch 100, Training Loss 0.14986771901550194\n",
      "2022-03-26 22:10:19.887667 Epoch 100, Training Loss 0.15107894241047637\n",
      "2022-03-26 22:10:19.901670 Epoch 100, Training Loss 0.152055920389912\n",
      "2022-03-26 22:10:19.915679 Epoch 100, Training Loss 0.15309732146275318\n",
      "2022-03-26 22:10:19.929686 Epoch 100, Training Loss 0.1539230653849404\n",
      "2022-03-26 22:10:19.943709 Epoch 100, Training Loss 0.15507198477645054\n",
      "2022-03-26 22:10:19.957717 Epoch 100, Training Loss 0.1559430167193303\n",
      "2022-03-26 22:10:19.970724 Epoch 100, Training Loss 0.1567416037135112\n",
      "2022-03-26 22:10:19.985733 Epoch 100, Training Loss 0.1576787328628628\n",
      "2022-03-26 22:10:19.999740 Epoch 100, Training Loss 0.15840190328905346\n",
      "2022-03-26 22:10:20.013665 Epoch 100, Training Loss 0.15944866801771668\n",
      "2022-03-26 22:10:20.027673 Epoch 100, Training Loss 0.16045171578826806\n",
      "2022-03-26 22:10:20.041679 Epoch 100, Training Loss 0.16122438482311377\n",
      "2022-03-26 22:10:20.055689 Epoch 100, Training Loss 0.16235551954535268\n",
      "2022-03-26 22:10:20.069698 Epoch 100, Training Loss 0.1632106402493499\n",
      "2022-03-26 22:10:20.082708 Epoch 100, Training Loss 0.16424814887973657\n",
      "2022-03-26 22:10:20.096719 Epoch 100, Training Loss 0.16503396813217028\n",
      "2022-03-26 22:10:20.110724 Epoch 100, Training Loss 0.16590307405232774\n",
      "2022-03-26 22:10:20.125725 Epoch 100, Training Loss 0.1666097566294853\n",
      "2022-03-26 22:10:20.139794 Epoch 100, Training Loss 0.16738753024574435\n",
      "2022-03-26 22:10:20.154666 Epoch 100, Training Loss 0.16845224062195213\n",
      "2022-03-26 22:10:20.169720 Epoch 100, Training Loss 0.16915298567708495\n",
      "2022-03-26 22:10:20.183697 Epoch 100, Training Loss 0.17002946313689737\n",
      "2022-03-26 22:10:20.197715 Epoch 100, Training Loss 0.170996336299745\n",
      "2022-03-26 22:10:20.212573 Epoch 100, Training Loss 0.17188406104931747\n",
      "2022-03-26 22:10:20.226587 Epoch 100, Training Loss 0.17288226262687723\n",
      "2022-03-26 22:10:20.241616 Epoch 100, Training Loss 0.17397922955815445\n",
      "2022-03-26 22:10:20.255614 Epoch 100, Training Loss 0.17504891997103192\n",
      "2022-03-26 22:10:20.270157 Epoch 100, Training Loss 0.1758765751291114\n",
      "2022-03-26 22:10:20.284242 Epoch 100, Training Loss 0.17683091119427205\n",
      "2022-03-26 22:10:20.298235 Epoch 100, Training Loss 0.17779529925502474\n",
      "2022-03-26 22:10:20.313339 Epoch 100, Training Loss 0.17871694834640875\n",
      "2022-03-26 22:10:20.327350 Epoch 100, Training Loss 0.17945956276810687\n",
      "2022-03-26 22:10:20.341606 Epoch 100, Training Loss 0.18041453954508846\n",
      "2022-03-26 22:10:20.355607 Epoch 100, Training Loss 0.18132781799492018\n",
      "2022-03-26 22:10:20.369640 Epoch 100, Training Loss 0.18229632373051266\n",
      "2022-03-26 22:10:20.384642 Epoch 100, Training Loss 0.18324802705394033\n",
      "2022-03-26 22:10:20.398655 Epoch 100, Training Loss 0.18417534216895431\n",
      "2022-03-26 22:10:20.412659 Epoch 100, Training Loss 0.18506703520065074\n",
      "2022-03-26 22:10:20.427521 Epoch 100, Training Loss 0.18592589460980252\n",
      "2022-03-26 22:10:20.441544 Epoch 100, Training Loss 0.1866709538128065\n",
      "2022-03-26 22:10:20.455547 Epoch 100, Training Loss 0.18773251360334703\n",
      "2022-03-26 22:10:20.469429 Epoch 100, Training Loss 0.18852210913777656\n",
      "2022-03-26 22:10:20.484432 Epoch 100, Training Loss 0.18961927569125925\n",
      "2022-03-26 22:10:20.498359 Epoch 100, Training Loss 0.19046164206836536\n",
      "2022-03-26 22:10:20.512369 Epoch 100, Training Loss 0.19133161225587206\n",
      "2022-03-26 22:10:20.527377 Epoch 100, Training Loss 0.19216362609887672\n",
      "2022-03-26 22:10:20.541381 Epoch 100, Training Loss 0.19308703398460622\n",
      "2022-03-26 22:10:20.555249 Epoch 100, Training Loss 0.1936501566406406\n",
      "2022-03-26 22:10:20.570260 Epoch 100, Training Loss 0.19474066462358244\n",
      "2022-03-26 22:10:20.584269 Epoch 100, Training Loss 0.19548902113724242\n",
      "2022-03-26 22:10:20.598607 Epoch 100, Training Loss 0.1964068263388046\n",
      "2022-03-26 22:10:20.612611 Epoch 100, Training Loss 0.197156308709508\n",
      "2022-03-26 22:10:20.626644 Epoch 100, Training Loss 0.19796855781999084\n",
      "2022-03-26 22:10:20.640648 Epoch 100, Training Loss 0.19947389172166205\n",
      "2022-03-26 22:10:20.654634 Epoch 100, Training Loss 0.2005492035690171\n",
      "2022-03-26 22:10:20.669637 Epoch 100, Training Loss 0.20139253322425707\n",
      "2022-03-26 22:10:20.683659 Epoch 100, Training Loss 0.20238741584446118\n",
      "2022-03-26 22:10:20.698673 Epoch 100, Training Loss 0.20327922374086307\n",
      "2022-03-26 22:10:20.712676 Epoch 100, Training Loss 0.20423876842879274\n",
      "2022-03-26 22:10:20.727612 Epoch 100, Training Loss 0.20484725471652682\n",
      "2022-03-26 22:10:20.741616 Epoch 100, Training Loss 0.20584762142137494\n",
      "2022-03-26 22:10:20.755622 Epoch 100, Training Loss 0.20672265800368755\n",
      "2022-03-26 22:10:20.769635 Epoch 100, Training Loss 0.2080946514368667\n",
      "2022-03-26 22:10:20.784671 Epoch 100, Training Loss 0.20888803796390135\n",
      "2022-03-26 22:10:20.798684 Epoch 100, Training Loss 0.2099454365575405\n",
      "2022-03-26 22:10:20.812616 Epoch 100, Training Loss 0.21096530602411237\n",
      "2022-03-26 22:10:20.826643 Epoch 100, Training Loss 0.21194457070296988\n",
      "2022-03-26 22:10:20.841329 Epoch 100, Training Loss 0.21294731488618093\n",
      "2022-03-26 22:10:20.855261 Epoch 100, Training Loss 0.2135770787363467\n",
      "2022-03-26 22:10:20.869317 Epoch 100, Training Loss 0.21454759830099238\n",
      "2022-03-26 22:10:20.883370 Epoch 100, Training Loss 0.21533769056620194\n",
      "2022-03-26 22:10:20.898383 Epoch 100, Training Loss 0.21618844221924882\n",
      "2022-03-26 22:10:20.912231 Epoch 100, Training Loss 0.21723457827897327\n",
      "2022-03-26 22:10:20.926786 Epoch 100, Training Loss 0.21808837114087762\n",
      "2022-03-26 22:10:20.941782 Epoch 100, Training Loss 0.21896598901590117\n",
      "2022-03-26 22:10:20.957862 Epoch 100, Training Loss 0.21972817373092826\n",
      "2022-03-26 22:10:20.972891 Epoch 100, Training Loss 0.22033741391833175\n",
      "2022-03-26 22:10:20.989178 Epoch 100, Training Loss 0.2210951002357561\n",
      "2022-03-26 22:10:21.004271 Epoch 100, Training Loss 0.22213638972138505\n",
      "2022-03-26 22:10:21.020263 Epoch 100, Training Loss 0.22322882494658156\n",
      "2022-03-26 22:10:21.035290 Epoch 100, Training Loss 0.224159923203461\n",
      "2022-03-26 22:10:21.051396 Epoch 100, Training Loss 0.22501096159905729\n",
      "2022-03-26 22:10:21.068364 Epoch 100, Training Loss 0.22587594603333633\n",
      "2022-03-26 22:10:21.084404 Epoch 100, Training Loss 0.22662468906253805\n",
      "2022-03-26 22:10:21.099134 Epoch 100, Training Loss 0.22753907614351843\n",
      "2022-03-26 22:10:21.115230 Epoch 100, Training Loss 0.22825816563328208\n",
      "2022-03-26 22:10:21.132051 Epoch 100, Training Loss 0.22887062283275683\n",
      "2022-03-26 22:10:21.148166 Epoch 100, Training Loss 0.22973026152309553\n",
      "2022-03-26 22:10:21.163067 Epoch 100, Training Loss 0.23082855599158256\n",
      "2022-03-26 22:10:21.179852 Epoch 100, Training Loss 0.23189623768219864\n",
      "2022-03-26 22:10:21.194879 Epoch 100, Training Loss 0.2327309181851804\n",
      "2022-03-26 22:10:21.210903 Epoch 100, Training Loss 0.23350103588207907\n",
      "2022-03-26 22:10:21.226581 Epoch 100, Training Loss 0.23436286485256136\n",
      "2022-03-26 22:10:21.242609 Epoch 100, Training Loss 0.2351799431588034\n",
      "2022-03-26 22:10:21.258630 Epoch 100, Training Loss 0.23638658835302534\n",
      "2022-03-26 22:10:21.274653 Epoch 100, Training Loss 0.2372383383457618\n",
      "2022-03-26 22:10:21.290630 Epoch 100, Training Loss 0.2380086268534136\n",
      "2022-03-26 22:10:21.306662 Epoch 100, Training Loss 0.23903913917901265\n",
      "2022-03-26 22:10:21.321683 Epoch 100, Training Loss 0.23998769816687648\n",
      "2022-03-26 22:10:21.340726 Epoch 100, Training Loss 0.24090432194645142\n",
      "2022-03-26 22:10:21.359748 Epoch 100, Training Loss 0.2417075011660071\n",
      "2022-03-26 22:10:21.377788 Epoch 100, Training Loss 0.24267730478892852\n",
      "2022-03-26 22:10:21.397727 Epoch 100, Training Loss 0.2436549555691307\n",
      "2022-03-26 22:10:21.416592 Epoch 100, Training Loss 0.2446204832828868\n",
      "2022-03-26 22:10:21.436693 Epoch 100, Training Loss 0.24561338629716498\n",
      "2022-03-26 22:10:21.455714 Epoch 100, Training Loss 0.24649780512313405\n",
      "2022-03-26 22:10:21.474825 Epoch 100, Training Loss 0.24754461421228735\n",
      "2022-03-26 22:10:21.494982 Epoch 100, Training Loss 0.24847208214995195\n",
      "2022-03-26 22:10:21.514042 Epoch 100, Training Loss 0.24936267592565484\n",
      "2022-03-26 22:10:21.533516 Epoch 100, Training Loss 0.2502917172887441\n",
      "2022-03-26 22:10:21.552480 Epoch 100, Training Loss 0.25107482986529467\n",
      "2022-03-26 22:10:21.571507 Epoch 100, Training Loss 0.2521654421182545\n",
      "2022-03-26 22:10:21.590535 Epoch 100, Training Loss 0.25307219363082095\n",
      "2022-03-26 22:10:21.609384 Epoch 100, Training Loss 0.2539192398109704\n",
      "2022-03-26 22:10:21.628232 Epoch 100, Training Loss 0.25497871275295686\n",
      "2022-03-26 22:10:21.648183 Epoch 100, Training Loss 0.25591219595783504\n",
      "2022-03-26 22:10:21.666242 Epoch 100, Training Loss 0.25691638155209134\n",
      "2022-03-26 22:10:21.686132 Epoch 100, Training Loss 0.25768090632108165\n",
      "2022-03-26 22:10:21.706072 Epoch 100, Training Loss 0.2585349404217337\n",
      "2022-03-26 22:10:21.725976 Epoch 100, Training Loss 0.2594911525087893\n",
      "2022-03-26 22:10:21.744006 Epoch 100, Training Loss 0.2603794565362394\n",
      "2022-03-26 22:10:21.763968 Epoch 100, Training Loss 0.2613522746145268\n",
      "2022-03-26 22:10:21.783129 Epoch 100, Training Loss 0.26211930422679236\n",
      "2022-03-26 22:10:21.801676 Epoch 100, Training Loss 0.2628701925658814\n",
      "2022-03-26 22:10:21.820737 Epoch 100, Training Loss 0.2635250718468595\n",
      "2022-03-26 22:10:21.840735 Epoch 100, Training Loss 0.2644073654090047\n",
      "2022-03-26 22:10:21.859811 Epoch 100, Training Loss 0.2654200373479472\n",
      "2022-03-26 22:10:21.878848 Epoch 100, Training Loss 0.2662095516310323\n",
      "2022-03-26 22:10:21.898847 Epoch 100, Training Loss 0.26742747086850577\n",
      "2022-03-26 22:10:21.917840 Epoch 100, Training Loss 0.2681576150381352\n",
      "2022-03-26 22:10:21.936838 Epoch 100, Training Loss 0.2692643804547122\n",
      "2022-03-26 22:10:21.956930 Epoch 100, Training Loss 0.27024567725560855\n",
      "2022-03-26 22:10:21.976069 Epoch 100, Training Loss 0.2709821501122716\n",
      "2022-03-26 22:10:21.995179 Epoch 100, Training Loss 0.2719343597702968\n",
      "2022-03-26 22:10:22.014141 Epoch 100, Training Loss 0.2726734313742279\n",
      "2022-03-26 22:10:22.033219 Epoch 100, Training Loss 0.2733613021309723\n",
      "2022-03-26 22:10:22.052162 Epoch 100, Training Loss 0.27441826840038497\n",
      "2022-03-26 22:10:22.071159 Epoch 100, Training Loss 0.2750971236878344\n",
      "2022-03-26 22:10:22.090859 Epoch 100, Training Loss 0.27596898369319606\n",
      "2022-03-26 22:10:22.109661 Epoch 100, Training Loss 0.27706074893779464\n",
      "2022-03-26 22:10:22.129015 Epoch 100, Training Loss 0.27800005960190083\n",
      "2022-03-26 22:10:22.149039 Epoch 100, Training Loss 0.27889403719883743\n",
      "2022-03-26 22:10:22.167845 Epoch 100, Training Loss 0.27964504749116387\n",
      "2022-03-26 22:10:22.187890 Epoch 100, Training Loss 0.2803530947250478\n",
      "2022-03-26 22:10:22.206898 Epoch 100, Training Loss 0.2811709483489966\n",
      "2022-03-26 22:10:22.225946 Epoch 100, Training Loss 0.28184588596491555\n",
      "2022-03-26 22:10:22.245058 Epoch 100, Training Loss 0.28303542302545076\n",
      "2022-03-26 22:10:22.264011 Epoch 100, Training Loss 0.2838171579115226\n",
      "2022-03-26 22:10:22.283982 Epoch 100, Training Loss 0.2845415106743498\n",
      "2022-03-26 22:10:22.303014 Epoch 100, Training Loss 0.285578525089242\n",
      "2022-03-26 22:10:22.323058 Epoch 100, Training Loss 0.2862243715225888\n",
      "2022-03-26 22:10:22.342141 Epoch 100, Training Loss 0.2871536202061817\n",
      "2022-03-26 22:10:22.361185 Epoch 100, Training Loss 0.28831661490680616\n",
      "2022-03-26 22:10:22.379908 Epoch 100, Training Loss 0.2891505043906019\n",
      "2022-03-26 22:10:22.399994 Epoch 100, Training Loss 0.2900527591062019\n",
      "2022-03-26 22:10:22.418964 Epoch 100, Training Loss 0.2910277839283199\n",
      "2022-03-26 22:10:22.439022 Epoch 100, Training Loss 0.2918527872894731\n",
      "2022-03-26 22:10:22.457884 Epoch 100, Training Loss 0.2930121674485829\n",
      "2022-03-26 22:10:22.476879 Epoch 100, Training Loss 0.29410472790451003\n",
      "2022-03-26 22:10:22.496450 Epoch 100, Training Loss 0.2950609043202437\n",
      "2022-03-26 22:10:22.515352 Epoch 100, Training Loss 0.295863861356245\n",
      "2022-03-26 22:10:22.535966 Epoch 100, Training Loss 0.2965309057013153\n",
      "2022-03-26 22:10:22.555039 Epoch 100, Training Loss 0.29731295888533676\n",
      "2022-03-26 22:10:22.575090 Epoch 100, Training Loss 0.2983981258500263\n",
      "2022-03-26 22:10:22.595155 Epoch 100, Training Loss 0.2994367300778094\n",
      "2022-03-26 22:10:22.615867 Epoch 100, Training Loss 0.30071745721427984\n",
      "2022-03-26 22:10:22.634940 Epoch 100, Training Loss 0.3016997492298141\n",
      "2022-03-26 22:10:22.656033 Epoch 100, Training Loss 0.30275502073033084\n",
      "2022-03-26 22:10:22.674047 Epoch 100, Training Loss 0.30354515506940727\n",
      "2022-03-26 22:10:22.694109 Epoch 100, Training Loss 0.30445237229089905\n",
      "2022-03-26 22:10:22.712123 Epoch 100, Training Loss 0.30527910571116623\n",
      "2022-03-26 22:10:22.732081 Epoch 100, Training Loss 0.30608850431716655\n",
      "2022-03-26 22:10:22.751948 Epoch 100, Training Loss 0.3066829253950387\n",
      "2022-03-26 22:10:22.771989 Epoch 100, Training Loss 0.3075275511083091\n",
      "2022-03-26 22:10:22.791049 Epoch 100, Training Loss 0.30817315218698643\n",
      "2022-03-26 22:10:22.809911 Epoch 100, Training Loss 0.3089547350888362\n",
      "2022-03-26 22:10:22.828968 Epoch 100, Training Loss 0.30992609887476774\n",
      "2022-03-26 22:10:22.848963 Epoch 100, Training Loss 0.31085135054100504\n",
      "2022-03-26 22:10:22.868136 Epoch 100, Training Loss 0.31174396233790364\n",
      "2022-03-26 22:10:22.887190 Epoch 100, Training Loss 0.31240606071699\n",
      "2022-03-26 22:10:22.906433 Epoch 100, Training Loss 0.313295924297684\n",
      "2022-03-26 22:10:22.926422 Epoch 100, Training Loss 0.31425511920848465\n",
      "2022-03-26 22:10:22.945660 Epoch 100, Training Loss 0.3151746874727556\n",
      "2022-03-26 22:10:22.965615 Epoch 100, Training Loss 0.3160489476702707\n",
      "2022-03-26 22:10:22.983849 Epoch 100, Training Loss 0.3170798372124772\n",
      "2022-03-26 22:10:23.002909 Epoch 100, Training Loss 0.31807663183078133\n",
      "2022-03-26 22:10:23.021927 Epoch 100, Training Loss 0.31903450232942393\n",
      "2022-03-26 22:10:23.040961 Epoch 100, Training Loss 0.3200765115678158\n",
      "2022-03-26 22:10:23.060576 Epoch 100, Training Loss 0.32119328682989723\n",
      "2022-03-26 22:10:23.079597 Epoch 100, Training Loss 0.32201263476210784\n",
      "2022-03-26 22:10:23.099857 Epoch 100, Training Loss 0.322933203195367\n",
      "2022-03-26 22:10:23.118978 Epoch 100, Training Loss 0.3239191692046192\n",
      "2022-03-26 22:10:23.139187 Epoch 100, Training Loss 0.3251085184571688\n",
      "2022-03-26 22:10:23.158462 Epoch 100, Training Loss 0.32600613689178704\n",
      "2022-03-26 22:10:23.177497 Epoch 100, Training Loss 0.3271244079865458\n",
      "2022-03-26 22:10:23.196487 Epoch 100, Training Loss 0.32789289051919335\n",
      "2022-03-26 22:10:23.215495 Epoch 100, Training Loss 0.3288311821115596\n",
      "2022-03-26 22:10:23.235536 Epoch 100, Training Loss 0.32963486728460895\n",
      "2022-03-26 22:10:23.254436 Epoch 100, Training Loss 0.3302977511949856\n",
      "2022-03-26 22:10:23.273472 Epoch 100, Training Loss 0.33128099909523867\n",
      "2022-03-26 22:10:23.292352 Epoch 100, Training Loss 0.33183908291027675\n",
      "2022-03-26 22:10:23.312388 Epoch 100, Training Loss 0.3328504521599816\n",
      "2022-03-26 22:10:23.332407 Epoch 100, Training Loss 0.3340232159246874\n",
      "2022-03-26 22:10:23.351429 Epoch 100, Training Loss 0.33498953229478556\n",
      "2022-03-26 22:10:23.370450 Epoch 100, Training Loss 0.33564932796808766\n",
      "2022-03-26 22:10:23.390467 Epoch 100, Training Loss 0.33655844453503103\n",
      "2022-03-26 22:10:23.409566 Epoch 100, Training Loss 0.33758040992042904\n",
      "2022-03-26 22:10:23.429605 Epoch 100, Training Loss 0.3383530189695261\n",
      "2022-03-26 22:10:23.447609 Epoch 100, Training Loss 0.3394320074783262\n",
      "2022-03-26 22:10:23.467624 Epoch 100, Training Loss 0.34012547718442004\n",
      "2022-03-26 22:10:23.486656 Epoch 100, Training Loss 0.34069360445832353\n",
      "2022-03-26 22:10:23.506679 Epoch 100, Training Loss 0.34171253861978534\n",
      "2022-03-26 22:10:23.525715 Epoch 100, Training Loss 0.3426042613775834\n",
      "2022-03-26 22:10:23.544990 Epoch 100, Training Loss 0.34352425274336734\n",
      "2022-03-26 22:10:23.564908 Epoch 100, Training Loss 0.34461606890344254\n",
      "2022-03-26 22:10:23.584534 Epoch 100, Training Loss 0.3454610623057236\n",
      "2022-03-26 22:10:23.604550 Epoch 100, Training Loss 0.3463321510330795\n",
      "2022-03-26 22:10:23.623399 Epoch 100, Training Loss 0.34726586053743386\n",
      "2022-03-26 22:10:23.643268 Epoch 100, Training Loss 0.3482743949079148\n",
      "2022-03-26 22:10:23.663208 Epoch 100, Training Loss 0.34909302888014127\n",
      "2022-03-26 22:10:23.682167 Epoch 100, Training Loss 0.3497717459793286\n",
      "2022-03-26 22:10:23.702200 Epoch 100, Training Loss 0.3506890278490608\n",
      "2022-03-26 22:10:23.721679 Epoch 100, Training Loss 0.3515412575753449\n",
      "2022-03-26 22:10:23.741612 Epoch 100, Training Loss 0.3526178320198108\n",
      "2022-03-26 22:10:23.760640 Epoch 100, Training Loss 0.3533960594545545\n",
      "2022-03-26 22:10:23.780976 Epoch 100, Training Loss 0.3541913134667575\n",
      "2022-03-26 22:10:23.799956 Epoch 100, Training Loss 0.3549127488032631\n",
      "2022-03-26 22:10:23.823949 Epoch 100, Training Loss 0.3557051406492053\n",
      "2022-03-26 22:10:23.844462 Epoch 100, Training Loss 0.3566364278573819\n",
      "2022-03-26 22:10:23.869396 Epoch 100, Training Loss 0.35774011746087037\n",
      "2022-03-26 22:10:23.889407 Epoch 100, Training Loss 0.3586168246500937\n",
      "2022-03-26 22:10:23.908377 Epoch 100, Training Loss 0.3595103901975295\n",
      "2022-03-26 22:10:23.929239 Epoch 100, Training Loss 0.3601543036720637\n",
      "2022-03-26 22:10:23.951031 Epoch 100, Training Loss 0.3611155966358721\n",
      "2022-03-26 22:10:23.970372 Epoch 100, Training Loss 0.362076174115281\n",
      "2022-03-26 22:10:23.990232 Epoch 100, Training Loss 0.36322321603670144\n",
      "2022-03-26 22:10:24.009079 Epoch 100, Training Loss 0.3641965600383251\n",
      "2022-03-26 22:10:24.028015 Epoch 100, Training Loss 0.3649540184556371\n",
      "2022-03-26 22:10:24.047066 Epoch 100, Training Loss 0.36582474650629343\n",
      "2022-03-26 22:10:24.066898 Epoch 100, Training Loss 0.366764582712632\n",
      "2022-03-26 22:10:24.085946 Epoch 100, Training Loss 0.3676727658799847\n",
      "2022-03-26 22:10:24.106058 Epoch 100, Training Loss 0.36881246972266973\n",
      "2022-03-26 22:10:24.125064 Epoch 100, Training Loss 0.3696548989056931\n",
      "2022-03-26 22:10:24.145140 Epoch 100, Training Loss 0.370571420778094\n",
      "2022-03-26 22:10:24.164204 Epoch 100, Training Loss 0.37162350929911486\n",
      "2022-03-26 22:10:24.183934 Epoch 100, Training Loss 0.3726717001184478\n",
      "2022-03-26 22:10:24.202942 Epoch 100, Training Loss 0.37358944853553383\n",
      "2022-03-26 22:10:24.223237 Epoch 100, Training Loss 0.37438026367855803\n",
      "2022-03-26 22:10:24.243171 Epoch 100, Training Loss 0.3751570048844418\n",
      "2022-03-26 22:10:24.262065 Epoch 100, Training Loss 0.37606287612329664\n",
      "2022-03-26 22:10:24.281113 Epoch 100, Training Loss 0.3771226193441454\n",
      "2022-03-26 22:10:24.300906 Epoch 100, Training Loss 0.37794791417353596\n",
      "2022-03-26 22:10:24.319935 Epoch 100, Training Loss 0.3789067731031676\n",
      "2022-03-26 22:10:24.340052 Epoch 100, Training Loss 0.37995898837933456\n",
      "2022-03-26 22:10:24.358958 Epoch 100, Training Loss 0.3811154630787842\n",
      "2022-03-26 22:10:24.378912 Epoch 100, Training Loss 0.3819617737284707\n",
      "2022-03-26 22:10:24.397985 Epoch 100, Training Loss 0.38284886264435164\n",
      "2022-03-26 22:10:24.417628 Epoch 100, Training Loss 0.3835457247084059\n",
      "2022-03-26 22:10:24.437453 Epoch 100, Training Loss 0.38439318690153645\n",
      "2022-03-26 22:10:24.456451 Epoch 100, Training Loss 0.3855513507295448\n",
      "2022-03-26 22:10:24.476485 Epoch 100, Training Loss 0.3861898418963718\n",
      "2022-03-26 22:10:24.495541 Epoch 100, Training Loss 0.3868574956265252\n",
      "2022-03-26 22:10:24.514119 Epoch 100, Training Loss 0.3875651384329857\n",
      "2022-03-26 22:10:24.534108 Epoch 100, Training Loss 0.3883935420028389\n",
      "2022-03-26 22:10:24.554359 Epoch 100, Training Loss 0.3891260131164585\n",
      "2022-03-26 22:10:24.574397 Epoch 100, Training Loss 0.39013281971444863\n",
      "2022-03-26 22:10:24.593405 Epoch 100, Training Loss 0.39104518656383086\n",
      "2022-03-26 22:10:24.613378 Epoch 100, Training Loss 0.3919914603766883\n",
      "2022-03-26 22:10:24.632393 Epoch 100, Training Loss 0.39295080643328256\n",
      "2022-03-26 22:10:24.652431 Epoch 100, Training Loss 0.3939888369473045\n",
      "2022-03-26 22:10:24.671455 Epoch 100, Training Loss 0.3946417972940923\n",
      "2022-03-26 22:10:24.690469 Epoch 100, Training Loss 0.3956708992305009\n",
      "2022-03-26 22:10:24.709509 Epoch 100, Training Loss 0.39655612050877204\n",
      "2022-03-26 22:10:24.729524 Epoch 100, Training Loss 0.39761433237806304\n",
      "2022-03-26 22:10:24.748481 Epoch 100, Training Loss 0.3985207505009668\n",
      "2022-03-26 22:10:24.768460 Epoch 100, Training Loss 0.39950132861619103\n",
      "2022-03-26 22:10:24.789029 Epoch 100, Training Loss 0.4002490242957459\n",
      "2022-03-26 22:10:24.807965 Epoch 100, Training Loss 0.4012609977475213\n",
      "2022-03-26 22:10:24.827668 Epoch 100, Training Loss 0.40224381530528786\n",
      "2022-03-26 22:10:24.847703 Epoch 100, Training Loss 0.40297474310068826\n",
      "2022-03-26 22:10:24.866600 Epoch 100, Training Loss 0.40377641878926845\n",
      "2022-03-26 22:10:24.885597 Epoch 100, Training Loss 0.4046868544329158\n",
      "2022-03-26 22:10:24.905170 Epoch 100, Training Loss 0.4056495886553279\n",
      "2022-03-26 22:10:24.925356 Epoch 100, Training Loss 0.4064312928244281\n",
      "2022-03-26 22:10:24.944383 Epoch 100, Training Loss 0.4072232351964697\n",
      "2022-03-26 22:10:24.963463 Epoch 100, Training Loss 0.40821449466221166\n",
      "2022-03-26 22:10:24.982485 Epoch 100, Training Loss 0.4091769766700847\n",
      "2022-03-26 22:10:25.003401 Epoch 100, Training Loss 0.40987202269799267\n",
      "2022-03-26 22:10:25.022272 Epoch 100, Training Loss 0.41067009619282335\n",
      "2022-03-26 22:10:25.041297 Epoch 100, Training Loss 0.41153197196286045\n",
      "2022-03-26 22:10:25.061082 Epoch 100, Training Loss 0.4126321522857222\n",
      "2022-03-26 22:10:25.079688 Epoch 100, Training Loss 0.4131874780139655\n",
      "2022-03-26 22:10:25.099728 Epoch 100, Training Loss 0.4140906889191674\n",
      "2022-03-26 22:10:25.119576 Epoch 100, Training Loss 0.4148470340558635\n",
      "2022-03-26 22:10:25.138600 Epoch 100, Training Loss 0.41561982946475146\n",
      "2022-03-26 22:10:25.157957 Epoch 100, Training Loss 0.41664982417507856\n",
      "2022-03-26 22:10:25.176898 Epoch 100, Training Loss 0.41777885489908934\n",
      "2022-03-26 22:10:25.196954 Epoch 100, Training Loss 0.4187044960534786\n",
      "2022-03-26 22:10:25.216327 Epoch 100, Training Loss 0.4196833113346563\n",
      "2022-03-26 22:10:25.236458 Epoch 100, Training Loss 0.4205895482807818\n",
      "2022-03-26 22:10:25.254916 Epoch 100, Training Loss 0.42107868095493073\n",
      "2022-03-26 22:10:25.275933 Epoch 100, Training Loss 0.4220854747478309\n",
      "2022-03-26 22:10:25.295223 Epoch 100, Training Loss 0.422979931468549\n",
      "2022-03-26 22:10:25.315935 Epoch 100, Training Loss 0.4237884731243943\n",
      "2022-03-26 22:10:25.335145 Epoch 100, Training Loss 0.4251574213852358\n",
      "2022-03-26 22:10:25.354666 Epoch 100, Training Loss 0.4262152552757117\n",
      "2022-03-26 22:10:25.373667 Epoch 100, Training Loss 0.42733132282791236\n",
      "2022-03-26 22:10:25.393719 Epoch 100, Training Loss 0.42816013059652674\n",
      "2022-03-26 22:10:25.413620 Epoch 100, Training Loss 0.42935931034710095\n",
      "2022-03-26 22:10:25.433634 Epoch 100, Training Loss 0.43012174095034295\n",
      "2022-03-26 22:10:25.452556 Epoch 100, Training Loss 0.43111126890877627\n",
      "2022-03-26 22:10:25.472559 Epoch 100, Training Loss 0.4318484105265049\n",
      "2022-03-26 22:10:25.491601 Epoch 100, Training Loss 0.432736902209499\n",
      "2022-03-26 22:10:25.510612 Epoch 100, Training Loss 0.433710808110664\n",
      "2022-03-26 22:10:25.530616 Epoch 100, Training Loss 0.43456585335609554\n",
      "2022-03-26 22:10:25.550664 Epoch 100, Training Loss 0.43545021105300435\n",
      "2022-03-26 22:10:25.570679 Epoch 100, Training Loss 0.4360643600106544\n",
      "2022-03-26 22:10:25.589709 Epoch 100, Training Loss 0.43683843059308086\n",
      "2022-03-26 22:10:25.609690 Epoch 100, Training Loss 0.43764121163531644\n",
      "2022-03-26 22:10:25.629665 Epoch 100, Training Loss 0.438384617609746\n",
      "2022-03-26 22:10:25.649668 Epoch 100, Training Loss 0.4393724764094633\n",
      "2022-03-26 22:10:25.668580 Epoch 100, Training Loss 0.4406008322525512\n",
      "2022-03-26 22:10:25.688612 Epoch 100, Training Loss 0.44178070490012694\n",
      "2022-03-26 22:10:25.707504 Epoch 100, Training Loss 0.4425278305245178\n",
      "2022-03-26 22:10:25.726541 Epoch 100, Training Loss 0.44360175119031725\n",
      "2022-03-26 22:10:25.746499 Epoch 100, Training Loss 0.44468798547449623\n",
      "2022-03-26 22:10:25.766396 Epoch 100, Training Loss 0.4455177921163457\n",
      "2022-03-26 22:10:25.785401 Epoch 100, Training Loss 0.4464389756512459\n",
      "2022-03-26 22:10:25.805421 Epoch 100, Training Loss 0.4472138486097536\n",
      "2022-03-26 22:10:25.825472 Epoch 100, Training Loss 0.4484663019552255\n",
      "2022-03-26 22:10:25.844544 Epoch 100, Training Loss 0.44930531980131594\n",
      "2022-03-26 22:10:25.863368 Epoch 100, Training Loss 0.45030485463264347\n",
      "2022-03-26 22:10:25.883385 Epoch 100, Training Loss 0.45139880436460683\n",
      "2022-03-26 22:10:25.902362 Epoch 100, Training Loss 0.4521268222795423\n",
      "2022-03-26 22:10:25.922371 Epoch 100, Training Loss 0.4528883953228631\n",
      "2022-03-26 22:10:25.942311 Epoch 100, Training Loss 0.45403343972647586\n",
      "2022-03-26 22:10:25.961128 Epoch 100, Training Loss 0.45505605916233016\n",
      "2022-03-26 22:10:25.981075 Epoch 100, Training Loss 0.45595110133480843\n",
      "2022-03-26 22:10:26.001002 Epoch 100, Training Loss 0.4567049533662284\n",
      "2022-03-26 22:10:26.021004 Epoch 100, Training Loss 0.4576708496836445\n",
      "2022-03-26 22:10:26.041032 Epoch 100, Training Loss 0.45875995314639545\n",
      "2022-03-26 22:10:26.060855 Epoch 100, Training Loss 0.4599197999290798\n",
      "2022-03-26 22:10:26.080474 Epoch 100, Training Loss 0.4605255224134611\n",
      "2022-03-26 22:10:26.099501 Epoch 100, Training Loss 0.46145440211229005\n",
      "2022-03-26 22:10:26.119361 Epoch 100, Training Loss 0.46216388001009023\n",
      "2022-03-26 22:10:26.138395 Epoch 100, Training Loss 0.4633573416020254\n",
      "2022-03-26 22:10:26.157381 Epoch 100, Training Loss 0.4641506957352314\n",
      "2022-03-26 22:10:26.176422 Epoch 100, Training Loss 0.46495715740239224\n",
      "2022-03-26 22:10:26.195411 Epoch 100, Training Loss 0.465867057137782\n",
      "2022-03-26 22:10:26.222272 Epoch 100, Training Loss 0.4667802206466875\n",
      "2022-03-26 22:10:26.248434 Epoch 100, Training Loss 0.46738228521993397\n",
      "2022-03-26 22:10:26.275464 Epoch 100, Training Loss 0.46821233584447897\n",
      "2022-03-26 22:10:26.301391 Epoch 100, Training Loss 0.4691007337759218\n",
      "2022-03-26 22:10:26.329164 Epoch 100, Training Loss 0.47008103528595946\n",
      "2022-03-26 22:10:26.355150 Epoch 100, Training Loss 0.47101625243721107\n",
      "2022-03-26 22:10:26.381287 Epoch 100, Training Loss 0.4723638437897958\n",
      "2022-03-26 22:10:26.407352 Epoch 100, Training Loss 0.47324107438706986\n",
      "2022-03-26 22:10:26.433391 Epoch 100, Training Loss 0.4741261585441697\n",
      "2022-03-26 22:10:26.459525 Epoch 100, Training Loss 0.47513640727228523\n",
      "2022-03-26 22:10:26.486555 Epoch 100, Training Loss 0.47596582480708655\n",
      "2022-03-26 22:10:26.512473 Epoch 100, Training Loss 0.4769602568862993\n",
      "2022-03-26 22:10:26.539054 Epoch 100, Training Loss 0.4778405818183099\n",
      "2022-03-26 22:10:26.565018 Epoch 100, Training Loss 0.47867663147504375\n",
      "2022-03-26 22:10:26.591378 Epoch 100, Training Loss 0.47956089458197276\n",
      "2022-03-26 22:10:26.609370 Epoch 100, Training Loss 0.4804117268765979\n",
      "2022-03-26 22:10:26.627283 Epoch 100, Training Loss 0.4814366653294819\n",
      "2022-03-26 22:10:26.645313 Epoch 100, Training Loss 0.48232826018882224\n",
      "2022-03-26 22:10:26.663244 Epoch 100, Training Loss 0.4832253652765318\n",
      "2022-03-26 22:10:26.682255 Epoch 100, Training Loss 0.48417937153440604\n",
      "2022-03-26 22:10:26.701301 Epoch 100, Training Loss 0.48505618535649137\n",
      "2022-03-26 22:10:26.719353 Epoch 100, Training Loss 0.4860307891350573\n",
      "2022-03-26 22:10:26.738408 Epoch 100, Training Loss 0.4870895088633613\n",
      "2022-03-26 22:10:26.756422 Epoch 100, Training Loss 0.4883864274262772\n",
      "2022-03-26 22:10:26.774378 Epoch 100, Training Loss 0.4894029948565051\n",
      "2022-03-26 22:10:26.792402 Epoch 100, Training Loss 0.4903866424584937\n",
      "2022-03-26 22:10:26.810520 Epoch 100, Training Loss 0.4914768146889289\n",
      "2022-03-26 22:10:26.828430 Epoch 100, Training Loss 0.49238734073041346\n",
      "2022-03-26 22:10:26.846462 Epoch 100, Training Loss 0.4932912300767191\n",
      "2022-03-26 22:10:26.865476 Epoch 100, Training Loss 0.49402294317474754\n",
      "2022-03-26 22:10:26.884610 Epoch 100, Training Loss 0.4949008956589662\n",
      "2022-03-26 22:10:26.903642 Epoch 100, Training Loss 0.4959125877036463\n",
      "2022-03-26 22:10:26.922650 Epoch 100, Training Loss 0.49677743036728683\n",
      "2022-03-26 22:10:26.940673 Epoch 100, Training Loss 0.4977599358772073\n",
      "2022-03-26 22:10:26.959713 Epoch 100, Training Loss 0.4986795750267975\n",
      "2022-03-26 22:10:26.977729 Epoch 100, Training Loss 0.49945401230736464\n",
      "2022-03-26 22:10:26.995744 Epoch 100, Training Loss 0.5005621891040022\n",
      "2022-03-26 22:10:27.013791 Epoch 100, Training Loss 0.501355445064852\n",
      "2022-03-26 22:10:27.032738 Epoch 100, Training Loss 0.502211338037725\n",
      "2022-03-26 22:10:27.050440 Epoch 100, Training Loss 0.5031760296095973\n",
      "2022-03-26 22:10:27.069456 Epoch 100, Training Loss 0.5042696427506255\n",
      "2022-03-26 22:10:27.088480 Epoch 100, Training Loss 0.5052244996323305\n",
      "2022-03-26 22:10:27.106502 Epoch 100, Training Loss 0.5061930164961559\n",
      "2022-03-26 22:10:27.124367 Epoch 100, Training Loss 0.5071338959362196\n",
      "2022-03-26 22:10:27.142414 Epoch 100, Training Loss 0.5081906868216327\n",
      "2022-03-26 22:10:27.160254 Epoch 100, Training Loss 0.5089478561335512\n",
      "2022-03-26 22:10:27.178247 Epoch 100, Training Loss 0.5103028812981627\n",
      "2022-03-26 22:10:27.196300 Epoch 100, Training Loss 0.5112338540194284\n",
      "2022-03-26 22:10:27.215137 Epoch 100, Training Loss 0.5119990255216809\n",
      "2022-03-26 22:10:27.233178 Epoch 100, Training Loss 0.5127492059222267\n",
      "2022-03-26 22:10:27.252103 Epoch 100, Training Loss 0.5137131435182088\n",
      "2022-03-26 22:10:27.270205 Epoch 100, Training Loss 0.514766220698881\n",
      "2022-03-26 22:10:27.289292 Epoch 100, Training Loss 0.5154837273880649\n",
      "2022-03-26 22:10:27.308240 Epoch 100, Training Loss 0.5163794068424293\n",
      "2022-03-26 22:10:27.327286 Epoch 100, Training Loss 0.5175816906077783\n",
      "2022-03-26 22:10:27.345281 Epoch 100, Training Loss 0.5185415385781652\n",
      "2022-03-26 22:10:27.364407 Epoch 100, Training Loss 0.5192376621391462\n",
      "2022-03-26 22:10:27.382866 Epoch 100, Training Loss 0.520338096048521\n",
      "2022-03-26 22:10:27.400911 Epoch 100, Training Loss 0.5209865861231714\n",
      "2022-03-26 22:10:27.419139 Epoch 100, Training Loss 0.5219040664717974\n",
      "2022-03-26 22:10:27.436847 Epoch 100, Training Loss 0.5229383372437314\n",
      "2022-03-26 22:10:27.454875 Epoch 100, Training Loss 0.5240010674804678\n",
      "2022-03-26 22:10:27.474249 Epoch 100, Training Loss 0.5246208986205518\n",
      "2022-03-26 22:10:27.492292 Epoch 100, Training Loss 0.5253980013415637\n",
      "2022-03-26 22:10:27.511289 Epoch 100, Training Loss 0.5262417983063652\n",
      "2022-03-26 22:10:27.529235 Epoch 100, Training Loss 0.5271073124750191\n",
      "2022-03-26 22:10:27.547243 Epoch 100, Training Loss 0.5280014628644489\n",
      "2022-03-26 22:10:27.566159 Epoch 100, Training Loss 0.5291894593507128\n",
      "2022-03-26 22:10:27.584402 Epoch 100, Training Loss 0.5301706559212921\n",
      "2022-03-26 22:10:27.602249 Epoch 100, Training Loss 0.5311259229469787\n",
      "2022-03-26 22:10:27.620155 Epoch 100, Training Loss 0.5319749993436477\n",
      "2022-03-26 22:10:27.638181 Epoch 100, Training Loss 0.5327748583863153\n",
      "2022-03-26 22:10:27.656225 Epoch 100, Training Loss 0.5335229629903193\n",
      "2022-03-26 22:10:27.675203 Epoch 100, Training Loss 0.5341891279001065\n",
      "2022-03-26 22:10:27.693289 Epoch 100, Training Loss 0.5351934916985309\n",
      "2022-03-26 22:10:27.712277 Epoch 100, Training Loss 0.5361180540240938\n",
      "2022-03-26 22:10:27.730369 Epoch 100, Training Loss 0.5371910092775779\n",
      "2022-03-26 22:10:27.748403 Epoch 100, Training Loss 0.5383929685710946\n",
      "2022-03-26 22:10:27.766384 Epoch 100, Training Loss 0.5393269128354309\n",
      "2022-03-26 22:10:27.785457 Epoch 100, Training Loss 0.5401474751170029\n",
      "2022-03-26 22:10:27.804424 Epoch 100, Training Loss 0.5410878341978468\n",
      "2022-03-26 22:10:27.822371 Epoch 100, Training Loss 0.5418778674681778\n",
      "2022-03-26 22:10:27.841395 Epoch 100, Training Loss 0.5429204697041865\n",
      "2022-03-26 22:10:27.859176 Epoch 100, Training Loss 0.5439674544822225\n",
      "2022-03-26 22:10:27.877296 Epoch 100, Training Loss 0.5446921910928644\n",
      "2022-03-26 22:10:27.895286 Epoch 100, Training Loss 0.5455241456361073\n",
      "2022-03-26 22:10:27.913532 Epoch 100, Training Loss 0.5467566577217463\n",
      "2022-03-26 22:10:27.932565 Epoch 100, Training Loss 0.5476672712646787\n",
      "2022-03-26 22:10:27.951565 Epoch 100, Training Loss 0.548654229363517\n",
      "2022-03-26 22:10:27.969588 Epoch 100, Training Loss 0.5497693747205807\n",
      "2022-03-26 22:10:27.987581 Epoch 100, Training Loss 0.5509367438076097\n",
      "2022-03-26 22:10:28.005591 Epoch 100, Training Loss 0.5520175637491523\n",
      "2022-03-26 22:10:28.024592 Epoch 100, Training Loss 0.5529135867305424\n",
      "2022-03-26 22:10:28.042606 Epoch 100, Training Loss 0.5537278389229494\n",
      "2022-03-26 22:10:28.060126 Epoch 100, Training Loss 0.55476254400085\n",
      "2022-03-26 22:10:28.078147 Epoch 100, Training Loss 0.5560346792268631\n",
      "2022-03-26 22:10:28.096718 Epoch 100, Training Loss 0.5568765391931509\n",
      "2022-03-26 22:10:28.115732 Epoch 100, Training Loss 0.5577814240589776\n",
      "2022-03-26 22:10:28.134820 Epoch 100, Training Loss 0.5585935908510252\n",
      "2022-03-26 22:10:28.153736 Epoch 100, Training Loss 0.5594916508325836\n",
      "2022-03-26 22:10:28.171759 Epoch 100, Training Loss 0.5603255913842975\n",
      "2022-03-26 22:10:28.190358 Epoch 100, Training Loss 0.5614051270058088\n",
      "2022-03-26 22:10:28.209248 Epoch 100, Training Loss 0.5622229455682017\n",
      "2022-03-26 22:10:28.227219 Epoch 100, Training Loss 0.5630669045783675\n",
      "2022-03-26 22:10:28.245350 Epoch 100, Training Loss 0.5638872198284129\n",
      "2022-03-26 22:10:28.263372 Epoch 100, Training Loss 0.5647944142598935\n",
      "2022-03-26 22:10:28.282252 Epoch 100, Training Loss 0.5657430121203517\n",
      "2022-03-26 22:10:28.300521 Epoch 100, Training Loss 0.5664818898948563\n",
      "2022-03-26 22:10:28.319547 Epoch 100, Training Loss 0.5675412353957096\n",
      "2022-03-26 22:10:28.338562 Epoch 100, Training Loss 0.5685436166918186\n",
      "2022-03-26 22:10:28.357949 Epoch 100, Training Loss 0.5694696200473229\n",
      "2022-03-26 22:10:28.376026 Epoch 100, Training Loss 0.5702194835218932\n",
      "2022-03-26 22:10:28.395121 Epoch 100, Training Loss 0.5710278436960772\n",
      "2022-03-26 22:10:28.412851 Epoch 100, Training Loss 0.5718657123615675\n",
      "2022-03-26 22:10:28.438881 Epoch 100, Training Loss 0.5729530234166118\n",
      "2022-03-26 22:10:28.463968 Epoch 100, Training Loss 0.5737396323162577\n",
      "2022-03-26 22:10:28.490966 Epoch 100, Training Loss 0.5746069875214715\n",
      "2022-03-26 22:10:28.516993 Epoch 100, Training Loss 0.5757648285545046\n",
      "2022-03-26 22:10:28.542947 Epoch 100, Training Loss 0.576832373276391\n",
      "2022-03-26 22:10:28.569878 Epoch 100, Training Loss 0.5776262791717753\n",
      "2022-03-26 22:10:28.595884 Epoch 100, Training Loss 0.5784262395118509\n",
      "2022-03-26 22:10:28.621805 Epoch 100, Training Loss 0.5794098180578188\n",
      "2022-03-26 22:10:28.636834 Epoch 100, Training Loss 0.5803117196425758\n",
      "2022-03-26 22:10:28.650834 Epoch 100, Training Loss 0.5808461741413302\n",
      "2022-03-26 22:10:28.664854 Epoch 100, Training Loss 0.5821109835601523\n",
      "2022-03-26 22:10:28.678892 Epoch 100, Training Loss 0.5829448624492606\n",
      "2022-03-26 22:10:28.693001 Epoch 100, Training Loss 0.5841694950600109\n",
      "2022-03-26 22:10:28.706758 Epoch 100, Training Loss 0.5848674494439684\n",
      "2022-03-26 22:10:28.721258 Epoch 100, Training Loss 0.5858750300639121\n",
      "2022-03-26 22:10:28.735387 Epoch 100, Training Loss 0.5868700068167714\n",
      "2022-03-26 22:10:28.750279 Epoch 100, Training Loss 0.5879629183455807\n",
      "2022-03-26 22:10:28.764194 Epoch 100, Training Loss 0.5886980159508298\n",
      "2022-03-26 22:10:28.779210 Epoch 100, Training Loss 0.5896788437653075\n",
      "2022-03-26 22:10:28.794296 Epoch 100, Training Loss 0.5906621236020647\n",
      "2022-03-26 22:10:28.808287 Epoch 100, Training Loss 0.5916280385935703\n",
      "2022-03-26 22:10:28.822279 Epoch 100, Training Loss 0.5927662535396683\n",
      "2022-03-26 22:10:28.837318 Epoch 100, Training Loss 0.5937056911113622\n",
      "2022-03-26 22:10:28.851276 Epoch 100, Training Loss 0.5945175287821104\n",
      "2022-03-26 22:10:28.866206 Epoch 100, Training Loss 0.5954804472301317\n",
      "2022-03-26 22:10:28.879260 Epoch 100, Training Loss 0.5963156503789565\n",
      "2022-03-26 22:10:28.894299 Epoch 100, Training Loss 0.5971529935784352\n",
      "2022-03-26 22:10:28.908656 Epoch 100, Training Loss 0.598212432449736\n",
      "2022-03-26 22:10:28.922684 Epoch 100, Training Loss 0.599293043089035\n",
      "2022-03-26 22:10:28.936840 Epoch 100, Training Loss 0.6000848130496872\n",
      "2022-03-26 22:10:28.950949 Epoch 100, Training Loss 0.6008463776325021\n",
      "2022-03-26 22:10:28.964911 Epoch 100, Training Loss 0.6015073432184547\n",
      "2022-03-26 22:10:28.979994 Epoch 100, Training Loss 0.602351615114895\n",
      "2022-03-26 22:10:28.995079 Epoch 100, Training Loss 0.6034046309378446\n",
      "2022-03-26 22:10:29.009016 Epoch 100, Training Loss 0.6046218141875304\n",
      "2022-03-26 22:10:29.022938 Epoch 100, Training Loss 0.6056103905296082\n",
      "2022-03-26 22:10:29.037876 Epoch 100, Training Loss 0.6066420314561985\n",
      "2022-03-26 22:10:29.051415 Epoch 100, Training Loss 0.6076035132188626\n",
      "2022-03-26 22:10:29.066430 Epoch 100, Training Loss 0.6083988170794514\n",
      "2022-03-26 22:10:29.080460 Epoch 100, Training Loss 0.609303456438167\n",
      "2022-03-26 22:10:29.095058 Epoch 100, Training Loss 0.610326705914934\n",
      "2022-03-26 22:10:29.109096 Epoch 100, Training Loss 0.611311421522399\n",
      "2022-03-26 22:10:29.123224 Epoch 100, Training Loss 0.6124025327165413\n",
      "2022-03-26 22:10:29.137261 Epoch 100, Training Loss 0.6132002200769342\n",
      "2022-03-26 22:10:29.152702 Epoch 100, Training Loss 0.6139735557386637\n",
      "2022-03-26 22:10:29.166548 Epoch 100, Training Loss 0.6150257209377825\n",
      "2022-03-26 22:10:29.181577 Epoch 100, Training Loss 0.6158511666843044\n",
      "2022-03-26 22:10:29.195581 Epoch 100, Training Loss 0.6165290080830265\n",
      "2022-03-26 22:10:29.210293 Epoch 100, Training Loss 0.6174390835072988\n",
      "2022-03-26 22:10:29.224253 Epoch 100, Training Loss 0.6185489562923646\n",
      "2022-03-26 22:10:29.238293 Epoch 100, Training Loss 0.6196042865590976\n",
      "2022-03-26 22:10:29.252939 Epoch 100, Training Loss 0.620523156488643\n",
      "2022-03-26 22:10:29.266964 Epoch 100, Training Loss 0.6214290475448989\n",
      "2022-03-26 22:10:29.281036 Epoch 100, Training Loss 0.6224058796377743\n",
      "2022-03-26 22:10:29.295064 Epoch 100, Training Loss 0.6235611986016374\n",
      "2022-03-26 22:10:29.309955 Epoch 100, Training Loss 0.6245256735540717\n",
      "2022-03-26 22:10:29.323983 Epoch 100, Training Loss 0.6253547105185516\n",
      "2022-03-26 22:10:29.337431 Epoch 100, Training Loss 0.6261897427804025\n",
      "2022-03-26 22:10:29.351459 Epoch 100, Training Loss 0.6271185839877409\n",
      "2022-03-26 22:10:29.366364 Epoch 100, Training Loss 0.6279562133200028\n",
      "2022-03-26 22:10:29.380387 Epoch 100, Training Loss 0.6290788219865325\n",
      "2022-03-26 22:10:29.395391 Epoch 100, Training Loss 0.6299984507700976\n",
      "2022-03-26 22:10:29.409381 Epoch 100, Training Loss 0.6308727193518978\n",
      "2022-03-26 22:10:29.423383 Epoch 100, Training Loss 0.6319459862721241\n",
      "2022-03-26 22:10:29.437382 Epoch 100, Training Loss 0.632893284339734\n",
      "2022-03-26 22:10:29.452422 Epoch 100, Training Loss 0.6338305598329705\n",
      "2022-03-26 22:10:29.466373 Epoch 100, Training Loss 0.6346975546663679\n",
      "2022-03-26 22:10:29.480378 Epoch 100, Training Loss 0.6356817371857441\n",
      "2022-03-26 22:10:29.494371 Epoch 100, Training Loss 0.6365437378816288\n",
      "2022-03-26 22:10:29.508410 Epoch 100, Training Loss 0.637572641186702\n",
      "2022-03-26 22:10:29.523412 Epoch 100, Training Loss 0.6384458384093117\n",
      "2022-03-26 22:10:29.537399 Epoch 100, Training Loss 0.6394204359377742\n",
      "2022-03-26 22:10:29.551426 Epoch 100, Training Loss 0.6405896376770781\n",
      "2022-03-26 22:10:29.566245 Epoch 100, Training Loss 0.641445621809996\n",
      "2022-03-26 22:10:29.580217 Epoch 100, Training Loss 0.6423297006150951\n",
      "2022-03-26 22:10:29.594190 Epoch 100, Training Loss 0.6434465970682062\n",
      "2022-03-26 22:10:29.608380 Epoch 100, Training Loss 0.6443057689825287\n",
      "2022-03-26 22:10:29.622370 Epoch 100, Training Loss 0.64514291827636\n",
      "2022-03-26 22:10:29.636378 Epoch 100, Training Loss 0.6459282114530158\n",
      "2022-03-26 22:10:29.651960 Epoch 100, Training Loss 0.6467457631664812\n",
      "2022-03-26 22:10:29.666136 Epoch 100, Training Loss 0.6477277346736635\n",
      "2022-03-26 22:10:29.681101 Epoch 100, Training Loss 0.6484780494514328\n",
      "2022-03-26 22:10:29.695129 Epoch 100, Training Loss 0.6494611417850875\n",
      "2022-03-26 22:10:29.708528 Epoch 100, Training Loss 0.6504832878899391\n",
      "2022-03-26 22:10:29.723556 Epoch 100, Training Loss 0.6512353024671754\n",
      "2022-03-26 22:10:29.737584 Epoch 100, Training Loss 0.6524092428519598\n",
      "2022-03-26 22:10:29.751492 Epoch 100, Training Loss 0.6532146645628888\n",
      "2022-03-26 22:10:29.765512 Epoch 100, Training Loss 0.6540247935925603\n",
      "2022-03-26 22:10:29.780489 Epoch 100, Training Loss 0.6549470620539487\n",
      "2022-03-26 22:10:29.795517 Epoch 100, Training Loss 0.6559191479554871\n",
      "2022-03-26 22:10:29.809531 Epoch 100, Training Loss 0.6569857992936888\n",
      "2022-03-26 22:10:29.823552 Epoch 100, Training Loss 0.6579091699074602\n",
      "2022-03-26 22:10:29.837628 Epoch 100, Training Loss 0.6588004932684057\n",
      "2022-03-26 22:10:29.851542 Epoch 100, Training Loss 0.6596807272690336\n",
      "2022-03-26 22:10:29.865562 Epoch 100, Training Loss 0.6606546900308955\n",
      "2022-03-26 22:10:29.879592 Epoch 100, Training Loss 0.6615721474370688\n",
      "2022-03-26 22:10:29.894612 Epoch 100, Training Loss 0.662424946837413\n",
      "2022-03-26 22:10:29.908006 Epoch 100, Training Loss 0.6632304674066851\n",
      "2022-03-26 22:10:29.922066 Epoch 100, Training Loss 0.6641966185301466\n",
      "2022-03-26 22:10:29.936950 Epoch 100, Training Loss 0.6649500726891295\n",
      "2022-03-26 22:10:29.950980 Epoch 100, Training Loss 0.6659780214814579\n",
      "2022-03-26 22:10:29.964899 Epoch 100, Training Loss 0.6669656641190619\n",
      "2022-03-26 22:10:29.979978 Epoch 100, Training Loss 0.6680304530026663\n",
      "2022-03-26 22:10:29.994942 Epoch 100, Training Loss 0.6689449528141704\n",
      "2022-03-26 22:10:30.008949 Epoch 100, Training Loss 0.6696689987883848\n",
      "2022-03-26 22:10:30.023053 Epoch 100, Training Loss 0.6704881305584822\n",
      "2022-03-26 22:10:30.037905 Epoch 100, Training Loss 0.6712957341652697\n",
      "2022-03-26 22:10:30.051881 Epoch 100, Training Loss 0.6719493772977453\n",
      "2022-03-26 22:10:30.066842 Epoch 100, Training Loss 0.6730774984792676\n",
      "2022-03-26 22:10:30.080915 Epoch 100, Training Loss 0.6738121712299259\n",
      "2022-03-26 22:10:30.094943 Epoch 100, Training Loss 0.674670561347776\n",
      "2022-03-26 22:10:30.108978 Epoch 100, Training Loss 0.6755091029664745\n",
      "2022-03-26 22:10:30.123074 Epoch 100, Training Loss 0.6765834042025954\n",
      "2022-03-26 22:10:30.137054 Epoch 100, Training Loss 0.6776831849761631\n",
      "2022-03-26 22:10:30.151167 Epoch 100, Training Loss 0.6786247741078477\n",
      "2022-03-26 22:10:30.164992 Epoch 100, Training Loss 0.6794304444509394\n",
      "2022-03-26 22:10:30.180078 Epoch 100, Training Loss 0.6800383755465602\n",
      "2022-03-26 22:10:30.195099 Epoch 100, Training Loss 0.6808898162353983\n",
      "2022-03-26 22:10:30.208457 Epoch 100, Training Loss 0.6818145143863795\n",
      "2022-03-26 22:10:30.223460 Epoch 100, Training Loss 0.6827436180980614\n",
      "2022-03-26 22:10:30.237479 Epoch 100, Training Loss 0.6836140011734975\n",
      "2022-03-26 22:10:30.251540 Epoch 100, Training Loss 0.6843709796285995\n",
      "2022-03-26 22:10:30.265544 Epoch 100, Training Loss 0.685122571409206\n",
      "2022-03-26 22:10:30.280525 Epoch 100, Training Loss 0.6861188780621189\n",
      "2022-03-26 22:10:30.294528 Epoch 100, Training Loss 0.6869483679304342\n",
      "2022-03-26 22:10:30.308480 Epoch 100, Training Loss 0.6881936252726923\n",
      "2022-03-26 22:10:30.322483 Epoch 100, Training Loss 0.6890795796423617\n",
      "2022-03-26 22:10:30.336353 Epoch 100, Training Loss 0.6901446301156603\n",
      "2022-03-26 22:10:30.351357 Epoch 100, Training Loss 0.69071455962975\n",
      "2022-03-26 22:10:30.365263 Epoch 100, Training Loss 0.6913887689561795\n",
      "2022-03-26 22:10:30.379638 Epoch 100, Training Loss 0.6921463436863916\n",
      "2022-03-26 22:10:30.394666 Epoch 100, Training Loss 0.6931322831708147\n",
      "2022-03-26 22:10:30.408690 Epoch 100, Training Loss 0.6940401289088037\n",
      "2022-03-26 22:10:30.422694 Epoch 100, Training Loss 0.6950047660590438\n",
      "2022-03-26 22:10:30.441714 Epoch 100, Training Loss 0.695705052874887\n",
      "2022-03-26 22:10:30.460712 Epoch 100, Training Loss 0.6963389244149712\n",
      "2022-03-26 22:10:30.479740 Epoch 100, Training Loss 0.6972069306218106\n",
      "2022-03-26 22:10:30.498741 Epoch 100, Training Loss 0.6983833658649489\n",
      "2022-03-26 22:10:30.517785 Epoch 100, Training Loss 0.699332825096367\n",
      "2022-03-26 22:10:30.537851 Epoch 100, Training Loss 0.7003232015444495\n",
      "2022-03-26 22:10:30.556781 Epoch 100, Training Loss 0.7012562922123448\n",
      "2022-03-26 22:10:30.575922 Epoch 100, Training Loss 0.7026146142302877\n",
      "2022-03-26 22:10:30.595938 Epoch 100, Training Loss 0.7036006435027817\n",
      "2022-03-26 22:10:30.616036 Epoch 100, Training Loss 0.7045252664238596\n",
      "2022-03-26 22:10:30.635140 Epoch 100, Training Loss 0.7053836371054125\n",
      "2022-03-26 22:10:30.654101 Epoch 100, Training Loss 0.7066154669389091\n",
      "2022-03-26 22:10:30.672609 Epoch 100, Training Loss 0.7074245570794396\n",
      "2022-03-26 22:10:30.691657 Epoch 100, Training Loss 0.7080798789744487\n",
      "2022-03-26 22:10:30.710921 Epoch 100, Training Loss 0.7090503585231883\n",
      "2022-03-26 22:10:30.729971 Epoch 100, Training Loss 0.710179694358955\n",
      "2022-03-26 22:10:30.737178 Epoch 100, Training Loss 0.7113321757758669\n",
      "2022-03-26 22:21:49.145155 Epoch 150, Training Loss 0.001327885996045359\n",
      "2022-03-26 22:21:49.161159 Epoch 150, Training Loss 0.0020831197576449655\n",
      "2022-03-26 22:21:49.176163 Epoch 150, Training Loss 0.0030832008632552593\n",
      "2022-03-26 22:21:49.190165 Epoch 150, Training Loss 0.004006823386682574\n",
      "2022-03-26 22:21:49.204168 Epoch 150, Training Loss 0.0047841251963544685\n",
      "2022-03-26 22:21:49.219172 Epoch 150, Training Loss 0.0055838343127609215\n",
      "2022-03-26 22:21:49.235175 Epoch 150, Training Loss 0.006638480345611377\n",
      "2022-03-26 22:21:49.250179 Epoch 150, Training Loss 0.007508442987261526\n",
      "2022-03-26 22:21:49.265182 Epoch 150, Training Loss 0.008053387865386046\n",
      "2022-03-26 22:21:49.280185 Epoch 150, Training Loss 0.008667926349298423\n",
      "2022-03-26 22:21:49.294188 Epoch 150, Training Loss 0.009637423336048565\n",
      "2022-03-26 22:21:49.308191 Epoch 150, Training Loss 0.010332407548909297\n",
      "2022-03-26 22:21:49.322195 Epoch 150, Training Loss 0.010894286586805378\n",
      "2022-03-26 22:21:49.336294 Epoch 150, Training Loss 0.011768971791352762\n",
      "2022-03-26 22:21:49.351297 Epoch 150, Training Loss 0.012660918562003718\n",
      "2022-03-26 22:21:49.365300 Epoch 150, Training Loss 0.013369659100042279\n",
      "2022-03-26 22:21:49.385660 Epoch 150, Training Loss 0.014163185415975273\n",
      "2022-03-26 22:21:49.404686 Epoch 150, Training Loss 0.015145361804596299\n",
      "2022-03-26 22:21:49.423690 Epoch 150, Training Loss 0.016084812714925508\n",
      "2022-03-26 22:21:49.442695 Epoch 150, Training Loss 0.017243490880712524\n",
      "2022-03-26 22:21:49.461699 Epoch 150, Training Loss 0.01832167221152264\n",
      "2022-03-26 22:21:49.480691 Epoch 150, Training Loss 0.01912837755649596\n",
      "2022-03-26 22:21:49.500702 Epoch 150, Training Loss 0.019788790160737684\n",
      "2022-03-26 22:21:49.519694 Epoch 150, Training Loss 0.020898783298404625\n",
      "2022-03-26 22:21:49.538717 Epoch 150, Training Loss 0.022016299731286285\n",
      "2022-03-26 22:21:49.558721 Epoch 150, Training Loss 0.022922766025718827\n",
      "2022-03-26 22:21:49.577726 Epoch 150, Training Loss 0.023383882573193602\n",
      "2022-03-26 22:21:49.596729 Epoch 150, Training Loss 0.024128869671345976\n",
      "2022-03-26 22:21:49.616734 Epoch 150, Training Loss 0.02501455010355586\n",
      "2022-03-26 22:21:49.635726 Epoch 150, Training Loss 0.026059719302770123\n",
      "2022-03-26 22:21:49.655743 Epoch 150, Training Loss 0.02671097931654557\n",
      "2022-03-26 22:21:49.674747 Epoch 150, Training Loss 0.02771145227315176\n",
      "2022-03-26 22:21:49.693751 Epoch 150, Training Loss 0.028577230013240022\n",
      "2022-03-26 22:21:49.712756 Epoch 150, Training Loss 0.029326048348565847\n",
      "2022-03-26 22:21:49.732972 Epoch 150, Training Loss 0.03012393548360566\n",
      "2022-03-26 22:21:49.752759 Epoch 150, Training Loss 0.0307009437733599\n",
      "2022-03-26 22:21:49.771757 Epoch 150, Training Loss 0.03127598339486915\n",
      "2022-03-26 22:21:49.790773 Epoch 150, Training Loss 0.03224884126040027\n",
      "2022-03-26 22:21:49.809778 Epoch 150, Training Loss 0.032814118715808215\n",
      "2022-03-26 22:21:49.828782 Epoch 150, Training Loss 0.033687756646929495\n",
      "2022-03-26 22:21:49.848787 Epoch 150, Training Loss 0.034748437993056935\n",
      "2022-03-26 22:21:49.868781 Epoch 150, Training Loss 0.03548610362860248\n",
      "2022-03-26 22:21:49.887796 Epoch 150, Training Loss 0.036145706844451786\n",
      "2022-03-26 22:21:49.906795 Epoch 150, Training Loss 0.03702938381363364\n",
      "2022-03-26 22:21:49.925798 Epoch 150, Training Loss 0.037724919102685835\n",
      "2022-03-26 22:21:49.945796 Epoch 150, Training Loss 0.03841397966570256\n",
      "2022-03-26 22:21:49.964813 Epoch 150, Training Loss 0.039192539575459706\n",
      "2022-03-26 22:21:49.984805 Epoch 150, Training Loss 0.04002465730737847\n",
      "2022-03-26 22:21:50.004816 Epoch 150, Training Loss 0.04089345948775406\n",
      "2022-03-26 22:21:50.023826 Epoch 150, Training Loss 0.0417504445518679\n",
      "2022-03-26 22:21:50.043831 Epoch 150, Training Loss 0.04269217385355469\n",
      "2022-03-26 22:21:50.063835 Epoch 150, Training Loss 0.043555293241730124\n",
      "2022-03-26 22:21:50.082826 Epoch 150, Training Loss 0.0445367519355491\n",
      "2022-03-26 22:21:50.102826 Epoch 150, Training Loss 0.045297312004791805\n",
      "2022-03-26 22:21:50.121849 Epoch 150, Training Loss 0.04604224224224725\n",
      "2022-03-26 22:21:50.140854 Epoch 150, Training Loss 0.04681554985473223\n",
      "2022-03-26 22:21:50.158857 Epoch 150, Training Loss 0.047656080073407846\n",
      "2022-03-26 22:21:50.178857 Epoch 150, Training Loss 0.04854368378439218\n",
      "2022-03-26 22:21:50.198866 Epoch 150, Training Loss 0.048936050223267594\n",
      "2022-03-26 22:21:50.218853 Epoch 150, Training Loss 0.04970768730506263\n",
      "2022-03-26 22:21:50.238863 Epoch 150, Training Loss 0.050481583723021896\n",
      "2022-03-26 22:21:50.257867 Epoch 150, Training Loss 0.05119433865675231\n",
      "2022-03-26 22:21:50.276884 Epoch 150, Training Loss 0.051780182496665994\n",
      "2022-03-26 22:21:50.295882 Epoch 150, Training Loss 0.05253402507671005\n",
      "2022-03-26 22:21:50.315892 Epoch 150, Training Loss 0.05337431291332635\n",
      "2022-03-26 22:21:50.335885 Epoch 150, Training Loss 0.05394629974042058\n",
      "2022-03-26 22:21:50.354901 Epoch 150, Training Loss 0.05445566342767242\n",
      "2022-03-26 22:21:50.373906 Epoch 150, Training Loss 0.05510470518828048\n",
      "2022-03-26 22:21:50.393898 Epoch 150, Training Loss 0.05588256074187091\n",
      "2022-03-26 22:21:50.413915 Epoch 150, Training Loss 0.05697099330937466\n",
      "2022-03-26 22:21:50.433901 Epoch 150, Training Loss 0.05773989261721101\n",
      "2022-03-26 22:21:50.452913 Epoch 150, Training Loss 0.05850609317612465\n",
      "2022-03-26 22:21:50.471916 Epoch 150, Training Loss 0.05953664178281184\n",
      "2022-03-26 22:21:50.490932 Epoch 150, Training Loss 0.06025345509162035\n",
      "2022-03-26 22:21:50.509937 Epoch 150, Training Loss 0.061103480665580086\n",
      "2022-03-26 22:21:50.528928 Epoch 150, Training Loss 0.061671661294024925\n",
      "2022-03-26 22:21:50.547945 Epoch 150, Training Loss 0.06253367281325943\n",
      "2022-03-26 22:21:50.566944 Epoch 150, Training Loss 0.06303398356870617\n",
      "2022-03-26 22:21:50.584941 Epoch 150, Training Loss 0.06388751087743608\n",
      "2022-03-26 22:21:50.604958 Epoch 150, Training Loss 0.06462329424098325\n",
      "2022-03-26 22:21:50.623956 Epoch 150, Training Loss 0.06545736497777807\n",
      "2022-03-26 22:21:50.642954 Epoch 150, Training Loss 0.06638820217850873\n",
      "2022-03-26 22:21:50.661965 Epoch 150, Training Loss 0.06737873609870901\n",
      "2022-03-26 22:21:50.681975 Epoch 150, Training Loss 0.06832586023051415\n",
      "2022-03-26 22:21:50.701980 Epoch 150, Training Loss 0.06910236980146764\n",
      "2022-03-26 22:21:50.721972 Epoch 150, Training Loss 0.06971042643270224\n",
      "2022-03-26 22:21:50.740989 Epoch 150, Training Loss 0.07046790809735008\n",
      "2022-03-26 22:21:50.760993 Epoch 150, Training Loss 0.07122589487706305\n",
      "2022-03-26 22:21:50.779998 Epoch 150, Training Loss 0.071992187815554\n",
      "2022-03-26 22:21:50.799996 Epoch 150, Training Loss 0.07287451278065782\n",
      "2022-03-26 22:21:50.818994 Epoch 150, Training Loss 0.07379333491977828\n",
      "2022-03-26 22:21:50.838005 Epoch 150, Training Loss 0.07442632252755373\n",
      "2022-03-26 22:21:50.857003 Epoch 150, Training Loss 0.07519411693905931\n",
      "2022-03-26 22:21:50.877020 Epoch 150, Training Loss 0.07583467022079945\n",
      "2022-03-26 22:21:50.896024 Epoch 150, Training Loss 0.076464949895049\n",
      "2022-03-26 22:21:50.915331 Epoch 150, Training Loss 0.07728758949757841\n",
      "2022-03-26 22:21:50.934336 Epoch 150, Training Loss 0.07820172482134437\n",
      "2022-03-26 22:21:50.953327 Epoch 150, Training Loss 0.07915340703161781\n",
      "2022-03-26 22:21:50.972338 Epoch 150, Training Loss 0.08004109839649151\n",
      "2022-03-26 22:21:50.992349 Epoch 150, Training Loss 0.0807873070849787\n",
      "2022-03-26 22:21:51.011353 Epoch 150, Training Loss 0.08160363362573297\n",
      "2022-03-26 22:21:51.031358 Epoch 150, Training Loss 0.082154385612139\n",
      "2022-03-26 22:21:51.052358 Epoch 150, Training Loss 0.08293700808911678\n",
      "2022-03-26 22:21:51.071368 Epoch 150, Training Loss 0.0840415744601613\n",
      "2022-03-26 22:21:51.089364 Epoch 150, Training Loss 0.08479750937665514\n",
      "2022-03-26 22:21:51.109375 Epoch 150, Training Loss 0.0859651925694912\n",
      "2022-03-26 22:21:51.128818 Epoch 150, Training Loss 0.08700101672078642\n",
      "2022-03-26 22:21:51.148828 Epoch 150, Training Loss 0.08790113900780983\n",
      "2022-03-26 22:21:51.168830 Epoch 150, Training Loss 0.08831967710686461\n",
      "2022-03-26 22:21:51.187837 Epoch 150, Training Loss 0.08914076874170766\n",
      "2022-03-26 22:21:51.206841 Epoch 150, Training Loss 0.08981098059345693\n",
      "2022-03-26 22:21:51.226846 Epoch 150, Training Loss 0.09064073136547947\n",
      "2022-03-26 22:21:51.245850 Epoch 150, Training Loss 0.09119827782406527\n",
      "2022-03-26 22:21:51.265836 Epoch 150, Training Loss 0.09194834831425601\n",
      "2022-03-26 22:21:51.284853 Epoch 150, Training Loss 0.09273172819705876\n",
      "2022-03-26 22:21:51.304864 Epoch 150, Training Loss 0.09352042066776539\n",
      "2022-03-26 22:21:51.323868 Epoch 150, Training Loss 0.09411972825941832\n",
      "2022-03-26 22:21:51.342872 Epoch 150, Training Loss 0.09520734595063397\n",
      "2022-03-26 22:21:51.361877 Epoch 150, Training Loss 0.09569663137121273\n",
      "2022-03-26 22:21:51.380881 Epoch 150, Training Loss 0.09675210950624608\n",
      "2022-03-26 22:21:51.400885 Epoch 150, Training Loss 0.09759310368076919\n",
      "2022-03-26 22:21:51.419877 Epoch 150, Training Loss 0.09823110844473094\n",
      "2022-03-26 22:21:51.439888 Epoch 150, Training Loss 0.09908136763536107\n",
      "2022-03-26 22:21:51.459899 Epoch 150, Training Loss 0.10004563747769427\n",
      "2022-03-26 22:21:51.480904 Epoch 150, Training Loss 0.10123709568282223\n",
      "2022-03-26 22:21:51.499908 Epoch 150, Training Loss 0.10191673871196444\n",
      "2022-03-26 22:21:51.518906 Epoch 150, Training Loss 0.10269672288309278\n",
      "2022-03-26 22:21:51.537917 Epoch 150, Training Loss 0.10346812581467202\n",
      "2022-03-26 22:21:51.556901 Epoch 150, Training Loss 0.10423481090904196\n",
      "2022-03-26 22:21:51.576905 Epoch 150, Training Loss 0.10503290231575442\n",
      "2022-03-26 22:21:51.594929 Epoch 150, Training Loss 0.10608597324632318\n",
      "2022-03-26 22:21:51.614934 Epoch 150, Training Loss 0.10686538179817102\n",
      "2022-03-26 22:21:51.633938 Epoch 150, Training Loss 0.10751985764259572\n",
      "2022-03-26 22:21:51.652937 Epoch 150, Training Loss 0.10828111139709687\n",
      "2022-03-26 22:21:51.671947 Epoch 150, Training Loss 0.10917482755678085\n",
      "2022-03-26 22:21:51.691952 Epoch 150, Training Loss 0.11003417287336285\n",
      "2022-03-26 22:21:51.710956 Epoch 150, Training Loss 0.11070980447942338\n",
      "2022-03-26 22:21:51.729960 Epoch 150, Training Loss 0.11138472044864274\n",
      "2022-03-26 22:21:51.749244 Epoch 150, Training Loss 0.11238171430804846\n",
      "2022-03-26 22:21:51.767963 Epoch 150, Training Loss 0.113240573793421\n",
      "2022-03-26 22:21:51.787973 Epoch 150, Training Loss 0.11407647512452987\n",
      "2022-03-26 22:21:51.806977 Epoch 150, Training Loss 0.1145309680486884\n",
      "2022-03-26 22:21:51.826982 Epoch 150, Training Loss 0.11533794108101779\n",
      "2022-03-26 22:21:51.845986 Epoch 150, Training Loss 0.11598698146965193\n",
      "2022-03-26 22:21:51.864991 Epoch 150, Training Loss 0.11678639027621131\n",
      "2022-03-26 22:21:51.884989 Epoch 150, Training Loss 0.1176171294792229\n",
      "2022-03-26 22:21:51.905000 Epoch 150, Training Loss 0.11848948842576702\n",
      "2022-03-26 22:21:51.925004 Epoch 150, Training Loss 0.1194057800352116\n",
      "2022-03-26 22:21:51.944009 Epoch 150, Training Loss 0.120131738998396\n",
      "2022-03-26 22:21:51.963921 Epoch 150, Training Loss 0.1210487765043288\n",
      "2022-03-26 22:21:51.982939 Epoch 150, Training Loss 0.1220210611134234\n",
      "2022-03-26 22:21:52.002931 Epoch 150, Training Loss 0.1227330006373203\n",
      "2022-03-26 22:21:52.021941 Epoch 150, Training Loss 0.12345928712116788\n",
      "2022-03-26 22:21:52.040952 Epoch 150, Training Loss 0.12422682070518698\n",
      "2022-03-26 22:21:52.059957 Epoch 150, Training Loss 0.12504994735845823\n",
      "2022-03-26 22:21:52.079961 Epoch 150, Training Loss 0.12611746410732075\n",
      "2022-03-26 22:21:52.099338 Epoch 150, Training Loss 0.12678272557228118\n",
      "2022-03-26 22:21:52.119960 Epoch 150, Training Loss 0.127722945778876\n",
      "2022-03-26 22:21:52.138968 Epoch 150, Training Loss 0.12843210415919418\n",
      "2022-03-26 22:21:52.157979 Epoch 150, Training Loss 0.12914777377529826\n",
      "2022-03-26 22:21:52.176983 Epoch 150, Training Loss 0.13001692115955646\n",
      "2022-03-26 22:21:52.196988 Epoch 150, Training Loss 0.13071683929551897\n",
      "2022-03-26 22:21:52.215977 Epoch 150, Training Loss 0.13147805455852957\n",
      "2022-03-26 22:21:52.235984 Epoch 150, Training Loss 0.13227655527079502\n",
      "2022-03-26 22:21:52.254995 Epoch 150, Training Loss 0.1330066496682594\n",
      "2022-03-26 22:21:52.275005 Epoch 150, Training Loss 0.13383143816305243\n",
      "2022-03-26 22:21:52.295010 Epoch 150, Training Loss 0.1345762690086194\n",
      "2022-03-26 22:21:52.315014 Epoch 150, Training Loss 0.13543627935144908\n",
      "2022-03-26 22:21:52.335015 Epoch 150, Training Loss 0.13652014781904342\n",
      "2022-03-26 22:21:52.355017 Epoch 150, Training Loss 0.13740437136739111\n",
      "2022-03-26 22:21:52.375028 Epoch 150, Training Loss 0.13803347838504235\n",
      "2022-03-26 22:21:52.395033 Epoch 150, Training Loss 0.13863828145634488\n",
      "2022-03-26 22:21:52.414037 Epoch 150, Training Loss 0.13960987039844094\n",
      "2022-03-26 22:21:52.434043 Epoch 150, Training Loss 0.14046275577581752\n",
      "2022-03-26 22:21:52.453033 Epoch 150, Training Loss 0.1413690393690563\n",
      "2022-03-26 22:21:52.473035 Epoch 150, Training Loss 0.1419907124222392\n",
      "2022-03-26 22:21:52.493055 Epoch 150, Training Loss 0.1426563286186789\n",
      "2022-03-26 22:21:52.512059 Epoch 150, Training Loss 0.14350378768675773\n",
      "2022-03-26 22:21:52.531063 Epoch 150, Training Loss 0.14463631763025317\n",
      "2022-03-26 22:21:52.553062 Epoch 150, Training Loss 0.14533970868953353\n",
      "2022-03-26 22:21:52.572066 Epoch 150, Training Loss 0.14620237052440643\n",
      "2022-03-26 22:21:52.591070 Epoch 150, Training Loss 0.14708663401243938\n",
      "2022-03-26 22:21:52.611082 Epoch 150, Training Loss 0.14790044785917872\n",
      "2022-03-26 22:21:52.631086 Epoch 150, Training Loss 0.14872009556769106\n",
      "2022-03-26 22:21:52.651086 Epoch 150, Training Loss 0.14967783832031747\n",
      "2022-03-26 22:21:52.671089 Epoch 150, Training Loss 0.15068113982982342\n",
      "2022-03-26 22:21:52.690099 Epoch 150, Training Loss 0.1516710317424496\n",
      "2022-03-26 22:21:52.710104 Epoch 150, Training Loss 0.152569292489525\n",
      "2022-03-26 22:21:52.729108 Epoch 150, Training Loss 0.15335615101220357\n",
      "2022-03-26 22:21:52.750114 Epoch 150, Training Loss 0.15440418733202893\n",
      "2022-03-26 22:21:52.770119 Epoch 150, Training Loss 0.15501275166983494\n",
      "2022-03-26 22:21:52.789122 Epoch 150, Training Loss 0.15560689309368964\n",
      "2022-03-26 22:21:52.810127 Epoch 150, Training Loss 0.15636373046414018\n",
      "2022-03-26 22:21:52.829131 Epoch 150, Training Loss 0.15724214926705032\n",
      "2022-03-26 22:21:52.849188 Epoch 150, Training Loss 0.1579377324227482\n",
      "2022-03-26 22:21:52.868134 Epoch 150, Training Loss 0.15849341364467845\n",
      "2022-03-26 22:21:52.887144 Epoch 150, Training Loss 0.1591860296018898\n",
      "2022-03-26 22:21:52.907148 Epoch 150, Training Loss 0.15989806737436357\n",
      "2022-03-26 22:21:52.926151 Epoch 150, Training Loss 0.16076543477490124\n",
      "2022-03-26 22:21:52.946157 Epoch 150, Training Loss 0.161392042246621\n",
      "2022-03-26 22:21:52.965147 Epoch 150, Training Loss 0.16225399499963922\n",
      "2022-03-26 22:21:52.986161 Epoch 150, Training Loss 0.16314356993226445\n",
      "2022-03-26 22:21:53.005164 Epoch 150, Training Loss 0.16428914993925167\n",
      "2022-03-26 22:21:53.025175 Epoch 150, Training Loss 0.16515951838029924\n",
      "2022-03-26 22:21:53.044180 Epoch 150, Training Loss 0.16620392239916965\n",
      "2022-03-26 22:21:53.064184 Epoch 150, Training Loss 0.16669143191383928\n",
      "2022-03-26 22:21:53.085903 Epoch 150, Training Loss 0.16755814816031006\n",
      "2022-03-26 22:21:53.105897 Epoch 150, Training Loss 0.16828800520628615\n",
      "2022-03-26 22:21:53.124908 Epoch 150, Training Loss 0.169144077145535\n",
      "2022-03-26 22:21:53.143897 Epoch 150, Training Loss 0.17013105925391703\n",
      "2022-03-26 22:21:53.163917 Epoch 150, Training Loss 0.17086515089739923\n",
      "2022-03-26 22:21:53.184915 Epoch 150, Training Loss 0.17208983411874307\n",
      "2022-03-26 22:21:53.203911 Epoch 150, Training Loss 0.1731877778954518\n",
      "2022-03-26 22:21:53.223930 Epoch 150, Training Loss 0.1738857947804434\n",
      "2022-03-26 22:21:53.242934 Epoch 150, Training Loss 0.17459103259284173\n",
      "2022-03-26 22:21:53.262939 Epoch 150, Training Loss 0.17540062944907361\n",
      "2022-03-26 22:21:53.282930 Epoch 150, Training Loss 0.17621209988813571\n",
      "2022-03-26 22:21:53.302942 Epoch 150, Training Loss 0.17726973956808104\n",
      "2022-03-26 22:21:53.321946 Epoch 150, Training Loss 0.1781538235561927\n",
      "2022-03-26 22:21:53.341957 Epoch 150, Training Loss 0.17881218620273462\n",
      "2022-03-26 22:21:53.360961 Epoch 150, Training Loss 0.17964427824825277\n",
      "2022-03-26 22:21:53.381959 Epoch 150, Training Loss 0.18074681882358268\n",
      "2022-03-26 22:21:53.400957 Epoch 150, Training Loss 0.1815893005227189\n",
      "2022-03-26 22:21:53.420975 Epoch 150, Training Loss 0.18265969170938673\n",
      "2022-03-26 22:21:53.440979 Epoch 150, Training Loss 0.18358686360556756\n",
      "2022-03-26 22:21:53.460984 Epoch 150, Training Loss 0.18431680563770597\n",
      "2022-03-26 22:21:53.479988 Epoch 150, Training Loss 0.185150211729357\n",
      "2022-03-26 22:21:53.500994 Epoch 150, Training Loss 0.18572526133578757\n",
      "2022-03-26 22:21:53.519998 Epoch 150, Training Loss 0.18654404096591198\n",
      "2022-03-26 22:21:53.538995 Epoch 150, Training Loss 0.18740021648919186\n",
      "2022-03-26 22:21:53.559006 Epoch 150, Training Loss 0.18832206139174265\n",
      "2022-03-26 22:21:53.579004 Epoch 150, Training Loss 0.18882355597013098\n",
      "2022-03-26 22:21:53.598015 Epoch 150, Training Loss 0.1896802866093033\n",
      "2022-03-26 22:21:53.619006 Epoch 150, Training Loss 0.19043427324660903\n",
      "2022-03-26 22:21:53.638018 Epoch 150, Training Loss 0.19112594597174992\n",
      "2022-03-26 22:21:53.658029 Epoch 150, Training Loss 0.1924626618395071\n",
      "2022-03-26 22:21:53.678033 Epoch 150, Training Loss 0.19345336009169478\n",
      "2022-03-26 22:21:53.697037 Epoch 150, Training Loss 0.1944747468852021\n",
      "2022-03-26 22:21:53.717043 Epoch 150, Training Loss 0.19534514695787064\n",
      "2022-03-26 22:21:53.736033 Epoch 150, Training Loss 0.1958438819631591\n",
      "2022-03-26 22:21:53.755044 Epoch 150, Training Loss 0.19663730118890552\n",
      "2022-03-26 22:21:53.775055 Epoch 150, Training Loss 0.19733162250970027\n",
      "2022-03-26 22:21:53.794059 Epoch 150, Training Loss 0.19814922986433026\n",
      "2022-03-26 22:21:53.814064 Epoch 150, Training Loss 0.19898607145489938\n",
      "2022-03-26 22:21:53.834055 Epoch 150, Training Loss 0.19978837908991157\n",
      "2022-03-26 22:21:53.854066 Epoch 150, Training Loss 0.20046228246615672\n",
      "2022-03-26 22:21:53.873077 Epoch 150, Training Loss 0.2015421236567485\n",
      "2022-03-26 22:21:53.892075 Epoch 150, Training Loss 0.20240882679324626\n",
      "2022-03-26 22:21:53.912087 Epoch 150, Training Loss 0.2036079506739936\n",
      "2022-03-26 22:21:53.932431 Epoch 150, Training Loss 0.20440166502657448\n",
      "2022-03-26 22:21:53.952081 Epoch 150, Training Loss 0.20512124888427422\n",
      "2022-03-26 22:21:53.971100 Epoch 150, Training Loss 0.20569661777952444\n",
      "2022-03-26 22:21:53.990089 Epoch 150, Training Loss 0.20661304925408813\n",
      "2022-03-26 22:21:54.011108 Epoch 150, Training Loss 0.20751721848307364\n",
      "2022-03-26 22:21:54.030113 Epoch 150, Training Loss 0.2080809772014618\n",
      "2022-03-26 22:21:54.058119 Epoch 150, Training Loss 0.209095702832922\n",
      "2022-03-26 22:21:54.085119 Epoch 150, Training Loss 0.2099010926835677\n",
      "2022-03-26 22:21:54.111131 Epoch 150, Training Loss 0.21061751292184797\n",
      "2022-03-26 22:21:54.138137 Epoch 150, Training Loss 0.21156604393668796\n",
      "2022-03-26 22:21:54.165143 Epoch 150, Training Loss 0.21221745410538695\n",
      "2022-03-26 22:21:54.192149 Epoch 150, Training Loss 0.2131707967089875\n",
      "2022-03-26 22:21:54.219145 Epoch 150, Training Loss 0.21398106339337575\n",
      "2022-03-26 22:21:54.244161 Epoch 150, Training Loss 0.2149161561523252\n",
      "2022-03-26 22:21:54.271153 Epoch 150, Training Loss 0.21551587644135556\n",
      "2022-03-26 22:21:54.297167 Epoch 150, Training Loss 0.21660274526347284\n",
      "2022-03-26 22:21:54.324179 Epoch 150, Training Loss 0.21737794757194226\n",
      "2022-03-26 22:21:54.350186 Epoch 150, Training Loss 0.21817342856960834\n",
      "2022-03-26 22:21:54.376185 Epoch 150, Training Loss 0.21890247218749104\n",
      "2022-03-26 22:21:54.402190 Epoch 150, Training Loss 0.21968356796237817\n",
      "2022-03-26 22:21:54.428203 Epoch 150, Training Loss 0.22041003051621225\n",
      "2022-03-26 22:21:54.446207 Epoch 150, Training Loss 0.22138700354129762\n",
      "2022-03-26 22:21:54.464211 Epoch 150, Training Loss 0.2222214155947156\n",
      "2022-03-26 22:21:54.483218 Epoch 150, Training Loss 0.22297753969116893\n",
      "2022-03-26 22:21:54.502206 Epoch 150, Training Loss 0.22365078375772443\n",
      "2022-03-26 22:21:54.520224 Epoch 150, Training Loss 0.2245181857624932\n",
      "2022-03-26 22:21:54.538228 Epoch 150, Training Loss 0.2255332597228877\n",
      "2022-03-26 22:21:54.557232 Epoch 150, Training Loss 0.2263131277335574\n",
      "2022-03-26 22:21:54.576237 Epoch 150, Training Loss 0.22701164043467978\n",
      "2022-03-26 22:21:54.596241 Epoch 150, Training Loss 0.2276567459258887\n",
      "2022-03-26 22:21:54.615245 Epoch 150, Training Loss 0.22860801501957048\n",
      "2022-03-26 22:21:54.634252 Epoch 150, Training Loss 0.2293705284747931\n",
      "2022-03-26 22:21:54.652254 Epoch 150, Training Loss 0.2303548035261881\n",
      "2022-03-26 22:21:54.670258 Epoch 150, Training Loss 0.2312630500330035\n",
      "2022-03-26 22:21:54.688262 Epoch 150, Training Loss 0.2320932307664086\n",
      "2022-03-26 22:21:54.707266 Epoch 150, Training Loss 0.23271731247225075\n",
      "2022-03-26 22:21:54.725270 Epoch 150, Training Loss 0.23343864978884188\n",
      "2022-03-26 22:21:54.744275 Epoch 150, Training Loss 0.2343186202942563\n",
      "2022-03-26 22:21:54.763279 Epoch 150, Training Loss 0.235201546641262\n",
      "2022-03-26 22:21:54.781283 Epoch 150, Training Loss 0.2360024127890082\n",
      "2022-03-26 22:21:54.800288 Epoch 150, Training Loss 0.23699488713765693\n",
      "2022-03-26 22:21:54.818416 Epoch 150, Training Loss 0.23798079018855034\n",
      "2022-03-26 22:21:54.837425 Epoch 150, Training Loss 0.23903752825296748\n",
      "2022-03-26 22:21:54.855422 Epoch 150, Training Loss 0.23990157433330556\n",
      "2022-03-26 22:21:54.874433 Epoch 150, Training Loss 0.24066662578783987\n",
      "2022-03-26 22:21:54.895438 Epoch 150, Training Loss 0.2413963115657382\n",
      "2022-03-26 22:21:54.914442 Epoch 150, Training Loss 0.24213205514204167\n",
      "2022-03-26 22:21:54.934644 Epoch 150, Training Loss 0.24295151405169835\n",
      "2022-03-26 22:21:54.952639 Epoch 150, Training Loss 0.24370276078086375\n",
      "2022-03-26 22:21:54.970652 Epoch 150, Training Loss 0.24480602415778752\n",
      "2022-03-26 22:21:54.988656 Epoch 150, Training Loss 0.24564953270318257\n",
      "2022-03-26 22:21:55.007660 Epoch 150, Training Loss 0.24646642625026996\n",
      "2022-03-26 22:21:55.025658 Epoch 150, Training Loss 0.24732006110651109\n",
      "2022-03-26 22:21:55.044675 Epoch 150, Training Loss 0.24790515630598872\n",
      "2022-03-26 22:21:55.063679 Epoch 150, Training Loss 0.2485220950963857\n",
      "2022-03-26 22:21:55.081683 Epoch 150, Training Loss 0.24954782876059833\n",
      "2022-03-26 22:21:55.101682 Epoch 150, Training Loss 0.2503663987073752\n",
      "2022-03-26 22:21:55.119673 Epoch 150, Training Loss 0.2513173329631996\n",
      "2022-03-26 22:21:55.137690 Epoch 150, Training Loss 0.2522506331620009\n",
      "2022-03-26 22:21:55.155694 Epoch 150, Training Loss 0.25319864717133517\n",
      "2022-03-26 22:21:55.174705 Epoch 150, Training Loss 0.25395308381608683\n",
      "2022-03-26 22:21:55.193709 Epoch 150, Training Loss 0.2548476871855728\n",
      "2022-03-26 22:21:55.212714 Epoch 150, Training Loss 0.25580130265954204\n",
      "2022-03-26 22:21:55.230711 Epoch 150, Training Loss 0.2564071907716639\n",
      "2022-03-26 22:21:55.248873 Epoch 150, Training Loss 0.25717597155619765\n",
      "2022-03-26 22:21:55.266726 Epoch 150, Training Loss 0.25791490847802223\n",
      "2022-03-26 22:21:55.285730 Epoch 150, Training Loss 0.2586000193567837\n",
      "2022-03-26 22:21:55.303734 Epoch 150, Training Loss 0.25937158799232424\n",
      "2022-03-26 22:21:55.322738 Epoch 150, Training Loss 0.25992783016103616\n",
      "2022-03-26 22:21:55.340744 Epoch 150, Training Loss 0.26065256090267847\n",
      "2022-03-26 22:21:55.358751 Epoch 150, Training Loss 0.2618272240890566\n",
      "2022-03-26 22:21:55.377750 Epoch 150, Training Loss 0.26244998168762385\n",
      "2022-03-26 22:21:55.396742 Epoch 150, Training Loss 0.2632792887785246\n",
      "2022-03-26 22:21:55.415759 Epoch 150, Training Loss 0.26421456035140833\n",
      "2022-03-26 22:21:55.434757 Epoch 150, Training Loss 0.26494729069187817\n",
      "2022-03-26 22:21:55.452761 Epoch 150, Training Loss 0.26593991626254126\n",
      "2022-03-26 22:21:55.470766 Epoch 150, Training Loss 0.2666120373684427\n",
      "2022-03-26 22:21:55.488770 Epoch 150, Training Loss 0.26726912003953746\n",
      "2022-03-26 22:21:55.507774 Epoch 150, Training Loss 0.2678681315134858\n",
      "2022-03-26 22:21:55.526784 Epoch 150, Training Loss 0.2685189688830729\n",
      "2022-03-26 22:21:55.545789 Epoch 150, Training Loss 0.2693168676036703\n",
      "2022-03-26 22:21:55.564787 Epoch 150, Training Loss 0.2703361844696352\n",
      "2022-03-26 22:21:55.584785 Epoch 150, Training Loss 0.2711499463338071\n",
      "2022-03-26 22:21:55.602796 Epoch 150, Training Loss 0.2719958072809307\n",
      "2022-03-26 22:21:55.620800 Epoch 150, Training Loss 0.2726746853583914\n",
      "2022-03-26 22:21:55.638804 Epoch 150, Training Loss 0.27358636294332\n",
      "2022-03-26 22:21:55.657814 Epoch 150, Training Loss 0.2746263042359096\n",
      "2022-03-26 22:21:55.675818 Epoch 150, Training Loss 0.27526115060157486\n",
      "2022-03-26 22:21:55.694822 Epoch 150, Training Loss 0.27613865208747745\n",
      "2022-03-26 22:21:55.713821 Epoch 150, Training Loss 0.2771105080309426\n",
      "2022-03-26 22:21:55.732812 Epoch 150, Training Loss 0.2780119638766169\n",
      "2022-03-26 22:21:55.750823 Epoch 150, Training Loss 0.2785658213259924\n",
      "2022-03-26 22:21:55.769833 Epoch 150, Training Loss 0.27925084150203355\n",
      "2022-03-26 22:21:55.787837 Epoch 150, Training Loss 0.2799349654742214\n",
      "2022-03-26 22:21:55.805841 Epoch 150, Training Loss 0.28064602365731584\n",
      "2022-03-26 22:21:55.823852 Epoch 150, Training Loss 0.2813628337648519\n",
      "2022-03-26 22:21:55.842856 Epoch 150, Training Loss 0.2822380509141766\n",
      "2022-03-26 22:21:55.861860 Epoch 150, Training Loss 0.28339112243231607\n",
      "2022-03-26 22:21:55.879864 Epoch 150, Training Loss 0.28458526315134197\n",
      "2022-03-26 22:21:55.898869 Epoch 150, Training Loss 0.2855917424573313\n",
      "2022-03-26 22:21:55.917856 Epoch 150, Training Loss 0.28640423303522416\n",
      "2022-03-26 22:21:55.935871 Epoch 150, Training Loss 0.2872199248093778\n",
      "2022-03-26 22:21:55.953869 Epoch 150, Training Loss 0.28807018819215047\n",
      "2022-03-26 22:21:55.971879 Epoch 150, Training Loss 0.2888904662464586\n",
      "2022-03-26 22:21:55.990883 Epoch 150, Training Loss 0.28940142317653617\n",
      "2022-03-26 22:21:56.008887 Epoch 150, Training Loss 0.29048581268933726\n",
      "2022-03-26 22:21:56.026898 Epoch 150, Training Loss 0.2911684550059116\n",
      "2022-03-26 22:21:56.045902 Epoch 150, Training Loss 0.29208842007552877\n",
      "2022-03-26 22:21:56.065888 Epoch 150, Training Loss 0.29268115015743335\n",
      "2022-03-26 22:21:56.083898 Epoch 150, Training Loss 0.2935065256284021\n",
      "2022-03-26 22:21:56.102896 Epoch 150, Training Loss 0.2941877125093089\n",
      "2022-03-26 22:21:56.121913 Epoch 150, Training Loss 0.29487958066451275\n",
      "2022-03-26 22:21:56.140923 Epoch 150, Training Loss 0.29560266679052805\n",
      "2022-03-26 22:21:56.159928 Epoch 150, Training Loss 0.29626954733715644\n",
      "2022-03-26 22:21:56.178932 Epoch 150, Training Loss 0.2970943858327768\n",
      "2022-03-26 22:21:56.196936 Epoch 150, Training Loss 0.29793876055104046\n",
      "2022-03-26 22:21:56.215943 Epoch 150, Training Loss 0.2987006645449592\n",
      "2022-03-26 22:21:56.233944 Epoch 150, Training Loss 0.2994860043687284\n",
      "2022-03-26 22:21:56.252931 Epoch 150, Training Loss 0.3002979335806254\n",
      "2022-03-26 22:21:56.278955 Epoch 150, Training Loss 0.3009754210100759\n",
      "2022-03-26 22:21:56.304955 Epoch 150, Training Loss 0.3019666659176502\n",
      "2022-03-26 22:21:56.331952 Epoch 150, Training Loss 0.3027347058362668\n",
      "2022-03-26 22:21:56.357973 Epoch 150, Training Loss 0.3032130929438964\n",
      "2022-03-26 22:21:56.383966 Epoch 150, Training Loss 0.3038039937653505\n",
      "2022-03-26 22:21:56.409984 Epoch 150, Training Loss 0.30439872407090024\n",
      "2022-03-26 22:21:56.435984 Epoch 150, Training Loss 0.30511403971773277\n",
      "2022-03-26 22:21:56.461996 Epoch 150, Training Loss 0.30564020951385695\n",
      "2022-03-26 22:21:56.475988 Epoch 150, Training Loss 0.30640007345877646\n",
      "2022-03-26 22:21:56.489991 Epoch 150, Training Loss 0.3072283120106553\n",
      "2022-03-26 22:21:56.503994 Epoch 150, Training Loss 0.30840838549996885\n",
      "2022-03-26 22:21:56.518998 Epoch 150, Training Loss 0.3091638576039268\n",
      "2022-03-26 22:21:56.534001 Epoch 150, Training Loss 0.30973913179486606\n",
      "2022-03-26 22:21:56.548010 Epoch 150, Training Loss 0.3106387081887106\n",
      "2022-03-26 22:21:56.562007 Epoch 150, Training Loss 0.3112154156731827\n",
      "2022-03-26 22:21:56.577010 Epoch 150, Training Loss 0.31197826091743186\n",
      "2022-03-26 22:21:56.591013 Epoch 150, Training Loss 0.3129646633668324\n",
      "2022-03-26 22:21:56.606023 Epoch 150, Training Loss 0.3136335364769182\n",
      "2022-03-26 22:21:56.620020 Epoch 150, Training Loss 0.31432690458072116\n",
      "2022-03-26 22:21:56.634023 Epoch 150, Training Loss 0.3149112442036724\n",
      "2022-03-26 22:21:56.650027 Epoch 150, Training Loss 0.3155564901697666\n",
      "2022-03-26 22:21:56.664031 Epoch 150, Training Loss 0.31626554793866396\n",
      "2022-03-26 22:21:56.679034 Epoch 150, Training Loss 0.3170337251308934\n",
      "2022-03-26 22:21:56.694037 Epoch 150, Training Loss 0.3176557508575947\n",
      "2022-03-26 22:21:56.708039 Epoch 150, Training Loss 0.31844859697934613\n",
      "2022-03-26 22:21:56.723044 Epoch 150, Training Loss 0.3193671417510723\n",
      "2022-03-26 22:21:56.737039 Epoch 150, Training Loss 0.3201780055489991\n",
      "2022-03-26 22:21:56.752050 Epoch 150, Training Loss 0.3207663467625523\n",
      "2022-03-26 22:21:56.766046 Epoch 150, Training Loss 0.3216179841772065\n",
      "2022-03-26 22:21:56.781056 Epoch 150, Training Loss 0.32238162211749866\n",
      "2022-03-26 22:21:56.795059 Epoch 150, Training Loss 0.32335695418555416\n",
      "2022-03-26 22:21:56.809063 Epoch 150, Training Loss 0.3243248013736647\n",
      "2022-03-26 22:21:56.824066 Epoch 150, Training Loss 0.32514023125324104\n",
      "2022-03-26 22:21:56.838071 Epoch 150, Training Loss 0.32587601309237274\n",
      "2022-03-26 22:21:56.852073 Epoch 150, Training Loss 0.326698949727256\n",
      "2022-03-26 22:21:56.867076 Epoch 150, Training Loss 0.3273016831758992\n",
      "2022-03-26 22:21:56.882080 Epoch 150, Training Loss 0.3282375670302555\n",
      "2022-03-26 22:21:56.896083 Epoch 150, Training Loss 0.3292392499916389\n",
      "2022-03-26 22:21:56.911085 Epoch 150, Training Loss 0.33033277951847867\n",
      "2022-03-26 22:21:56.926089 Epoch 150, Training Loss 0.33113306936095743\n",
      "2022-03-26 22:21:56.940092 Epoch 150, Training Loss 0.3321699817924548\n",
      "2022-03-26 22:21:56.955096 Epoch 150, Training Loss 0.33296645663278485\n",
      "2022-03-26 22:21:56.968099 Epoch 150, Training Loss 0.33382088532838067\n",
      "2022-03-26 22:21:56.983102 Epoch 150, Training Loss 0.33478724895535833\n",
      "2022-03-26 22:21:56.997106 Epoch 150, Training Loss 0.3355421557298402\n",
      "2022-03-26 22:21:57.013110 Epoch 150, Training Loss 0.3363847339244755\n",
      "2022-03-26 22:21:57.028113 Epoch 150, Training Loss 0.3376419798035146\n",
      "2022-03-26 22:21:57.043116 Epoch 150, Training Loss 0.3384764518426812\n",
      "2022-03-26 22:21:57.057120 Epoch 150, Training Loss 0.33912828153051683\n",
      "2022-03-26 22:21:57.072123 Epoch 150, Training Loss 0.34001073812889626\n",
      "2022-03-26 22:21:57.086126 Epoch 150, Training Loss 0.34061947407777354\n",
      "2022-03-26 22:21:57.100130 Epoch 150, Training Loss 0.3414755986855768\n",
      "2022-03-26 22:21:57.115132 Epoch 150, Training Loss 0.3422131012848881\n",
      "2022-03-26 22:21:57.130137 Epoch 150, Training Loss 0.34303591977757264\n",
      "2022-03-26 22:21:57.145139 Epoch 150, Training Loss 0.3436212139513791\n",
      "2022-03-26 22:21:57.160142 Epoch 150, Training Loss 0.3443403186090767\n",
      "2022-03-26 22:21:57.174146 Epoch 150, Training Loss 0.3451820988484356\n",
      "2022-03-26 22:21:57.189149 Epoch 150, Training Loss 0.3458557958188264\n",
      "2022-03-26 22:21:57.203153 Epoch 150, Training Loss 0.34648791367135695\n",
      "2022-03-26 22:21:57.218155 Epoch 150, Training Loss 0.3473873707796911\n",
      "2022-03-26 22:21:57.232160 Epoch 150, Training Loss 0.34844336843551577\n",
      "2022-03-26 22:21:57.247162 Epoch 150, Training Loss 0.3491104361041428\n",
      "2022-03-26 22:21:57.261166 Epoch 150, Training Loss 0.3503898575787654\n",
      "2022-03-26 22:21:57.276169 Epoch 150, Training Loss 0.35109132985629693\n",
      "2022-03-26 22:21:57.290172 Epoch 150, Training Loss 0.3519756502050268\n",
      "2022-03-26 22:21:57.305175 Epoch 150, Training Loss 0.3528802035867101\n",
      "2022-03-26 22:21:57.318179 Epoch 150, Training Loss 0.353628197899255\n",
      "2022-03-26 22:21:57.334178 Epoch 150, Training Loss 0.3545104402410405\n",
      "2022-03-26 22:21:57.348186 Epoch 150, Training Loss 0.3550592944064104\n",
      "2022-03-26 22:21:57.363188 Epoch 150, Training Loss 0.35560841507771435\n",
      "2022-03-26 22:21:57.377193 Epoch 150, Training Loss 0.3563589111847036\n",
      "2022-03-26 22:21:57.392196 Epoch 150, Training Loss 0.35758447704260304\n",
      "2022-03-26 22:21:57.406199 Epoch 150, Training Loss 0.3584510903910298\n",
      "2022-03-26 22:21:57.421203 Epoch 150, Training Loss 0.35934592310882285\n",
      "2022-03-26 22:21:57.435206 Epoch 150, Training Loss 0.36005761849758267\n",
      "2022-03-26 22:21:57.450209 Epoch 150, Training Loss 0.3606530752633234\n",
      "2022-03-26 22:21:57.464212 Epoch 150, Training Loss 0.36160303930492355\n",
      "2022-03-26 22:21:57.480215 Epoch 150, Training Loss 0.36236629431205025\n",
      "2022-03-26 22:21:57.494218 Epoch 150, Training Loss 0.362934810807333\n",
      "2022-03-26 22:21:57.509222 Epoch 150, Training Loss 0.36362044052089876\n",
      "2022-03-26 22:21:57.523225 Epoch 150, Training Loss 0.36435702839470885\n",
      "2022-03-26 22:21:57.538229 Epoch 150, Training Loss 0.3648250160162406\n",
      "2022-03-26 22:21:57.553231 Epoch 150, Training Loss 0.36532652225640727\n",
      "2022-03-26 22:21:57.567235 Epoch 150, Training Loss 0.36628179759015816\n",
      "2022-03-26 22:21:57.582238 Epoch 150, Training Loss 0.3672097185078789\n",
      "2022-03-26 22:21:57.596241 Epoch 150, Training Loss 0.3679207379708205\n",
      "2022-03-26 22:21:57.611244 Epoch 150, Training Loss 0.3686916312140882\n",
      "2022-03-26 22:21:57.625248 Epoch 150, Training Loss 0.3694410598491464\n",
      "2022-03-26 22:21:57.639251 Epoch 150, Training Loss 0.370462781861615\n",
      "2022-03-26 22:21:57.653255 Epoch 150, Training Loss 0.37118547461221896\n",
      "2022-03-26 22:21:57.667259 Epoch 150, Training Loss 0.371959112794198\n",
      "2022-03-26 22:21:57.682261 Epoch 150, Training Loss 0.37301453284900205\n",
      "2022-03-26 22:21:57.696264 Epoch 150, Training Loss 0.373707376630105\n",
      "2022-03-26 22:21:57.710268 Epoch 150, Training Loss 0.37458399258306263\n",
      "2022-03-26 22:21:57.724271 Epoch 150, Training Loss 0.3755262815738883\n",
      "2022-03-26 22:21:57.740276 Epoch 150, Training Loss 0.37649105790326054\n",
      "2022-03-26 22:21:57.754277 Epoch 150, Training Loss 0.37724193755318136\n",
      "2022-03-26 22:21:57.768281 Epoch 150, Training Loss 0.3784319372738109\n",
      "2022-03-26 22:21:57.782283 Epoch 150, Training Loss 0.37915569993541065\n",
      "2022-03-26 22:21:57.796288 Epoch 150, Training Loss 0.37984329843155257\n",
      "2022-03-26 22:21:57.810290 Epoch 150, Training Loss 0.3808417838552724\n",
      "2022-03-26 22:21:57.825293 Epoch 150, Training Loss 0.38168025451243076\n",
      "2022-03-26 22:21:57.839297 Epoch 150, Training Loss 0.38258392464779223\n",
      "2022-03-26 22:21:57.853300 Epoch 150, Training Loss 0.38347410614533195\n",
      "2022-03-26 22:21:57.868303 Epoch 150, Training Loss 0.38456792835994147\n",
      "2022-03-26 22:21:57.883307 Epoch 150, Training Loss 0.38562129113985144\n",
      "2022-03-26 22:21:57.898302 Epoch 150, Training Loss 0.3864274419786985\n",
      "2022-03-26 22:21:57.913313 Epoch 150, Training Loss 0.38719539134703634\n",
      "2022-03-26 22:21:57.927317 Epoch 150, Training Loss 0.3882399244838968\n",
      "2022-03-26 22:21:57.942320 Epoch 150, Training Loss 0.38912952845663673\n",
      "2022-03-26 22:21:57.957323 Epoch 150, Training Loss 0.389788896912504\n",
      "2022-03-26 22:21:57.971326 Epoch 150, Training Loss 0.3905202959047254\n",
      "2022-03-26 22:21:57.985330 Epoch 150, Training Loss 0.39127307802514955\n",
      "2022-03-26 22:21:58.000333 Epoch 150, Training Loss 0.3920249749175118\n",
      "2022-03-26 22:21:58.014337 Epoch 150, Training Loss 0.3931064575986789\n",
      "2022-03-26 22:21:58.029339 Epoch 150, Training Loss 0.3940463078296398\n",
      "2022-03-26 22:21:58.044343 Epoch 150, Training Loss 0.3949267013603464\n",
      "2022-03-26 22:21:58.058347 Epoch 150, Training Loss 0.3957846002353122\n",
      "2022-03-26 22:21:58.073349 Epoch 150, Training Loss 0.39674999608713035\n",
      "2022-03-26 22:21:58.087353 Epoch 150, Training Loss 0.3976300030260745\n",
      "2022-03-26 22:21:58.101356 Epoch 150, Training Loss 0.3985931762801412\n",
      "2022-03-26 22:21:58.116359 Epoch 150, Training Loss 0.39940485480191457\n",
      "2022-03-26 22:21:58.131362 Epoch 150, Training Loss 0.40030476717692814\n",
      "2022-03-26 22:21:58.146366 Epoch 150, Training Loss 0.40102088794378976\n",
      "2022-03-26 22:21:58.161370 Epoch 150, Training Loss 0.40191892368714216\n",
      "2022-03-26 22:21:58.176373 Epoch 150, Training Loss 0.40281392896876617\n",
      "2022-03-26 22:21:58.190376 Epoch 150, Training Loss 0.40380332788543016\n",
      "2022-03-26 22:21:58.204380 Epoch 150, Training Loss 0.4046055513727086\n",
      "2022-03-26 22:21:58.218382 Epoch 150, Training Loss 0.405265212059021\n",
      "2022-03-26 22:21:58.233386 Epoch 150, Training Loss 0.4060490161866483\n",
      "2022-03-26 22:21:58.247390 Epoch 150, Training Loss 0.40675725221938797\n",
      "2022-03-26 22:21:58.265390 Epoch 150, Training Loss 0.40757699756671095\n",
      "2022-03-26 22:21:58.284401 Epoch 150, Training Loss 0.40832814071184537\n",
      "2022-03-26 22:21:58.305408 Epoch 150, Training Loss 0.40909476528692124\n",
      "2022-03-26 22:21:58.323418 Epoch 150, Training Loss 0.4101759237249184\n",
      "2022-03-26 22:21:58.343423 Epoch 150, Training Loss 0.41076988953611127\n",
      "2022-03-26 22:21:58.363427 Epoch 150, Training Loss 0.4114715436001873\n",
      "2022-03-26 22:21:58.383419 Epoch 150, Training Loss 0.4122738950621441\n",
      "2022-03-26 22:21:58.403430 Epoch 150, Training Loss 0.4133902717658016\n",
      "2022-03-26 22:21:58.421434 Epoch 150, Training Loss 0.4142685365265288\n",
      "2022-03-26 22:21:58.440439 Epoch 150, Training Loss 0.4151817152795889\n",
      "2022-03-26 22:21:58.460443 Epoch 150, Training Loss 0.41602314017770237\n",
      "2022-03-26 22:21:58.480448 Epoch 150, Training Loss 0.4170723803284223\n",
      "2022-03-26 22:21:58.500446 Epoch 150, Training Loss 0.41769287218828033\n",
      "2022-03-26 22:21:58.520463 Epoch 150, Training Loss 0.418570478935071\n",
      "2022-03-26 22:21:58.538461 Epoch 150, Training Loss 0.4194029724354024\n",
      "2022-03-26 22:21:58.558472 Epoch 150, Training Loss 0.42031921053786414\n",
      "2022-03-26 22:21:58.578476 Epoch 150, Training Loss 0.42111361308780776\n",
      "2022-03-26 22:21:58.598486 Epoch 150, Training Loss 0.4219218613698964\n",
      "2022-03-26 22:21:58.617479 Epoch 150, Training Loss 0.42286619063838365\n",
      "2022-03-26 22:21:58.636486 Epoch 150, Training Loss 0.42384924554763853\n",
      "2022-03-26 22:21:58.655487 Epoch 150, Training Loss 0.4246039373795395\n",
      "2022-03-26 22:21:58.674498 Epoch 150, Training Loss 0.42544048445304034\n",
      "2022-03-26 22:21:58.694496 Epoch 150, Training Loss 0.42631943847822107\n",
      "2022-03-26 22:21:58.713507 Epoch 150, Training Loss 0.42736001179346345\n",
      "2022-03-26 22:21:58.733646 Epoch 150, Training Loss 0.42819870105179986\n",
      "2022-03-26 22:21:58.753651 Epoch 150, Training Loss 0.42899204070305885\n",
      "2022-03-26 22:21:58.772667 Epoch 150, Training Loss 0.4299231107582522\n",
      "2022-03-26 22:21:58.792672 Epoch 150, Training Loss 0.4307865793137904\n",
      "2022-03-26 22:21:58.811676 Epoch 150, Training Loss 0.4317004391756814\n",
      "2022-03-26 22:21:58.831662 Epoch 150, Training Loss 0.4323772952684661\n",
      "2022-03-26 22:21:58.850679 Epoch 150, Training Loss 0.43339553765018884\n",
      "2022-03-26 22:21:58.869689 Epoch 150, Training Loss 0.4341411698047462\n",
      "2022-03-26 22:21:58.888693 Epoch 150, Training Loss 0.43494651620955116\n",
      "2022-03-26 22:21:58.907698 Epoch 150, Training Loss 0.4357556747201154\n",
      "2022-03-26 22:21:58.926696 Epoch 150, Training Loss 0.43705603358385814\n",
      "2022-03-26 22:21:58.945706 Epoch 150, Training Loss 0.4376632507957156\n",
      "2022-03-26 22:21:58.964695 Epoch 150, Training Loss 0.438681331513178\n",
      "2022-03-26 22:21:58.985696 Epoch 150, Training Loss 0.4394003716118805\n",
      "2022-03-26 22:21:59.004700 Epoch 150, Training Loss 0.4403159817313904\n",
      "2022-03-26 22:21:59.022711 Epoch 150, Training Loss 0.4411427904577816\n",
      "2022-03-26 22:21:59.042728 Epoch 150, Training Loss 0.44215264794466747\n",
      "2022-03-26 22:21:59.061732 Epoch 150, Training Loss 0.4429139464407626\n",
      "2022-03-26 22:21:59.080737 Epoch 150, Training Loss 0.44373931589029025\n",
      "2022-03-26 22:21:59.099735 Epoch 150, Training Loss 0.4447170764284061\n",
      "2022-03-26 22:21:59.119726 Epoch 150, Training Loss 0.4453986674318533\n",
      "2022-03-26 22:21:59.137738 Epoch 150, Training Loss 0.44608827228741266\n",
      "2022-03-26 22:21:59.157735 Epoch 150, Training Loss 0.4468406065345725\n",
      "2022-03-26 22:21:59.176759 Epoch 150, Training Loss 0.447745763889664\n",
      "2022-03-26 22:21:59.197763 Epoch 150, Training Loss 0.4487445083115717\n",
      "2022-03-26 22:21:59.216756 Epoch 150, Training Loss 0.44957478310141114\n",
      "2022-03-26 22:21:59.236760 Epoch 150, Training Loss 0.45056748679836695\n",
      "2022-03-26 22:21:59.255776 Epoch 150, Training Loss 0.45153375842687116\n",
      "2022-03-26 22:21:59.274769 Epoch 150, Training Loss 0.4522424188568769\n",
      "2022-03-26 22:21:59.293785 Epoch 150, Training Loss 0.45331796546421393\n",
      "2022-03-26 22:21:59.313790 Epoch 150, Training Loss 0.4541411106391331\n",
      "2022-03-26 22:21:59.334783 Epoch 150, Training Loss 0.4549295458647296\n",
      "2022-03-26 22:21:59.353793 Epoch 150, Training Loss 0.4558542298386469\n",
      "2022-03-26 22:21:59.372803 Epoch 150, Training Loss 0.45671503981361\n",
      "2022-03-26 22:21:59.391807 Epoch 150, Training Loss 0.4575147290363946\n",
      "2022-03-26 22:21:59.411812 Epoch 150, Training Loss 0.4586365095642217\n",
      "2022-03-26 22:21:59.431816 Epoch 150, Training Loss 0.45974738183228864\n",
      "2022-03-26 22:21:59.451939 Epoch 150, Training Loss 0.4605657943069478\n",
      "2022-03-26 22:21:59.470937 Epoch 150, Training Loss 0.46179035481284647\n",
      "2022-03-26 22:21:59.489947 Epoch 150, Training Loss 0.4625014484386005\n",
      "2022-03-26 22:21:59.508951 Epoch 150, Training Loss 0.4633254630638815\n",
      "2022-03-26 22:21:59.528956 Epoch 150, Training Loss 0.4638156002516027\n",
      "2022-03-26 22:21:59.548961 Epoch 150, Training Loss 0.46460021308163546\n",
      "2022-03-26 22:21:59.567962 Epoch 150, Training Loss 0.4653720499380775\n",
      "2022-03-26 22:21:59.587970 Epoch 150, Training Loss 0.4662287327868249\n",
      "2022-03-26 22:21:59.606974 Epoch 150, Training Loss 0.4669695059433008\n",
      "2022-03-26 22:21:59.626979 Epoch 150, Training Loss 0.4678680850645465\n",
      "2022-03-26 22:21:59.646983 Epoch 150, Training Loss 0.46881980972979076\n",
      "2022-03-26 22:21:59.666975 Epoch 150, Training Loss 0.46968980339329564\n",
      "2022-03-26 22:21:59.686992 Epoch 150, Training Loss 0.47058775933349833\n",
      "2022-03-26 22:21:59.705990 Epoch 150, Training Loss 0.471327573022879\n",
      "2022-03-26 22:21:59.725001 Epoch 150, Training Loss 0.47227837023375285\n",
      "2022-03-26 22:21:59.744999 Epoch 150, Training Loss 0.4729434770467641\n",
      "2022-03-26 22:21:59.764009 Epoch 150, Training Loss 0.47370600963340087\n",
      "2022-03-26 22:21:59.784008 Epoch 150, Training Loss 0.47420474997414347\n",
      "2022-03-26 22:21:59.804012 Epoch 150, Training Loss 0.47535239983245237\n",
      "2022-03-26 22:21:59.824023 Epoch 150, Training Loss 0.4761144802012407\n",
      "2022-03-26 22:21:59.843027 Epoch 150, Training Loss 0.47698952615870843\n",
      "2022-03-26 22:21:59.863032 Epoch 150, Training Loss 0.47761224305538263\n",
      "2022-03-26 22:21:59.883032 Epoch 150, Training Loss 0.47853808482284743\n",
      "2022-03-26 22:21:59.903035 Epoch 150, Training Loss 0.4791943295989805\n",
      "2022-03-26 22:21:59.922045 Epoch 150, Training Loss 0.48000000466776016\n",
      "2022-03-26 22:21:59.942050 Epoch 150, Training Loss 0.48069693639760125\n",
      "2022-03-26 22:21:59.961054 Epoch 150, Training Loss 0.4818995455494317\n",
      "2022-03-26 22:21:59.980058 Epoch 150, Training Loss 0.4828469266214639\n",
      "2022-03-26 22:22:00.000050 Epoch 150, Training Loss 0.48367602037041996\n",
      "2022-03-26 22:22:00.021062 Epoch 150, Training Loss 0.4843976992322966\n",
      "2022-03-26 22:22:00.041072 Epoch 150, Training Loss 0.48546222492557045\n",
      "2022-03-26 22:22:00.060077 Epoch 150, Training Loss 0.4860150030506846\n",
      "2022-03-26 22:22:00.080081 Epoch 150, Training Loss 0.48681819393201864\n",
      "2022-03-26 22:22:00.099085 Epoch 150, Training Loss 0.4878981227764998\n",
      "2022-03-26 22:22:00.118084 Epoch 150, Training Loss 0.488580551903571\n",
      "2022-03-26 22:22:00.137094 Epoch 150, Training Loss 0.48948788231291124\n",
      "2022-03-26 22:22:00.157099 Epoch 150, Training Loss 0.49045635169119484\n",
      "2022-03-26 22:22:00.176103 Epoch 150, Training Loss 0.4912901549692959\n",
      "2022-03-26 22:22:00.195107 Epoch 150, Training Loss 0.49192828191515736\n",
      "2022-03-26 22:22:00.215106 Epoch 150, Training Loss 0.49276396822746454\n",
      "2022-03-26 22:22:00.235111 Epoch 150, Training Loss 0.49334802956837215\n",
      "2022-03-26 22:22:00.254108 Epoch 150, Training Loss 0.4942367581455299\n",
      "2022-03-26 22:22:00.274119 Epoch 150, Training Loss 0.4947673025948312\n",
      "2022-03-26 22:22:00.293129 Epoch 150, Training Loss 0.4957303298098962\n",
      "2022-03-26 22:22:00.312134 Epoch 150, Training Loss 0.4965488702592338\n",
      "2022-03-26 22:22:00.332138 Epoch 150, Training Loss 0.497428976361404\n",
      "2022-03-26 22:22:00.352131 Epoch 150, Training Loss 0.4984843413848096\n",
      "2022-03-26 22:22:00.371141 Epoch 150, Training Loss 0.49911543753598353\n",
      "2022-03-26 22:22:00.391152 Epoch 150, Training Loss 0.500161768339784\n",
      "2022-03-26 22:22:00.411150 Epoch 150, Training Loss 0.50123534273461\n",
      "2022-03-26 22:22:00.431160 Epoch 150, Training Loss 0.5021199621736546\n",
      "2022-03-26 22:22:00.450148 Epoch 150, Training Loss 0.5028704606434878\n",
      "2022-03-26 22:22:00.470164 Epoch 150, Training Loss 0.5034800312098335\n",
      "2022-03-26 22:22:00.489168 Epoch 150, Training Loss 0.5043210320917847\n",
      "2022-03-26 22:22:00.508178 Epoch 150, Training Loss 0.5051316291932255\n",
      "2022-03-26 22:22:00.528183 Epoch 150, Training Loss 0.5059360867113714\n",
      "2022-03-26 22:22:00.548298 Epoch 150, Training Loss 0.5067510296926474\n",
      "2022-03-26 22:22:00.568186 Epoch 150, Training Loss 0.5073337453176908\n",
      "2022-03-26 22:22:00.587190 Epoch 150, Training Loss 0.5082314100183184\n",
      "2022-03-26 22:22:00.607194 Epoch 150, Training Loss 0.5089284073742454\n",
      "2022-03-26 22:22:00.626199 Epoch 150, Training Loss 0.5099146855075646\n",
      "2022-03-26 22:22:00.646209 Epoch 150, Training Loss 0.5105790048837662\n",
      "2022-03-26 22:22:00.666202 Epoch 150, Training Loss 0.5113424369517494\n",
      "2022-03-26 22:22:00.686212 Epoch 150, Training Loss 0.5121547126632822\n",
      "2022-03-26 22:22:00.705223 Epoch 150, Training Loss 0.5129837539723462\n",
      "2022-03-26 22:22:00.724227 Epoch 150, Training Loss 0.513867603505359\n",
      "2022-03-26 22:22:00.743231 Epoch 150, Training Loss 0.5147975927499859\n",
      "2022-03-26 22:22:00.763236 Epoch 150, Training Loss 0.5156236241769303\n",
      "2022-03-26 22:22:00.783228 Epoch 150, Training Loss 0.5164242792693551\n",
      "2022-03-26 22:22:00.803245 Epoch 150, Training Loss 0.5175630765421616\n",
      "2022-03-26 22:22:00.822254 Epoch 150, Training Loss 0.5188286669952485\n",
      "2022-03-26 22:22:00.841254 Epoch 150, Training Loss 0.5196368214495651\n",
      "2022-03-26 22:22:00.860252 Epoch 150, Training Loss 0.5207605470553078\n",
      "2022-03-26 22:22:00.881897 Epoch 150, Training Loss 0.5219600986108146\n",
      "2022-03-26 22:22:00.901261 Epoch 150, Training Loss 0.5227872976637862\n",
      "2022-03-26 22:22:00.920266 Epoch 150, Training Loss 0.5234225868721447\n",
      "2022-03-26 22:22:00.939276 Epoch 150, Training Loss 0.5244186698170878\n",
      "2022-03-26 22:22:00.959280 Epoch 150, Training Loss 0.5251978675422766\n",
      "2022-03-26 22:22:00.978285 Epoch 150, Training Loss 0.5260489206484822\n",
      "2022-03-26 22:22:00.998284 Epoch 150, Training Loss 0.5268885435350715\n",
      "2022-03-26 22:22:01.018288 Epoch 150, Training Loss 0.527829308720196\n",
      "2022-03-26 22:22:01.037292 Epoch 150, Training Loss 0.5286205256991374\n",
      "2022-03-26 22:22:01.056302 Epoch 150, Training Loss 0.5293455753484955\n",
      "2022-03-26 22:22:01.077307 Epoch 150, Training Loss 0.5302692431470623\n",
      "2022-03-26 22:22:01.097312 Epoch 150, Training Loss 0.5309547371876514\n",
      "2022-03-26 22:22:01.117305 Epoch 150, Training Loss 0.5320077140617858\n",
      "2022-03-26 22:22:01.136315 Epoch 150, Training Loss 0.5329531551626943\n",
      "2022-03-26 22:22:01.156325 Epoch 150, Training Loss 0.5339848417455278\n",
      "2022-03-26 22:22:01.175329 Epoch 150, Training Loss 0.5350366614358809\n",
      "2022-03-26 22:22:01.195328 Epoch 150, Training Loss 0.5356845151432945\n",
      "2022-03-26 22:22:01.214914 Epoch 150, Training Loss 0.536424143921079\n",
      "2022-03-26 22:22:01.234459 Epoch 150, Training Loss 0.537541121091989\n",
      "2022-03-26 22:22:01.253464 Epoch 150, Training Loss 0.5382082403620796\n",
      "2022-03-26 22:22:01.273451 Epoch 150, Training Loss 0.5391198450037281\n",
      "2022-03-26 22:22:01.293473 Epoch 150, Training Loss 0.5397392469827477\n",
      "2022-03-26 22:22:01.312477 Epoch 150, Training Loss 0.540524656014979\n",
      "2022-03-26 22:22:01.332470 Epoch 150, Training Loss 0.5412700094301682\n",
      "2022-03-26 22:22:01.351480 Epoch 150, Training Loss 0.5420572582032065\n",
      "2022-03-26 22:22:01.370490 Epoch 150, Training Loss 0.5427974536824409\n",
      "2022-03-26 22:22:01.390495 Epoch 150, Training Loss 0.5438532761067075\n",
      "2022-03-26 22:22:01.410493 Epoch 150, Training Loss 0.544675421646184\n",
      "2022-03-26 22:22:01.429504 Epoch 150, Training Loss 0.5456672145048981\n",
      "2022-03-26 22:22:01.449497 Epoch 150, Training Loss 0.5466296716647989\n",
      "2022-03-26 22:22:01.470513 Epoch 150, Training Loss 0.5473783957912489\n",
      "2022-03-26 22:22:01.490518 Epoch 150, Training Loss 0.548371911620545\n",
      "2022-03-26 22:22:01.510522 Epoch 150, Training Loss 0.5492238334911254\n",
      "2022-03-26 22:22:01.529520 Epoch 150, Training Loss 0.5501032416396738\n",
      "2022-03-26 22:22:01.550345 Epoch 150, Training Loss 0.5512660929690236\n",
      "2022-03-26 22:22:01.570349 Epoch 150, Training Loss 0.5521625951504159\n",
      "2022-03-26 22:22:01.589354 Epoch 150, Training Loss 0.552816962227797\n",
      "2022-03-26 22:22:01.608364 Epoch 150, Training Loss 0.5534396482169476\n",
      "2022-03-26 22:22:01.628368 Epoch 150, Training Loss 0.5543697977538609\n",
      "2022-03-26 22:22:01.648373 Epoch 150, Training Loss 0.5551431558244978\n",
      "2022-03-26 22:22:01.668373 Epoch 150, Training Loss 0.5556792798630722\n",
      "2022-03-26 22:22:01.686376 Epoch 150, Training Loss 0.5564713742955566\n",
      "2022-03-26 22:22:01.706386 Epoch 150, Training Loss 0.5574491934474471\n",
      "2022-03-26 22:22:01.726391 Epoch 150, Training Loss 0.5583679793054795\n",
      "2022-03-26 22:22:01.753380 Epoch 150, Training Loss 0.5593274435042725\n",
      "2022-03-26 22:22:01.779384 Epoch 150, Training Loss 0.5600386603790171\n",
      "2022-03-26 22:22:01.802390 Epoch 150, Training Loss 0.5607506717028825\n",
      "2022-03-26 22:22:01.823392 Epoch 150, Training Loss 0.5614866674937251\n",
      "2022-03-26 22:22:01.843399 Epoch 150, Training Loss 0.562421406283403\n",
      "2022-03-26 22:22:01.864405 Epoch 150, Training Loss 0.5634379139565446\n",
      "2022-03-26 22:22:01.884407 Epoch 150, Training Loss 0.5643068703315447\n",
      "2022-03-26 22:22:01.903411 Epoch 150, Training Loss 0.5651526414143765\n",
      "2022-03-26 22:22:01.923436 Epoch 150, Training Loss 0.5659789501324944\n",
      "2022-03-26 22:22:01.943440 Epoch 150, Training Loss 0.5669671262392912\n",
      "2022-03-26 22:22:01.963438 Epoch 150, Training Loss 0.5676958376870436\n",
      "2022-03-26 22:22:01.982450 Epoch 150, Training Loss 0.5687102850364603\n",
      "2022-03-26 22:22:02.002447 Epoch 150, Training Loss 0.569409518023891\n",
      "2022-03-26 22:22:02.021452 Epoch 150, Training Loss 0.570087091735257\n",
      "2022-03-26 22:22:02.040462 Epoch 150, Training Loss 0.5710238954981269\n",
      "2022-03-26 22:22:02.060466 Epoch 150, Training Loss 0.5716736858229503\n",
      "2022-03-26 22:22:02.080471 Epoch 150, Training Loss 0.5725066662978029\n",
      "2022-03-26 22:22:02.100463 Epoch 150, Training Loss 0.5732225407953457\n",
      "2022-03-26 22:22:02.119479 Epoch 150, Training Loss 0.573834603323656\n",
      "2022-03-26 22:22:02.138478 Epoch 150, Training Loss 0.5747986983917558\n",
      "2022-03-26 22:22:02.159483 Epoch 150, Training Loss 0.5755520597138368\n",
      "2022-03-26 22:22:02.179487 Epoch 150, Training Loss 0.5767473813213045\n",
      "2022-03-26 22:22:02.199486 Epoch 150, Training Loss 0.5776042242336761\n",
      "2022-03-26 22:22:02.219502 Epoch 150, Training Loss 0.578350944211111\n",
      "2022-03-26 22:22:02.238501 Epoch 150, Training Loss 0.5791379426751295\n",
      "2022-03-26 22:22:02.258511 Epoch 150, Training Loss 0.580009744219158\n",
      "2022-03-26 22:22:02.277515 Epoch 150, Training Loss 0.5808570733308183\n",
      "2022-03-26 22:22:02.297520 Epoch 150, Training Loss 0.5818338079373245\n",
      "2022-03-26 22:22:02.317519 Epoch 150, Training Loss 0.5831118505781568\n",
      "2022-03-26 22:22:02.337523 Epoch 150, Training Loss 0.5838910564589683\n",
      "2022-03-26 22:22:02.356533 Epoch 150, Training Loss 0.5848128339061347\n",
      "2022-03-26 22:22:02.377538 Epoch 150, Training Loss 0.5854082492458851\n",
      "2022-03-26 22:22:02.397543 Epoch 150, Training Loss 0.5865250590359768\n",
      "2022-03-26 22:22:02.417541 Epoch 150, Training Loss 0.5873531569605288\n",
      "2022-03-26 22:22:02.437539 Epoch 150, Training Loss 0.5880908772463689\n",
      "2022-03-26 22:22:02.457556 Epoch 150, Training Loss 0.5889394872481256\n",
      "2022-03-26 22:22:02.476561 Epoch 150, Training Loss 0.5897258611591271\n",
      "2022-03-26 22:22:02.495559 Epoch 150, Training Loss 0.5902493357505945\n",
      "2022-03-26 22:22:02.514570 Epoch 150, Training Loss 0.590917683623331\n",
      "2022-03-26 22:22:02.541569 Epoch 150, Training Loss 0.5917872332246102\n",
      "2022-03-26 22:22:02.568582 Epoch 150, Training Loss 0.5926766201205875\n",
      "2022-03-26 22:22:02.595587 Epoch 150, Training Loss 0.5934584000531364\n",
      "2022-03-26 22:22:02.621581 Epoch 150, Training Loss 0.5941581304573342\n",
      "2022-03-26 22:22:02.649601 Epoch 150, Training Loss 0.5946805003049124\n",
      "2022-03-26 22:22:02.675606 Epoch 150, Training Loss 0.5954567667315988\n",
      "2022-03-26 22:22:02.702612 Epoch 150, Training Loss 0.5962066228889749\n",
      "2022-03-26 22:22:02.728703 Epoch 150, Training Loss 0.5969485021613138\n",
      "2022-03-26 22:22:02.754618 Epoch 150, Training Loss 0.5977239627819841\n",
      "2022-03-26 22:22:02.780623 Epoch 150, Training Loss 0.5987189875539306\n",
      "2022-03-26 22:22:02.807636 Epoch 150, Training Loss 0.5994291662255211\n",
      "2022-03-26 22:22:02.834638 Epoch 150, Training Loss 0.6006752185504455\n",
      "2022-03-26 22:22:02.860647 Epoch 150, Training Loss 0.6013480876870168\n",
      "2022-03-26 22:22:02.886653 Epoch 150, Training Loss 0.6025397669323875\n",
      "2022-03-26 22:22:02.912659 Epoch 150, Training Loss 0.6033274429228604\n",
      "2022-03-26 22:22:02.930663 Epoch 150, Training Loss 0.6041775446413727\n",
      "2022-03-26 22:22:02.949662 Epoch 150, Training Loss 0.6048706146457311\n",
      "2022-03-26 22:22:02.968667 Epoch 150, Training Loss 0.6056477097446656\n",
      "2022-03-26 22:22:02.986664 Epoch 150, Training Loss 0.6063887265027331\n",
      "2022-03-26 22:22:03.004680 Epoch 150, Training Loss 0.6070223800895159\n",
      "2022-03-26 22:22:03.023684 Epoch 150, Training Loss 0.6080030797768736\n",
      "2022-03-26 22:22:03.041689 Epoch 150, Training Loss 0.6087179980085938\n",
      "2022-03-26 22:22:03.060687 Epoch 150, Training Loss 0.6096128292781923\n",
      "2022-03-26 22:22:03.079697 Epoch 150, Training Loss 0.6105134459331517\n",
      "2022-03-26 22:22:03.098048 Epoch 150, Training Loss 0.6115313742090674\n",
      "2022-03-26 22:22:03.116694 Epoch 150, Training Loss 0.6123145562989633\n",
      "2022-03-26 22:22:03.134705 Epoch 150, Training Loss 0.6131103167982053\n",
      "2022-03-26 22:22:03.152708 Epoch 150, Training Loss 0.6138819813194787\n",
      "2022-03-26 22:22:03.170712 Epoch 150, Training Loss 0.6147501208364506\n",
      "2022-03-26 22:22:03.189722 Epoch 150, Training Loss 0.6154121450527245\n",
      "2022-03-26 22:22:03.207726 Epoch 150, Training Loss 0.6166213482161007\n",
      "2022-03-26 22:22:03.226731 Epoch 150, Training Loss 0.6171917169905075\n",
      "2022-03-26 22:22:03.245735 Epoch 150, Training Loss 0.6178825777357496\n",
      "2022-03-26 22:22:03.264635 Epoch 150, Training Loss 0.6184926575711925\n",
      "2022-03-26 22:22:03.282645 Epoch 150, Training Loss 0.6191266340291713\n",
      "2022-03-26 22:22:03.300642 Epoch 150, Training Loss 0.6200044085378842\n",
      "2022-03-26 22:22:03.319665 Epoch 150, Training Loss 0.620910420022962\n",
      "2022-03-26 22:22:03.337664 Epoch 150, Training Loss 0.6217113119333296\n",
      "2022-03-26 22:22:03.356674 Epoch 150, Training Loss 0.6224764649901549\n",
      "2022-03-26 22:22:03.375672 Epoch 150, Training Loss 0.6232179202463316\n",
      "2022-03-26 22:22:03.393676 Epoch 150, Training Loss 0.6239586273956177\n",
      "2022-03-26 22:22:03.411686 Epoch 150, Training Loss 0.6248145526860986\n",
      "2022-03-26 22:22:03.431692 Epoch 150, Training Loss 0.6254891079786183\n",
      "2022-03-26 22:22:03.451689 Epoch 150, Training Loss 0.6263240738521756\n",
      "2022-03-26 22:22:03.469694 Epoch 150, Training Loss 0.6273590833558451\n",
      "2022-03-26 22:22:03.487697 Epoch 150, Training Loss 0.628130649857204\n",
      "2022-03-26 22:22:03.506708 Epoch 150, Training Loss 0.6286633190748941\n",
      "2022-03-26 22:22:03.525712 Epoch 150, Training Loss 0.6295470065625427\n",
      "2022-03-26 22:22:03.543716 Epoch 150, Training Loss 0.6307216772185568\n",
      "2022-03-26 22:22:03.563721 Epoch 150, Training Loss 0.631407178957444\n",
      "2022-03-26 22:22:03.581708 Epoch 150, Training Loss 0.6320970010422075\n",
      "2022-03-26 22:22:03.600723 Epoch 150, Training Loss 0.6330729800721874\n",
      "2022-03-26 22:22:03.619717 Epoch 150, Training Loss 0.6339464381985043\n",
      "2022-03-26 22:22:03.638727 Epoch 150, Training Loss 0.6347970981579607\n",
      "2022-03-26 22:22:03.656736 Epoch 150, Training Loss 0.6356875749347765\n",
      "2022-03-26 22:22:03.674736 Epoch 150, Training Loss 0.6363904181191379\n",
      "2022-03-26 22:22:03.692745 Epoch 150, Training Loss 0.6373120720886514\n",
      "2022-03-26 22:22:03.710748 Epoch 150, Training Loss 0.6380461581680171\n",
      "2022-03-26 22:22:03.728747 Epoch 150, Training Loss 0.6389882027188225\n",
      "2022-03-26 22:22:03.746750 Epoch 150, Training Loss 0.6400328297596758\n",
      "2022-03-26 22:22:03.765755 Epoch 150, Training Loss 0.6407574669022085\n",
      "2022-03-26 22:22:03.783760 Epoch 150, Training Loss 0.6414775131913402\n",
      "2022-03-26 22:22:03.801763 Epoch 150, Training Loss 0.6426140778052533\n",
      "2022-03-26 22:22:03.820768 Epoch 150, Training Loss 0.6436693221711747\n",
      "2022-03-26 22:22:03.838773 Epoch 150, Training Loss 0.6443157060371946\n",
      "2022-03-26 22:22:03.846766 Epoch 150, Training Loss 0.6448226516585216\n",
      "2022-03-26 22:33:26.522027 Epoch 200, Training Loss 0.0009438740780286472\n",
      "2022-03-26 22:33:26.543031 Epoch 200, Training Loss 0.001664943478601363\n",
      "2022-03-26 22:33:26.560036 Epoch 200, Training Loss 0.002686317238356451\n",
      "2022-03-26 22:33:26.577039 Epoch 200, Training Loss 0.003286686051837014\n",
      "2022-03-26 22:33:26.595043 Epoch 200, Training Loss 0.004206499899439799\n",
      "2022-03-26 22:33:26.612047 Epoch 200, Training Loss 0.0052328818213299415\n",
      "2022-03-26 22:33:26.630051 Epoch 200, Training Loss 0.0058131867738635944\n",
      "2022-03-26 22:33:26.648055 Epoch 200, Training Loss 0.006603608084151812\n",
      "2022-03-26 22:33:26.665058 Epoch 200, Training Loss 0.00742416929863298\n",
      "2022-03-26 22:33:26.683062 Epoch 200, Training Loss 0.008280617311177655\n",
      "2022-03-26 22:33:26.701068 Epoch 200, Training Loss 0.009071762490150568\n",
      "2022-03-26 22:33:26.719091 Epoch 200, Training Loss 0.009920354427583992\n",
      "2022-03-26 22:33:26.738096 Epoch 200, Training Loss 0.010792058256581007\n",
      "2022-03-26 22:33:26.757100 Epoch 200, Training Loss 0.011534641199099744\n",
      "2022-03-26 22:33:26.776104 Epoch 200, Training Loss 0.012359975434630119\n",
      "2022-03-26 22:33:26.795103 Epoch 200, Training Loss 0.013129008052599095\n",
      "2022-03-26 22:33:26.813113 Epoch 200, Training Loss 0.01371763860020796\n",
      "2022-03-26 22:33:26.832117 Epoch 200, Training Loss 0.014646503435986122\n",
      "2022-03-26 22:33:26.850121 Epoch 200, Training Loss 0.015392328505320928\n",
      "2022-03-26 22:33:26.869125 Epoch 200, Training Loss 0.016151024519330096\n",
      "2022-03-26 22:33:26.888123 Epoch 200, Training Loss 0.016813879999358333\n",
      "2022-03-26 22:33:26.907128 Epoch 200, Training Loss 0.01742170533865614\n",
      "2022-03-26 22:33:26.925138 Epoch 200, Training Loss 0.01817144914661222\n",
      "2022-03-26 22:33:26.944136 Epoch 200, Training Loss 0.019032770486743858\n",
      "2022-03-26 22:33:26.962140 Epoch 200, Training Loss 0.019713753553302696\n",
      "2022-03-26 22:33:26.980150 Epoch 200, Training Loss 0.020479869354716348\n",
      "2022-03-26 22:33:26.999155 Epoch 200, Training Loss 0.021237829152275536\n",
      "2022-03-26 22:33:27.018159 Epoch 200, Training Loss 0.02216038328912252\n",
      "2022-03-26 22:33:27.036164 Epoch 200, Training Loss 0.022967333302778357\n",
      "2022-03-26 22:33:27.054170 Epoch 200, Training Loss 0.023888650147811226\n",
      "2022-03-26 22:33:27.072165 Epoch 200, Training Loss 0.024831426722924117\n",
      "2022-03-26 22:33:27.091170 Epoch 200, Training Loss 0.025506272797694293\n",
      "2022-03-26 22:33:27.109181 Epoch 200, Training Loss 0.026109746693040402\n",
      "2022-03-26 22:33:27.127178 Epoch 200, Training Loss 0.02658997963913871\n",
      "2022-03-26 22:33:27.145182 Epoch 200, Training Loss 0.027381449213723087\n",
      "2022-03-26 22:33:27.163186 Epoch 200, Training Loss 0.028094545464076654\n",
      "2022-03-26 22:33:27.181196 Epoch 200, Training Loss 0.02894494612046215\n",
      "2022-03-26 22:33:27.200200 Epoch 200, Training Loss 0.029792422871760395\n",
      "2022-03-26 22:33:27.218198 Epoch 200, Training Loss 0.03072264954409636\n",
      "2022-03-26 22:33:27.236202 Epoch 200, Training Loss 0.03183169053186236\n",
      "2022-03-26 22:33:27.254213 Epoch 200, Training Loss 0.03253587344875726\n",
      "2022-03-26 22:33:27.272217 Epoch 200, Training Loss 0.0335563105314284\n",
      "2022-03-26 22:33:27.290221 Epoch 200, Training Loss 0.034065337551524265\n",
      "2022-03-26 22:33:27.309210 Epoch 200, Training Loss 0.034678615229513945\n",
      "2022-03-26 22:33:27.327229 Epoch 200, Training Loss 0.035195736476527455\n",
      "2022-03-26 22:33:27.345227 Epoch 200, Training Loss 0.03613663512422605\n",
      "2022-03-26 22:33:27.363237 Epoch 200, Training Loss 0.036627592409358305\n",
      "2022-03-26 22:33:27.382242 Epoch 200, Training Loss 0.03720950935502796\n",
      "2022-03-26 22:33:27.400245 Epoch 200, Training Loss 0.03805156658067728\n",
      "2022-03-26 22:33:27.418250 Epoch 200, Training Loss 0.03866366283667972\n",
      "2022-03-26 22:33:27.437248 Epoch 200, Training Loss 0.03941121323944053\n",
      "2022-03-26 22:33:27.455252 Epoch 200, Training Loss 0.04000705343378169\n",
      "2022-03-26 22:33:27.473262 Epoch 200, Training Loss 0.04057788010448446\n",
      "2022-03-26 22:33:27.492481 Epoch 200, Training Loss 0.0416202011620602\n",
      "2022-03-26 22:33:27.510270 Epoch 200, Training Loss 0.04247913641088149\n",
      "2022-03-26 22:33:27.529275 Epoch 200, Training Loss 0.04303590107299483\n",
      "2022-03-26 22:33:27.547273 Epoch 200, Training Loss 0.04381845914341909\n",
      "2022-03-26 22:33:27.564283 Epoch 200, Training Loss 0.0448119193315506\n",
      "2022-03-26 22:33:27.582287 Epoch 200, Training Loss 0.04570332356273671\n",
      "2022-03-26 22:33:27.601291 Epoch 200, Training Loss 0.046607033256679545\n",
      "2022-03-26 22:33:27.620295 Epoch 200, Training Loss 0.04718395614105722\n",
      "2022-03-26 22:33:27.637293 Epoch 200, Training Loss 0.048199905039709244\n",
      "2022-03-26 22:33:27.656304 Epoch 200, Training Loss 0.048802284671522464\n",
      "2022-03-26 22:33:27.674308 Epoch 200, Training Loss 0.04929272662800596\n",
      "2022-03-26 22:33:27.692414 Epoch 200, Training Loss 0.04982246668137553\n",
      "2022-03-26 22:33:27.710310 Epoch 200, Training Loss 0.05037499182974286\n",
      "2022-03-26 22:33:27.729320 Epoch 200, Training Loss 0.05107152865975714\n",
      "2022-03-26 22:33:27.748324 Epoch 200, Training Loss 0.05173284912963048\n",
      "2022-03-26 22:33:27.766329 Epoch 200, Training Loss 0.05250396021186848\n",
      "2022-03-26 22:33:27.784326 Epoch 200, Training Loss 0.05305418993353539\n",
      "2022-03-26 22:33:27.802337 Epoch 200, Training Loss 0.053659614166030496\n",
      "2022-03-26 22:33:27.821341 Epoch 200, Training Loss 0.05432875267684917\n",
      "2022-03-26 22:33:27.839339 Epoch 200, Training Loss 0.054918984470465\n",
      "2022-03-26 22:33:27.856343 Epoch 200, Training Loss 0.0558888862657425\n",
      "2022-03-26 22:33:27.875353 Epoch 200, Training Loss 0.056476290878432485\n",
      "2022-03-26 22:33:27.893358 Epoch 200, Training Loss 0.05707684048758748\n",
      "2022-03-26 22:33:27.911355 Epoch 200, Training Loss 0.057607019267728564\n",
      "2022-03-26 22:33:27.929359 Epoch 200, Training Loss 0.05840243646860732\n",
      "2022-03-26 22:33:27.947370 Epoch 200, Training Loss 0.059029198554165836\n",
      "2022-03-26 22:33:27.965374 Epoch 200, Training Loss 0.05970974853429038\n",
      "2022-03-26 22:33:27.983378 Epoch 200, Training Loss 0.06056634929326489\n",
      "2022-03-26 22:33:28.001382 Epoch 200, Training Loss 0.06108151349570135\n",
      "2022-03-26 22:33:28.020386 Epoch 200, Training Loss 0.061700604532075966\n",
      "2022-03-26 22:33:28.039391 Epoch 200, Training Loss 0.06223446184107105\n",
      "2022-03-26 22:33:28.057395 Epoch 200, Training Loss 0.06275351135931967\n",
      "2022-03-26 22:33:28.076399 Epoch 200, Training Loss 0.06370031734561676\n",
      "2022-03-26 22:33:28.094397 Epoch 200, Training Loss 0.06451935040981263\n",
      "2022-03-26 22:33:28.112401 Epoch 200, Training Loss 0.06516138267943926\n",
      "2022-03-26 22:33:28.130411 Epoch 200, Training Loss 0.06598110569407568\n",
      "2022-03-26 22:33:28.148409 Epoch 200, Training Loss 0.06674006688015541\n",
      "2022-03-26 22:33:28.166413 Epoch 200, Training Loss 0.06742514925234763\n",
      "2022-03-26 22:33:28.185424 Epoch 200, Training Loss 0.06822747700964399\n",
      "2022-03-26 22:33:28.203428 Epoch 200, Training Loss 0.06896135416786994\n",
      "2022-03-26 22:33:28.221432 Epoch 200, Training Loss 0.06995280395688304\n",
      "2022-03-26 22:33:28.240436 Epoch 200, Training Loss 0.07062853968051998\n",
      "2022-03-26 22:33:28.258440 Epoch 200, Training Loss 0.07139421241057803\n",
      "2022-03-26 22:33:28.277438 Epoch 200, Training Loss 0.07200119192795375\n",
      "2022-03-26 22:33:28.295442 Epoch 200, Training Loss 0.07283640399460903\n",
      "2022-03-26 22:33:28.313453 Epoch 200, Training Loss 0.0735571489233495\n",
      "2022-03-26 22:33:28.331457 Epoch 200, Training Loss 0.07424349869456133\n",
      "2022-03-26 22:33:28.350461 Epoch 200, Training Loss 0.07502712393203355\n",
      "2022-03-26 22:33:28.376467 Epoch 200, Training Loss 0.07578863637983951\n",
      "2022-03-26 22:33:28.402473 Epoch 200, Training Loss 0.07649687683338399\n",
      "2022-03-26 22:33:28.428473 Epoch 200, Training Loss 0.07714665305736425\n",
      "2022-03-26 22:33:28.453484 Epoch 200, Training Loss 0.07809322763739339\n",
      "2022-03-26 22:33:28.479490 Epoch 200, Training Loss 0.0787712904574621\n",
      "2022-03-26 22:33:28.507496 Epoch 200, Training Loss 0.07945251468654789\n",
      "2022-03-26 22:33:28.533497 Epoch 200, Training Loss 0.0799604032350623\n",
      "2022-03-26 22:33:28.559510 Epoch 200, Training Loss 0.08062843791664104\n",
      "2022-03-26 22:33:28.573500 Epoch 200, Training Loss 0.08157099055512176\n",
      "2022-03-26 22:33:28.587503 Epoch 200, Training Loss 0.08226233415896325\n",
      "2022-03-26 22:33:28.602506 Epoch 200, Training Loss 0.08303859212514385\n",
      "2022-03-26 22:33:28.616509 Epoch 200, Training Loss 0.08356067267677668\n",
      "2022-03-26 22:33:28.630512 Epoch 200, Training Loss 0.08425303085533249\n",
      "2022-03-26 22:33:28.643516 Epoch 200, Training Loss 0.08483591679569401\n",
      "2022-03-26 22:33:28.658519 Epoch 200, Training Loss 0.08532355706710035\n",
      "2022-03-26 22:33:28.672522 Epoch 200, Training Loss 0.08590282339726568\n",
      "2022-03-26 22:33:28.686525 Epoch 200, Training Loss 0.08650722646194955\n",
      "2022-03-26 22:33:28.701528 Epoch 200, Training Loss 0.08768225428850754\n",
      "2022-03-26 22:33:28.715532 Epoch 200, Training Loss 0.0882435529052144\n",
      "2022-03-26 22:33:28.729535 Epoch 200, Training Loss 0.08919429317917056\n",
      "2022-03-26 22:33:28.743539 Epoch 200, Training Loss 0.08986838535427133\n",
      "2022-03-26 22:33:28.757542 Epoch 200, Training Loss 0.09063510269002842\n",
      "2022-03-26 22:33:28.772544 Epoch 200, Training Loss 0.09143910456039107\n",
      "2022-03-26 22:33:28.786548 Epoch 200, Training Loss 0.09245345785337336\n",
      "2022-03-26 22:33:28.800551 Epoch 200, Training Loss 0.09313693715026007\n",
      "2022-03-26 22:33:28.814555 Epoch 200, Training Loss 0.09391252124858329\n",
      "2022-03-26 22:33:28.828557 Epoch 200, Training Loss 0.09483431866559226\n",
      "2022-03-26 22:33:28.842560 Epoch 200, Training Loss 0.09557379088590821\n",
      "2022-03-26 22:33:28.856564 Epoch 200, Training Loss 0.0963524396690871\n",
      "2022-03-26 22:33:28.870567 Epoch 200, Training Loss 0.09722234758422198\n",
      "2022-03-26 22:33:28.885570 Epoch 200, Training Loss 0.09765390522034882\n",
      "2022-03-26 22:33:28.900573 Epoch 200, Training Loss 0.0981534857426763\n",
      "2022-03-26 22:33:28.914577 Epoch 200, Training Loss 0.09912666800381888\n",
      "2022-03-26 22:33:28.928580 Epoch 200, Training Loss 0.09987428921567815\n",
      "2022-03-26 22:33:28.942583 Epoch 200, Training Loss 0.1005957452080134\n",
      "2022-03-26 22:33:28.956587 Epoch 200, Training Loss 0.10174101499645301\n",
      "2022-03-26 22:33:28.970590 Epoch 200, Training Loss 0.10262473205776165\n",
      "2022-03-26 22:33:28.984593 Epoch 200, Training Loss 0.10334992934675778\n",
      "2022-03-26 22:33:28.998596 Epoch 200, Training Loss 0.10395758242710777\n",
      "2022-03-26 22:33:29.012599 Epoch 200, Training Loss 0.10442427265674561\n",
      "2022-03-26 22:33:29.026603 Epoch 200, Training Loss 0.10534982646212858\n",
      "2022-03-26 22:33:29.041606 Epoch 200, Training Loss 0.1061367355191799\n",
      "2022-03-26 22:33:29.055609 Epoch 200, Training Loss 0.10680662411862932\n",
      "2022-03-26 22:33:29.069613 Epoch 200, Training Loss 0.10743516664523298\n",
      "2022-03-26 22:33:29.086616 Epoch 200, Training Loss 0.1083985239724674\n",
      "2022-03-26 22:33:29.100619 Epoch 200, Training Loss 0.10925594296144403\n",
      "2022-03-26 22:33:29.114623 Epoch 200, Training Loss 0.11010427910196202\n",
      "2022-03-26 22:33:29.128625 Epoch 200, Training Loss 0.11080576334615498\n",
      "2022-03-26 22:33:29.142629 Epoch 200, Training Loss 0.11154509211897545\n",
      "2022-03-26 22:33:29.156632 Epoch 200, Training Loss 0.11213766850169053\n",
      "2022-03-26 22:33:29.170635 Epoch 200, Training Loss 0.11274587128625806\n",
      "2022-03-26 22:33:29.184638 Epoch 200, Training Loss 0.11361668439929748\n",
      "2022-03-26 22:33:29.198641 Epoch 200, Training Loss 0.1146113543635439\n",
      "2022-03-26 22:33:29.212644 Epoch 200, Training Loss 0.11561731186211871\n",
      "2022-03-26 22:33:29.226648 Epoch 200, Training Loss 0.11700125747477003\n",
      "2022-03-26 22:33:29.240651 Epoch 200, Training Loss 0.1180134581406708\n",
      "2022-03-26 22:33:29.254654 Epoch 200, Training Loss 0.11897458585784258\n",
      "2022-03-26 22:33:29.268658 Epoch 200, Training Loss 0.11949702727672694\n",
      "2022-03-26 22:33:29.282660 Epoch 200, Training Loss 0.12022441781847679\n",
      "2022-03-26 22:33:29.297663 Epoch 200, Training Loss 0.1209027771373539\n",
      "2022-03-26 22:33:29.311668 Epoch 200, Training Loss 0.12151423325319119\n",
      "2022-03-26 22:33:29.325671 Epoch 200, Training Loss 0.12230771048294614\n",
      "2022-03-26 22:33:29.339673 Epoch 200, Training Loss 0.12309923508892888\n",
      "2022-03-26 22:33:29.354683 Epoch 200, Training Loss 0.12388803533580907\n",
      "2022-03-26 22:33:29.368680 Epoch 200, Training Loss 0.12443235383161803\n",
      "2022-03-26 22:33:29.382684 Epoch 200, Training Loss 0.12526176580230294\n",
      "2022-03-26 22:33:29.396686 Epoch 200, Training Loss 0.12622001199313745\n",
      "2022-03-26 22:33:29.410690 Epoch 200, Training Loss 0.12680158480201537\n",
      "2022-03-26 22:33:29.424694 Epoch 200, Training Loss 0.12757785397264965\n",
      "2022-03-26 22:33:29.439696 Epoch 200, Training Loss 0.12865442643537545\n",
      "2022-03-26 22:33:29.453699 Epoch 200, Training Loss 0.129683139950723\n",
      "2022-03-26 22:33:29.467703 Epoch 200, Training Loss 0.130589701597343\n",
      "2022-03-26 22:33:29.482706 Epoch 200, Training Loss 0.13131326059703632\n",
      "2022-03-26 22:33:29.496709 Epoch 200, Training Loss 0.13207781082376494\n",
      "2022-03-26 22:33:29.511712 Epoch 200, Training Loss 0.1330143645062776\n",
      "2022-03-26 22:33:29.524715 Epoch 200, Training Loss 0.13405831844148125\n",
      "2022-03-26 22:33:29.539718 Epoch 200, Training Loss 0.13477872502620872\n",
      "2022-03-26 22:33:29.553721 Epoch 200, Training Loss 0.13581744571933357\n",
      "2022-03-26 22:33:29.568725 Epoch 200, Training Loss 0.13641830139300404\n",
      "2022-03-26 22:33:29.582728 Epoch 200, Training Loss 0.1368941163163051\n",
      "2022-03-26 22:33:29.597731 Epoch 200, Training Loss 0.13758255293607102\n",
      "2022-03-26 22:33:29.611734 Epoch 200, Training Loss 0.1383333228280782\n",
      "2022-03-26 22:33:29.625738 Epoch 200, Training Loss 0.13879774160244884\n",
      "2022-03-26 22:33:29.639741 Epoch 200, Training Loss 0.13954005136971584\n",
      "2022-03-26 22:33:29.653745 Epoch 200, Training Loss 0.1402020019567226\n",
      "2022-03-26 22:33:29.667748 Epoch 200, Training Loss 0.14096440207165525\n",
      "2022-03-26 22:33:29.681751 Epoch 200, Training Loss 0.1417309028641952\n",
      "2022-03-26 22:33:29.696754 Epoch 200, Training Loss 0.14250031361342086\n",
      "2022-03-26 22:33:29.710758 Epoch 200, Training Loss 0.14337143160955376\n",
      "2022-03-26 22:33:29.724761 Epoch 200, Training Loss 0.14400282166802975\n",
      "2022-03-26 22:33:29.738765 Epoch 200, Training Loss 0.14479776157442567\n",
      "2022-03-26 22:33:29.752767 Epoch 200, Training Loss 0.14536561117605176\n",
      "2022-03-26 22:33:29.767770 Epoch 200, Training Loss 0.14610979597434362\n",
      "2022-03-26 22:33:29.781773 Epoch 200, Training Loss 0.14680084071653274\n",
      "2022-03-26 22:33:29.795776 Epoch 200, Training Loss 0.14769626681304648\n",
      "2022-03-26 22:33:29.809780 Epoch 200, Training Loss 0.14842397870157686\n",
      "2022-03-26 22:33:29.823783 Epoch 200, Training Loss 0.14898030063532808\n",
      "2022-03-26 22:33:29.837786 Epoch 200, Training Loss 0.14975473982141452\n",
      "2022-03-26 22:33:29.852789 Epoch 200, Training Loss 0.15053267174821985\n",
      "2022-03-26 22:33:29.866792 Epoch 200, Training Loss 0.15151381039101144\n",
      "2022-03-26 22:33:29.880796 Epoch 200, Training Loss 0.1523175614950297\n",
      "2022-03-26 22:33:29.895799 Epoch 200, Training Loss 0.15294292824500053\n",
      "2022-03-26 22:33:29.909803 Epoch 200, Training Loss 0.1536089088529577\n",
      "2022-03-26 22:33:29.923805 Epoch 200, Training Loss 0.1542993135693128\n",
      "2022-03-26 22:33:29.937809 Epoch 200, Training Loss 0.1550927001725682\n",
      "2022-03-26 22:33:29.952818 Epoch 200, Training Loss 0.15589613114933834\n",
      "2022-03-26 22:33:29.966815 Epoch 200, Training Loss 0.15669610307497137\n",
      "2022-03-26 22:33:29.980819 Epoch 200, Training Loss 0.1574909799467877\n",
      "2022-03-26 22:33:29.994828 Epoch 200, Training Loss 0.1584128388739608\n",
      "2022-03-26 22:33:30.008831 Epoch 200, Training Loss 0.15912268019240836\n",
      "2022-03-26 22:33:30.022829 Epoch 200, Training Loss 0.15990588873091255\n",
      "2022-03-26 22:33:30.036832 Epoch 200, Training Loss 0.1604606841912355\n",
      "2022-03-26 22:33:30.050834 Epoch 200, Training Loss 0.16118999336229262\n",
      "2022-03-26 22:33:30.064838 Epoch 200, Training Loss 0.16200881034059597\n",
      "2022-03-26 22:33:30.078840 Epoch 200, Training Loss 0.16284974795930526\n",
      "2022-03-26 22:33:30.093844 Epoch 200, Training Loss 0.16386147857169667\n",
      "2022-03-26 22:33:30.107847 Epoch 200, Training Loss 0.16457523908609015\n",
      "2022-03-26 22:33:30.122851 Epoch 200, Training Loss 0.16528987309054646\n",
      "2022-03-26 22:33:30.136854 Epoch 200, Training Loss 0.1661216499631667\n",
      "2022-03-26 22:33:30.150857 Epoch 200, Training Loss 0.16675569970741907\n",
      "2022-03-26 22:33:30.165860 Epoch 200, Training Loss 0.16751411553386533\n",
      "2022-03-26 22:33:30.179863 Epoch 200, Training Loss 0.168111991661284\n",
      "2022-03-26 22:33:30.193867 Epoch 200, Training Loss 0.16874949885603716\n",
      "2022-03-26 22:33:30.207869 Epoch 200, Training Loss 0.16966312792142638\n",
      "2022-03-26 22:33:30.221873 Epoch 200, Training Loss 0.17043475570428707\n",
      "2022-03-26 22:33:30.236397 Epoch 200, Training Loss 0.1711717701857657\n",
      "2022-03-26 22:33:30.250399 Epoch 200, Training Loss 0.17212869867187022\n",
      "2022-03-26 22:33:30.264403 Epoch 200, Training Loss 0.17290245293808715\n",
      "2022-03-26 22:33:30.279406 Epoch 200, Training Loss 0.17343247188326646\n",
      "2022-03-26 22:33:30.293410 Epoch 200, Training Loss 0.17403954397077145\n",
      "2022-03-26 22:33:30.308412 Epoch 200, Training Loss 0.17480401736696052\n",
      "2022-03-26 22:33:30.323422 Epoch 200, Training Loss 0.1761581252907853\n",
      "2022-03-26 22:33:30.337419 Epoch 200, Training Loss 0.17705714031863395\n",
      "2022-03-26 22:33:30.351422 Epoch 200, Training Loss 0.17801176731848656\n",
      "2022-03-26 22:33:30.369432 Epoch 200, Training Loss 0.1786723691026878\n",
      "2022-03-26 22:33:30.388442 Epoch 200, Training Loss 0.1792933938600828\n",
      "2022-03-26 22:33:30.407446 Epoch 200, Training Loss 0.1801449740329362\n",
      "2022-03-26 22:33:30.427435 Epoch 200, Training Loss 0.18082997843127727\n",
      "2022-03-26 22:33:30.452439 Epoch 200, Training Loss 0.18167707087743618\n",
      "2022-03-26 22:33:30.474442 Epoch 200, Training Loss 0.1827342992700884\n",
      "2022-03-26 22:33:30.500450 Epoch 200, Training Loss 0.18335561629603891\n",
      "2022-03-26 22:33:30.520455 Epoch 200, Training Loss 0.18392821361341744\n",
      "2022-03-26 22:33:30.540462 Epoch 200, Training Loss 0.18470125818801353\n",
      "2022-03-26 22:33:30.561461 Epoch 200, Training Loss 0.18552801737090205\n",
      "2022-03-26 22:33:30.580467 Epoch 200, Training Loss 0.18637358006613944\n",
      "2022-03-26 22:33:30.600490 Epoch 200, Training Loss 0.18740280677595406\n",
      "2022-03-26 22:33:30.619495 Epoch 200, Training Loss 0.18814290736032568\n",
      "2022-03-26 22:33:30.638499 Epoch 200, Training Loss 0.1889094085339695\n",
      "2022-03-26 22:33:30.658502 Epoch 200, Training Loss 0.18945171396293298\n",
      "2022-03-26 22:33:30.677502 Epoch 200, Training Loss 0.19038005500955654\n",
      "2022-03-26 22:33:30.696512 Epoch 200, Training Loss 0.19102171585535455\n",
      "2022-03-26 22:33:30.714516 Epoch 200, Training Loss 0.1915014228781166\n",
      "2022-03-26 22:33:30.735521 Epoch 200, Training Loss 0.1924168050975141\n",
      "2022-03-26 22:33:30.753525 Epoch 200, Training Loss 0.19327480001065434\n",
      "2022-03-26 22:33:30.773530 Epoch 200, Training Loss 0.193974171567451\n",
      "2022-03-26 22:33:30.792521 Epoch 200, Training Loss 0.19473053095743176\n",
      "2022-03-26 22:33:30.811538 Epoch 200, Training Loss 0.1954749602719646\n",
      "2022-03-26 22:33:30.830542 Epoch 200, Training Loss 0.1962923735685056\n",
      "2022-03-26 22:33:30.849541 Epoch 200, Training Loss 0.19689956063504718\n",
      "2022-03-26 22:33:30.868551 Epoch 200, Training Loss 0.19757087738312723\n",
      "2022-03-26 22:33:30.888556 Epoch 200, Training Loss 0.19857071793597678\n",
      "2022-03-26 22:33:30.907556 Epoch 200, Training Loss 0.19941035248434452\n",
      "2022-03-26 22:33:30.927558 Epoch 200, Training Loss 0.20003710363222205\n",
      "2022-03-26 22:33:30.946563 Epoch 200, Training Loss 0.20077244064692037\n",
      "2022-03-26 22:33:30.965573 Epoch 200, Training Loss 0.20186771334284712\n",
      "2022-03-26 22:33:30.984577 Epoch 200, Training Loss 0.20242146976158748\n",
      "2022-03-26 22:33:31.003582 Epoch 200, Training Loss 0.20308805685823836\n",
      "2022-03-26 22:33:31.022586 Epoch 200, Training Loss 0.20386344301121315\n",
      "2022-03-26 22:33:31.041590 Epoch 200, Training Loss 0.20448739601827948\n",
      "2022-03-26 22:33:31.060588 Epoch 200, Training Loss 0.20512141455012514\n",
      "2022-03-26 22:33:31.079599 Epoch 200, Training Loss 0.20592828674237137\n",
      "2022-03-26 22:33:31.098603 Epoch 200, Training Loss 0.2067785006578621\n",
      "2022-03-26 22:33:31.117601 Epoch 200, Training Loss 0.2075890485969041\n",
      "2022-03-26 22:33:31.136612 Epoch 200, Training Loss 0.20849523104517662\n",
      "2022-03-26 22:33:31.157617 Epoch 200, Training Loss 0.20910466106041617\n",
      "2022-03-26 22:33:31.176615 Epoch 200, Training Loss 0.21037998848863879\n",
      "2022-03-26 22:33:31.195625 Epoch 200, Training Loss 0.21134993708347116\n",
      "2022-03-26 22:33:31.215037 Epoch 200, Training Loss 0.21221778322668636\n",
      "2022-03-26 22:33:31.234042 Epoch 200, Training Loss 0.21288818136200577\n",
      "2022-03-26 22:33:31.253046 Epoch 200, Training Loss 0.21360389267087287\n",
      "2022-03-26 22:33:31.273051 Epoch 200, Training Loss 0.2143592563126703\n",
      "2022-03-26 22:33:31.292040 Epoch 200, Training Loss 0.2152691737312795\n",
      "2022-03-26 22:33:31.311053 Epoch 200, Training Loss 0.2160660216722952\n",
      "2022-03-26 22:33:31.330615 Epoch 200, Training Loss 0.2166481141925163\n",
      "2022-03-26 22:33:31.349619 Epoch 200, Training Loss 0.21766373290277807\n",
      "2022-03-26 22:33:31.369624 Epoch 200, Training Loss 0.21822642544498833\n",
      "2022-03-26 22:33:31.389628 Epoch 200, Training Loss 0.21892233131944067\n",
      "2022-03-26 22:33:31.408799 Epoch 200, Training Loss 0.21980316136651637\n",
      "2022-03-26 22:33:31.427631 Epoch 200, Training Loss 0.22057403517348687\n",
      "2022-03-26 22:33:31.446645 Epoch 200, Training Loss 0.22105544546376105\n",
      "2022-03-26 22:33:31.465646 Epoch 200, Training Loss 0.2215717491667594\n",
      "2022-03-26 22:33:31.485650 Epoch 200, Training Loss 0.22254692368647633\n",
      "2022-03-26 22:33:31.504648 Epoch 200, Training Loss 0.22358860144072482\n",
      "2022-03-26 22:33:31.523659 Epoch 200, Training Loss 0.2242731976768245\n",
      "2022-03-26 22:33:31.542663 Epoch 200, Training Loss 0.2251812293935005\n",
      "2022-03-26 22:33:31.562668 Epoch 200, Training Loss 0.22597137318395288\n",
      "2022-03-26 22:33:31.581672 Epoch 200, Training Loss 0.22650011287778235\n",
      "2022-03-26 22:33:31.600676 Epoch 200, Training Loss 0.22707962120890313\n",
      "2022-03-26 22:33:31.619681 Epoch 200, Training Loss 0.2279376477536643\n",
      "2022-03-26 22:33:31.639685 Epoch 200, Training Loss 0.2288562547977623\n",
      "2022-03-26 22:33:31.658679 Epoch 200, Training Loss 0.22931943224061785\n",
      "2022-03-26 22:33:31.677682 Epoch 200, Training Loss 0.22996498911124666\n",
      "2022-03-26 22:33:31.696695 Epoch 200, Training Loss 0.23062547000930134\n",
      "2022-03-26 22:33:31.715699 Epoch 200, Training Loss 0.23136663791316245\n",
      "2022-03-26 22:33:31.734702 Epoch 200, Training Loss 0.2320642220928236\n",
      "2022-03-26 22:33:31.753707 Epoch 200, Training Loss 0.2329109258892591\n",
      "2022-03-26 22:33:31.773702 Epoch 200, Training Loss 0.23366215752671138\n",
      "2022-03-26 22:33:31.792714 Epoch 200, Training Loss 0.2343543287357101\n",
      "2022-03-26 22:33:31.811718 Epoch 200, Training Loss 0.23546749067580913\n",
      "2022-03-26 22:33:31.830723 Epoch 200, Training Loss 0.23626171639355856\n",
      "2022-03-26 22:33:31.849729 Epoch 200, Training Loss 0.23693185332028763\n",
      "2022-03-26 22:33:31.868733 Epoch 200, Training Loss 0.2377110196425177\n",
      "2022-03-26 22:33:31.887738 Epoch 200, Training Loss 0.23829139525170825\n",
      "2022-03-26 22:33:31.906731 Epoch 200, Training Loss 0.2390773840953627\n",
      "2022-03-26 22:33:31.925740 Epoch 200, Training Loss 0.24018691243875362\n",
      "2022-03-26 22:33:31.944750 Epoch 200, Training Loss 0.24067314186364488\n",
      "2022-03-26 22:33:31.963755 Epoch 200, Training Loss 0.24155907618725087\n",
      "2022-03-26 22:33:31.982758 Epoch 200, Training Loss 0.24234201345602266\n",
      "2022-03-26 22:33:32.002750 Epoch 200, Training Loss 0.243353748794102\n",
      "2022-03-26 22:33:32.020766 Epoch 200, Training Loss 0.2441049439980246\n",
      "2022-03-26 22:33:32.039772 Epoch 200, Training Loss 0.24472358613215445\n",
      "2022-03-26 22:33:32.058776 Epoch 200, Training Loss 0.24536132858232465\n",
      "2022-03-26 22:33:32.077781 Epoch 200, Training Loss 0.24618483519615114\n",
      "2022-03-26 22:33:32.096785 Epoch 200, Training Loss 0.24697042250877146\n",
      "2022-03-26 22:33:32.115783 Epoch 200, Training Loss 0.24762753711637023\n",
      "2022-03-26 22:33:32.134794 Epoch 200, Training Loss 0.2483633774930559\n",
      "2022-03-26 22:33:32.153799 Epoch 200, Training Loss 0.2491663516787312\n",
      "2022-03-26 22:33:32.172802 Epoch 200, Training Loss 0.249931767544783\n",
      "2022-03-26 22:33:32.191796 Epoch 200, Training Loss 0.25075977766300406\n",
      "2022-03-26 22:33:32.210808 Epoch 200, Training Loss 0.25157548208980607\n",
      "2022-03-26 22:33:32.230815 Epoch 200, Training Loss 0.25252150772782544\n",
      "2022-03-26 22:33:32.251807 Epoch 200, Training Loss 0.2533749641512361\n",
      "2022-03-26 22:33:32.270829 Epoch 200, Training Loss 0.2544204261144409\n",
      "2022-03-26 22:33:32.289825 Epoch 200, Training Loss 0.25570844620694894\n",
      "2022-03-26 22:33:32.308834 Epoch 200, Training Loss 0.25644615521211456\n",
      "2022-03-26 22:33:32.326832 Epoch 200, Training Loss 0.2570389358665022\n",
      "2022-03-26 22:33:32.346845 Epoch 200, Training Loss 0.2577129272777406\n",
      "2022-03-26 22:33:32.365850 Epoch 200, Training Loss 0.25866405764961486\n",
      "2022-03-26 22:33:32.384854 Epoch 200, Training Loss 0.25934177907683964\n",
      "2022-03-26 22:33:32.403858 Epoch 200, Training Loss 0.26020870927502127\n",
      "2022-03-26 22:33:32.423863 Epoch 200, Training Loss 0.26100056925240683\n",
      "2022-03-26 22:33:32.444862 Epoch 200, Training Loss 0.26157560761627335\n",
      "2022-03-26 22:33:32.463872 Epoch 200, Training Loss 0.262225296262585\n",
      "2022-03-26 22:33:32.482876 Epoch 200, Training Loss 0.26304427155143467\n",
      "2022-03-26 22:33:32.501876 Epoch 200, Training Loss 0.26360871019723164\n",
      "2022-03-26 22:33:32.520881 Epoch 200, Training Loss 0.2643229356583427\n",
      "2022-03-26 22:33:32.540877 Epoch 200, Training Loss 0.26510069814636883\n",
      "2022-03-26 22:33:32.559881 Epoch 200, Training Loss 0.2657940567225751\n",
      "2022-03-26 22:33:32.578893 Epoch 200, Training Loss 0.2665562938774943\n",
      "2022-03-26 22:33:32.597883 Epoch 200, Training Loss 0.2674831037250016\n",
      "2022-03-26 22:33:32.617901 Epoch 200, Training Loss 0.26824336630456586\n",
      "2022-03-26 22:33:32.636911 Epoch 200, Training Loss 0.26894516709363064\n",
      "2022-03-26 22:33:32.657910 Epoch 200, Training Loss 0.2697478338047062\n",
      "2022-03-26 22:33:32.677915 Epoch 200, Training Loss 0.2706882735271283\n",
      "2022-03-26 22:33:32.696913 Epoch 200, Training Loss 0.27161984869738676\n",
      "2022-03-26 22:33:32.716924 Epoch 200, Training Loss 0.2723827201615819\n",
      "2022-03-26 22:33:32.735934 Epoch 200, Training Loss 0.2729139060849119\n",
      "2022-03-26 22:33:32.754938 Epoch 200, Training Loss 0.2735179536559088\n",
      "2022-03-26 22:33:32.775547 Epoch 200, Training Loss 0.2747254510745978\n",
      "2022-03-26 22:33:32.794564 Epoch 200, Training Loss 0.27572834465052465\n",
      "2022-03-26 22:33:32.813574 Epoch 200, Training Loss 0.27670037445357387\n",
      "2022-03-26 22:33:32.833579 Epoch 200, Training Loss 0.277345997483834\n",
      "2022-03-26 22:33:32.853583 Epoch 200, Training Loss 0.27816150030669046\n",
      "2022-03-26 22:33:32.872581 Epoch 200, Training Loss 0.27906846066417595\n",
      "2022-03-26 22:33:32.892572 Epoch 200, Training Loss 0.2799483127225086\n",
      "2022-03-26 22:33:32.911591 Epoch 200, Training Loss 0.28066801860966645\n",
      "2022-03-26 22:33:32.930600 Epoch 200, Training Loss 0.281333933515317\n",
      "2022-03-26 22:33:32.949605 Epoch 200, Training Loss 0.28214491172062467\n",
      "2022-03-26 22:33:32.969603 Epoch 200, Training Loss 0.282845315230472\n",
      "2022-03-26 22:33:32.988614 Epoch 200, Training Loss 0.2836330259013969\n",
      "2022-03-26 22:33:33.007288 Epoch 200, Training Loss 0.2846754863667671\n",
      "2022-03-26 22:33:33.027286 Epoch 200, Training Loss 0.28541100936015246\n",
      "2022-03-26 22:33:33.047297 Epoch 200, Training Loss 0.2860921792438268\n",
      "2022-03-26 22:33:33.066284 Epoch 200, Training Loss 0.28703466412204\n",
      "2022-03-26 22:33:33.086306 Epoch 200, Training Loss 0.28767561298959393\n",
      "2022-03-26 22:33:33.105310 Epoch 200, Training Loss 0.2883429959073396\n",
      "2022-03-26 22:33:33.125310 Epoch 200, Training Loss 0.2893191950629129\n",
      "2022-03-26 22:33:33.144308 Epoch 200, Training Loss 0.28980077067604454\n",
      "2022-03-26 22:33:33.163324 Epoch 200, Training Loss 0.2905956811612219\n",
      "2022-03-26 22:33:33.182322 Epoch 200, Training Loss 0.29142168187119466\n",
      "2022-03-26 22:33:33.202326 Epoch 200, Training Loss 0.29211757646497255\n",
      "2022-03-26 22:33:33.222324 Epoch 200, Training Loss 0.29270938160779225\n",
      "2022-03-26 22:33:33.241336 Epoch 200, Training Loss 0.2935076033520272\n",
      "2022-03-26 22:33:33.261340 Epoch 200, Training Loss 0.29426763886990753\n",
      "2022-03-26 22:33:33.280343 Epoch 200, Training Loss 0.29495882926999456\n",
      "2022-03-26 22:33:33.299349 Epoch 200, Training Loss 0.29555448588660305\n",
      "2022-03-26 22:33:33.319346 Epoch 200, Training Loss 0.2965517314270024\n",
      "2022-03-26 22:33:33.339358 Epoch 200, Training Loss 0.2973481032168469\n",
      "2022-03-26 22:33:33.358369 Epoch 200, Training Loss 0.2981805144749639\n",
      "2022-03-26 22:33:33.377360 Epoch 200, Training Loss 0.2988549241858065\n",
      "2022-03-26 22:33:33.397364 Epoch 200, Training Loss 0.29933512443319304\n",
      "2022-03-26 22:33:33.417371 Epoch 200, Training Loss 0.3000007152481152\n",
      "2022-03-26 22:33:33.436371 Epoch 200, Training Loss 0.3008070275225603\n",
      "2022-03-26 22:33:33.455389 Epoch 200, Training Loss 0.3015081938499075\n",
      "2022-03-26 22:33:33.475380 Epoch 200, Training Loss 0.3024359720061197\n",
      "2022-03-26 22:33:33.495393 Epoch 200, Training Loss 0.30308832441601913\n",
      "2022-03-26 22:33:33.515397 Epoch 200, Training Loss 0.3038470822450755\n",
      "2022-03-26 22:33:33.534402 Epoch 200, Training Loss 0.30468250216577974\n",
      "2022-03-26 22:33:33.554412 Epoch 200, Training Loss 0.30530090386148\n",
      "2022-03-26 22:33:33.573416 Epoch 200, Training Loss 0.30594384971329625\n",
      "2022-03-26 22:33:33.592406 Epoch 200, Training Loss 0.3065958104246413\n",
      "2022-03-26 22:33:33.612419 Epoch 200, Training Loss 0.3072533373104032\n",
      "2022-03-26 22:33:33.631423 Epoch 200, Training Loss 0.3079324973285046\n",
      "2022-03-26 22:33:33.651597 Epoch 200, Training Loss 0.3089739625030162\n",
      "2022-03-26 22:33:33.670438 Epoch 200, Training Loss 0.3096556850635182\n",
      "2022-03-26 22:33:33.690437 Epoch 200, Training Loss 0.3104980432850016\n",
      "2022-03-26 22:33:33.710819 Epoch 200, Training Loss 0.31126609006348777\n",
      "2022-03-26 22:33:33.728829 Epoch 200, Training Loss 0.3120416331550349\n",
      "2022-03-26 22:33:33.748834 Epoch 200, Training Loss 0.31259649401278145\n",
      "2022-03-26 22:33:33.767832 Epoch 200, Training Loss 0.3130660486571929\n",
      "2022-03-26 22:33:33.787837 Epoch 200, Training Loss 0.31367720930320225\n",
      "2022-03-26 22:33:33.807833 Epoch 200, Training Loss 0.3147161214629098\n",
      "2022-03-26 22:33:33.826839 Epoch 200, Training Loss 0.315507309249295\n",
      "2022-03-26 22:33:33.846850 Epoch 200, Training Loss 0.31625818394486555\n",
      "2022-03-26 22:33:33.865848 Epoch 200, Training Loss 0.31703548537343357\n",
      "2022-03-26 22:33:33.885852 Epoch 200, Training Loss 0.3177976574358123\n",
      "2022-03-26 22:33:33.904858 Epoch 200, Training Loss 0.3185812037085633\n",
      "2022-03-26 22:33:33.923874 Epoch 200, Training Loss 0.3191813993484468\n",
      "2022-03-26 22:33:33.943872 Epoch 200, Training Loss 0.32002547055559083\n",
      "2022-03-26 22:33:33.963879 Epoch 200, Training Loss 0.32078461185135804\n",
      "2022-03-26 22:33:33.983881 Epoch 200, Training Loss 0.3216905236396643\n",
      "2022-03-26 22:33:34.002879 Epoch 200, Training Loss 0.3224856413691245\n",
      "2022-03-26 22:33:34.021923 Epoch 200, Training Loss 0.3231343225292537\n",
      "2022-03-26 22:33:34.041901 Epoch 200, Training Loss 0.3239376466444996\n",
      "2022-03-26 22:33:34.061899 Epoch 200, Training Loss 0.32499656889139844\n",
      "2022-03-26 22:33:34.080903 Epoch 200, Training Loss 0.32584279905194824\n",
      "2022-03-26 22:33:34.100908 Epoch 200, Training Loss 0.32664991858060405\n",
      "2022-03-26 22:33:34.119983 Epoch 200, Training Loss 0.3273609932273855\n",
      "2022-03-26 22:33:34.140923 Epoch 200, Training Loss 0.3281217712118193\n",
      "2022-03-26 22:33:34.159915 Epoch 200, Training Loss 0.3286340948565842\n",
      "2022-03-26 22:33:34.179927 Epoch 200, Training Loss 0.3293553108297041\n",
      "2022-03-26 22:33:34.198930 Epoch 200, Training Loss 0.3299645167558699\n",
      "2022-03-26 22:33:34.217935 Epoch 200, Training Loss 0.33062670545657274\n",
      "2022-03-26 22:33:34.237945 Epoch 200, Training Loss 0.33125254607109156\n",
      "2022-03-26 22:33:34.256943 Epoch 200, Training Loss 0.33191369763573114\n",
      "2022-03-26 22:33:34.276941 Epoch 200, Training Loss 0.3326828885642464\n",
      "2022-03-26 22:33:34.296953 Epoch 200, Training Loss 0.3332718675170103\n",
      "2022-03-26 22:33:34.315962 Epoch 200, Training Loss 0.3341474746118116\n",
      "2022-03-26 22:33:34.335961 Epoch 200, Training Loss 0.3347494990167106\n",
      "2022-03-26 22:33:34.355971 Epoch 200, Training Loss 0.3354037527538017\n",
      "2022-03-26 22:33:34.375966 Epoch 200, Training Loss 0.3363803500866951\n",
      "2022-03-26 22:33:34.395975 Epoch 200, Training Loss 0.3371614572184775\n",
      "2022-03-26 22:33:34.414979 Epoch 200, Training Loss 0.33783932857196347\n",
      "2022-03-26 22:33:34.433983 Epoch 200, Training Loss 0.3387620732607439\n",
      "2022-03-26 22:33:34.453994 Epoch 200, Training Loss 0.3396710581944117\n",
      "2022-03-26 22:33:34.473985 Epoch 200, Training Loss 0.3402209211798275\n",
      "2022-03-26 22:33:34.492992 Epoch 200, Training Loss 0.34093325011565556\n",
      "2022-03-26 22:33:34.513002 Epoch 200, Training Loss 0.34143067057937615\n",
      "2022-03-26 22:33:34.532012 Epoch 200, Training Loss 0.3422070780144933\n",
      "2022-03-26 22:33:34.551010 Epoch 200, Training Loss 0.3431349018269488\n",
      "2022-03-26 22:33:34.572021 Epoch 200, Training Loss 0.34407155695931074\n",
      "2022-03-26 22:33:34.591019 Epoch 200, Training Loss 0.3448472432696911\n",
      "2022-03-26 22:33:34.611023 Epoch 200, Training Loss 0.34558349904959157\n",
      "2022-03-26 22:33:34.637029 Epoch 200, Training Loss 0.3465675519174322\n",
      "2022-03-26 22:33:34.663041 Epoch 200, Training Loss 0.3472021001836528\n",
      "2022-03-26 22:33:34.689041 Epoch 200, Training Loss 0.34806682950700335\n",
      "2022-03-26 22:33:34.716039 Epoch 200, Training Loss 0.34877799470406357\n",
      "2022-03-26 22:33:34.743054 Epoch 200, Training Loss 0.3498282170356692\n",
      "2022-03-26 22:33:34.769065 Epoch 200, Training Loss 0.3506256196352527\n",
      "2022-03-26 22:33:34.795065 Epoch 200, Training Loss 0.3514270928814588\n",
      "2022-03-26 22:33:34.822065 Epoch 200, Training Loss 0.3521500085016041\n",
      "2022-03-26 22:33:34.848083 Epoch 200, Training Loss 0.3531538141353051\n",
      "2022-03-26 22:33:34.875089 Epoch 200, Training Loss 0.35380660611040454\n",
      "2022-03-26 22:33:34.901095 Epoch 200, Training Loss 0.35492233348929364\n",
      "2022-03-26 22:33:34.927088 Epoch 200, Training Loss 0.3556658682768302\n",
      "2022-03-26 22:33:34.953107 Epoch 200, Training Loss 0.3563513626223025\n",
      "2022-03-26 22:33:34.979113 Epoch 200, Training Loss 0.3570946261401067\n",
      "2022-03-26 22:33:35.005113 Epoch 200, Training Loss 0.3576912346398434\n",
      "2022-03-26 22:33:35.023107 Epoch 200, Training Loss 0.35831088418393486\n",
      "2022-03-26 22:33:35.041127 Epoch 200, Training Loss 0.3592233671175549\n",
      "2022-03-26 22:33:35.059125 Epoch 200, Training Loss 0.3601793827074568\n",
      "2022-03-26 22:33:35.077129 Epoch 200, Training Loss 0.36085003602992544\n",
      "2022-03-26 22:33:35.095134 Epoch 200, Training Loss 0.36181675332129154\n",
      "2022-03-26 22:33:35.113143 Epoch 200, Training Loss 0.36267414311771196\n",
      "2022-03-26 22:33:35.132148 Epoch 200, Training Loss 0.3634836028527726\n",
      "2022-03-26 22:33:35.151146 Epoch 200, Training Loss 0.3640788120534414\n",
      "2022-03-26 22:33:35.169440 Epoch 200, Training Loss 0.3648490456059156\n",
      "2022-03-26 22:33:35.187444 Epoch 200, Training Loss 0.36587039505124397\n",
      "2022-03-26 22:33:35.206443 Epoch 200, Training Loss 0.36653887608167157\n",
      "2022-03-26 22:33:35.224453 Epoch 200, Training Loss 0.3671492535592345\n",
      "2022-03-26 22:33:35.242451 Epoch 200, Training Loss 0.367741055234009\n",
      "2022-03-26 22:33:35.261455 Epoch 200, Training Loss 0.3684745096718259\n",
      "2022-03-26 22:33:35.279465 Epoch 200, Training Loss 0.3691007554378656\n",
      "2022-03-26 22:33:35.297457 Epoch 200, Training Loss 0.3698729759896808\n",
      "2022-03-26 22:33:35.315473 Epoch 200, Training Loss 0.3703508664046407\n",
      "2022-03-26 22:33:35.333472 Epoch 200, Training Loss 0.3710534696460075\n",
      "2022-03-26 22:33:35.352476 Epoch 200, Training Loss 0.3717028794767302\n",
      "2022-03-26 22:33:35.370486 Epoch 200, Training Loss 0.37238610320536375\n",
      "2022-03-26 22:33:35.389484 Epoch 200, Training Loss 0.3730069420984029\n",
      "2022-03-26 22:33:35.407482 Epoch 200, Training Loss 0.3738932347358645\n",
      "2022-03-26 22:33:35.425499 Epoch 200, Training Loss 0.3744336234029297\n",
      "2022-03-26 22:33:35.443497 Epoch 200, Training Loss 0.37552893169395757\n",
      "2022-03-26 22:33:35.461500 Epoch 200, Training Loss 0.37624304625384336\n",
      "2022-03-26 22:33:35.479505 Epoch 200, Training Loss 0.37719059645977165\n",
      "2022-03-26 22:33:35.498515 Epoch 200, Training Loss 0.3780500994771338\n",
      "2022-03-26 22:33:35.516519 Epoch 200, Training Loss 0.3789229155958766\n",
      "2022-03-26 22:33:35.535518 Epoch 200, Training Loss 0.37983967527709045\n",
      "2022-03-26 22:33:35.553515 Epoch 200, Training Loss 0.38037720101568706\n",
      "2022-03-26 22:33:35.573532 Epoch 200, Training Loss 0.3810255771402813\n",
      "2022-03-26 22:33:35.591523 Epoch 200, Training Loss 0.3819104667057467\n",
      "2022-03-26 22:33:35.608540 Epoch 200, Training Loss 0.38279998935092135\n",
      "2022-03-26 22:33:35.627538 Epoch 200, Training Loss 0.38348135047251614\n",
      "2022-03-26 22:33:35.645548 Epoch 200, Training Loss 0.3844223561341805\n",
      "2022-03-26 22:33:35.662546 Epoch 200, Training Loss 0.38508079180022337\n",
      "2022-03-26 22:33:35.680705 Epoch 200, Training Loss 0.3859048167153088\n",
      "2022-03-26 22:33:35.698715 Epoch 200, Training Loss 0.38673787097186996\n",
      "2022-03-26 22:33:35.717707 Epoch 200, Training Loss 0.3876453160172533\n",
      "2022-03-26 22:33:35.735717 Epoch 200, Training Loss 0.38824004300719944\n",
      "2022-03-26 22:33:35.754722 Epoch 200, Training Loss 0.38921464000211653\n",
      "2022-03-26 22:33:35.773720 Epoch 200, Training Loss 0.38990293492746475\n",
      "2022-03-26 22:33:35.791730 Epoch 200, Training Loss 0.3905697052588548\n",
      "2022-03-26 22:33:35.811075 Epoch 200, Training Loss 0.3912491174152745\n",
      "2022-03-26 22:33:35.829732 Epoch 200, Training Loss 0.39215892637172317\n",
      "2022-03-26 22:33:35.847736 Epoch 200, Training Loss 0.393172763573849\n",
      "2022-03-26 22:33:35.866747 Epoch 200, Training Loss 0.3940258126734468\n",
      "2022-03-26 22:33:35.885745 Epoch 200, Training Loss 0.39489551799376604\n",
      "2022-03-26 22:33:35.904762 Epoch 200, Training Loss 0.39595661962123785\n",
      "2022-03-26 22:33:35.922760 Epoch 200, Training Loss 0.39682254827845737\n",
      "2022-03-26 22:33:35.941758 Epoch 200, Training Loss 0.39756869256039107\n",
      "2022-03-26 22:33:35.960769 Epoch 200, Training Loss 0.39850468845928416\n",
      "2022-03-26 22:33:35.978773 Epoch 200, Training Loss 0.3994424337011469\n",
      "2022-03-26 22:33:35.996773 Epoch 200, Training Loss 0.40017745737224586\n",
      "2022-03-26 22:33:36.015781 Epoch 200, Training Loss 0.40098064375655423\n",
      "2022-03-26 22:33:36.033785 Epoch 200, Training Loss 0.40187884116416694\n",
      "2022-03-26 22:33:36.052789 Epoch 200, Training Loss 0.40249276919590543\n",
      "2022-03-26 22:33:36.070799 Epoch 200, Training Loss 0.40347008300406856\n",
      "2022-03-26 22:33:36.089804 Epoch 200, Training Loss 0.4040430409600363\n",
      "2022-03-26 22:33:36.108788 Epoch 200, Training Loss 0.40473735793624693\n",
      "2022-03-26 22:33:36.127806 Epoch 200, Training Loss 0.4057324834720558\n",
      "2022-03-26 22:33:36.145816 Epoch 200, Training Loss 0.4063885171928674\n",
      "2022-03-26 22:33:36.163814 Epoch 200, Training Loss 0.4071339318133376\n",
      "2022-03-26 22:33:36.182812 Epoch 200, Training Loss 0.4078235535899087\n",
      "2022-03-26 22:33:36.200829 Epoch 200, Training Loss 0.4085062471268427\n",
      "2022-03-26 22:33:36.219827 Epoch 200, Training Loss 0.4090643763694617\n",
      "2022-03-26 22:33:36.236837 Epoch 200, Training Loss 0.40959871310712126\n",
      "2022-03-26 22:33:36.255836 Epoch 200, Training Loss 0.41039444045032686\n",
      "2022-03-26 22:33:36.272845 Epoch 200, Training Loss 0.4111641764335925\n",
      "2022-03-26 22:33:36.291843 Epoch 200, Training Loss 0.41183009507406093\n",
      "2022-03-26 22:33:36.309841 Epoch 200, Training Loss 0.4123247340893197\n",
      "2022-03-26 22:33:36.327852 Epoch 200, Training Loss 0.41297615965461487\n",
      "2022-03-26 22:33:36.345862 Epoch 200, Training Loss 0.41368749939724614\n",
      "2022-03-26 22:33:36.362860 Epoch 200, Training Loss 0.41422199090118605\n",
      "2022-03-26 22:33:36.382864 Epoch 200, Training Loss 0.4149379785865774\n",
      "2022-03-26 22:33:36.401868 Epoch 200, Training Loss 0.4156750328552997\n",
      "2022-03-26 22:33:36.419866 Epoch 200, Training Loss 0.41620860841420604\n",
      "2022-03-26 22:33:36.437877 Epoch 200, Training Loss 0.4169800989234539\n",
      "2022-03-26 22:33:36.456870 Epoch 200, Training Loss 0.4178033777896096\n",
      "2022-03-26 22:33:36.474885 Epoch 200, Training Loss 0.4187085147175338\n",
      "2022-03-26 22:33:36.493883 Epoch 200, Training Loss 0.4194846081992854\n",
      "2022-03-26 22:33:36.510893 Epoch 200, Training Loss 0.42009782509120835\n",
      "2022-03-26 22:33:36.529899 Epoch 200, Training Loss 0.42074436856352765\n",
      "2022-03-26 22:33:36.548896 Epoch 200, Training Loss 0.42157904525547074\n",
      "2022-03-26 22:33:36.566906 Epoch 200, Training Loss 0.422333561093606\n",
      "2022-03-26 22:33:36.585910 Epoch 200, Training Loss 0.42284685026501756\n",
      "2022-03-26 22:33:36.605914 Epoch 200, Training Loss 0.4236008225728179\n",
      "2022-03-26 22:33:36.623925 Epoch 200, Training Loss 0.42434771854401854\n",
      "2022-03-26 22:33:36.642917 Epoch 200, Training Loss 0.42497284420768316\n",
      "2022-03-26 22:33:36.661928 Epoch 200, Training Loss 0.4257880670716391\n",
      "2022-03-26 22:33:36.679938 Epoch 200, Training Loss 0.426555910797985\n",
      "2022-03-26 22:33:36.698936 Epoch 200, Training Loss 0.4273545388751628\n",
      "2022-03-26 22:33:36.717940 Epoch 200, Training Loss 0.42803086858728656\n",
      "2022-03-26 22:33:36.736938 Epoch 200, Training Loss 0.4288368938142991\n",
      "2022-03-26 22:33:36.754948 Epoch 200, Training Loss 0.43004443239220574\n",
      "2022-03-26 22:33:36.773959 Epoch 200, Training Loss 0.43083567627708014\n",
      "2022-03-26 22:33:36.793951 Epoch 200, Training Loss 0.43167992195357446\n",
      "2022-03-26 22:33:36.811962 Epoch 200, Training Loss 0.4325859586677283\n",
      "2022-03-26 22:33:36.833966 Epoch 200, Training Loss 0.4334480827650451\n",
      "2022-03-26 22:33:36.859972 Epoch 200, Training Loss 0.4343967150772929\n",
      "2022-03-26 22:33:36.886615 Epoch 200, Training Loss 0.43515812592280795\n",
      "2022-03-26 22:33:36.912620 Epoch 200, Training Loss 0.43576119763924337\n",
      "2022-03-26 22:33:36.937632 Epoch 200, Training Loss 0.43660028652309457\n",
      "2022-03-26 22:33:36.963632 Epoch 200, Training Loss 0.4376669798589424\n",
      "2022-03-26 22:33:36.989632 Epoch 200, Training Loss 0.4385412315197308\n",
      "2022-03-26 22:33:37.017638 Epoch 200, Training Loss 0.43907372760193425\n",
      "2022-03-26 22:33:37.032644 Epoch 200, Training Loss 0.4400555673996201\n",
      "2022-03-26 22:33:37.046646 Epoch 200, Training Loss 0.4409575311805281\n",
      "2022-03-26 22:33:37.061648 Epoch 200, Training Loss 0.4421141855323406\n",
      "2022-03-26 22:33:37.075652 Epoch 200, Training Loss 0.4428652871371535\n",
      "2022-03-26 22:33:37.089655 Epoch 200, Training Loss 0.44371910080732896\n",
      "2022-03-26 22:33:37.103651 Epoch 200, Training Loss 0.4447630015404328\n",
      "2022-03-26 22:33:37.118653 Epoch 200, Training Loss 0.4454567694984129\n",
      "2022-03-26 22:33:37.131657 Epoch 200, Training Loss 0.44617525592941765\n",
      "2022-03-26 22:33:37.146661 Epoch 200, Training Loss 0.44699683514854793\n",
      "2022-03-26 22:33:37.160671 Epoch 200, Training Loss 0.4477249590865791\n",
      "2022-03-26 22:33:37.174681 Epoch 200, Training Loss 0.4484303782281973\n",
      "2022-03-26 22:33:37.188677 Epoch 200, Training Loss 0.44916624002292027\n",
      "2022-03-26 22:33:37.203680 Epoch 200, Training Loss 0.4499906828565061\n",
      "2022-03-26 22:33:37.218683 Epoch 200, Training Loss 0.4508091806222106\n",
      "2022-03-26 22:33:37.232688 Epoch 200, Training Loss 0.4514036579891239\n",
      "2022-03-26 22:33:37.247683 Epoch 200, Training Loss 0.4522547864776743\n",
      "2022-03-26 22:33:37.261693 Epoch 200, Training Loss 0.4530864553454587\n",
      "2022-03-26 22:33:37.275690 Epoch 200, Training Loss 0.4536042153606634\n",
      "2022-03-26 22:33:37.290700 Epoch 200, Training Loss 0.45442780551245754\n",
      "2022-03-26 22:33:37.304703 Epoch 200, Training Loss 0.45525113994355704\n",
      "2022-03-26 22:33:37.318706 Epoch 200, Training Loss 0.45596472629348334\n",
      "2022-03-26 22:33:37.333710 Epoch 200, Training Loss 0.4567947551188871\n",
      "2022-03-26 22:33:37.347714 Epoch 200, Training Loss 0.4574146027226582\n",
      "2022-03-26 22:33:37.361716 Epoch 200, Training Loss 0.45821846190773313\n",
      "2022-03-26 22:33:37.376719 Epoch 200, Training Loss 0.45919444977932267\n",
      "2022-03-26 22:33:37.390723 Epoch 200, Training Loss 0.4600554819378402\n",
      "2022-03-26 22:33:37.405726 Epoch 200, Training Loss 0.4610197413379274\n",
      "2022-03-26 22:33:37.420730 Epoch 200, Training Loss 0.4618502664367866\n",
      "2022-03-26 22:33:37.434733 Epoch 200, Training Loss 0.462626874256317\n",
      "2022-03-26 22:33:37.449736 Epoch 200, Training Loss 0.46347963112546964\n",
      "2022-03-26 22:33:37.463739 Epoch 200, Training Loss 0.464105228252728\n",
      "2022-03-26 22:33:37.477742 Epoch 200, Training Loss 0.4650542716998273\n",
      "2022-03-26 22:33:37.491745 Epoch 200, Training Loss 0.4657860307589821\n",
      "2022-03-26 22:33:37.505750 Epoch 200, Training Loss 0.466574953873749\n",
      "2022-03-26 22:33:37.520752 Epoch 200, Training Loss 0.4672183880720602\n",
      "2022-03-26 22:33:37.534756 Epoch 200, Training Loss 0.46804138995192546\n",
      "2022-03-26 22:33:37.548759 Epoch 200, Training Loss 0.4685874916327274\n",
      "2022-03-26 22:33:37.563762 Epoch 200, Training Loss 0.46945287729315743\n",
      "2022-03-26 22:33:37.577765 Epoch 200, Training Loss 0.4702654342593439\n",
      "2022-03-26 22:33:37.591769 Epoch 200, Training Loss 0.4710146054968505\n",
      "2022-03-26 22:33:37.606772 Epoch 200, Training Loss 0.4715435094845569\n",
      "2022-03-26 22:33:37.620776 Epoch 200, Training Loss 0.472300462024596\n",
      "2022-03-26 22:33:37.635971 Epoch 200, Training Loss 0.47331404213405326\n",
      "2022-03-26 22:33:37.649974 Epoch 200, Training Loss 0.47392469328230297\n",
      "2022-03-26 22:33:37.664983 Epoch 200, Training Loss 0.47449422172268335\n",
      "2022-03-26 22:33:37.678501 Epoch 200, Training Loss 0.4752624961726196\n",
      "2022-03-26 22:33:37.693502 Epoch 200, Training Loss 0.47587161379701953\n",
      "2022-03-26 22:33:37.707512 Epoch 200, Training Loss 0.47678925984960685\n",
      "2022-03-26 22:33:37.721509 Epoch 200, Training Loss 0.4774127598385067\n",
      "2022-03-26 22:33:37.735512 Epoch 200, Training Loss 0.4782255122347561\n",
      "2022-03-26 22:33:37.749515 Epoch 200, Training Loss 0.478859785953751\n",
      "2022-03-26 22:33:37.764519 Epoch 200, Training Loss 0.4795723132160314\n",
      "2022-03-26 22:33:37.777524 Epoch 200, Training Loss 0.48051790660604493\n",
      "2022-03-26 22:33:37.791526 Epoch 200, Training Loss 0.48125674947143515\n",
      "2022-03-26 22:33:37.806528 Epoch 200, Training Loss 0.4821880792870241\n",
      "2022-03-26 22:33:37.821531 Epoch 200, Training Loss 0.4827744663142792\n",
      "2022-03-26 22:33:37.836535 Epoch 200, Training Loss 0.4835021268300083\n",
      "2022-03-26 22:33:37.850538 Epoch 200, Training Loss 0.484466486307971\n",
      "2022-03-26 22:33:37.864541 Epoch 200, Training Loss 0.485056155554169\n",
      "2022-03-26 22:33:37.878545 Epoch 200, Training Loss 0.4858095812828035\n",
      "2022-03-26 22:33:37.892549 Epoch 200, Training Loss 0.48657540957946\n",
      "2022-03-26 22:33:37.907544 Epoch 200, Training Loss 0.4873935119117922\n",
      "2022-03-26 22:33:37.921554 Epoch 200, Training Loss 0.4881431301841346\n",
      "2022-03-26 22:33:37.935550 Epoch 200, Training Loss 0.4890578883840605\n",
      "2022-03-26 22:33:37.950561 Epoch 200, Training Loss 0.4898606045624179\n",
      "2022-03-26 22:33:37.965564 Epoch 200, Training Loss 0.4903844762641146\n",
      "2022-03-26 22:33:37.979567 Epoch 200, Training Loss 0.49136143747498007\n",
      "2022-03-26 22:33:37.993572 Epoch 200, Training Loss 0.49201571468806937\n",
      "2022-03-26 22:33:38.008570 Epoch 200, Training Loss 0.492629424728396\n",
      "2022-03-26 22:33:38.023578 Epoch 200, Training Loss 0.49324164690111605\n",
      "2022-03-26 22:33:38.037580 Epoch 200, Training Loss 0.49383774464545044\n",
      "2022-03-26 22:33:38.052585 Epoch 200, Training Loss 0.49465544216925533\n",
      "2022-03-26 22:33:38.067588 Epoch 200, Training Loss 0.49554564302687143\n",
      "2022-03-26 22:33:38.081591 Epoch 200, Training Loss 0.4961994817418516\n",
      "2022-03-26 22:33:38.095601 Epoch 200, Training Loss 0.4967432304111588\n",
      "2022-03-26 22:33:38.109597 Epoch 200, Training Loss 0.4974225517124166\n",
      "2022-03-26 22:33:38.124601 Epoch 200, Training Loss 0.4980919604258769\n",
      "2022-03-26 22:33:38.138604 Epoch 200, Training Loss 0.4991608604293345\n",
      "2022-03-26 22:33:38.153607 Epoch 200, Training Loss 0.49993629650691584\n",
      "2022-03-26 22:33:38.167610 Epoch 200, Training Loss 0.5007503948095814\n",
      "2022-03-26 22:33:38.182615 Epoch 200, Training Loss 0.5013351621454024\n",
      "2022-03-26 22:33:38.196617 Epoch 200, Training Loss 0.5021010758855459\n",
      "2022-03-26 22:33:38.212622 Epoch 200, Training Loss 0.5032995963264304\n",
      "2022-03-26 22:33:38.226623 Epoch 200, Training Loss 0.5039543934414149\n",
      "2022-03-26 22:33:38.240627 Epoch 200, Training Loss 0.5047917855746301\n",
      "2022-03-26 22:33:38.255630 Epoch 200, Training Loss 0.5052358825569567\n",
      "2022-03-26 22:33:38.269633 Epoch 200, Training Loss 0.5061005784193878\n",
      "2022-03-26 22:33:38.283636 Epoch 200, Training Loss 0.5068688938761001\n",
      "2022-03-26 22:33:38.297639 Epoch 200, Training Loss 0.5076763310167186\n",
      "2022-03-26 22:33:38.311643 Epoch 200, Training Loss 0.5085768068156888\n",
      "2022-03-26 22:33:38.326646 Epoch 200, Training Loss 0.5093260676888249\n",
      "2022-03-26 22:33:38.340649 Epoch 200, Training Loss 0.5105599580747088\n",
      "2022-03-26 22:33:38.354653 Epoch 200, Training Loss 0.5113533231074853\n",
      "2022-03-26 22:33:38.368656 Epoch 200, Training Loss 0.5121417267776817\n",
      "2022-03-26 22:33:38.383659 Epoch 200, Training Loss 0.5125770989586326\n",
      "2022-03-26 22:33:38.397662 Epoch 200, Training Loss 0.5132205319374114\n",
      "2022-03-26 22:33:38.412669 Epoch 200, Training Loss 0.514106251165995\n",
      "2022-03-26 22:33:38.426670 Epoch 200, Training Loss 0.5149226637600023\n",
      "2022-03-26 22:33:38.440667 Epoch 200, Training Loss 0.5158604876617031\n",
      "2022-03-26 22:33:38.455668 Epoch 200, Training Loss 0.516914214395806\n",
      "2022-03-26 22:33:38.470671 Epoch 200, Training Loss 0.5179098876540923\n",
      "2022-03-26 22:33:38.484674 Epoch 200, Training Loss 0.5188905307856362\n",
      "2022-03-26 22:33:38.498685 Epoch 200, Training Loss 0.5196453087470111\n",
      "2022-03-26 22:33:38.512690 Epoch 200, Training Loss 0.5202574758883327\n",
      "2022-03-26 22:33:38.527692 Epoch 200, Training Loss 0.5211359094018522\n",
      "2022-03-26 22:33:38.541689 Epoch 200, Training Loss 0.5221146026535717\n",
      "2022-03-26 22:33:38.556698 Epoch 200, Training Loss 0.5228965838851831\n",
      "2022-03-26 22:33:38.571702 Epoch 200, Training Loss 0.5238549357942303\n",
      "2022-03-26 22:33:38.585705 Epoch 200, Training Loss 0.5251798946076952\n",
      "2022-03-26 22:33:38.600708 Epoch 200, Training Loss 0.5260132518418305\n",
      "2022-03-26 22:33:38.614713 Epoch 200, Training Loss 0.5269108950482\n",
      "2022-03-26 22:33:38.629715 Epoch 200, Training Loss 0.5275911162119082\n",
      "2022-03-26 22:33:38.644718 Epoch 200, Training Loss 0.5283741844279687\n",
      "2022-03-26 22:33:38.659722 Epoch 200, Training Loss 0.5290893178309322\n",
      "2022-03-26 22:33:38.674728 Epoch 200, Training Loss 0.5296694647396922\n",
      "2022-03-26 22:33:38.689728 Epoch 200, Training Loss 0.530652137249327\n",
      "2022-03-26 22:33:38.703732 Epoch 200, Training Loss 0.5312724013615142\n",
      "2022-03-26 22:33:38.718735 Epoch 200, Training Loss 0.5319543171416768\n",
      "2022-03-26 22:33:38.732739 Epoch 200, Training Loss 0.5327892452097305\n",
      "2022-03-26 22:33:38.746743 Epoch 200, Training Loss 0.5334547319070763\n",
      "2022-03-26 22:33:38.761745 Epoch 200, Training Loss 0.5341786120248877\n",
      "2022-03-26 22:33:38.775748 Epoch 200, Training Loss 0.5351101045718278\n",
      "2022-03-26 22:33:38.789752 Epoch 200, Training Loss 0.5356006182520591\n",
      "2022-03-26 22:33:38.804762 Epoch 200, Training Loss 0.5363672141681242\n",
      "2022-03-26 22:33:38.818758 Epoch 200, Training Loss 0.5371947216865657\n",
      "2022-03-26 22:33:38.836779 Epoch 200, Training Loss 0.5378155314632694\n",
      "2022-03-26 22:33:38.855777 Epoch 200, Training Loss 0.5383779439322479\n",
      "2022-03-26 22:33:38.875771 Epoch 200, Training Loss 0.5390952701306404\n",
      "2022-03-26 22:33:38.894781 Epoch 200, Training Loss 0.5399200269175918\n",
      "2022-03-26 22:33:38.913773 Epoch 200, Training Loss 0.5408622856487704\n",
      "2022-03-26 22:33:38.932775 Epoch 200, Training Loss 0.5415678175971331\n",
      "2022-03-26 22:33:38.952782 Epoch 200, Training Loss 0.5424100069133827\n",
      "2022-03-26 22:33:38.971806 Epoch 200, Training Loss 0.5430515117352576\n",
      "2022-03-26 22:33:38.991215 Epoch 200, Training Loss 0.5436100602683509\n",
      "2022-03-26 22:33:39.010797 Epoch 200, Training Loss 0.5445017515469694\n",
      "2022-03-26 22:33:39.029806 Epoch 200, Training Loss 0.5452250584083445\n",
      "2022-03-26 22:33:39.050817 Epoch 200, Training Loss 0.545873793304119\n",
      "2022-03-26 22:33:39.069827 Epoch 200, Training Loss 0.5467454991910768\n",
      "2022-03-26 22:33:39.089831 Epoch 200, Training Loss 0.5475262777351052\n",
      "2022-03-26 22:33:39.107909 Epoch 200, Training Loss 0.5483042489156089\n",
      "2022-03-26 22:33:39.127834 Epoch 200, Training Loss 0.5490667115315757\n",
      "2022-03-26 22:33:39.146832 Epoch 200, Training Loss 0.5497567365160378\n",
      "2022-03-26 22:33:39.166849 Epoch 200, Training Loss 0.5504891638789335\n",
      "2022-03-26 22:33:39.185847 Epoch 200, Training Loss 0.5514686470065275\n",
      "2022-03-26 22:33:39.205858 Epoch 200, Training Loss 0.552339834439785\n",
      "2022-03-26 22:33:39.224849 Epoch 200, Training Loss 0.5531656505430446\n",
      "2022-03-26 22:33:39.243860 Epoch 200, Training Loss 0.5539751235405197\n",
      "2022-03-26 22:33:39.262871 Epoch 200, Training Loss 0.554790600524534\n",
      "2022-03-26 22:33:39.282875 Epoch 200, Training Loss 0.5554905233861845\n",
      "2022-03-26 22:33:39.302874 Epoch 200, Training Loss 0.5565016662601925\n",
      "2022-03-26 22:33:39.321879 Epoch 200, Training Loss 0.5574878997281384\n",
      "2022-03-26 22:33:39.341115 Epoch 200, Training Loss 0.5580176315496644\n",
      "2022-03-26 22:33:39.359759 Epoch 200, Training Loss 0.5587419773764013\n",
      "2022-03-26 22:33:39.378764 Epoch 200, Training Loss 0.5595536287635794\n",
      "2022-03-26 22:33:39.397774 Epoch 200, Training Loss 0.5604455131094169\n",
      "2022-03-26 22:33:39.417772 Epoch 200, Training Loss 0.5612596060766284\n",
      "2022-03-26 22:33:39.436519 Epoch 200, Training Loss 0.5621111749688072\n",
      "2022-03-26 22:33:39.456524 Epoch 200, Training Loss 0.5627996785104122\n",
      "2022-03-26 22:33:39.476536 Epoch 200, Training Loss 0.5636374838364399\n",
      "2022-03-26 22:33:39.495545 Epoch 200, Training Loss 0.5642209605259054\n",
      "2022-03-26 22:33:39.514549 Epoch 200, Training Loss 0.5650682327768687\n",
      "2022-03-26 22:33:39.534549 Epoch 200, Training Loss 0.5658772853405579\n",
      "2022-03-26 22:33:39.553558 Epoch 200, Training Loss 0.5665527773863824\n",
      "2022-03-26 22:33:39.572563 Epoch 200, Training Loss 0.5672105603739429\n",
      "2022-03-26 22:33:39.591554 Epoch 200, Training Loss 0.5680234846480362\n",
      "2022-03-26 22:33:39.610565 Epoch 200, Training Loss 0.568982534891809\n",
      "2022-03-26 22:33:39.630576 Epoch 200, Training Loss 0.5697658040258281\n",
      "2022-03-26 22:33:39.649580 Epoch 200, Training Loss 0.5704440031286395\n",
      "2022-03-26 22:33:39.669585 Epoch 200, Training Loss 0.5711673896407228\n",
      "2022-03-26 22:33:39.689577 Epoch 200, Training Loss 0.571792521668822\n",
      "2022-03-26 22:33:39.709588 Epoch 200, Training Loss 0.5726985881852982\n",
      "2022-03-26 22:33:39.729598 Epoch 200, Training Loss 0.5732890698687195\n",
      "2022-03-26 22:33:39.749603 Epoch 200, Training Loss 0.5741463464772915\n",
      "2022-03-26 22:33:39.769599 Epoch 200, Training Loss 0.5748444774647807\n",
      "2022-03-26 22:33:39.790600 Epoch 200, Training Loss 0.5756653858648847\n",
      "2022-03-26 22:33:39.809604 Epoch 200, Training Loss 0.5764114645969532\n",
      "2022-03-26 22:33:39.829609 Epoch 200, Training Loss 0.5773020081431665\n",
      "2022-03-26 22:33:39.848605 Epoch 200, Training Loss 0.5781518861537089\n",
      "2022-03-26 22:33:39.869610 Epoch 200, Training Loss 0.5787640660238997\n",
      "2022-03-26 22:33:39.889634 Epoch 200, Training Loss 0.5796747288054518\n",
      "2022-03-26 22:33:39.908640 Epoch 200, Training Loss 0.580283859768487\n",
      "2022-03-26 22:33:39.927637 Epoch 200, Training Loss 0.5807948625453597\n",
      "2022-03-26 22:33:39.946647 Epoch 200, Training Loss 0.5814933879753513\n",
      "2022-03-26 22:33:39.966646 Epoch 200, Training Loss 0.5824691098364417\n",
      "2022-03-26 22:33:39.986656 Epoch 200, Training Loss 0.5832074738829337\n",
      "2022-03-26 22:33:40.005661 Epoch 200, Training Loss 0.5838853322788883\n",
      "2022-03-26 22:33:40.024659 Epoch 200, Training Loss 0.5845518391912855\n",
      "2022-03-26 22:33:40.044664 Epoch 200, Training Loss 0.5851857431633088\n",
      "2022-03-26 22:33:40.062674 Epoch 200, Training Loss 0.5859197264970721\n",
      "2022-03-26 22:33:40.082678 Epoch 200, Training Loss 0.586651828244824\n",
      "2022-03-26 22:33:40.102683 Epoch 200, Training Loss 0.5871026815889436\n",
      "2022-03-26 22:33:40.121675 Epoch 200, Training Loss 0.5878754002435128\n",
      "2022-03-26 22:33:40.140700 Epoch 200, Training Loss 0.5886900577017719\n",
      "2022-03-26 22:33:40.159690 Epoch 200, Training Loss 0.5894905529592348\n",
      "2022-03-26 22:33:40.179694 Epoch 200, Training Loss 0.5905750936940503\n",
      "2022-03-26 22:33:40.199700 Epoch 200, Training Loss 0.5912172236024876\n",
      "2022-03-26 22:33:40.219703 Epoch 200, Training Loss 0.5921022424383846\n",
      "2022-03-26 22:33:40.238714 Epoch 200, Training Loss 0.592802210041629\n",
      "2022-03-26 22:33:40.257718 Epoch 200, Training Loss 0.5935277249425879\n",
      "2022-03-26 22:33:40.277717 Epoch 200, Training Loss 0.5943523961336107\n",
      "2022-03-26 22:33:40.296727 Epoch 200, Training Loss 0.5950975091484807\n",
      "2022-03-26 22:33:40.315731 Epoch 200, Training Loss 0.5958169928520841\n",
      "2022-03-26 22:33:40.335735 Epoch 200, Training Loss 0.5966885436297683\n",
      "2022-03-26 22:33:40.355744 Epoch 200, Training Loss 0.5976096982007746\n",
      "2022-03-26 22:33:40.375739 Epoch 200, Training Loss 0.598391901387278\n",
      "2022-03-26 22:33:40.394743 Epoch 200, Training Loss 0.5991888837055173\n",
      "2022-03-26 22:33:40.402739 Epoch 200, Training Loss 0.5997704614687454\n",
      "2022-03-26 22:44:54.646883 Epoch 250, Training Loss 0.0012207366621402828\n",
      "2022-03-26 22:44:54.668888 Epoch 250, Training Loss 0.0020403803309516225\n",
      "2022-03-26 22:44:54.693895 Epoch 250, Training Loss 0.0031618702289698372\n",
      "2022-03-26 22:44:54.727901 Epoch 250, Training Loss 0.003622575565372282\n",
      "2022-03-26 22:44:54.753911 Epoch 250, Training Loss 0.004549146232092777\n",
      "2022-03-26 22:44:54.788916 Epoch 250, Training Loss 0.005422768004410102\n",
      "2022-03-26 22:44:54.814923 Epoch 250, Training Loss 0.006042215258569059\n",
      "2022-03-26 22:44:54.843930 Epoch 250, Training Loss 0.006644188100114808\n",
      "2022-03-26 22:44:54.861934 Epoch 250, Training Loss 0.007501667608385501\n",
      "2022-03-26 22:44:54.877918 Epoch 250, Training Loss 0.008221649490963772\n",
      "2022-03-26 22:44:54.892922 Epoch 250, Training Loss 0.00893401615607464\n",
      "2022-03-26 22:44:54.906931 Epoch 250, Training Loss 0.009546193282317627\n",
      "2022-03-26 22:44:54.921876 Epoch 250, Training Loss 0.010227919730079143\n",
      "2022-03-26 22:44:54.939907 Epoch 250, Training Loss 0.010978645718920871\n",
      "2022-03-26 22:44:54.955773 Epoch 250, Training Loss 0.011678186943159079\n",
      "2022-03-26 22:44:54.972117 Epoch 250, Training Loss 0.012438661340252517\n",
      "2022-03-26 22:44:54.988900 Epoch 250, Training Loss 0.013066367496309989\n",
      "2022-03-26 22:44:55.005888 Epoch 250, Training Loss 0.014186052638856346\n",
      "2022-03-26 22:44:55.021891 Epoch 250, Training Loss 0.015139792505127694\n",
      "2022-03-26 22:44:55.039895 Epoch 250, Training Loss 0.015958552546513356\n",
      "2022-03-26 22:44:55.056853 Epoch 250, Training Loss 0.01668130261513888\n",
      "2022-03-26 22:44:55.072875 Epoch 250, Training Loss 0.017615516624792153\n",
      "2022-03-26 22:44:55.088938 Epoch 250, Training Loss 0.01829729848505591\n",
      "2022-03-26 22:44:55.105873 Epoch 250, Training Loss 0.01916840443830661\n",
      "2022-03-26 22:44:55.122832 Epoch 250, Training Loss 0.0198449589254911\n",
      "2022-03-26 22:44:55.139603 Epoch 250, Training Loss 0.020509103832342435\n",
      "2022-03-26 22:44:55.156247 Epoch 250, Training Loss 0.021027337254770576\n",
      "2022-03-26 22:44:55.172999 Epoch 250, Training Loss 0.02154274665943497\n",
      "2022-03-26 22:44:55.193005 Epoch 250, Training Loss 0.022328308133213112\n",
      "2022-03-26 22:44:55.209939 Epoch 250, Training Loss 0.023162419953004783\n",
      "2022-03-26 22:44:55.223942 Epoch 250, Training Loss 0.0236046024981667\n",
      "2022-03-26 22:44:55.238877 Epoch 250, Training Loss 0.02418103471131581\n",
      "2022-03-26 22:44:55.255825 Epoch 250, Training Loss 0.025255639763439402\n",
      "2022-03-26 22:44:55.273018 Epoch 250, Training Loss 0.02593677915880442\n",
      "2022-03-26 22:44:55.294943 Epoch 250, Training Loss 0.026748076119386326\n",
      "2022-03-26 22:44:55.311105 Epoch 250, Training Loss 0.02750312542671438\n",
      "2022-03-26 22:44:55.325069 Epoch 250, Training Loss 0.028369627035487337\n",
      "2022-03-26 22:44:55.340093 Epoch 250, Training Loss 0.029161496311807266\n",
      "2022-03-26 22:44:55.355096 Epoch 250, Training Loss 0.029869470678631913\n",
      "2022-03-26 22:44:55.372067 Epoch 250, Training Loss 0.03062671903149246\n",
      "2022-03-26 22:44:55.390076 Epoch 250, Training Loss 0.031161404441079826\n",
      "2022-03-26 22:44:55.405924 Epoch 250, Training Loss 0.03205258740336084\n",
      "2022-03-26 22:44:55.421941 Epoch 250, Training Loss 0.03285144856366355\n",
      "2022-03-26 22:44:55.438905 Epoch 250, Training Loss 0.03360906239513241\n",
      "2022-03-26 22:44:55.455927 Epoch 250, Training Loss 0.034580835181733834\n",
      "2022-03-26 22:44:55.472420 Epoch 250, Training Loss 0.03546243772634765\n",
      "2022-03-26 22:44:55.491008 Epoch 250, Training Loss 0.036396194487581475\n",
      "2022-03-26 22:44:55.506896 Epoch 250, Training Loss 0.03703005333690692\n",
      "2022-03-26 22:44:55.521959 Epoch 250, Training Loss 0.037594968743641356\n",
      "2022-03-26 22:44:55.538965 Epoch 250, Training Loss 0.038491362996418456\n",
      "2022-03-26 22:44:55.556133 Epoch 250, Training Loss 0.03936202138128793\n",
      "2022-03-26 22:44:55.572721 Epoch 250, Training Loss 0.04028613430917111\n",
      "2022-03-26 22:44:55.588610 Epoch 250, Training Loss 0.04108060347607069\n",
      "2022-03-26 22:44:55.605785 Epoch 250, Training Loss 0.04156608182146116\n",
      "2022-03-26 22:44:55.622779 Epoch 250, Training Loss 0.04242262876856967\n",
      "2022-03-26 22:44:55.637775 Epoch 250, Training Loss 0.04319762787245728\n",
      "2022-03-26 22:44:55.654762 Epoch 250, Training Loss 0.04384284274047598\n",
      "2022-03-26 22:44:55.671829 Epoch 250, Training Loss 0.04440995292437961\n",
      "2022-03-26 22:44:55.689317 Epoch 250, Training Loss 0.045236905403149404\n",
      "2022-03-26 22:44:55.706174 Epoch 250, Training Loss 0.04588362014354647\n",
      "2022-03-26 22:44:55.722114 Epoch 250, Training Loss 0.046617232663247285\n",
      "2022-03-26 22:44:55.737963 Epoch 250, Training Loss 0.04726604980123622\n",
      "2022-03-26 22:44:55.755986 Epoch 250, Training Loss 0.04785856574087802\n",
      "2022-03-26 22:44:55.772545 Epoch 250, Training Loss 0.0483154893836097\n",
      "2022-03-26 22:44:55.788930 Epoch 250, Training Loss 0.04908345925533558\n",
      "2022-03-26 22:44:55.804818 Epoch 250, Training Loss 0.04984342792759771\n",
      "2022-03-26 22:44:55.822092 Epoch 250, Training Loss 0.05064129783674274\n",
      "2022-03-26 22:44:55.838991 Epoch 250, Training Loss 0.051572108512644266\n",
      "2022-03-26 22:44:55.854660 Epoch 250, Training Loss 0.052243885786637016\n",
      "2022-03-26 22:44:55.872648 Epoch 250, Training Loss 0.052934755068605815\n",
      "2022-03-26 22:44:55.887071 Epoch 250, Training Loss 0.05349201665205114\n",
      "2022-03-26 22:44:55.905926 Epoch 250, Training Loss 0.05438751111859861\n",
      "2022-03-26 22:44:55.921939 Epoch 250, Training Loss 0.05496727456064785\n",
      "2022-03-26 22:44:55.938963 Epoch 250, Training Loss 0.05571843035843061\n",
      "2022-03-26 22:44:55.956491 Epoch 250, Training Loss 0.05642679741467966\n",
      "2022-03-26 22:44:55.974070 Epoch 250, Training Loss 0.057203428329104355\n",
      "2022-03-26 22:44:55.989796 Epoch 250, Training Loss 0.0578283009398014\n",
      "2022-03-26 22:44:56.004796 Epoch 250, Training Loss 0.058250550761857\n",
      "2022-03-26 22:44:56.022840 Epoch 250, Training Loss 0.05898389406978627\n",
      "2022-03-26 22:44:56.037839 Epoch 250, Training Loss 0.05960986345930173\n",
      "2022-03-26 22:44:56.055861 Epoch 250, Training Loss 0.060422082081475224\n",
      "2022-03-26 22:44:56.071847 Epoch 250, Training Loss 0.061252385758987775\n",
      "2022-03-26 22:44:56.088843 Epoch 250, Training Loss 0.06199294206736338\n",
      "2022-03-26 22:44:56.106269 Epoch 250, Training Loss 0.062431665790050536\n",
      "2022-03-26 22:44:56.122158 Epoch 250, Training Loss 0.06290228962136046\n",
      "2022-03-26 22:44:56.139176 Epoch 250, Training Loss 0.06388329632599335\n",
      "2022-03-26 22:44:56.155489 Epoch 250, Training Loss 0.06455941635476964\n",
      "2022-03-26 22:44:56.171948 Epoch 250, Training Loss 0.06516507077400031\n",
      "2022-03-26 22:44:56.188951 Epoch 250, Training Loss 0.06591138891551805\n",
      "2022-03-26 22:44:56.204918 Epoch 250, Training Loss 0.06672889725936343\n",
      "2022-03-26 22:44:56.222893 Epoch 250, Training Loss 0.0673388836862486\n",
      "2022-03-26 22:44:56.237908 Epoch 250, Training Loss 0.06803305187950963\n",
      "2022-03-26 22:44:56.256096 Epoch 250, Training Loss 0.06861859365649846\n",
      "2022-03-26 22:44:56.271967 Epoch 250, Training Loss 0.06946374232049489\n",
      "2022-03-26 22:44:56.287970 Epoch 250, Training Loss 0.070317546691736\n",
      "2022-03-26 22:44:56.306831 Epoch 250, Training Loss 0.07090285359441167\n",
      "2022-03-26 22:44:56.322212 Epoch 250, Training Loss 0.0714593254544241\n",
      "2022-03-26 22:44:56.339224 Epoch 250, Training Loss 0.07239559582432213\n",
      "2022-03-26 22:44:56.354377 Epoch 250, Training Loss 0.07293752628519103\n",
      "2022-03-26 22:44:56.372359 Epoch 250, Training Loss 0.07364687064419621\n",
      "2022-03-26 22:44:56.389813 Epoch 250, Training Loss 0.07447466360943397\n",
      "2022-03-26 22:44:56.404813 Epoch 250, Training Loss 0.07525316940244202\n",
      "2022-03-26 22:44:56.421784 Epoch 250, Training Loss 0.07573778729152192\n",
      "2022-03-26 22:44:56.438633 Epoch 250, Training Loss 0.07666836900022024\n",
      "2022-03-26 22:44:56.457560 Epoch 250, Training Loss 0.07741555666832058\n",
      "2022-03-26 22:44:56.473227 Epoch 250, Training Loss 0.07812341723753058\n",
      "2022-03-26 22:44:56.488962 Epoch 250, Training Loss 0.078617186459434\n",
      "2022-03-26 22:44:56.505892 Epoch 250, Training Loss 0.07936720008892781\n",
      "2022-03-26 22:44:56.521869 Epoch 250, Training Loss 0.08003324761872402\n",
      "2022-03-26 22:44:56.538857 Epoch 250, Training Loss 0.08054893378101652\n",
      "2022-03-26 22:44:56.554855 Epoch 250, Training Loss 0.08125087923710914\n",
      "2022-03-26 22:44:56.572148 Epoch 250, Training Loss 0.08194820830584182\n",
      "2022-03-26 22:44:56.590170 Epoch 250, Training Loss 0.08258075070808\n",
      "2022-03-26 22:44:56.605899 Epoch 250, Training Loss 0.08325145471736294\n",
      "2022-03-26 22:44:56.621980 Epoch 250, Training Loss 0.08384356783021746\n",
      "2022-03-26 22:44:56.638983 Epoch 250, Training Loss 0.08455275311646863\n",
      "2022-03-26 22:44:56.656005 Epoch 250, Training Loss 0.08547315020542925\n",
      "2022-03-26 22:44:56.672087 Epoch 250, Training Loss 0.08601241526396378\n",
      "2022-03-26 22:44:56.690067 Epoch 250, Training Loss 0.08672649986908564\n",
      "2022-03-26 22:44:56.704961 Epoch 250, Training Loss 0.08750056176234389\n",
      "2022-03-26 22:44:56.721923 Epoch 250, Training Loss 0.08823612652471303\n",
      "2022-03-26 22:44:56.738925 Epoch 250, Training Loss 0.0890794930708073\n",
      "2022-03-26 22:44:56.755865 Epoch 250, Training Loss 0.08963885975768195\n",
      "2022-03-26 22:44:56.772844 Epoch 250, Training Loss 0.09023826735098954\n",
      "2022-03-26 22:44:56.788924 Epoch 250, Training Loss 0.09106620803208607\n",
      "2022-03-26 22:44:56.804894 Epoch 250, Training Loss 0.09181157554811833\n",
      "2022-03-26 22:44:56.821904 Epoch 250, Training Loss 0.09262471003910465\n",
      "2022-03-26 22:44:56.837917 Epoch 250, Training Loss 0.09334991251111335\n",
      "2022-03-26 22:44:56.854837 Epoch 250, Training Loss 0.09403294309630723\n",
      "2022-03-26 22:44:56.872489 Epoch 250, Training Loss 0.09458410396905201\n",
      "2022-03-26 22:44:56.888898 Epoch 250, Training Loss 0.09536642560263729\n",
      "2022-03-26 22:44:56.905789 Epoch 250, Training Loss 0.09598702546733115\n",
      "2022-03-26 22:44:56.921792 Epoch 250, Training Loss 0.09665907385861477\n",
      "2022-03-26 22:44:56.938870 Epoch 250, Training Loss 0.09738003723609173\n",
      "2022-03-26 22:44:56.955900 Epoch 250, Training Loss 0.0981443320470088\n",
      "2022-03-26 22:44:56.972865 Epoch 250, Training Loss 0.09884333675322325\n",
      "2022-03-26 22:44:56.988871 Epoch 250, Training Loss 0.0994742933060507\n",
      "2022-03-26 22:44:57.004804 Epoch 250, Training Loss 0.10017571565897568\n",
      "2022-03-26 22:44:57.022888 Epoch 250, Training Loss 0.10085742457595932\n",
      "2022-03-26 22:44:57.037872 Epoch 250, Training Loss 0.10174619941912649\n",
      "2022-03-26 22:44:57.055848 Epoch 250, Training Loss 0.10238933353625296\n",
      "2022-03-26 22:44:57.071838 Epoch 250, Training Loss 0.1030368629242758\n",
      "2022-03-26 22:44:57.087835 Epoch 250, Training Loss 0.10366741398258893\n",
      "2022-03-26 22:44:57.105864 Epoch 250, Training Loss 0.10457790900221871\n",
      "2022-03-26 22:44:57.121780 Epoch 250, Training Loss 0.10583562988911749\n",
      "2022-03-26 22:44:57.138786 Epoch 250, Training Loss 0.10655233801325874\n",
      "2022-03-26 22:44:57.154797 Epoch 250, Training Loss 0.10734968992602795\n",
      "2022-03-26 22:44:57.172821 Epoch 250, Training Loss 0.1082000228983667\n",
      "2022-03-26 22:44:57.188824 Epoch 250, Training Loss 0.10886066553690245\n",
      "2022-03-26 22:44:57.205248 Epoch 250, Training Loss 0.10942740227712695\n",
      "2022-03-26 22:44:57.223077 Epoch 250, Training Loss 0.10990430448976014\n",
      "2022-03-26 22:44:57.238710 Epoch 250, Training Loss 0.11082041865724432\n",
      "2022-03-26 22:44:57.255769 Epoch 250, Training Loss 0.11153390721591842\n",
      "2022-03-26 22:44:57.271765 Epoch 250, Training Loss 0.11250000986296807\n",
      "2022-03-26 22:44:57.287724 Epoch 250, Training Loss 0.11306644960894914\n",
      "2022-03-26 22:44:57.305717 Epoch 250, Training Loss 0.11384301855588508\n",
      "2022-03-26 22:44:57.322186 Epoch 250, Training Loss 0.11457756630447515\n",
      "2022-03-26 22:44:57.339106 Epoch 250, Training Loss 0.11544688381349949\n",
      "2022-03-26 22:44:57.355342 Epoch 250, Training Loss 0.11617765505143139\n",
      "2022-03-26 22:44:57.371709 Epoch 250, Training Loss 0.11681803904683388\n",
      "2022-03-26 22:44:57.388952 Epoch 250, Training Loss 0.11740089533731456\n",
      "2022-03-26 22:44:57.405379 Epoch 250, Training Loss 0.11816294697087135\n",
      "2022-03-26 22:44:57.421788 Epoch 250, Training Loss 0.11899374341568374\n",
      "2022-03-26 22:44:57.438791 Epoch 250, Training Loss 0.11949089074226292\n",
      "2022-03-26 22:44:57.455775 Epoch 250, Training Loss 0.12013468233978047\n",
      "2022-03-26 22:44:57.471778 Epoch 250, Training Loss 0.12072445638954182\n",
      "2022-03-26 22:44:57.488727 Epoch 250, Training Loss 0.12130340861390009\n",
      "2022-03-26 22:44:57.505743 Epoch 250, Training Loss 0.12190260244604877\n",
      "2022-03-26 22:44:57.521963 Epoch 250, Training Loss 0.12257508697259761\n",
      "2022-03-26 22:44:57.538987 Epoch 250, Training Loss 0.1233362350088861\n",
      "2022-03-26 22:44:57.553897 Epoch 250, Training Loss 0.12414522522398273\n",
      "2022-03-26 22:44:57.571986 Epoch 250, Training Loss 0.12465832700662295\n",
      "2022-03-26 22:44:57.588904 Epoch 250, Training Loss 0.1255552492406972\n",
      "2022-03-26 22:44:57.604866 Epoch 250, Training Loss 0.12620957584484763\n",
      "2022-03-26 22:44:57.621842 Epoch 250, Training Loss 0.12714242984724167\n",
      "2022-03-26 22:44:57.639842 Epoch 250, Training Loss 0.127901495760664\n",
      "2022-03-26 22:44:57.653717 Epoch 250, Training Loss 0.12841700726305433\n",
      "2022-03-26 22:44:57.671741 Epoch 250, Training Loss 0.129019327214002\n",
      "2022-03-26 22:44:57.688793 Epoch 250, Training Loss 0.12990289564480256\n",
      "2022-03-26 22:44:57.705816 Epoch 250, Training Loss 0.1307006473355281\n",
      "2022-03-26 22:44:57.721839 Epoch 250, Training Loss 0.13150121462162193\n",
      "2022-03-26 22:44:57.738843 Epoch 250, Training Loss 0.1318576941481027\n",
      "2022-03-26 22:44:57.755864 Epoch 250, Training Loss 0.1326373274750112\n",
      "2022-03-26 22:44:57.772409 Epoch 250, Training Loss 0.1335326255206257\n",
      "2022-03-26 22:44:57.794391 Epoch 250, Training Loss 0.13412955853030506\n",
      "2022-03-26 22:44:57.816318 Epoch 250, Training Loss 0.13483580855457375\n",
      "2022-03-26 22:44:57.835970 Epoch 250, Training Loss 0.1357554573842022\n",
      "2022-03-26 22:44:57.853968 Epoch 250, Training Loss 0.13643691606838684\n",
      "2022-03-26 22:44:57.871918 Epoch 250, Training Loss 0.13715530325994468\n",
      "2022-03-26 22:44:57.890098 Epoch 250, Training Loss 0.13766079283583804\n",
      "2022-03-26 22:44:57.913226 Epoch 250, Training Loss 0.1383542161997017\n",
      "2022-03-26 22:44:57.940223 Epoch 250, Training Loss 0.139177857419414\n",
      "2022-03-26 22:44:57.956227 Epoch 250, Training Loss 0.13973365198163426\n",
      "2022-03-26 22:44:57.972769 Epoch 250, Training Loss 0.14066264890801267\n",
      "2022-03-26 22:44:58.011779 Epoch 250, Training Loss 0.14146283196518794\n",
      "2022-03-26 22:44:58.026757 Epoch 250, Training Loss 0.14212292291776604\n",
      "2022-03-26 22:44:58.040770 Epoch 250, Training Loss 0.14288646078018277\n",
      "2022-03-26 22:44:58.055775 Epoch 250, Training Loss 0.1436375525525159\n",
      "2022-03-26 22:44:58.073965 Epoch 250, Training Loss 0.14444468084656065\n",
      "2022-03-26 22:44:58.088987 Epoch 250, Training Loss 0.1454756274019056\n",
      "2022-03-26 22:44:58.105893 Epoch 250, Training Loss 0.1463884239077873\n",
      "2022-03-26 22:44:58.121889 Epoch 250, Training Loss 0.147010309921811\n",
      "2022-03-26 22:44:58.138936 Epoch 250, Training Loss 0.14802390878157848\n",
      "2022-03-26 22:44:58.155985 Epoch 250, Training Loss 0.14868931933437163\n",
      "2022-03-26 22:44:58.172300 Epoch 250, Training Loss 0.14983365480857128\n",
      "2022-03-26 22:44:58.188269 Epoch 250, Training Loss 0.15048984462952675\n",
      "2022-03-26 22:44:58.205720 Epoch 250, Training Loss 0.15112561448608214\n",
      "2022-03-26 22:44:58.223385 Epoch 250, Training Loss 0.1517950002570896\n",
      "2022-03-26 22:44:58.239332 Epoch 250, Training Loss 0.1524204749356755\n",
      "2022-03-26 22:44:58.257144 Epoch 250, Training Loss 0.15306526746438898\n",
      "2022-03-26 22:44:58.272789 Epoch 250, Training Loss 0.1536548292392965\n",
      "2022-03-26 22:44:58.288911 Epoch 250, Training Loss 0.15448938123405437\n",
      "2022-03-26 22:44:58.304983 Epoch 250, Training Loss 0.15509852991841944\n",
      "2022-03-26 22:44:58.321956 Epoch 250, Training Loss 0.15560263162836088\n",
      "2022-03-26 22:44:58.338957 Epoch 250, Training Loss 0.1563277686267253\n",
      "2022-03-26 22:44:58.355962 Epoch 250, Training Loss 0.15692716143320284\n",
      "2022-03-26 22:44:58.373702 Epoch 250, Training Loss 0.1576102648092353\n",
      "2022-03-26 22:44:58.388701 Epoch 250, Training Loss 0.1580050899015973\n",
      "2022-03-26 22:44:58.405718 Epoch 250, Training Loss 0.15864382890026893\n",
      "2022-03-26 22:44:58.422843 Epoch 250, Training Loss 0.15921328695076506\n",
      "2022-03-26 22:44:58.438845 Epoch 250, Training Loss 0.15968831451347723\n",
      "2022-03-26 22:44:58.455879 Epoch 250, Training Loss 0.1603215089463212\n",
      "2022-03-26 22:44:58.471845 Epoch 250, Training Loss 0.16080125236450254\n",
      "2022-03-26 22:44:58.488875 Epoch 250, Training Loss 0.16161375803410855\n",
      "2022-03-26 22:44:58.505873 Epoch 250, Training Loss 0.1623715508319533\n",
      "2022-03-26 22:44:58.521840 Epoch 250, Training Loss 0.16313128725951895\n",
      "2022-03-26 22:44:58.538843 Epoch 250, Training Loss 0.16384881125081835\n",
      "2022-03-26 22:44:58.556783 Epoch 250, Training Loss 0.16446809889867786\n",
      "2022-03-26 22:44:58.572544 Epoch 250, Training Loss 0.16532554029656188\n",
      "2022-03-26 22:44:58.588259 Epoch 250, Training Loss 0.1661305629155215\n",
      "2022-03-26 22:44:58.606204 Epoch 250, Training Loss 0.1669108359633809\n",
      "2022-03-26 22:44:58.621873 Epoch 250, Training Loss 0.16797436903352322\n",
      "2022-03-26 22:44:58.638894 Epoch 250, Training Loss 0.16855857961470513\n",
      "2022-03-26 22:44:58.655924 Epoch 250, Training Loss 0.16914695310775582\n",
      "2022-03-26 22:44:58.675053 Epoch 250, Training Loss 0.16999724796970786\n",
      "2022-03-26 22:44:58.689064 Epoch 250, Training Loss 0.17097682324821686\n",
      "2022-03-26 22:44:58.705087 Epoch 250, Training Loss 0.17165404611536303\n",
      "2022-03-26 22:44:58.722773 Epoch 250, Training Loss 0.1723767051001644\n",
      "2022-03-26 22:44:58.738959 Epoch 250, Training Loss 0.17293520046926825\n",
      "2022-03-26 22:44:58.756984 Epoch 250, Training Loss 0.17377636606431068\n",
      "2022-03-26 22:44:58.771965 Epoch 250, Training Loss 0.17447592344735285\n",
      "2022-03-26 22:44:58.788943 Epoch 250, Training Loss 0.17532811033756227\n",
      "2022-03-26 22:44:58.805873 Epoch 250, Training Loss 0.17577498236580577\n",
      "2022-03-26 22:44:58.821982 Epoch 250, Training Loss 0.1764302335279372\n",
      "2022-03-26 22:44:58.838952 Epoch 250, Training Loss 0.17759302288979825\n",
      "2022-03-26 22:44:58.855895 Epoch 250, Training Loss 0.17834852319544234\n",
      "2022-03-26 22:44:58.871824 Epoch 250, Training Loss 0.17896437923164318\n",
      "2022-03-26 22:44:58.887831 Epoch 250, Training Loss 0.17957358412882862\n",
      "2022-03-26 22:44:58.905836 Epoch 250, Training Loss 0.17998587936544053\n",
      "2022-03-26 22:44:58.921882 Epoch 250, Training Loss 0.18060241982607586\n",
      "2022-03-26 22:44:58.938864 Epoch 250, Training Loss 0.18148711182729668\n",
      "2022-03-26 22:44:58.955789 Epoch 250, Training Loss 0.18243809814190926\n",
      "2022-03-26 22:44:58.973248 Epoch 250, Training Loss 0.18306605037673354\n",
      "2022-03-26 22:44:58.989984 Epoch 250, Training Loss 0.18374199235378325\n",
      "2022-03-26 22:44:59.004893 Epoch 250, Training Loss 0.1844927940298529\n",
      "2022-03-26 22:44:59.022721 Epoch 250, Training Loss 0.1851186263363075\n",
      "2022-03-26 22:44:59.038736 Epoch 250, Training Loss 0.1857540814772896\n",
      "2022-03-26 22:44:59.054961 Epoch 250, Training Loss 0.18664623526356106\n",
      "2022-03-26 22:44:59.072571 Epoch 250, Training Loss 0.18721842053143875\n",
      "2022-03-26 22:44:59.088605 Epoch 250, Training Loss 0.187848099967098\n",
      "2022-03-26 22:44:59.105659 Epoch 250, Training Loss 0.18847595094262487\n",
      "2022-03-26 22:44:59.122102 Epoch 250, Training Loss 0.18912592495951203\n",
      "2022-03-26 22:44:59.138108 Epoch 250, Training Loss 0.18976695736503357\n",
      "2022-03-26 22:44:59.154643 Epoch 250, Training Loss 0.19028733571624512\n",
      "2022-03-26 22:44:59.172154 Epoch 250, Training Loss 0.19137231929375387\n",
      "2022-03-26 22:44:59.189088 Epoch 250, Training Loss 0.19208029983446118\n",
      "2022-03-26 22:44:59.205288 Epoch 250, Training Loss 0.1925669875748627\n",
      "2022-03-26 22:44:59.223072 Epoch 250, Training Loss 0.19317456207159536\n",
      "2022-03-26 22:44:59.239073 Epoch 250, Training Loss 0.1940065848324305\n",
      "2022-03-26 22:44:59.256114 Epoch 250, Training Loss 0.19468932646467252\n",
      "2022-03-26 22:44:59.272923 Epoch 250, Training Loss 0.19559116524351222\n",
      "2022-03-26 22:44:59.287894 Epoch 250, Training Loss 0.19642818351383404\n",
      "2022-03-26 22:44:59.305856 Epoch 250, Training Loss 0.19741088376783045\n",
      "2022-03-26 22:44:59.321896 Epoch 250, Training Loss 0.19794186503838396\n",
      "2022-03-26 22:44:59.337902 Epoch 250, Training Loss 0.19841589205100407\n",
      "2022-03-26 22:44:59.356858 Epoch 250, Training Loss 0.19928015490322162\n",
      "2022-03-26 22:44:59.371536 Epoch 250, Training Loss 0.19994596759681507\n",
      "2022-03-26 22:44:59.388404 Epoch 250, Training Loss 0.20061324357681568\n",
      "2022-03-26 22:44:59.405643 Epoch 250, Training Loss 0.20129096279363803\n",
      "2022-03-26 22:44:59.422950 Epoch 250, Training Loss 0.2020994900437572\n",
      "2022-03-26 22:44:59.438953 Epoch 250, Training Loss 0.20294615313829972\n",
      "2022-03-26 22:44:59.453950 Epoch 250, Training Loss 0.20358189505994168\n",
      "2022-03-26 22:44:59.472959 Epoch 250, Training Loss 0.2041915284322046\n",
      "2022-03-26 22:44:59.488959 Epoch 250, Training Loss 0.20490651930232182\n",
      "2022-03-26 22:44:59.505989 Epoch 250, Training Loss 0.20566372199893912\n",
      "2022-03-26 22:44:59.522099 Epoch 250, Training Loss 0.20639524858473512\n",
      "2022-03-26 22:44:59.538999 Epoch 250, Training Loss 0.20674497247352014\n",
      "2022-03-26 22:44:59.554981 Epoch 250, Training Loss 0.2073888703227958\n",
      "2022-03-26 22:44:59.571907 Epoch 250, Training Loss 0.20812574143299972\n",
      "2022-03-26 22:44:59.588925 Epoch 250, Training Loss 0.20859161167955764\n",
      "2022-03-26 22:44:59.605950 Epoch 250, Training Loss 0.20934045646349184\n",
      "2022-03-26 22:44:59.621953 Epoch 250, Training Loss 0.20992417675454902\n",
      "2022-03-26 22:44:59.638970 Epoch 250, Training Loss 0.21084236214532875\n",
      "2022-03-26 22:44:59.655969 Epoch 250, Training Loss 0.2117243710228854\n",
      "2022-03-26 22:44:59.670960 Epoch 250, Training Loss 0.2123335634960848\n",
      "2022-03-26 22:44:59.690104 Epoch 250, Training Loss 0.21299361077415974\n",
      "2022-03-26 22:44:59.705087 Epoch 250, Training Loss 0.21397002693027486\n",
      "2022-03-26 22:44:59.721906 Epoch 250, Training Loss 0.21470064991880256\n",
      "2022-03-26 22:44:59.738901 Epoch 250, Training Loss 0.21563410393112456\n",
      "2022-03-26 22:44:59.755855 Epoch 250, Training Loss 0.21640761962632085\n",
      "2022-03-26 22:44:59.775847 Epoch 250, Training Loss 0.2171276788729841\n",
      "2022-03-26 22:44:59.790850 Epoch 250, Training Loss 0.21769712213665018\n",
      "2022-03-26 22:44:59.805801 Epoch 250, Training Loss 0.2184042945847182\n",
      "2022-03-26 22:44:59.821845 Epoch 250, Training Loss 0.21905192839520057\n",
      "2022-03-26 22:44:59.839801 Epoch 250, Training Loss 0.21977590897199137\n",
      "2022-03-26 22:44:59.855804 Epoch 250, Training Loss 0.22031126150389765\n",
      "2022-03-26 22:44:59.872784 Epoch 250, Training Loss 0.2208133411529424\n",
      "2022-03-26 22:44:59.889788 Epoch 250, Training Loss 0.2218043601421444\n",
      "2022-03-26 22:44:59.906306 Epoch 250, Training Loss 0.22280094721128263\n",
      "2022-03-26 22:44:59.921790 Epoch 250, Training Loss 0.22317914973439462\n",
      "2022-03-26 22:44:59.939867 Epoch 250, Training Loss 0.22363385169402414\n",
      "2022-03-26 22:44:59.956904 Epoch 250, Training Loss 0.2242855649927388\n",
      "2022-03-26 22:44:59.971835 Epoch 250, Training Loss 0.22500008504713892\n",
      "2022-03-26 22:44:59.988836 Epoch 250, Training Loss 0.22573392661026373\n",
      "2022-03-26 22:45:00.005866 Epoch 250, Training Loss 0.22663854470338357\n",
      "2022-03-26 22:45:00.024823 Epoch 250, Training Loss 0.2273869147843412\n",
      "2022-03-26 22:45:00.039834 Epoch 250, Training Loss 0.2281471760681523\n",
      "2022-03-26 22:45:00.054729 Epoch 250, Training Loss 0.22885986278429055\n",
      "2022-03-26 22:45:00.073755 Epoch 250, Training Loss 0.2296313309608518\n",
      "2022-03-26 22:45:00.089204 Epoch 250, Training Loss 0.2302967871698882\n",
      "2022-03-26 22:45:00.105122 Epoch 250, Training Loss 0.23094401236080453\n",
      "2022-03-26 22:45:00.122975 Epoch 250, Training Loss 0.2316047485222292\n",
      "2022-03-26 22:45:00.139087 Epoch 250, Training Loss 0.23233894420706708\n",
      "2022-03-26 22:45:00.156110 Epoch 250, Training Loss 0.23293871998482044\n",
      "2022-03-26 22:45:00.173331 Epoch 250, Training Loss 0.23340940685070993\n",
      "2022-03-26 22:45:00.191670 Epoch 250, Training Loss 0.23401103387860692\n",
      "2022-03-26 22:45:00.206747 Epoch 250, Training Loss 0.235011061248572\n",
      "2022-03-26 22:45:00.222555 Epoch 250, Training Loss 0.23563102425059393\n",
      "2022-03-26 22:45:00.239381 Epoch 250, Training Loss 0.23642987733149468\n",
      "2022-03-26 22:45:00.255768 Epoch 250, Training Loss 0.2371307068392444\n",
      "2022-03-26 22:45:00.272919 Epoch 250, Training Loss 0.23795429668615542\n",
      "2022-03-26 22:45:00.288923 Epoch 250, Training Loss 0.23879416873845297\n",
      "2022-03-26 22:45:00.305947 Epoch 250, Training Loss 0.2395759378476521\n",
      "2022-03-26 22:45:00.322958 Epoch 250, Training Loss 0.2404325457713793\n",
      "2022-03-26 22:45:00.338984 Epoch 250, Training Loss 0.24095327637689498\n",
      "2022-03-26 22:45:00.355799 Epoch 250, Training Loss 0.2416492466579008\n",
      "2022-03-26 22:45:00.373850 Epoch 250, Training Loss 0.24236866816535324\n",
      "2022-03-26 22:45:00.388802 Epoch 250, Training Loss 0.24309503750118147\n",
      "2022-03-26 22:45:00.405677 Epoch 250, Training Loss 0.24369460439590543\n",
      "2022-03-26 22:45:00.423677 Epoch 250, Training Loss 0.244580374387524\n",
      "2022-03-26 22:45:00.439689 Epoch 250, Training Loss 0.24532521354115527\n",
      "2022-03-26 22:45:00.456575 Epoch 250, Training Loss 0.2459734087938543\n",
      "2022-03-26 22:45:00.471955 Epoch 250, Training Loss 0.24649040248540358\n",
      "2022-03-26 22:45:00.488884 Epoch 250, Training Loss 0.24710267443028863\n",
      "2022-03-26 22:45:00.505896 Epoch 250, Training Loss 0.2481251570498547\n",
      "2022-03-26 22:45:00.521900 Epoch 250, Training Loss 0.24905079363099755\n",
      "2022-03-26 22:45:00.541445 Epoch 250, Training Loss 0.24983776686594006\n",
      "2022-03-26 22:45:00.556306 Epoch 250, Training Loss 0.25080856593216166\n",
      "2022-03-26 22:45:00.574148 Epoch 250, Training Loss 0.2514842176605064\n",
      "2022-03-26 22:45:00.590125 Epoch 250, Training Loss 0.2520564462598937\n",
      "2022-03-26 22:45:00.605935 Epoch 250, Training Loss 0.2525206497105796\n",
      "2022-03-26 22:45:00.621887 Epoch 250, Training Loss 0.2535660247821027\n",
      "2022-03-26 22:45:00.638895 Epoch 250, Training Loss 0.25416689711001217\n",
      "2022-03-26 22:45:00.655834 Epoch 250, Training Loss 0.254857852826338\n",
      "2022-03-26 22:45:00.672654 Epoch 250, Training Loss 0.2557359688803363\n",
      "2022-03-26 22:45:00.687666 Epoch 250, Training Loss 0.25650666219651547\n",
      "2022-03-26 22:45:00.705748 Epoch 250, Training Loss 0.25685674550435733\n",
      "2022-03-26 22:45:00.721754 Epoch 250, Training Loss 0.2577276742443099\n",
      "2022-03-26 22:45:00.739752 Epoch 250, Training Loss 0.25834298343457224\n",
      "2022-03-26 22:45:00.755791 Epoch 250, Training Loss 0.2591436658902546\n",
      "2022-03-26 22:45:00.774915 Epoch 250, Training Loss 0.25993499963942085\n",
      "2022-03-26 22:45:00.788910 Epoch 250, Training Loss 0.26069870198626655\n",
      "2022-03-26 22:45:00.805086 Epoch 250, Training Loss 0.26201194963034463\n",
      "2022-03-26 22:45:00.821679 Epoch 250, Training Loss 0.26254540014907224\n",
      "2022-03-26 22:45:00.838667 Epoch 250, Training Loss 0.26314007447046395\n",
      "2022-03-26 22:45:00.854686 Epoch 250, Training Loss 0.26398238665460017\n",
      "2022-03-26 22:45:00.873188 Epoch 250, Training Loss 0.2646124106843758\n",
      "2022-03-26 22:45:00.889191 Epoch 250, Training Loss 0.26538865188198624\n",
      "2022-03-26 22:45:00.905698 Epoch 250, Training Loss 0.26612170204482116\n",
      "2022-03-26 22:45:00.921722 Epoch 250, Training Loss 0.2667884935655862\n",
      "2022-03-26 22:45:00.939801 Epoch 250, Training Loss 0.26773270316745923\n",
      "2022-03-26 22:45:00.957797 Epoch 250, Training Loss 0.26839622847564387\n",
      "2022-03-26 22:45:00.972809 Epoch 250, Training Loss 0.2693110198316062\n",
      "2022-03-26 22:45:00.988795 Epoch 250, Training Loss 0.2700071885152851\n",
      "2022-03-26 22:45:01.005739 Epoch 250, Training Loss 0.2706140006137321\n",
      "2022-03-26 22:45:01.021689 Epoch 250, Training Loss 0.2713865361860036\n",
      "2022-03-26 22:45:01.039693 Epoch 250, Training Loss 0.2723290520860716\n",
      "2022-03-26 22:45:01.056685 Epoch 250, Training Loss 0.2729690637429962\n",
      "2022-03-26 22:45:01.072977 Epoch 250, Training Loss 0.27372573350396606\n",
      "2022-03-26 22:45:01.087994 Epoch 250, Training Loss 0.274295594724243\n",
      "2022-03-26 22:45:01.105126 Epoch 250, Training Loss 0.2749897751890485\n",
      "2022-03-26 22:45:01.121089 Epoch 250, Training Loss 0.27573902447662696\n",
      "2022-03-26 22:45:01.138809 Epoch 250, Training Loss 0.2762812764748283\n",
      "2022-03-26 22:45:01.155684 Epoch 250, Training Loss 0.27688022857279426\n",
      "2022-03-26 22:45:01.172333 Epoch 250, Training Loss 0.27772506755178844\n",
      "2022-03-26 22:45:01.189246 Epoch 250, Training Loss 0.278486942353151\n",
      "2022-03-26 22:45:01.206233 Epoch 250, Training Loss 0.27934585061981854\n",
      "2022-03-26 22:45:01.221743 Epoch 250, Training Loss 0.2799457923301955\n",
      "2022-03-26 22:45:01.238753 Epoch 250, Training Loss 0.28059520421887907\n",
      "2022-03-26 22:45:01.254735 Epoch 250, Training Loss 0.2810631853616451\n",
      "2022-03-26 22:45:01.274656 Epoch 250, Training Loss 0.281758624231419\n",
      "2022-03-26 22:45:01.289688 Epoch 250, Training Loss 0.28226902654103914\n",
      "2022-03-26 22:45:01.305282 Epoch 250, Training Loss 0.28286708742761246\n",
      "2022-03-26 22:45:01.322001 Epoch 250, Training Loss 0.28337504392694635\n",
      "2022-03-26 22:45:01.339788 Epoch 250, Training Loss 0.28405703806206395\n",
      "2022-03-26 22:45:01.361835 Epoch 250, Training Loss 0.28474464204610156\n",
      "2022-03-26 22:45:01.406844 Epoch 250, Training Loss 0.2854160274690984\n",
      "2022-03-26 22:45:01.433850 Epoch 250, Training Loss 0.2860124921783462\n",
      "2022-03-26 22:45:01.459845 Epoch 250, Training Loss 0.2867557008934143\n",
      "2022-03-26 22:45:01.476671 Epoch 250, Training Loss 0.28757783835348877\n",
      "2022-03-26 22:45:01.492959 Epoch 250, Training Loss 0.2881985518633557\n",
      "2022-03-26 22:45:01.508884 Epoch 250, Training Loss 0.28904627732303745\n",
      "2022-03-26 22:45:01.537888 Epoch 250, Training Loss 0.28956553126540024\n",
      "2022-03-26 22:45:01.554889 Epoch 250, Training Loss 0.29003504383594486\n",
      "2022-03-26 22:45:01.572995 Epoch 250, Training Loss 0.29075830229712873\n",
      "2022-03-26 22:45:01.588013 Epoch 250, Training Loss 0.29138854969188077\n",
      "2022-03-26 22:45:01.605973 Epoch 250, Training Loss 0.2924089733596958\n",
      "2022-03-26 22:45:01.621683 Epoch 250, Training Loss 0.2930857223622939\n",
      "2022-03-26 22:45:01.638152 Epoch 250, Training Loss 0.2937409183405854\n",
      "2022-03-26 22:45:01.656121 Epoch 250, Training Loss 0.2944085022525104\n",
      "2022-03-26 22:45:01.672952 Epoch 250, Training Loss 0.2952238946314663\n",
      "2022-03-26 22:45:01.687975 Epoch 250, Training Loss 0.2959339931188032\n",
      "2022-03-26 22:45:01.705849 Epoch 250, Training Loss 0.29667310649171813\n",
      "2022-03-26 22:45:01.723082 Epoch 250, Training Loss 0.29729717325828875\n",
      "2022-03-26 22:45:01.739949 Epoch 250, Training Loss 0.29828251456208243\n",
      "2022-03-26 22:45:01.754804 Epoch 250, Training Loss 0.2989050074077933\n",
      "2022-03-26 22:45:01.774819 Epoch 250, Training Loss 0.29976249476680367\n",
      "2022-03-26 22:45:01.789829 Epoch 250, Training Loss 0.3005129565744449\n",
      "2022-03-26 22:45:01.805616 Epoch 250, Training Loss 0.30121468670685275\n",
      "2022-03-26 22:45:01.822491 Epoch 250, Training Loss 0.30182815009675673\n",
      "2022-03-26 22:45:01.838569 Epoch 250, Training Loss 0.3026515712670963\n",
      "2022-03-26 22:45:01.855631 Epoch 250, Training Loss 0.30331807398735106\n",
      "2022-03-26 22:45:01.871855 Epoch 250, Training Loss 0.3040972185104399\n",
      "2022-03-26 22:45:01.888868 Epoch 250, Training Loss 0.30471622417954836\n",
      "2022-03-26 22:45:01.904736 Epoch 250, Training Loss 0.305388257098015\n",
      "2022-03-26 22:45:01.922666 Epoch 250, Training Loss 0.30626434872827263\n",
      "2022-03-26 22:45:01.938740 Epoch 250, Training Loss 0.30693669986846805\n",
      "2022-03-26 22:45:01.956682 Epoch 250, Training Loss 0.30750445766217266\n",
      "2022-03-26 22:45:01.970918 Epoch 250, Training Loss 0.3081253866100555\n",
      "2022-03-26 22:45:01.989853 Epoch 250, Training Loss 0.30870186253581816\n",
      "2022-03-26 22:45:02.004733 Epoch 250, Training Loss 0.3093110109915209\n",
      "2022-03-26 22:45:02.021740 Epoch 250, Training Loss 0.3101525427511586\n",
      "2022-03-26 22:45:02.038729 Epoch 250, Training Loss 0.3108332188004423\n",
      "2022-03-26 22:45:02.054331 Epoch 250, Training Loss 0.31132596231939846\n",
      "2022-03-26 22:45:02.071883 Epoch 250, Training Loss 0.31218470213815686\n",
      "2022-03-26 22:45:02.088115 Epoch 250, Training Loss 0.31281339562000215\n",
      "2022-03-26 22:45:02.106082 Epoch 250, Training Loss 0.3137676053111206\n",
      "2022-03-26 22:45:02.121445 Epoch 250, Training Loss 0.3144959458685897\n",
      "2022-03-26 22:45:02.139589 Epoch 250, Training Loss 0.3151557432187488\n",
      "2022-03-26 22:45:02.154671 Epoch 250, Training Loss 0.3161808338082965\n",
      "2022-03-26 22:45:02.172198 Epoch 250, Training Loss 0.3174335571277477\n",
      "2022-03-26 22:45:02.189210 Epoch 250, Training Loss 0.3180632798949166\n",
      "2022-03-26 22:45:02.205602 Epoch 250, Training Loss 0.3186696106210694\n",
      "2022-03-26 22:45:02.222214 Epoch 250, Training Loss 0.3193818735497077\n",
      "2022-03-26 22:45:02.238109 Epoch 250, Training Loss 0.3202074799696198\n",
      "2022-03-26 22:45:02.254961 Epoch 250, Training Loss 0.32084283057381124\n",
      "2022-03-26 22:45:02.271965 Epoch 250, Training Loss 0.321430104772758\n",
      "2022-03-26 22:45:02.287982 Epoch 250, Training Loss 0.3220238543075064\n",
      "2022-03-26 22:45:02.305966 Epoch 250, Training Loss 0.3226808593096331\n",
      "2022-03-26 22:45:02.323089 Epoch 250, Training Loss 0.3232226008192048\n",
      "2022-03-26 22:45:02.338971 Epoch 250, Training Loss 0.323820984767526\n",
      "2022-03-26 22:45:02.357986 Epoch 250, Training Loss 0.32454634021462686\n",
      "2022-03-26 22:45:02.373226 Epoch 250, Training Loss 0.3252581766880382\n",
      "2022-03-26 22:45:02.389092 Epoch 250, Training Loss 0.32585536263635395\n",
      "2022-03-26 22:45:02.405078 Epoch 250, Training Loss 0.32681170048768565\n",
      "2022-03-26 22:45:02.421876 Epoch 250, Training Loss 0.3273894142006974\n",
      "2022-03-26 22:45:02.438949 Epoch 250, Training Loss 0.328332892769133\n",
      "2022-03-26 22:45:02.454902 Epoch 250, Training Loss 0.32898449463307705\n",
      "2022-03-26 22:45:02.471857 Epoch 250, Training Loss 0.3296218119618838\n",
      "2022-03-26 22:45:02.488393 Epoch 250, Training Loss 0.33019541131565944\n",
      "2022-03-26 22:45:02.505794 Epoch 250, Training Loss 0.3309117670711654\n",
      "2022-03-26 22:45:02.521798 Epoch 250, Training Loss 0.33167083610963943\n",
      "2022-03-26 22:45:02.538798 Epoch 250, Training Loss 0.3322519413040727\n",
      "2022-03-26 22:45:02.554726 Epoch 250, Training Loss 0.3331294057466795\n",
      "2022-03-26 22:45:02.572639 Epoch 250, Training Loss 0.3339439657185694\n",
      "2022-03-26 22:45:02.587841 Epoch 250, Training Loss 0.33482129410709566\n",
      "2022-03-26 22:45:02.605828 Epoch 250, Training Loss 0.33546449057281474\n",
      "2022-03-26 22:45:02.621514 Epoch 250, Training Loss 0.33602774935915036\n",
      "2022-03-26 22:45:02.638042 Epoch 250, Training Loss 0.33663119615801157\n",
      "2022-03-26 22:45:02.655983 Epoch 250, Training Loss 0.3373603107374343\n",
      "2022-03-26 22:45:02.672554 Epoch 250, Training Loss 0.33815285159498837\n",
      "2022-03-26 22:45:02.687550 Epoch 250, Training Loss 0.339033211817217\n",
      "2022-03-26 22:45:02.705003 Epoch 250, Training Loss 0.3395972239315662\n",
      "2022-03-26 22:45:02.721961 Epoch 250, Training Loss 0.34013754693443515\n",
      "2022-03-26 22:45:02.738981 Epoch 250, Training Loss 0.34100892903554775\n",
      "2022-03-26 22:45:02.756003 Epoch 250, Training Loss 0.34172801608624664\n",
      "2022-03-26 22:45:02.772766 Epoch 250, Training Loss 0.3423688088536567\n",
      "2022-03-26 22:45:02.789343 Epoch 250, Training Loss 0.34303248218258325\n",
      "2022-03-26 22:45:02.805700 Epoch 250, Training Loss 0.34367421025510336\n",
      "2022-03-26 22:45:02.822298 Epoch 250, Training Loss 0.34424243623490836\n",
      "2022-03-26 22:45:02.839231 Epoch 250, Training Loss 0.344835017800636\n",
      "2022-03-26 22:45:02.858813 Epoch 250, Training Loss 0.3453982273864624\n",
      "2022-03-26 22:45:02.883572 Epoch 250, Training Loss 0.34626978205140596\n",
      "2022-03-26 22:45:02.905871 Epoch 250, Training Loss 0.34695282521302745\n",
      "2022-03-26 22:45:02.921883 Epoch 250, Training Loss 0.3478700164562601\n",
      "2022-03-26 22:45:02.938894 Epoch 250, Training Loss 0.34858795082020333\n",
      "2022-03-26 22:45:02.955811 Epoch 250, Training Loss 0.3492432198942165\n",
      "2022-03-26 22:45:02.977985 Epoch 250, Training Loss 0.3500557215241215\n",
      "2022-03-26 22:45:03.000988 Epoch 250, Training Loss 0.3506152057434287\n",
      "2022-03-26 22:45:03.020971 Epoch 250, Training Loss 0.35118251817915447\n",
      "2022-03-26 22:45:03.037951 Epoch 250, Training Loss 0.35191805877953847\n",
      "2022-03-26 22:45:03.085975 Epoch 250, Training Loss 0.3525141606016842\n",
      "2022-03-26 22:45:03.104984 Epoch 250, Training Loss 0.3532357206353751\n",
      "2022-03-26 22:45:03.121960 Epoch 250, Training Loss 0.3538691465507078\n",
      "2022-03-26 22:45:03.139962 Epoch 250, Training Loss 0.3546558210002187\n",
      "2022-03-26 22:45:03.154986 Epoch 250, Training Loss 0.3554252261853279\n",
      "2022-03-26 22:45:03.172922 Epoch 250, Training Loss 0.35626879967082187\n",
      "2022-03-26 22:45:03.188942 Epoch 250, Training Loss 0.3569104858981374\n",
      "2022-03-26 22:45:03.204965 Epoch 250, Training Loss 0.35781625698289604\n",
      "2022-03-26 22:45:03.222081 Epoch 250, Training Loss 0.3585808671953733\n",
      "2022-03-26 22:45:03.239147 Epoch 250, Training Loss 0.35911867529382485\n",
      "2022-03-26 22:45:03.254647 Epoch 250, Training Loss 0.35976281682091293\n",
      "2022-03-26 22:45:03.272481 Epoch 250, Training Loss 0.3604607427745219\n",
      "2022-03-26 22:45:03.289143 Epoch 250, Training Loss 0.36120111142735345\n",
      "2022-03-26 22:45:03.307167 Epoch 250, Training Loss 0.36200956154205\n",
      "2022-03-26 22:45:03.321787 Epoch 250, Training Loss 0.3628083650032273\n",
      "2022-03-26 22:45:03.339126 Epoch 250, Training Loss 0.36346915718692036\n",
      "2022-03-26 22:45:03.357791 Epoch 250, Training Loss 0.36453228483876915\n",
      "2022-03-26 22:45:03.374677 Epoch 250, Training Loss 0.3653429067119613\n",
      "2022-03-26 22:45:03.388687 Epoch 250, Training Loss 0.36558624538009427\n",
      "2022-03-26 22:45:03.404742 Epoch 250, Training Loss 0.36657464115516\n",
      "2022-03-26 22:45:03.421782 Epoch 250, Training Loss 0.367037034133816\n",
      "2022-03-26 22:45:03.440831 Epoch 250, Training Loss 0.36764024213299423\n",
      "2022-03-26 22:45:03.455825 Epoch 250, Training Loss 0.36848796812622137\n",
      "2022-03-26 22:45:03.471906 Epoch 250, Training Loss 0.3691670180053052\n",
      "2022-03-26 22:45:03.487908 Epoch 250, Training Loss 0.3697670247320019\n",
      "2022-03-26 22:45:03.504867 Epoch 250, Training Loss 0.3705793702617631\n",
      "2022-03-26 22:45:03.521889 Epoch 250, Training Loss 0.3715304195347344\n",
      "2022-03-26 22:45:03.539790 Epoch 250, Training Loss 0.37221902109625393\n",
      "2022-03-26 22:45:03.555806 Epoch 250, Training Loss 0.372773585645744\n",
      "2022-03-26 22:45:03.572289 Epoch 250, Training Loss 0.37361860580151646\n",
      "2022-03-26 22:45:03.588852 Epoch 250, Training Loss 0.3744622231901759\n",
      "2022-03-26 22:45:03.604871 Epoch 250, Training Loss 0.37485521216221784\n",
      "2022-03-26 22:45:03.621770 Epoch 250, Training Loss 0.3755799041074865\n",
      "2022-03-26 22:45:03.638681 Epoch 250, Training Loss 0.37602785500266667\n",
      "2022-03-26 22:45:03.655704 Epoch 250, Training Loss 0.37681189251830205\n",
      "2022-03-26 22:45:03.671726 Epoch 250, Training Loss 0.37765644483096766\n",
      "2022-03-26 22:45:03.687789 Epoch 250, Training Loss 0.3784433240094758\n",
      "2022-03-26 22:45:03.705829 Epoch 250, Training Loss 0.37910723232704663\n",
      "2022-03-26 22:45:03.724832 Epoch 250, Training Loss 0.37968573805011446\n",
      "2022-03-26 22:45:03.738851 Epoch 250, Training Loss 0.38026275403816684\n",
      "2022-03-26 22:45:03.754884 Epoch 250, Training Loss 0.38076016912832283\n",
      "2022-03-26 22:45:03.771888 Epoch 250, Training Loss 0.38152503201266386\n",
      "2022-03-26 22:45:03.788949 Epoch 250, Training Loss 0.3822396657884578\n",
      "2022-03-26 22:45:03.805997 Epoch 250, Training Loss 0.38298969603408023\n",
      "2022-03-26 22:45:03.822733 Epoch 250, Training Loss 0.38354221123563664\n",
      "2022-03-26 22:45:03.840735 Epoch 250, Training Loss 0.3840339054613162\n",
      "2022-03-26 22:45:03.855688 Epoch 250, Training Loss 0.38467946271304887\n",
      "2022-03-26 22:45:03.872280 Epoch 250, Training Loss 0.38534020786852485\n",
      "2022-03-26 22:45:03.887401 Epoch 250, Training Loss 0.3862295326445719\n",
      "2022-03-26 22:45:03.905645 Epoch 250, Training Loss 0.38708271421587376\n",
      "2022-03-26 22:45:03.921747 Epoch 250, Training Loss 0.38804335640671916\n",
      "2022-03-26 22:45:03.938732 Epoch 250, Training Loss 0.38864836230149963\n",
      "2022-03-26 22:45:03.955252 Epoch 250, Training Loss 0.38920066915357204\n",
      "2022-03-26 22:45:03.972214 Epoch 250, Training Loss 0.38979702574365277\n",
      "2022-03-26 22:45:03.988730 Epoch 250, Training Loss 0.39057019402456405\n",
      "2022-03-26 22:45:04.006722 Epoch 250, Training Loss 0.39127143062746433\n",
      "2022-03-26 22:45:04.022053 Epoch 250, Training Loss 0.39180049056287314\n",
      "2022-03-26 22:45:04.039052 Epoch 250, Training Loss 0.3924128285911687\n",
      "2022-03-26 22:45:04.057955 Epoch 250, Training Loss 0.3931340396861591\n",
      "2022-03-26 22:45:04.071963 Epoch 250, Training Loss 0.39393796091494354\n",
      "2022-03-26 22:45:04.089962 Epoch 250, Training Loss 0.3945487893908225\n",
      "2022-03-26 22:45:04.105094 Epoch 250, Training Loss 0.3950713157958692\n",
      "2022-03-26 22:45:04.122564 Epoch 250, Training Loss 0.39584153105535774\n",
      "2022-03-26 22:45:04.139503 Epoch 250, Training Loss 0.39663830460489863\n",
      "2022-03-26 22:45:04.155591 Epoch 250, Training Loss 0.3971091088888895\n",
      "2022-03-26 22:45:04.173610 Epoch 250, Training Loss 0.39760303584968343\n",
      "2022-03-26 22:45:04.189979 Epoch 250, Training Loss 0.3984165019010339\n",
      "2022-03-26 22:45:04.205400 Epoch 250, Training Loss 0.3990005729982005\n",
      "2022-03-26 22:45:04.221960 Epoch 250, Training Loss 0.39979148936241177\n",
      "2022-03-26 22:45:04.240965 Epoch 250, Training Loss 0.4002501329268946\n",
      "2022-03-26 22:45:04.254852 Epoch 250, Training Loss 0.4009105709888746\n",
      "2022-03-26 22:45:04.273730 Epoch 250, Training Loss 0.4015892796275561\n",
      "2022-03-26 22:45:04.288739 Epoch 250, Training Loss 0.40225926716156934\n",
      "2022-03-26 22:45:04.307516 Epoch 250, Training Loss 0.40299753600831534\n",
      "2022-03-26 22:45:04.322819 Epoch 250, Training Loss 0.4039663199497306\n",
      "2022-03-26 22:45:04.340839 Epoch 250, Training Loss 0.4046860996948179\n",
      "2022-03-26 22:45:04.355842 Epoch 250, Training Loss 0.4053600177054515\n",
      "2022-03-26 22:45:04.373384 Epoch 250, Training Loss 0.40614025592041747\n",
      "2022-03-26 22:45:04.389054 Epoch 250, Training Loss 0.40703423222160096\n",
      "2022-03-26 22:45:04.406245 Epoch 250, Training Loss 0.4076313646629338\n",
      "2022-03-26 22:45:04.422143 Epoch 250, Training Loss 0.40834557366035784\n",
      "2022-03-26 22:45:04.438362 Epoch 250, Training Loss 0.40926574300164764\n",
      "2022-03-26 22:45:04.455538 Epoch 250, Training Loss 0.4099047048317502\n",
      "2022-03-26 22:45:04.473101 Epoch 250, Training Loss 0.41079734803160745\n",
      "2022-03-26 22:45:04.489117 Epoch 250, Training Loss 0.411387451278889\n",
      "2022-03-26 22:45:04.506965 Epoch 250, Training Loss 0.41212071966179803\n",
      "2022-03-26 22:45:04.521984 Epoch 250, Training Loss 0.41290347079944123\n",
      "2022-03-26 22:45:04.540982 Epoch 250, Training Loss 0.4136403955690696\n",
      "2022-03-26 22:45:04.555979 Epoch 250, Training Loss 0.4142271384711156\n",
      "2022-03-26 22:45:04.574003 Epoch 250, Training Loss 0.41488677781561145\n",
      "2022-03-26 22:45:04.589968 Epoch 250, Training Loss 0.4156056138713037\n",
      "2022-03-26 22:45:04.604987 Epoch 250, Training Loss 0.41654500670140354\n",
      "2022-03-26 22:45:04.622971 Epoch 250, Training Loss 0.41717063603193866\n",
      "2022-03-26 22:45:04.639908 Epoch 250, Training Loss 0.4178775664790512\n",
      "2022-03-26 22:45:04.655342 Epoch 250, Training Loss 0.4184954352009937\n",
      "2022-03-26 22:45:04.671986 Epoch 250, Training Loss 0.41940268161504163\n",
      "2022-03-26 22:45:04.689008 Epoch 250, Training Loss 0.4200589630533667\n",
      "2022-03-26 22:45:04.706157 Epoch 250, Training Loss 0.42053777455826247\n",
      "2022-03-26 22:45:04.721270 Epoch 250, Training Loss 0.4214517091165113\n",
      "2022-03-26 22:45:04.738911 Epoch 250, Training Loss 0.42215899246580463\n",
      "2022-03-26 22:45:04.755920 Epoch 250, Training Loss 0.4227981898180969\n",
      "2022-03-26 22:45:04.771906 Epoch 250, Training Loss 0.4235280432054759\n",
      "2022-03-26 22:45:04.789851 Epoch 250, Training Loss 0.4241631026844234\n",
      "2022-03-26 22:45:04.806858 Epoch 250, Training Loss 0.4250672001134404\n",
      "2022-03-26 22:45:04.823882 Epoch 250, Training Loss 0.42590458050865654\n",
      "2022-03-26 22:45:04.841880 Epoch 250, Training Loss 0.4266987130084001\n",
      "2022-03-26 22:45:04.856892 Epoch 250, Training Loss 0.42746877407326417\n",
      "2022-03-26 22:45:04.872869 Epoch 250, Training Loss 0.4282997934638387\n",
      "2022-03-26 22:45:04.888873 Epoch 250, Training Loss 0.42932292952409484\n",
      "2022-03-26 22:45:04.905890 Epoch 250, Training Loss 0.4299260151508214\n",
      "2022-03-26 22:45:04.922853 Epoch 250, Training Loss 0.43048119705046534\n",
      "2022-03-26 22:45:04.939864 Epoch 250, Training Loss 0.43086642247941487\n",
      "2022-03-26 22:45:04.954854 Epoch 250, Training Loss 0.4315846653850487\n",
      "2022-03-26 22:45:04.971871 Epoch 250, Training Loss 0.4322556658931401\n",
      "2022-03-26 22:45:04.988891 Epoch 250, Training Loss 0.43289953919932667\n",
      "2022-03-26 22:45:05.004888 Epoch 250, Training Loss 0.4336073897073946\n",
      "2022-03-26 22:45:05.023265 Epoch 250, Training Loss 0.43439089344895404\n",
      "2022-03-26 22:45:05.040240 Epoch 250, Training Loss 0.43529333894514977\n",
      "2022-03-26 22:45:05.055088 Epoch 250, Training Loss 0.4360263958916335\n",
      "2022-03-26 22:45:05.071977 Epoch 250, Training Loss 0.43666330178070556\n",
      "2022-03-26 22:45:05.089294 Epoch 250, Training Loss 0.43713716274637093\n",
      "2022-03-26 22:45:05.106260 Epoch 250, Training Loss 0.43781172802381196\n",
      "2022-03-26 22:45:05.122867 Epoch 250, Training Loss 0.4385540755203618\n",
      "2022-03-26 22:45:05.139845 Epoch 250, Training Loss 0.4393470647847256\n",
      "2022-03-26 22:45:05.155852 Epoch 250, Training Loss 0.440112351304125\n",
      "2022-03-26 22:45:05.171077 Epoch 250, Training Loss 0.44050780228336756\n",
      "2022-03-26 22:45:05.188768 Epoch 250, Training Loss 0.4411838614879667\n",
      "2022-03-26 22:45:05.205762 Epoch 250, Training Loss 0.441838565156283\n",
      "2022-03-26 22:45:05.222766 Epoch 250, Training Loss 0.4426565026993032\n",
      "2022-03-26 22:45:05.240438 Epoch 250, Training Loss 0.44349841037979515\n",
      "2022-03-26 22:45:05.254399 Epoch 250, Training Loss 0.44433603064178506\n",
      "2022-03-26 22:45:05.274179 Epoch 250, Training Loss 0.44500129416470635\n",
      "2022-03-26 22:45:05.288732 Epoch 250, Training Loss 0.44589379269753576\n",
      "2022-03-26 22:45:05.306921 Epoch 250, Training Loss 0.4466611693429825\n",
      "2022-03-26 22:45:05.321965 Epoch 250, Training Loss 0.4471998079429807\n",
      "2022-03-26 22:45:05.338997 Epoch 250, Training Loss 0.44788483703685233\n",
      "2022-03-26 22:45:05.357137 Epoch 250, Training Loss 0.44836914668912475\n",
      "2022-03-26 22:45:05.372792 Epoch 250, Training Loss 0.448970167228328\n",
      "2022-03-26 22:45:05.388789 Epoch 250, Training Loss 0.44982720114995756\n",
      "2022-03-26 22:45:05.405373 Epoch 250, Training Loss 0.45041121149916785\n",
      "2022-03-26 22:45:05.421911 Epoch 250, Training Loss 0.4510602317655178\n",
      "2022-03-26 22:45:05.439970 Epoch 250, Training Loss 0.4515228072167053\n",
      "2022-03-26 22:45:05.455004 Epoch 250, Training Loss 0.45251061258566044\n",
      "2022-03-26 22:45:05.472957 Epoch 250, Training Loss 0.45326902521083423\n",
      "2022-03-26 22:45:05.488955 Epoch 250, Training Loss 0.45400224610820145\n",
      "2022-03-26 22:45:05.505862 Epoch 250, Training Loss 0.4545674287830777\n",
      "2022-03-26 22:45:05.521796 Epoch 250, Training Loss 0.4552652958180288\n",
      "2022-03-26 22:45:05.540857 Epoch 250, Training Loss 0.4562898942500429\n",
      "2022-03-26 22:45:05.555878 Epoch 250, Training Loss 0.45686513280777064\n",
      "2022-03-26 22:45:05.571848 Epoch 250, Training Loss 0.45739179216992215\n",
      "2022-03-26 22:45:05.589859 Epoch 250, Training Loss 0.45810287840226116\n",
      "2022-03-26 22:45:05.605995 Epoch 250, Training Loss 0.45892624896200723\n",
      "2022-03-26 22:45:05.622731 Epoch 250, Training Loss 0.4598517206013965\n",
      "2022-03-26 22:45:05.641916 Epoch 250, Training Loss 0.46075294290662117\n",
      "2022-03-26 22:45:05.655939 Epoch 250, Training Loss 0.4612576100222595\n",
      "2022-03-26 22:45:05.671942 Epoch 250, Training Loss 0.46202267816914316\n",
      "2022-03-26 22:45:05.688919 Epoch 250, Training Loss 0.46273161756717945\n",
      "2022-03-26 22:45:05.705939 Epoch 250, Training Loss 0.4636407858118072\n",
      "2022-03-26 22:45:05.721960 Epoch 250, Training Loss 0.464296652990229\n",
      "2022-03-26 22:45:05.738986 Epoch 250, Training Loss 0.46477809910426665\n",
      "2022-03-26 22:45:05.755970 Epoch 250, Training Loss 0.46565547734117874\n",
      "2022-03-26 22:45:05.773901 Epoch 250, Training Loss 0.4663321303818232\n",
      "2022-03-26 22:45:05.796906 Epoch 250, Training Loss 0.4671504756297602\n",
      "2022-03-26 22:45:05.821912 Epoch 250, Training Loss 0.46779215050018047\n",
      "2022-03-26 22:45:05.847921 Epoch 250, Training Loss 0.4682735666213438\n",
      "2022-03-26 22:45:05.896999 Epoch 250, Training Loss 0.4691432056295902\n",
      "2022-03-26 22:45:05.929914 Epoch 250, Training Loss 0.46982403053804433\n",
      "2022-03-26 22:45:05.962920 Epoch 250, Training Loss 0.4705670733204888\n",
      "2022-03-26 22:45:05.978925 Epoch 250, Training Loss 0.4714229653024917\n",
      "2022-03-26 22:45:05.995932 Epoch 250, Training Loss 0.4721625181643859\n",
      "2022-03-26 22:45:06.011257 Epoch 250, Training Loss 0.4730820465651924\n",
      "2022-03-26 22:45:06.030689 Epoch 250, Training Loss 0.47385272044507437\n",
      "2022-03-26 22:45:06.055316 Epoch 250, Training Loss 0.4744397411337289\n",
      "2022-03-26 22:45:06.073889 Epoch 250, Training Loss 0.4754521971392205\n",
      "2022-03-26 22:45:06.091956 Epoch 250, Training Loss 0.4760796449068562\n",
      "2022-03-26 22:45:06.124962 Epoch 250, Training Loss 0.4767294338597056\n",
      "2022-03-26 22:45:06.142968 Epoch 250, Training Loss 0.4774338494786216\n",
      "2022-03-26 22:45:06.158975 Epoch 250, Training Loss 0.47833412717980195\n",
      "2022-03-26 22:45:06.182611 Epoch 250, Training Loss 0.47927435767620113\n",
      "2022-03-26 22:45:06.207801 Epoch 250, Training Loss 0.47984581362560885\n",
      "2022-03-26 22:45:06.225288 Epoch 250, Training Loss 0.48040417587513207\n",
      "2022-03-26 22:45:06.242780 Epoch 250, Training Loss 0.4812824856061155\n",
      "2022-03-26 22:45:06.259785 Epoch 250, Training Loss 0.482083219129716\n",
      "2022-03-26 22:45:06.274825 Epoch 250, Training Loss 0.4826169170991844\n",
      "2022-03-26 22:45:06.290828 Epoch 250, Training Loss 0.48318337525248223\n",
      "2022-03-26 22:45:06.307684 Epoch 250, Training Loss 0.48414816392961973\n",
      "2022-03-26 22:45:06.322617 Epoch 250, Training Loss 0.48493055218016096\n",
      "2022-03-26 22:45:06.340943 Epoch 250, Training Loss 0.4860745585330612\n",
      "2022-03-26 22:45:06.355947 Epoch 250, Training Loss 0.487196113142516\n",
      "2022-03-26 22:45:06.373939 Epoch 250, Training Loss 0.48813515268933133\n",
      "2022-03-26 22:45:06.390156 Epoch 250, Training Loss 0.4889298832934836\n",
      "2022-03-26 22:45:06.406965 Epoch 250, Training Loss 0.4896523402932355\n",
      "2022-03-26 22:45:06.421970 Epoch 250, Training Loss 0.49047145849603524\n",
      "2022-03-26 22:45:06.442420 Epoch 250, Training Loss 0.49094779396910804\n",
      "2022-03-26 22:45:06.458314 Epoch 250, Training Loss 0.49159310350332724\n",
      "2022-03-26 22:45:06.472755 Epoch 250, Training Loss 0.4924577020318307\n",
      "2022-03-26 22:45:06.487735 Epoch 250, Training Loss 0.4932423419964588\n",
      "2022-03-26 22:45:06.505648 Epoch 250, Training Loss 0.4943180357098884\n",
      "2022-03-26 22:45:06.521584 Epoch 250, Training Loss 0.4951006482019449\n",
      "2022-03-26 22:45:06.539561 Epoch 250, Training Loss 0.49587679168452387\n",
      "2022-03-26 22:45:06.555901 Epoch 250, Training Loss 0.49681561956625153\n",
      "2022-03-26 22:45:06.572034 Epoch 250, Training Loss 0.4976442492069186\n",
      "2022-03-26 22:45:06.589064 Epoch 250, Training Loss 0.4983492262680512\n",
      "2022-03-26 22:45:06.607969 Epoch 250, Training Loss 0.49922014692860184\n",
      "2022-03-26 22:45:06.623878 Epoch 250, Training Loss 0.4999377795345033\n",
      "2022-03-26 22:45:06.640882 Epoch 250, Training Loss 0.5003575780964873\n",
      "2022-03-26 22:45:06.655789 Epoch 250, Training Loss 0.5010469345485463\n",
      "2022-03-26 22:45:06.672786 Epoch 250, Training Loss 0.5016025446183846\n",
      "2022-03-26 22:45:06.688781 Epoch 250, Training Loss 0.5024437055639599\n",
      "2022-03-26 22:45:06.706737 Epoch 250, Training Loss 0.5031983705661486\n",
      "2022-03-26 22:45:06.721499 Epoch 250, Training Loss 0.5039687788928561\n",
      "2022-03-26 22:45:06.739292 Epoch 250, Training Loss 0.5046642861326637\n",
      "2022-03-26 22:45:06.756077 Epoch 250, Training Loss 0.505352303233293\n",
      "2022-03-26 22:45:06.773086 Epoch 250, Training Loss 0.5060841917915417\n",
      "2022-03-26 22:45:06.789536 Epoch 250, Training Loss 0.5069970524753146\n",
      "2022-03-26 22:45:06.806195 Epoch 250, Training Loss 0.5077518976634116\n",
      "2022-03-26 22:45:06.824084 Epoch 250, Training Loss 0.5083867532899008\n",
      "2022-03-26 22:45:06.842848 Epoch 250, Training Loss 0.5090948775829867\n",
      "2022-03-26 22:45:06.856877 Epoch 250, Training Loss 0.5097310775914765\n",
      "2022-03-26 22:45:06.871907 Epoch 250, Training Loss 0.5103805878049578\n",
      "2022-03-26 22:45:06.889464 Epoch 250, Training Loss 0.5115968903236072\n",
      "2022-03-26 22:45:06.905975 Epoch 250, Training Loss 0.5122783986275153\n",
      "2022-03-26 22:45:06.922995 Epoch 250, Training Loss 0.5129587685741732\n",
      "2022-03-26 22:45:06.938980 Epoch 250, Training Loss 0.5138941969331878\n",
      "2022-03-26 22:45:06.955924 Epoch 250, Training Loss 0.5146568081034418\n",
      "2022-03-26 22:45:06.972733 Epoch 250, Training Loss 0.5154503953959936\n",
      "2022-03-26 22:45:06.987743 Epoch 250, Training Loss 0.5162415068853846\n",
      "2022-03-26 22:45:07.004743 Epoch 250, Training Loss 0.5169022710960539\n",
      "2022-03-26 22:45:07.021744 Epoch 250, Training Loss 0.5175241645797134\n",
      "2022-03-26 22:45:07.040698 Epoch 250, Training Loss 0.5182431040669951\n",
      "2022-03-26 22:45:07.054649 Epoch 250, Training Loss 0.5192548222554004\n",
      "2022-03-26 22:45:07.072178 Epoch 250, Training Loss 0.5199138474891253\n",
      "2022-03-26 22:45:07.089344 Epoch 250, Training Loss 0.5209619135350523\n",
      "2022-03-26 22:45:07.106684 Epoch 250, Training Loss 0.5221650766594635\n",
      "2022-03-26 22:45:07.120750 Epoch 250, Training Loss 0.5231568645638274\n",
      "2022-03-26 22:45:07.138676 Epoch 250, Training Loss 0.5237045027410893\n",
      "2022-03-26 22:45:07.154747 Epoch 250, Training Loss 0.5245604470867635\n",
      "2022-03-26 22:45:07.172133 Epoch 250, Training Loss 0.5252445484213817\n",
      "2022-03-26 22:45:07.190161 Epoch 250, Training Loss 0.5259639421082518\n",
      "2022-03-26 22:45:07.206213 Epoch 250, Training Loss 0.5265148508807888\n",
      "2022-03-26 22:45:07.221653 Epoch 250, Training Loss 0.5270750816063503\n",
      "2022-03-26 22:45:07.239628 Epoch 250, Training Loss 0.5277620896201609\n",
      "2022-03-26 22:45:07.255858 Epoch 250, Training Loss 0.5286478288948079\n",
      "2022-03-26 22:45:07.272789 Epoch 250, Training Loss 0.5294429117913746\n",
      "2022-03-26 22:45:07.289800 Epoch 250, Training Loss 0.529940613128645\n",
      "2022-03-26 22:45:07.305815 Epoch 250, Training Loss 0.5306331971493523\n",
      "2022-03-26 22:45:07.322822 Epoch 250, Training Loss 0.5313311950553714\n",
      "2022-03-26 22:45:07.338825 Epoch 250, Training Loss 0.5319365097205048\n",
      "2022-03-26 22:45:07.355746 Epoch 250, Training Loss 0.5325324010208744\n",
      "2022-03-26 22:45:07.372724 Epoch 250, Training Loss 0.5332254291038074\n",
      "2022-03-26 22:45:07.388167 Epoch 250, Training Loss 0.534227612149685\n",
      "2022-03-26 22:45:07.405180 Epoch 250, Training Loss 0.5349077089210911\n",
      "2022-03-26 22:45:07.421992 Epoch 250, Training Loss 0.5355413482545892\n",
      "2022-03-26 22:45:07.438888 Epoch 250, Training Loss 0.5363951640589463\n",
      "2022-03-26 22:45:07.454980 Epoch 250, Training Loss 0.537203104256669\n",
      "2022-03-26 22:45:07.472160 Epoch 250, Training Loss 0.53793482253771\n",
      "2022-03-26 22:45:07.487839 Epoch 250, Training Loss 0.5386681234668893\n",
      "2022-03-26 22:45:07.506213 Epoch 250, Training Loss 0.5393061681705362\n",
      "2022-03-26 22:45:07.521835 Epoch 250, Training Loss 0.5399320332519234\n",
      "2022-03-26 22:45:07.539833 Epoch 250, Training Loss 0.540621972335574\n",
      "2022-03-26 22:45:07.555959 Epoch 250, Training Loss 0.5417650291300795\n",
      "2022-03-26 22:45:07.572900 Epoch 250, Training Loss 0.5426255935217108\n",
      "2022-03-26 22:45:07.588909 Epoch 250, Training Loss 0.5436719175418625\n",
      "2022-03-26 22:45:07.605930 Epoch 250, Training Loss 0.5444267385679743\n",
      "2022-03-26 22:45:07.622061 Epoch 250, Training Loss 0.5453012409951071\n",
      "2022-03-26 22:45:07.640073 Epoch 250, Training Loss 0.5459690923657259\n",
      "2022-03-26 22:45:07.654923 Epoch 250, Training Loss 0.5466961586643058\n",
      "2022-03-26 22:45:07.672115 Epoch 250, Training Loss 0.5473441552856694\n",
      "2022-03-26 22:45:07.690062 Epoch 250, Training Loss 0.547823015762412\n",
      "2022-03-26 22:45:07.704979 Epoch 250, Training Loss 0.5489100357303229\n",
      "2022-03-26 22:45:07.722961 Epoch 250, Training Loss 0.5498018132146362\n",
      "2022-03-26 22:45:07.738965 Epoch 250, Training Loss 0.5506494854722181\n",
      "2022-03-26 22:45:07.755968 Epoch 250, Training Loss 0.5518198466057058\n",
      "2022-03-26 22:45:07.771955 Epoch 250, Training Loss 0.5525506064105217\n",
      "2022-03-26 22:45:07.788951 Epoch 250, Training Loss 0.5536286731815094\n",
      "2022-03-26 22:45:07.805829 Epoch 250, Training Loss 0.5546581075929314\n",
      "2022-03-26 22:45:07.821841 Epoch 250, Training Loss 0.5553183942042348\n",
      "2022-03-26 22:45:07.838848 Epoch 250, Training Loss 0.5563070001961935\n",
      "2022-03-26 22:45:07.855787 Epoch 250, Training Loss 0.5568559797066251\n",
      "2022-03-26 22:45:07.873295 Epoch 250, Training Loss 0.5576259886364803\n",
      "2022-03-26 22:45:07.888787 Epoch 250, Training Loss 0.5582777741924881\n",
      "2022-03-26 22:45:07.905960 Epoch 250, Training Loss 0.5591571742616346\n",
      "2022-03-26 22:45:07.920900 Epoch 250, Training Loss 0.5601070995068611\n",
      "2022-03-26 22:45:07.938648 Epoch 250, Training Loss 0.5607744651224912\n",
      "2022-03-26 22:45:07.964160 Epoch 250, Training Loss 0.5618334911820834\n",
      "2022-03-26 22:45:07.980164 Epoch 250, Training Loss 0.5626770834178876\n",
      "2022-03-26 22:45:07.995167 Epoch 250, Training Loss 0.5632141221819631\n",
      "2022-03-26 22:45:08.009170 Epoch 250, Training Loss 0.5638148691267004\n",
      "2022-03-26 22:45:08.024173 Epoch 250, Training Loss 0.5645779147553627\n",
      "2022-03-26 22:45:08.040177 Epoch 250, Training Loss 0.5652207241719945\n",
      "2022-03-26 22:45:08.050180 Epoch 250, Training Loss 0.5657473407742922\n",
      "2022-03-26 22:55:56.354796 Epoch 300, Training Loss 0.0006467719059770979\n",
      "2022-03-26 22:55:56.378802 Epoch 300, Training Loss 0.001194445868892133\n",
      "2022-03-26 22:55:56.397684 Epoch 300, Training Loss 0.0016844963173732123\n",
      "2022-03-26 22:55:56.424693 Epoch 300, Training Loss 0.0023605951567744967\n",
      "2022-03-26 22:55:56.518713 Epoch 300, Training Loss 0.003127735212940694\n",
      "2022-03-26 22:55:56.550724 Epoch 300, Training Loss 0.003904752414244825\n",
      "2022-03-26 22:55:56.566519 Epoch 300, Training Loss 0.004597919600089188\n",
      "2022-03-26 22:55:56.584557 Epoch 300, Training Loss 0.00521906505307883\n",
      "2022-03-26 22:55:56.605563 Epoch 300, Training Loss 0.0062055501257976915\n",
      "2022-03-26 22:55:56.633455 Epoch 300, Training Loss 0.006818901928489471\n",
      "2022-03-26 22:55:56.649466 Epoch 300, Training Loss 0.00765944937305987\n",
      "2022-03-26 22:55:56.665664 Epoch 300, Training Loss 0.008104687227922328\n",
      "2022-03-26 22:55:56.682702 Epoch 300, Training Loss 0.00872617582683368\n",
      "2022-03-26 22:55:56.698565 Epoch 300, Training Loss 0.009383528167024598\n",
      "2022-03-26 22:55:56.716161 Epoch 300, Training Loss 0.010119980977624274\n",
      "2022-03-26 22:55:56.732486 Epoch 300, Training Loss 0.010640054331411181\n",
      "2022-03-26 22:55:56.749348 Epoch 300, Training Loss 0.0113403654616812\n",
      "2022-03-26 22:55:56.765236 Epoch 300, Training Loss 0.011982300930925647\n",
      "2022-03-26 22:55:56.783176 Epoch 300, Training Loss 0.01258765759370516\n",
      "2022-03-26 22:55:56.799179 Epoch 300, Training Loss 0.013350630355308124\n",
      "2022-03-26 22:55:56.815242 Epoch 300, Training Loss 0.014424794012933131\n",
      "2022-03-26 22:55:56.840030 Epoch 300, Training Loss 0.01513441680642345\n",
      "2022-03-26 22:55:56.863038 Epoch 300, Training Loss 0.015846576455913846\n",
      "2022-03-26 22:55:56.883944 Epoch 300, Training Loss 0.01652872173682503\n",
      "2022-03-26 22:55:56.897946 Epoch 300, Training Loss 0.017218658533852422\n",
      "2022-03-26 22:55:56.914950 Epoch 300, Training Loss 0.01786697429159413\n",
      "2022-03-26 22:55:56.931970 Epoch 300, Training Loss 0.018801055067335554\n",
      "2022-03-26 22:55:56.958109 Epoch 300, Training Loss 0.019664536656626045\n",
      "2022-03-26 22:55:56.987605 Epoch 300, Training Loss 0.020317781383119276\n",
      "2022-03-26 22:55:57.003637 Epoch 300, Training Loss 0.020959344437665037\n",
      "2022-03-26 22:55:57.019639 Epoch 300, Training Loss 0.021607924147944928\n",
      "2022-03-26 22:55:57.043720 Epoch 300, Training Loss 0.022458013518691977\n",
      "2022-03-26 22:55:57.073858 Epoch 300, Training Loss 0.023464149907421884\n",
      "2022-03-26 22:55:57.089815 Epoch 300, Training Loss 0.0242051835865011\n",
      "2022-03-26 22:55:57.104954 Epoch 300, Training Loss 0.02555901787774947\n",
      "2022-03-26 22:55:57.126934 Epoch 300, Training Loss 0.025987275878486732\n",
      "2022-03-26 22:55:57.148958 Epoch 300, Training Loss 0.02652743729331609\n",
      "2022-03-26 22:55:57.163957 Epoch 300, Training Loss 0.027062622025189803\n",
      "2022-03-26 22:55:57.181717 Epoch 300, Training Loss 0.027691676374286642\n",
      "2022-03-26 22:55:57.200709 Epoch 300, Training Loss 0.028632252379451568\n",
      "2022-03-26 22:55:57.217717 Epoch 300, Training Loss 0.029342891805617097\n",
      "2022-03-26 22:55:57.234405 Epoch 300, Training Loss 0.029867621905663434\n",
      "2022-03-26 22:55:57.248807 Epoch 300, Training Loss 0.030779086758413583\n",
      "2022-03-26 22:55:57.264756 Epoch 300, Training Loss 0.03169739890433943\n",
      "2022-03-26 22:55:57.282673 Epoch 300, Training Loss 0.03249264826707523\n",
      "2022-03-26 22:55:57.299686 Epoch 300, Training Loss 0.03305827607126797\n",
      "2022-03-26 22:55:57.317716 Epoch 300, Training Loss 0.0336146967109207\n",
      "2022-03-26 22:55:57.332043 Epoch 300, Training Loss 0.03421868810720761\n",
      "2022-03-26 22:55:57.349059 Epoch 300, Training Loss 0.03527246949160495\n",
      "2022-03-26 22:55:57.366169 Epoch 300, Training Loss 0.03582451364878193\n",
      "2022-03-26 22:55:57.383856 Epoch 300, Training Loss 0.036445693820333844\n",
      "2022-03-26 22:55:57.399856 Epoch 300, Training Loss 0.03719450895438719\n",
      "2022-03-26 22:55:57.417863 Epoch 300, Training Loss 0.03823240455764029\n",
      "2022-03-26 22:55:57.432849 Epoch 300, Training Loss 0.03894736448212353\n",
      "2022-03-26 22:55:57.448851 Epoch 300, Training Loss 0.03958539272208348\n",
      "2022-03-26 22:55:57.465809 Epoch 300, Training Loss 0.04043231153732066\n",
      "2022-03-26 22:55:57.482837 Epoch 300, Training Loss 0.04095967971455411\n",
      "2022-03-26 22:55:57.498862 Epoch 300, Training Loss 0.041562434962338496\n",
      "2022-03-26 22:55:57.517082 Epoch 300, Training Loss 0.042152375363937726\n",
      "2022-03-26 22:55:57.534096 Epoch 300, Training Loss 0.04311316427977189\n",
      "2022-03-26 22:55:57.550105 Epoch 300, Training Loss 0.043866368358397426\n",
      "2022-03-26 22:55:57.567026 Epoch 300, Training Loss 0.04455881975495907\n",
      "2022-03-26 22:55:57.582055 Epoch 300, Training Loss 0.04552436614280467\n",
      "2022-03-26 22:55:57.599939 Epoch 300, Training Loss 0.04601892908973157\n",
      "2022-03-26 22:55:57.615962 Epoch 300, Training Loss 0.04647818398292717\n",
      "2022-03-26 22:55:57.633978 Epoch 300, Training Loss 0.04693251097446208\n",
      "2022-03-26 22:55:57.648985 Epoch 300, Training Loss 0.047666848353717636\n",
      "2022-03-26 22:55:57.667961 Epoch 300, Training Loss 0.04813730846280637\n",
      "2022-03-26 22:55:57.684006 Epoch 300, Training Loss 0.048585273077725755\n",
      "2022-03-26 22:55:57.699934 Epoch 300, Training Loss 0.049272837579402776\n",
      "2022-03-26 22:55:57.715910 Epoch 300, Training Loss 0.05017076325995843\n",
      "2022-03-26 22:55:57.731916 Epoch 300, Training Loss 0.050772735110634126\n",
      "2022-03-26 22:55:57.748804 Epoch 300, Training Loss 0.051492250140975505\n",
      "2022-03-26 22:55:57.765937 Epoch 300, Training Loss 0.05238686822107076\n",
      "2022-03-26 22:55:57.781955 Epoch 300, Training Loss 0.052948844516673664\n",
      "2022-03-26 22:55:57.798952 Epoch 300, Training Loss 0.05355765481891535\n",
      "2022-03-26 22:55:57.814922 Epoch 300, Training Loss 0.05424400741029579\n",
      "2022-03-26 22:55:57.832925 Epoch 300, Training Loss 0.054901244504677364\n",
      "2022-03-26 22:55:57.847942 Epoch 300, Training Loss 0.05559110729133382\n",
      "2022-03-26 22:55:57.866843 Epoch 300, Training Loss 0.0562755778393782\n",
      "2022-03-26 22:55:57.882532 Epoch 300, Training Loss 0.05679689347743988\n",
      "2022-03-26 22:55:57.898541 Epoch 300, Training Loss 0.05741210914481326\n",
      "2022-03-26 22:55:57.919729 Epoch 300, Training Loss 0.05820355547206176\n",
      "2022-03-26 22:55:57.935847 Epoch 300, Training Loss 0.05894936201974864\n",
      "2022-03-26 22:55:57.951859 Epoch 300, Training Loss 0.05941540886983847\n",
      "2022-03-26 22:55:57.968832 Epoch 300, Training Loss 0.059950328124758534\n",
      "2022-03-26 22:55:57.984368 Epoch 300, Training Loss 0.06069101949634455\n",
      "2022-03-26 22:55:58.002052 Epoch 300, Training Loss 0.061199811870789586\n",
      "2022-03-26 22:55:58.019072 Epoch 300, Training Loss 0.061788942998327566\n",
      "2022-03-26 22:55:58.033245 Epoch 300, Training Loss 0.06232307733172346\n",
      "2022-03-26 22:55:58.048190 Epoch 300, Training Loss 0.06303791972377416\n",
      "2022-03-26 22:55:58.066907 Epoch 300, Training Loss 0.06372227776995705\n",
      "2022-03-26 22:55:58.082665 Epoch 300, Training Loss 0.06455868452101413\n",
      "2022-03-26 22:55:58.099545 Epoch 300, Training Loss 0.06532333337742349\n",
      "2022-03-26 22:55:58.116800 Epoch 300, Training Loss 0.0660176544695559\n",
      "2022-03-26 22:55:58.132520 Epoch 300, Training Loss 0.0669701528518706\n",
      "2022-03-26 22:55:58.149482 Epoch 300, Training Loss 0.06756494729720114\n",
      "2022-03-26 22:55:58.167145 Epoch 300, Training Loss 0.06802472967625883\n",
      "2022-03-26 22:55:58.182824 Epoch 300, Training Loss 0.06853452610695149\n",
      "2022-03-26 22:55:58.198372 Epoch 300, Training Loss 0.06924734095020978\n",
      "2022-03-26 22:55:58.217060 Epoch 300, Training Loss 0.06986767560472269\n",
      "2022-03-26 22:55:58.231991 Epoch 300, Training Loss 0.07029772730891967\n",
      "2022-03-26 22:55:58.248002 Epoch 300, Training Loss 0.07083742758807014\n",
      "2022-03-26 22:55:58.265966 Epoch 300, Training Loss 0.07149935942476668\n",
      "2022-03-26 22:55:58.281971 Epoch 300, Training Loss 0.072361677732614\n",
      "2022-03-26 22:55:58.302822 Epoch 300, Training Loss 0.07336319377050375\n",
      "2022-03-26 22:55:58.318832 Epoch 300, Training Loss 0.07410472341815529\n",
      "2022-03-26 22:55:58.332922 Epoch 300, Training Loss 0.0748679971755923\n",
      "2022-03-26 22:55:58.348930 Epoch 300, Training Loss 0.07569223802412867\n",
      "2022-03-26 22:55:58.365945 Epoch 300, Training Loss 0.07645471573180859\n",
      "2022-03-26 22:55:58.382948 Epoch 300, Training Loss 0.07704631805115039\n",
      "2022-03-26 22:55:58.398982 Epoch 300, Training Loss 0.07776715993271459\n",
      "2022-03-26 22:55:58.417001 Epoch 300, Training Loss 0.07862570912331876\n",
      "2022-03-26 22:55:58.434020 Epoch 300, Training Loss 0.07925921930071643\n",
      "2022-03-26 22:55:58.449050 Epoch 300, Training Loss 0.08008850497357986\n",
      "2022-03-26 22:55:58.466990 Epoch 300, Training Loss 0.08056232649499498\n",
      "2022-03-26 22:55:58.483023 Epoch 300, Training Loss 0.08100010252669644\n",
      "2022-03-26 22:55:58.498911 Epoch 300, Training Loss 0.08191134146107432\n",
      "2022-03-26 22:55:58.517940 Epoch 300, Training Loss 0.08237901318561086\n",
      "2022-03-26 22:55:58.535245 Epoch 300, Training Loss 0.08325286727884541\n",
      "2022-03-26 22:55:58.549082 Epoch 300, Training Loss 0.08393441102541316\n",
      "2022-03-26 22:55:58.568009 Epoch 300, Training Loss 0.08469867062233292\n",
      "2022-03-26 22:55:58.584943 Epoch 300, Training Loss 0.08543652612382494\n",
      "2022-03-26 22:55:58.601126 Epoch 300, Training Loss 0.08610651548713674\n",
      "2022-03-26 22:55:58.619008 Epoch 300, Training Loss 0.08684545271384442\n",
      "2022-03-26 22:55:58.633949 Epoch 300, Training Loss 0.08751531806595796\n",
      "2022-03-26 22:55:58.650913 Epoch 300, Training Loss 0.08844590114663019\n",
      "2022-03-26 22:55:58.665971 Epoch 300, Training Loss 0.08917956801174241\n",
      "2022-03-26 22:55:58.683914 Epoch 300, Training Loss 0.08989131119092712\n",
      "2022-03-26 22:55:58.700919 Epoch 300, Training Loss 0.09062972028389611\n",
      "2022-03-26 22:55:58.717827 Epoch 300, Training Loss 0.09133191315261908\n",
      "2022-03-26 22:55:58.735831 Epoch 300, Training Loss 0.09198344138729603\n",
      "2022-03-26 22:55:58.750766 Epoch 300, Training Loss 0.09273713053492329\n",
      "2022-03-26 22:55:58.768769 Epoch 300, Training Loss 0.09341666464458036\n",
      "2022-03-26 22:55:58.783744 Epoch 300, Training Loss 0.09398989684289069\n",
      "2022-03-26 22:55:58.802130 Epoch 300, Training Loss 0.09496240317821503\n",
      "2022-03-26 22:55:58.819817 Epoch 300, Training Loss 0.09585558541138153\n",
      "2022-03-26 22:55:58.834074 Epoch 300, Training Loss 0.09645793950923569\n",
      "2022-03-26 22:55:58.851041 Epoch 300, Training Loss 0.09718906037185503\n",
      "2022-03-26 22:55:58.867947 Epoch 300, Training Loss 0.09788721818905657\n",
      "2022-03-26 22:55:58.885164 Epoch 300, Training Loss 0.09846356793132889\n",
      "2022-03-26 22:55:58.902229 Epoch 300, Training Loss 0.09924206633092192\n",
      "2022-03-26 22:55:58.917158 Epoch 300, Training Loss 0.09975301411450671\n",
      "2022-03-26 22:55:58.937109 Epoch 300, Training Loss 0.10030725796509277\n",
      "2022-03-26 22:55:58.951991 Epoch 300, Training Loss 0.1012549084775588\n",
      "2022-03-26 22:55:58.965961 Epoch 300, Training Loss 0.1018005094641005\n",
      "2022-03-26 22:55:58.983978 Epoch 300, Training Loss 0.10254710657364877\n",
      "2022-03-26 22:55:59.000812 Epoch 300, Training Loss 0.10314398779131262\n",
      "2022-03-26 22:55:59.017832 Epoch 300, Training Loss 0.10367989795439689\n",
      "2022-03-26 22:55:59.033211 Epoch 300, Training Loss 0.10455415906656124\n",
      "2022-03-26 22:55:59.050119 Epoch 300, Training Loss 0.10504958784336324\n",
      "2022-03-26 22:55:59.066792 Epoch 300, Training Loss 0.10583195642894491\n",
      "2022-03-26 22:55:59.084967 Epoch 300, Training Loss 0.10640610544882773\n",
      "2022-03-26 22:55:59.099943 Epoch 300, Training Loss 0.1071637567046963\n",
      "2022-03-26 22:55:59.116577 Epoch 300, Training Loss 0.10802335095832415\n",
      "2022-03-26 22:55:59.132551 Epoch 300, Training Loss 0.10863322370192584\n",
      "2022-03-26 22:55:59.150560 Epoch 300, Training Loss 0.10950582358233459\n",
      "2022-03-26 22:55:59.166049 Epoch 300, Training Loss 0.10999061281571303\n",
      "2022-03-26 22:55:59.183084 Epoch 300, Training Loss 0.1109579519161483\n",
      "2022-03-26 22:55:59.199087 Epoch 300, Training Loss 0.111841481626796\n",
      "2022-03-26 22:55:59.217831 Epoch 300, Training Loss 0.11283235450077544\n",
      "2022-03-26 22:55:59.233827 Epoch 300, Training Loss 0.1137819286731198\n",
      "2022-03-26 22:55:59.248830 Epoch 300, Training Loss 0.1144704886943178\n",
      "2022-03-26 22:55:59.269473 Epoch 300, Training Loss 0.11541147560567197\n",
      "2022-03-26 22:55:59.286506 Epoch 300, Training Loss 0.11617001856836821\n",
      "2022-03-26 22:55:59.301391 Epoch 300, Training Loss 0.11703957884055574\n",
      "2022-03-26 22:55:59.317391 Epoch 300, Training Loss 0.11772015603149638\n",
      "2022-03-26 22:55:59.333838 Epoch 300, Training Loss 0.11816705439401709\n",
      "2022-03-26 22:55:59.349841 Epoch 300, Training Loss 0.11894603603331329\n",
      "2022-03-26 22:55:59.367836 Epoch 300, Training Loss 0.11973634987231106\n",
      "2022-03-26 22:55:59.383822 Epoch 300, Training Loss 0.12053122773499744\n",
      "2022-03-26 22:55:59.401843 Epoch 300, Training Loss 0.12107612810019032\n",
      "2022-03-26 22:55:59.418661 Epoch 300, Training Loss 0.12169207323847524\n",
      "2022-03-26 22:55:59.435720 Epoch 300, Training Loss 0.12240758157142288\n",
      "2022-03-26 22:55:59.449959 Epoch 300, Training Loss 0.12314867798019857\n",
      "2022-03-26 22:55:59.467167 Epoch 300, Training Loss 0.12358104447117242\n",
      "2022-03-26 22:55:59.485025 Epoch 300, Training Loss 0.12412738403700807\n",
      "2022-03-26 22:55:59.501988 Epoch 300, Training Loss 0.12476717282439131\n",
      "2022-03-26 22:55:59.520934 Epoch 300, Training Loss 0.12532210273815847\n",
      "2022-03-26 22:55:59.535944 Epoch 300, Training Loss 0.12595724781303455\n",
      "2022-03-26 22:55:59.551830 Epoch 300, Training Loss 0.1266065056595351\n",
      "2022-03-26 22:55:59.566828 Epoch 300, Training Loss 0.1272715502763953\n",
      "2022-03-26 22:55:59.583432 Epoch 300, Training Loss 0.12818239484449176\n",
      "2022-03-26 22:55:59.599446 Epoch 300, Training Loss 0.1288501696894541\n",
      "2022-03-26 22:55:59.621022 Epoch 300, Training Loss 0.12922833425462094\n",
      "2022-03-26 22:55:59.636937 Epoch 300, Training Loss 0.12975588848676217\n",
      "2022-03-26 22:55:59.652025 Epoch 300, Training Loss 0.13047457926566033\n",
      "2022-03-26 22:55:59.666019 Epoch 300, Training Loss 0.1312580990135822\n",
      "2022-03-26 22:55:59.682958 Epoch 300, Training Loss 0.13183139603766028\n",
      "2022-03-26 22:55:59.698961 Epoch 300, Training Loss 0.13249647739293324\n",
      "2022-03-26 22:55:59.716917 Epoch 300, Training Loss 0.13300944735174594\n",
      "2022-03-26 22:55:59.732921 Epoch 300, Training Loss 0.13371463737371939\n",
      "2022-03-26 22:55:59.748924 Epoch 300, Training Loss 0.13423467505618433\n",
      "2022-03-26 22:55:59.767819 Epoch 300, Training Loss 0.13502097526169798\n",
      "2022-03-26 22:55:59.785816 Epoch 300, Training Loss 0.1358950970422886\n",
      "2022-03-26 22:55:59.800818 Epoch 300, Training Loss 0.13648929376431437\n",
      "2022-03-26 22:55:59.818145 Epoch 300, Training Loss 0.13731007030248032\n",
      "2022-03-26 22:55:59.834771 Epoch 300, Training Loss 0.13818108982137403\n",
      "2022-03-26 22:55:59.849780 Epoch 300, Training Loss 0.1387110602139207\n",
      "2022-03-26 22:55:59.866771 Epoch 300, Training Loss 0.13918529306073932\n",
      "2022-03-26 22:55:59.883771 Epoch 300, Training Loss 0.13996702367844788\n",
      "2022-03-26 22:55:59.899691 Epoch 300, Training Loss 0.14079448020519197\n",
      "2022-03-26 22:55:59.918635 Epoch 300, Training Loss 0.1413982775052795\n",
      "2022-03-26 22:55:59.937168 Epoch 300, Training Loss 0.14207145038163266\n",
      "2022-03-26 22:55:59.951483 Epoch 300, Training Loss 0.1428221614312028\n",
      "2022-03-26 22:55:59.967752 Epoch 300, Training Loss 0.14351305853375387\n",
      "2022-03-26 22:55:59.997115 Epoch 300, Training Loss 0.14407612737792228\n",
      "2022-03-26 22:56:00.019780 Epoch 300, Training Loss 0.14515383232889884\n",
      "2022-03-26 22:56:00.040693 Epoch 300, Training Loss 0.1458901603660925\n",
      "2022-03-26 22:56:00.057355 Epoch 300, Training Loss 0.14643968481694342\n",
      "2022-03-26 22:56:00.073848 Epoch 300, Training Loss 0.1472982061869653\n",
      "2022-03-26 22:56:00.090851 Epoch 300, Training Loss 0.14803314685364208\n",
      "2022-03-26 22:56:00.107756 Epoch 300, Training Loss 0.14871807419278127\n",
      "2022-03-26 22:56:00.122812 Epoch 300, Training Loss 0.14961862926135588\n",
      "2022-03-26 22:56:00.139808 Epoch 300, Training Loss 0.15050100918163728\n",
      "2022-03-26 22:56:00.155759 Epoch 300, Training Loss 0.1512204496680623\n",
      "2022-03-26 22:56:00.172770 Epoch 300, Training Loss 0.151908836591884\n",
      "2022-03-26 22:56:00.188989 Epoch 300, Training Loss 0.1526028168247179\n",
      "2022-03-26 22:56:00.208000 Epoch 300, Training Loss 0.15339578741499224\n",
      "2022-03-26 22:56:00.223910 Epoch 300, Training Loss 0.15433922284247015\n",
      "2022-03-26 22:56:00.239941 Epoch 300, Training Loss 0.1548555276887801\n",
      "2022-03-26 22:56:00.255958 Epoch 300, Training Loss 0.15548771524520785\n",
      "2022-03-26 22:56:00.269955 Epoch 300, Training Loss 0.15606208698219046\n",
      "2022-03-26 22:56:00.286958 Epoch 300, Training Loss 0.1569476738915114\n",
      "2022-03-26 22:56:00.300961 Epoch 300, Training Loss 0.15771944420721828\n",
      "2022-03-26 22:56:00.318945 Epoch 300, Training Loss 0.15848136047268158\n",
      "2022-03-26 22:56:00.331944 Epoch 300, Training Loss 0.15928642043982016\n",
      "2022-03-26 22:56:00.350823 Epoch 300, Training Loss 0.16005626549501248\n",
      "2022-03-26 22:56:00.366769 Epoch 300, Training Loss 0.16061315801747314\n",
      "2022-03-26 22:56:00.383453 Epoch 300, Training Loss 0.161159743204751\n",
      "2022-03-26 22:56:00.399468 Epoch 300, Training Loss 0.16222077112673494\n",
      "2022-03-26 22:56:00.419053 Epoch 300, Training Loss 0.16301261128671943\n",
      "2022-03-26 22:56:00.435051 Epoch 300, Training Loss 0.163613331554186\n",
      "2022-03-26 22:56:00.449941 Epoch 300, Training Loss 0.16442517600858303\n",
      "2022-03-26 22:56:00.465932 Epoch 300, Training Loss 0.1650097227614859\n",
      "2022-03-26 22:56:00.481935 Epoch 300, Training Loss 0.16565784278428158\n",
      "2022-03-26 22:56:00.500940 Epoch 300, Training Loss 0.1662390769442634\n",
      "2022-03-26 22:56:00.516762 Epoch 300, Training Loss 0.16686580655977246\n",
      "2022-03-26 22:56:00.532839 Epoch 300, Training Loss 0.1675217743496151\n",
      "2022-03-26 22:56:00.549839 Epoch 300, Training Loss 0.16803235688325388\n",
      "2022-03-26 22:56:00.564842 Epoch 300, Training Loss 0.1687667361839348\n",
      "2022-03-26 22:56:00.583486 Epoch 300, Training Loss 0.16938611163812525\n",
      "2022-03-26 22:56:00.599581 Epoch 300, Training Loss 0.17027503137698258\n",
      "2022-03-26 22:56:00.618372 Epoch 300, Training Loss 0.17088983773880298\n",
      "2022-03-26 22:56:00.634618 Epoch 300, Training Loss 0.1716389740671953\n",
      "2022-03-26 22:56:00.650188 Epoch 300, Training Loss 0.17215236941414416\n",
      "2022-03-26 22:56:00.665330 Epoch 300, Training Loss 0.1726987247195695\n",
      "2022-03-26 22:56:00.681643 Epoch 300, Training Loss 0.17319732550007608\n",
      "2022-03-26 22:56:00.697822 Epoch 300, Training Loss 0.17374061882648323\n",
      "2022-03-26 22:56:00.715879 Epoch 300, Training Loss 0.17456553727769486\n",
      "2022-03-26 22:56:00.731896 Epoch 300, Training Loss 0.17532364730639835\n",
      "2022-03-26 22:56:00.748789 Epoch 300, Training Loss 0.176069281061592\n",
      "2022-03-26 22:56:00.766627 Epoch 300, Training Loss 0.1768997675165191\n",
      "2022-03-26 22:56:00.781736 Epoch 300, Training Loss 0.1776467549526478\n",
      "2022-03-26 22:56:00.799712 Epoch 300, Training Loss 0.17841391955190303\n",
      "2022-03-26 22:56:00.816728 Epoch 300, Training Loss 0.17899350063575198\n",
      "2022-03-26 22:56:00.832968 Epoch 300, Training Loss 0.17961757700614003\n",
      "2022-03-26 22:56:00.849992 Epoch 300, Training Loss 0.1802189512859525\n",
      "2022-03-26 22:56:00.864996 Epoch 300, Training Loss 0.18072583833161524\n",
      "2022-03-26 22:56:00.883002 Epoch 300, Training Loss 0.1813278686055137\n",
      "2022-03-26 22:56:00.900021 Epoch 300, Training Loss 0.18192006133096603\n",
      "2022-03-26 22:56:00.916043 Epoch 300, Training Loss 0.18248274994780644\n",
      "2022-03-26 22:56:00.933998 Epoch 300, Training Loss 0.18301545941006497\n",
      "2022-03-26 22:56:00.950006 Epoch 300, Training Loss 0.18384522603600836\n",
      "2022-03-26 22:56:00.966006 Epoch 300, Training Loss 0.18449352784534853\n",
      "2022-03-26 22:56:00.982205 Epoch 300, Training Loss 0.18565668428645415\n",
      "2022-03-26 22:56:01.000123 Epoch 300, Training Loss 0.18610813150472957\n",
      "2022-03-26 22:56:01.014834 Epoch 300, Training Loss 0.1868652368674193\n",
      "2022-03-26 22:56:01.032791 Epoch 300, Training Loss 0.18768933868926505\n",
      "2022-03-26 22:56:01.047787 Epoch 300, Training Loss 0.18821502547434835\n",
      "2022-03-26 22:56:01.066967 Epoch 300, Training Loss 0.1887759909681652\n",
      "2022-03-26 22:56:01.083003 Epoch 300, Training Loss 0.18932328931510906\n",
      "2022-03-26 22:56:01.099027 Epoch 300, Training Loss 0.1899259362150641\n",
      "2022-03-26 22:56:01.117900 Epoch 300, Training Loss 0.19056666960649174\n",
      "2022-03-26 22:56:01.132826 Epoch 300, Training Loss 0.19116638349297713\n",
      "2022-03-26 22:56:01.148836 Epoch 300, Training Loss 0.19183745412417993\n",
      "2022-03-26 22:56:01.166831 Epoch 300, Training Loss 0.19245558355927772\n",
      "2022-03-26 22:56:01.181998 Epoch 300, Training Loss 0.19307526568774983\n",
      "2022-03-26 22:56:01.200941 Epoch 300, Training Loss 0.1939244321773729\n",
      "2022-03-26 22:56:01.214845 Epoch 300, Training Loss 0.1944645119216436\n",
      "2022-03-26 22:56:01.232941 Epoch 300, Training Loss 0.19509022600967865\n",
      "2022-03-26 22:56:01.247814 Epoch 300, Training Loss 0.19564352864804474\n",
      "2022-03-26 22:56:01.265688 Epoch 300, Training Loss 0.19618597317991965\n",
      "2022-03-26 22:56:01.284241 Epoch 300, Training Loss 0.19673760399184265\n",
      "2022-03-26 22:56:01.299463 Epoch 300, Training Loss 0.19722334781418677\n",
      "2022-03-26 22:56:01.316168 Epoch 300, Training Loss 0.19775222150413582\n",
      "2022-03-26 22:56:01.332522 Epoch 300, Training Loss 0.19844357326360004\n",
      "2022-03-26 22:56:01.348475 Epoch 300, Training Loss 0.1991419488435511\n",
      "2022-03-26 22:56:01.366301 Epoch 300, Training Loss 0.19991315893657371\n",
      "2022-03-26 22:56:01.382276 Epoch 300, Training Loss 0.2004339161431393\n",
      "2022-03-26 22:56:01.398291 Epoch 300, Training Loss 0.2011533441293575\n",
      "2022-03-26 22:56:01.415966 Epoch 300, Training Loss 0.20164005111550432\n",
      "2022-03-26 22:56:01.432989 Epoch 300, Training Loss 0.202345016057534\n",
      "2022-03-26 22:56:01.448064 Epoch 300, Training Loss 0.20298795924162316\n",
      "2022-03-26 22:56:01.467130 Epoch 300, Training Loss 0.2036818070789737\n",
      "2022-03-26 22:56:01.482477 Epoch 300, Training Loss 0.2041674088257963\n",
      "2022-03-26 22:56:01.499493 Epoch 300, Training Loss 0.2047149903329132\n",
      "2022-03-26 22:56:01.515192 Epoch 300, Training Loss 0.20532664336511852\n",
      "2022-03-26 22:56:01.533216 Epoch 300, Training Loss 0.2060603944541853\n",
      "2022-03-26 22:56:01.549121 Epoch 300, Training Loss 0.20687622616967888\n",
      "2022-03-26 22:56:01.567981 Epoch 300, Training Loss 0.20736658603638944\n",
      "2022-03-26 22:56:01.585027 Epoch 300, Training Loss 0.20795617925236598\n",
      "2022-03-26 22:56:01.600046 Epoch 300, Training Loss 0.20893212383055626\n",
      "2022-03-26 22:56:01.615162 Epoch 300, Training Loss 0.20956112932213736\n",
      "2022-03-26 22:56:01.633810 Epoch 300, Training Loss 0.2100600498868986\n",
      "2022-03-26 22:56:01.648818 Epoch 300, Training Loss 0.2107243577537634\n",
      "2022-03-26 22:56:01.665857 Epoch 300, Training Loss 0.21124427523606878\n",
      "2022-03-26 22:56:01.681126 Epoch 300, Training Loss 0.2118284720594011\n",
      "2022-03-26 22:56:01.699080 Epoch 300, Training Loss 0.21259682791312332\n",
      "2022-03-26 22:56:01.714933 Epoch 300, Training Loss 0.21315564824949446\n",
      "2022-03-26 22:56:01.732936 Epoch 300, Training Loss 0.21400585850638806\n",
      "2022-03-26 22:56:01.748961 Epoch 300, Training Loss 0.21456934012415463\n",
      "2022-03-26 22:56:01.765983 Epoch 300, Training Loss 0.2152819621288563\n",
      "2022-03-26 22:56:01.781931 Epoch 300, Training Loss 0.21596207765057263\n",
      "2022-03-26 22:56:01.798935 Epoch 300, Training Loss 0.21668009227499022\n",
      "2022-03-26 22:56:01.814826 Epoch 300, Training Loss 0.21739689552265665\n",
      "2022-03-26 22:56:01.832838 Epoch 300, Training Loss 0.2181151730325216\n",
      "2022-03-26 22:56:01.848746 Epoch 300, Training Loss 0.21908129054262204\n",
      "2022-03-26 22:56:01.867631 Epoch 300, Training Loss 0.21954974776033856\n",
      "2022-03-26 22:56:01.882160 Epoch 300, Training Loss 0.22022103173348606\n",
      "2022-03-26 22:56:01.900066 Epoch 300, Training Loss 0.22088556605226853\n",
      "2022-03-26 22:56:01.921200 Epoch 300, Training Loss 0.22164666904207994\n",
      "2022-03-26 22:56:01.942145 Epoch 300, Training Loss 0.22254254331674111\n",
      "2022-03-26 22:56:01.964951 Epoch 300, Training Loss 0.22311779163072787\n",
      "2022-03-26 22:56:01.981983 Epoch 300, Training Loss 0.22360717247971487\n",
      "2022-03-26 22:56:01.999990 Epoch 300, Training Loss 0.2243109479965761\n",
      "2022-03-26 22:56:02.019986 Epoch 300, Training Loss 0.2251625977589956\n",
      "2022-03-26 22:56:02.044408 Epoch 300, Training Loss 0.22592014718391096\n",
      "2022-03-26 22:56:02.071960 Epoch 300, Training Loss 0.22665651211195895\n",
      "2022-03-26 22:56:02.089935 Epoch 300, Training Loss 0.22735258170863246\n",
      "2022-03-26 22:56:02.113944 Epoch 300, Training Loss 0.22812640487842853\n",
      "2022-03-26 22:56:02.156921 Epoch 300, Training Loss 0.22849152665918745\n",
      "2022-03-26 22:56:02.170925 Epoch 300, Training Loss 0.2290869029357915\n",
      "2022-03-26 22:56:02.188931 Epoch 300, Training Loss 0.2298181447607782\n",
      "2022-03-26 22:56:02.203937 Epoch 300, Training Loss 0.23054108157029848\n",
      "2022-03-26 22:56:02.219020 Epoch 300, Training Loss 0.23107871016882875\n",
      "2022-03-26 22:56:02.239049 Epoch 300, Training Loss 0.23174203326330162\n",
      "2022-03-26 22:56:02.256438 Epoch 300, Training Loss 0.23253710442186926\n",
      "2022-03-26 22:56:02.278164 Epoch 300, Training Loss 0.2334186565083311\n",
      "2022-03-26 22:56:02.298177 Epoch 300, Training Loss 0.2341227080968335\n",
      "2022-03-26 22:56:02.318173 Epoch 300, Training Loss 0.23472477468993047\n",
      "2022-03-26 22:56:02.337427 Epoch 300, Training Loss 0.23526086016079348\n",
      "2022-03-26 22:56:02.352444 Epoch 300, Training Loss 0.23592346822819138\n",
      "2022-03-26 22:56:02.369654 Epoch 300, Training Loss 0.23656706576761993\n",
      "2022-03-26 22:56:02.385460 Epoch 300, Training Loss 0.2374009622637268\n",
      "2022-03-26 22:56:02.404827 Epoch 300, Training Loss 0.23825291934830453\n",
      "2022-03-26 22:56:02.418812 Epoch 300, Training Loss 0.2389255457979334\n",
      "2022-03-26 22:56:02.434885 Epoch 300, Training Loss 0.23936800346197679\n",
      "2022-03-26 22:56:02.449890 Epoch 300, Training Loss 0.2400209877420874\n",
      "2022-03-26 22:56:02.466821 Epoch 300, Training Loss 0.2409815585140682\n",
      "2022-03-26 22:56:02.487352 Epoch 300, Training Loss 0.2416340779999028\n",
      "2022-03-26 22:56:02.503122 Epoch 300, Training Loss 0.24234525905088392\n",
      "2022-03-26 22:56:02.519167 Epoch 300, Training Loss 0.24319348680546216\n",
      "2022-03-26 22:56:02.533195 Epoch 300, Training Loss 0.24391047763245186\n",
      "2022-03-26 22:56:02.550706 Epoch 300, Training Loss 0.24473596301377581\n",
      "2022-03-26 22:56:02.567631 Epoch 300, Training Loss 0.24550333500975538\n",
      "2022-03-26 22:56:02.584551 Epoch 300, Training Loss 0.24633048528143206\n",
      "2022-03-26 22:56:02.603168 Epoch 300, Training Loss 0.24710037949902322\n",
      "2022-03-26 22:56:02.619263 Epoch 300, Training Loss 0.24792362475181784\n",
      "2022-03-26 22:56:02.634056 Epoch 300, Training Loss 0.24859317439748807\n",
      "2022-03-26 22:56:02.650931 Epoch 300, Training Loss 0.24931436011096095\n",
      "2022-03-26 22:56:02.666937 Epoch 300, Training Loss 0.25026295103532886\n",
      "2022-03-26 22:56:02.684755 Epoch 300, Training Loss 0.25093188306407244\n",
      "2022-03-26 22:56:02.702478 Epoch 300, Training Loss 0.25187060278852275\n",
      "2022-03-26 22:56:02.717898 Epoch 300, Training Loss 0.2528605622327541\n",
      "2022-03-26 22:56:02.735914 Epoch 300, Training Loss 0.2534470052060569\n",
      "2022-03-26 22:56:02.751822 Epoch 300, Training Loss 0.25413411901430094\n",
      "2022-03-26 22:56:02.767521 Epoch 300, Training Loss 0.2548901077426608\n",
      "2022-03-26 22:56:02.787681 Epoch 300, Training Loss 0.2556485002455504\n",
      "2022-03-26 22:56:02.802366 Epoch 300, Training Loss 0.2564179413306439\n",
      "2022-03-26 22:56:02.818402 Epoch 300, Training Loss 0.25730587835507013\n",
      "2022-03-26 22:56:02.834042 Epoch 300, Training Loss 0.2578189541083163\n",
      "2022-03-26 22:56:02.850063 Epoch 300, Training Loss 0.25854791990478937\n",
      "2022-03-26 22:56:02.868353 Epoch 300, Training Loss 0.25947195905096393\n",
      "2022-03-26 22:56:02.884728 Epoch 300, Training Loss 0.2602122729010594\n",
      "2022-03-26 22:56:02.903039 Epoch 300, Training Loss 0.2609761623699037\n",
      "2022-03-26 22:56:02.917699 Epoch 300, Training Loss 0.2615418543138772\n",
      "2022-03-26 22:56:02.932699 Epoch 300, Training Loss 0.26219422776071005\n",
      "2022-03-26 22:56:02.949695 Epoch 300, Training Loss 0.2628236676344786\n",
      "2022-03-26 22:56:02.966770 Epoch 300, Training Loss 0.26363503487061357\n",
      "2022-03-26 22:56:02.983798 Epoch 300, Training Loss 0.26422550340595147\n",
      "2022-03-26 22:56:03.000810 Epoch 300, Training Loss 0.26494652280569686\n",
      "2022-03-26 22:56:03.017823 Epoch 300, Training Loss 0.2653720027116863\n",
      "2022-03-26 22:56:03.033925 Epoch 300, Training Loss 0.26607208426498696\n",
      "2022-03-26 22:56:03.049927 Epoch 300, Training Loss 0.2669237350944973\n",
      "2022-03-26 22:56:03.065941 Epoch 300, Training Loss 0.26775263188897497\n",
      "2022-03-26 22:56:03.084920 Epoch 300, Training Loss 0.268312750112675\n",
      "2022-03-26 22:56:03.100935 Epoch 300, Training Loss 0.26899297821247364\n",
      "2022-03-26 22:56:03.116941 Epoch 300, Training Loss 0.2698462469041195\n",
      "2022-03-26 22:56:03.134949 Epoch 300, Training Loss 0.27067752033853165\n",
      "2022-03-26 22:56:03.153953 Epoch 300, Training Loss 0.27141709888682647\n",
      "2022-03-26 22:56:03.168957 Epoch 300, Training Loss 0.2721002325225059\n",
      "2022-03-26 22:56:03.183968 Epoch 300, Training Loss 0.2729788226697146\n",
      "2022-03-26 22:56:03.200977 Epoch 300, Training Loss 0.2737926796574117\n",
      "2022-03-26 22:56:03.216989 Epoch 300, Training Loss 0.27460907090960257\n",
      "2022-03-26 22:56:03.233946 Epoch 300, Training Loss 0.2751647544562664\n",
      "2022-03-26 22:56:03.248954 Epoch 300, Training Loss 0.27577600359459364\n",
      "2022-03-26 22:56:03.266940 Epoch 300, Training Loss 0.2764877090444955\n",
      "2022-03-26 22:56:03.285951 Epoch 300, Training Loss 0.2771100418646927\n",
      "2022-03-26 22:56:03.301972 Epoch 300, Training Loss 0.2779721029274299\n",
      "2022-03-26 22:56:03.319152 Epoch 300, Training Loss 0.2787470412071404\n",
      "2022-03-26 22:56:03.334137 Epoch 300, Training Loss 0.27954943962109363\n",
      "2022-03-26 22:56:03.352891 Epoch 300, Training Loss 0.28051042511030233\n",
      "2022-03-26 22:56:03.367889 Epoch 300, Training Loss 0.28147862817320374\n",
      "2022-03-26 22:56:03.383846 Epoch 300, Training Loss 0.2820998774769971\n",
      "2022-03-26 22:56:03.404173 Epoch 300, Training Loss 0.28271867079503094\n",
      "2022-03-26 22:56:03.419119 Epoch 300, Training Loss 0.28367349101454403\n",
      "2022-03-26 22:56:03.435645 Epoch 300, Training Loss 0.28449872715393903\n",
      "2022-03-26 22:56:03.452191 Epoch 300, Training Loss 0.2849546577924353\n",
      "2022-03-26 22:56:03.468162 Epoch 300, Training Loss 0.2859556969931668\n",
      "2022-03-26 22:56:03.486135 Epoch 300, Training Loss 0.2872206778325083\n",
      "2022-03-26 22:56:03.502013 Epoch 300, Training Loss 0.28777555683079886\n",
      "2022-03-26 22:56:03.518930 Epoch 300, Training Loss 0.28840968896970726\n",
      "2022-03-26 22:56:03.534953 Epoch 300, Training Loss 0.2890841579040908\n",
      "2022-03-26 22:56:03.552082 Epoch 300, Training Loss 0.28972803479265374\n",
      "2022-03-26 22:56:03.568105 Epoch 300, Training Loss 0.29027151691791653\n",
      "2022-03-26 22:56:03.586157 Epoch 300, Training Loss 0.2909732890479705\n",
      "2022-03-26 22:56:03.601094 Epoch 300, Training Loss 0.2918282324624488\n",
      "2022-03-26 22:56:03.618020 Epoch 300, Training Loss 0.29253151749863343\n",
      "2022-03-26 22:56:03.634950 Epoch 300, Training Loss 0.29330409689784964\n",
      "2022-03-26 22:56:03.652961 Epoch 300, Training Loss 0.29394331429620535\n",
      "2022-03-26 22:56:03.668930 Epoch 300, Training Loss 0.2946558284485127\n",
      "2022-03-26 22:56:03.684317 Epoch 300, Training Loss 0.2951987208155415\n",
      "2022-03-26 22:56:03.703336 Epoch 300, Training Loss 0.29596852684569785\n",
      "2022-03-26 22:56:03.717337 Epoch 300, Training Loss 0.2965847850989198\n",
      "2022-03-26 22:56:03.736237 Epoch 300, Training Loss 0.2970864489636458\n",
      "2022-03-26 22:56:03.752052 Epoch 300, Training Loss 0.2979041268224911\n",
      "2022-03-26 22:56:03.768019 Epoch 300, Training Loss 0.29864984762180796\n",
      "2022-03-26 22:56:03.785338 Epoch 300, Training Loss 0.29947476714011045\n",
      "2022-03-26 22:56:03.801894 Epoch 300, Training Loss 0.30007777185848605\n",
      "2022-03-26 22:56:03.817893 Epoch 300, Training Loss 0.3009435669006899\n",
      "2022-03-26 22:56:03.835831 Epoch 300, Training Loss 0.3015706905394869\n",
      "2022-03-26 22:56:03.852834 Epoch 300, Training Loss 0.30224999400508373\n",
      "2022-03-26 22:56:03.868849 Epoch 300, Training Loss 0.3027880802712477\n",
      "2022-03-26 22:56:03.884844 Epoch 300, Training Loss 0.3033027856246285\n",
      "2022-03-26 22:56:03.903972 Epoch 300, Training Loss 0.3040464728537118\n",
      "2022-03-26 22:56:03.919013 Epoch 300, Training Loss 0.30470667531728135\n",
      "2022-03-26 22:56:03.933129 Epoch 300, Training Loss 0.30534449448365997\n",
      "2022-03-26 22:56:03.950162 Epoch 300, Training Loss 0.30604400133232934\n",
      "2022-03-26 22:56:03.969236 Epoch 300, Training Loss 0.3064903510958337\n",
      "2022-03-26 22:56:03.984927 Epoch 300, Training Loss 0.30734510571145646\n",
      "2022-03-26 22:56:04.001938 Epoch 300, Training Loss 0.308380160261603\n",
      "2022-03-26 22:56:04.019272 Epoch 300, Training Loss 0.3094148603851533\n",
      "2022-03-26 22:56:04.035309 Epoch 300, Training Loss 0.3100742481248763\n",
      "2022-03-26 22:56:04.050148 Epoch 300, Training Loss 0.31064877767697013\n",
      "2022-03-26 22:56:04.069297 Epoch 300, Training Loss 0.311228766084632\n",
      "2022-03-26 22:56:04.083378 Epoch 300, Training Loss 0.31200359300579256\n",
      "2022-03-26 22:56:04.101820 Epoch 300, Training Loss 0.3127253791102973\n",
      "2022-03-26 22:56:04.118803 Epoch 300, Training Loss 0.31336870385557797\n",
      "2022-03-26 22:56:04.133808 Epoch 300, Training Loss 0.31397851051576914\n",
      "2022-03-26 22:56:04.151944 Epoch 300, Training Loss 0.31464617453572696\n",
      "2022-03-26 22:56:04.169939 Epoch 300, Training Loss 0.315392433377483\n",
      "2022-03-26 22:56:04.186941 Epoch 300, Training Loss 0.3159116879677224\n",
      "2022-03-26 22:56:04.202943 Epoch 300, Training Loss 0.3165323009804996\n",
      "2022-03-26 22:56:04.216953 Epoch 300, Training Loss 0.31764472403642163\n",
      "2022-03-26 22:56:04.234968 Epoch 300, Training Loss 0.31834804215242185\n",
      "2022-03-26 22:56:04.251942 Epoch 300, Training Loss 0.3189315746736039\n",
      "2022-03-26 22:56:04.266941 Epoch 300, Training Loss 0.3196350908873941\n",
      "2022-03-26 22:56:04.285941 Epoch 300, Training Loss 0.32044561344491856\n",
      "2022-03-26 22:56:04.301959 Epoch 300, Training Loss 0.3209523463721775\n",
      "2022-03-26 22:56:04.316937 Epoch 300, Training Loss 0.3214716625488018\n",
      "2022-03-26 22:56:04.333940 Epoch 300, Training Loss 0.3220592706709567\n",
      "2022-03-26 22:56:04.349937 Epoch 300, Training Loss 0.3227298219338098\n",
      "2022-03-26 22:56:04.367876 Epoch 300, Training Loss 0.3232181367971708\n",
      "2022-03-26 22:56:04.382889 Epoch 300, Training Loss 0.3239468386411057\n",
      "2022-03-26 22:56:04.401938 Epoch 300, Training Loss 0.32456539323567735\n",
      "2022-03-26 22:56:04.417987 Epoch 300, Training Loss 0.3251016290901262\n",
      "2022-03-26 22:56:04.434169 Epoch 300, Training Loss 0.32578496684503677\n",
      "2022-03-26 22:56:04.449079 Epoch 300, Training Loss 0.3262761762684873\n",
      "2022-03-26 22:56:04.468833 Epoch 300, Training Loss 0.32725272946955297\n",
      "2022-03-26 22:56:04.482852 Epoch 300, Training Loss 0.3278808350224629\n",
      "2022-03-26 22:56:04.502023 Epoch 300, Training Loss 0.3287814294971773\n",
      "2022-03-26 22:56:04.517046 Epoch 300, Training Loss 0.32942704650599636\n",
      "2022-03-26 22:56:04.534029 Epoch 300, Training Loss 0.3300674485657221\n",
      "2022-03-26 22:56:04.552074 Epoch 300, Training Loss 0.3307744836639565\n",
      "2022-03-26 22:56:04.569031 Epoch 300, Training Loss 0.3313878122955332\n",
      "2022-03-26 22:56:04.582964 Epoch 300, Training Loss 0.332092399182527\n",
      "2022-03-26 22:56:04.601916 Epoch 300, Training Loss 0.3327181111363804\n",
      "2022-03-26 22:56:04.616923 Epoch 300, Training Loss 0.33321448966212897\n",
      "2022-03-26 22:56:04.634930 Epoch 300, Training Loss 0.3337330524726292\n",
      "2022-03-26 22:56:04.649944 Epoch 300, Training Loss 0.3346077635160188\n",
      "2022-03-26 22:56:04.667930 Epoch 300, Training Loss 0.33518600703962625\n",
      "2022-03-26 22:56:04.684055 Epoch 300, Training Loss 0.33564201100250646\n",
      "2022-03-26 22:56:04.703002 Epoch 300, Training Loss 0.3365742424336236\n",
      "2022-03-26 22:56:04.718240 Epoch 300, Training Loss 0.33714964947737086\n",
      "2022-03-26 22:56:04.734992 Epoch 300, Training Loss 0.3378054968383916\n",
      "2022-03-26 22:56:04.750995 Epoch 300, Training Loss 0.3386099641127964\n",
      "2022-03-26 22:56:04.767851 Epoch 300, Training Loss 0.33920541584796615\n",
      "2022-03-26 22:56:04.784832 Epoch 300, Training Loss 0.33981410046215255\n",
      "2022-03-26 22:56:04.799836 Epoch 300, Training Loss 0.3403637525446884\n",
      "2022-03-26 22:56:04.816762 Epoch 300, Training Loss 0.34080215354862115\n",
      "2022-03-26 22:56:04.834930 Epoch 300, Training Loss 0.3415891443905623\n",
      "2022-03-26 22:56:04.848937 Epoch 300, Training Loss 0.34201739526465724\n",
      "2022-03-26 22:56:04.868418 Epoch 300, Training Loss 0.3425609900823335\n",
      "2022-03-26 22:56:04.885454 Epoch 300, Training Loss 0.3431699207371763\n",
      "2022-03-26 22:56:04.903037 Epoch 300, Training Loss 0.34375117109407244\n",
      "2022-03-26 22:56:04.918957 Epoch 300, Training Loss 0.34439494165465656\n",
      "2022-03-26 22:56:04.932953 Epoch 300, Training Loss 0.34513376858990513\n",
      "2022-03-26 22:56:04.949816 Epoch 300, Training Loss 0.3458798806304517\n",
      "2022-03-26 22:56:04.967827 Epoch 300, Training Loss 0.34652064672059113\n",
      "2022-03-26 22:56:04.982751 Epoch 300, Training Loss 0.34735018422688974\n",
      "2022-03-26 22:56:05.001762 Epoch 300, Training Loss 0.34791373238539147\n",
      "2022-03-26 22:56:05.016651 Epoch 300, Training Loss 0.3485543060942989\n",
      "2022-03-26 22:56:05.034361 Epoch 300, Training Loss 0.3496819293255086\n",
      "2022-03-26 22:56:05.049678 Epoch 300, Training Loss 0.35045061216634865\n",
      "2022-03-26 22:56:05.067675 Epoch 300, Training Loss 0.3513523206838866\n",
      "2022-03-26 22:56:05.086809 Epoch 300, Training Loss 0.3519284736622325\n",
      "2022-03-26 22:56:05.102316 Epoch 300, Training Loss 0.35268851687841096\n",
      "2022-03-26 22:56:05.117211 Epoch 300, Training Loss 0.3533154444392685\n",
      "2022-03-26 22:56:05.134215 Epoch 300, Training Loss 0.35377518969880956\n",
      "2022-03-26 22:56:05.151940 Epoch 300, Training Loss 0.35470035675998846\n",
      "2022-03-26 22:56:05.168905 Epoch 300, Training Loss 0.3554484668518881\n",
      "2022-03-26 22:56:05.184292 Epoch 300, Training Loss 0.3563633318371175\n",
      "2022-03-26 22:56:05.201943 Epoch 300, Training Loss 0.3572752810728824\n",
      "2022-03-26 22:56:05.216841 Epoch 300, Training Loss 0.35776315187401786\n",
      "2022-03-26 22:56:05.233844 Epoch 300, Training Loss 0.35857863777586263\n",
      "2022-03-26 22:56:05.249850 Epoch 300, Training Loss 0.3591689496012905\n",
      "2022-03-26 22:56:05.268862 Epoch 300, Training Loss 0.3601722079698387\n",
      "2022-03-26 22:56:05.285830 Epoch 300, Training Loss 0.36084618512779243\n",
      "2022-03-26 22:56:05.302882 Epoch 300, Training Loss 0.361871758728381\n",
      "2022-03-26 22:56:05.317823 Epoch 300, Training Loss 0.3625937684646348\n",
      "2022-03-26 22:56:05.334845 Epoch 300, Training Loss 0.36315997817632184\n",
      "2022-03-26 22:56:05.351869 Epoch 300, Training Loss 0.36382201908494505\n",
      "2022-03-26 22:56:05.367882 Epoch 300, Training Loss 0.3644352356338745\n",
      "2022-03-26 22:56:05.384878 Epoch 300, Training Loss 0.36508793164702025\n",
      "2022-03-26 22:56:05.401230 Epoch 300, Training Loss 0.3656518963520484\n",
      "2022-03-26 22:56:05.417127 Epoch 300, Training Loss 0.3663149128865708\n",
      "2022-03-26 22:56:05.435094 Epoch 300, Training Loss 0.3669407716035233\n",
      "2022-03-26 22:56:05.452062 Epoch 300, Training Loss 0.36753387806360677\n",
      "2022-03-26 22:56:05.468961 Epoch 300, Training Loss 0.3682236443547642\n",
      "2022-03-26 22:56:05.484955 Epoch 300, Training Loss 0.36879178077516045\n",
      "2022-03-26 22:56:05.504993 Epoch 300, Training Loss 0.369429586336131\n",
      "2022-03-26 22:56:05.521013 Epoch 300, Training Loss 0.37019111288478\n",
      "2022-03-26 22:56:05.534830 Epoch 300, Training Loss 0.37084145253271705\n",
      "2022-03-26 22:56:05.550833 Epoch 300, Training Loss 0.3714950386520542\n",
      "2022-03-26 22:56:05.571847 Epoch 300, Training Loss 0.372162719821686\n",
      "2022-03-26 22:56:05.589835 Epoch 300, Training Loss 0.3727951397371414\n",
      "2022-03-26 22:56:05.603822 Epoch 300, Training Loss 0.37333555024145815\n",
      "2022-03-26 22:56:05.618830 Epoch 300, Training Loss 0.37403259759821245\n",
      "2022-03-26 22:56:05.638597 Epoch 300, Training Loss 0.3746734958551729\n",
      "2022-03-26 22:56:05.654221 Epoch 300, Training Loss 0.37530223731799506\n",
      "2022-03-26 22:56:05.668615 Epoch 300, Training Loss 0.37613966032062346\n",
      "2022-03-26 22:56:05.686252 Epoch 300, Training Loss 0.37682743206658326\n",
      "2022-03-26 22:56:05.704728 Epoch 300, Training Loss 0.37768850073485116\n",
      "2022-03-26 22:56:05.719672 Epoch 300, Training Loss 0.3782980044555786\n",
      "2022-03-26 22:56:05.733783 Epoch 300, Training Loss 0.37892571903403155\n",
      "2022-03-26 22:56:05.750780 Epoch 300, Training Loss 0.3797489930220577\n",
      "2022-03-26 22:56:05.767733 Epoch 300, Training Loss 0.3805320473659374\n",
      "2022-03-26 22:56:05.787667 Epoch 300, Training Loss 0.38112749460408146\n",
      "2022-03-26 22:56:05.803206 Epoch 300, Training Loss 0.3820974322231224\n",
      "2022-03-26 22:56:05.818477 Epoch 300, Training Loss 0.3825629855055943\n",
      "2022-03-26 22:56:05.835754 Epoch 300, Training Loss 0.38327342713885293\n",
      "2022-03-26 22:56:05.853091 Epoch 300, Training Loss 0.3840018007761377\n",
      "2022-03-26 22:56:05.868046 Epoch 300, Training Loss 0.3846359721687444\n",
      "2022-03-26 22:56:05.885057 Epoch 300, Training Loss 0.38530679424400527\n",
      "2022-03-26 22:56:05.902942 Epoch 300, Training Loss 0.3859008193168494\n",
      "2022-03-26 22:56:05.919349 Epoch 300, Training Loss 0.38661150119798565\n",
      "2022-03-26 22:56:05.936078 Epoch 300, Training Loss 0.3872824171010186\n",
      "2022-03-26 22:56:05.953030 Epoch 300, Training Loss 0.38772231450928446\n",
      "2022-03-26 22:56:05.968371 Epoch 300, Training Loss 0.3885660565951291\n",
      "2022-03-26 22:56:05.984939 Epoch 300, Training Loss 0.38916994063445676\n",
      "2022-03-26 22:56:06.001934 Epoch 300, Training Loss 0.38967991324946705\n",
      "2022-03-26 22:56:06.017018 Epoch 300, Training Loss 0.390247681332976\n",
      "2022-03-26 22:56:06.034829 Epoch 300, Training Loss 0.39102193332084306\n",
      "2022-03-26 22:56:06.051840 Epoch 300, Training Loss 0.3919541289281967\n",
      "2022-03-26 22:56:06.067837 Epoch 300, Training Loss 0.3925628478036207\n",
      "2022-03-26 22:56:06.085840 Epoch 300, Training Loss 0.3932662862722221\n",
      "2022-03-26 22:56:06.101874 Epoch 300, Training Loss 0.3939887171282488\n",
      "2022-03-26 22:56:06.117874 Epoch 300, Training Loss 0.3946981417477283\n",
      "2022-03-26 22:56:06.135854 Epoch 300, Training Loss 0.39548478128812503\n",
      "2022-03-26 22:56:06.150870 Epoch 300, Training Loss 0.3962460787171293\n",
      "2022-03-26 22:56:06.167758 Epoch 300, Training Loss 0.3967944982716495\n",
      "2022-03-26 22:56:06.185760 Epoch 300, Training Loss 0.39729310957062275\n",
      "2022-03-26 22:56:06.201789 Epoch 300, Training Loss 0.3979347052476595\n",
      "2022-03-26 22:56:06.218739 Epoch 300, Training Loss 0.3985660112727329\n",
      "2022-03-26 22:56:06.235779 Epoch 300, Training Loss 0.399137608001909\n",
      "2022-03-26 22:56:06.255810 Epoch 300, Training Loss 0.3996887790120166\n",
      "2022-03-26 22:56:06.270786 Epoch 300, Training Loss 0.40025769982039167\n",
      "2022-03-26 22:56:06.285802 Epoch 300, Training Loss 0.40103275197393756\n",
      "2022-03-26 22:56:06.301795 Epoch 300, Training Loss 0.4017014049584299\n",
      "2022-03-26 22:56:06.317809 Epoch 300, Training Loss 0.40228001014960696\n",
      "2022-03-26 22:56:06.334807 Epoch 300, Training Loss 0.402908421042935\n",
      "2022-03-26 22:56:06.352847 Epoch 300, Training Loss 0.4036423258693017\n",
      "2022-03-26 22:56:06.368862 Epoch 300, Training Loss 0.4045094109480948\n",
      "2022-03-26 22:56:06.385873 Epoch 300, Training Loss 0.40515507795774114\n",
      "2022-03-26 22:56:06.402951 Epoch 300, Training Loss 0.4058395748019523\n",
      "2022-03-26 22:56:06.418863 Epoch 300, Training Loss 0.4066877495068723\n",
      "2022-03-26 22:56:06.435545 Epoch 300, Training Loss 0.4074009894333837\n",
      "2022-03-26 22:56:06.451982 Epoch 300, Training Loss 0.40784809713625847\n",
      "2022-03-26 22:56:06.467855 Epoch 300, Training Loss 0.40849336406306536\n",
      "2022-03-26 22:56:06.484828 Epoch 300, Training Loss 0.4090098042774688\n",
      "2022-03-26 22:56:06.501944 Epoch 300, Training Loss 0.40975247998066877\n",
      "2022-03-26 22:56:06.518948 Epoch 300, Training Loss 0.41033549820218246\n",
      "2022-03-26 22:56:06.534983 Epoch 300, Training Loss 0.4110602101935145\n",
      "2022-03-26 22:56:06.552079 Epoch 300, Training Loss 0.41162475760635514\n",
      "2022-03-26 22:56:06.567761 Epoch 300, Training Loss 0.4124739880452071\n",
      "2022-03-26 22:56:06.585767 Epoch 300, Training Loss 0.4133686645104147\n",
      "2022-03-26 22:56:06.602794 Epoch 300, Training Loss 0.41418486040876346\n",
      "2022-03-26 22:56:06.618709 Epoch 300, Training Loss 0.4148025619023291\n",
      "2022-03-26 22:56:06.635724 Epoch 300, Training Loss 0.41541657728307385\n",
      "2022-03-26 22:56:06.653427 Epoch 300, Training Loss 0.4161364784478531\n",
      "2022-03-26 22:56:06.671128 Epoch 300, Training Loss 0.41689068833580406\n",
      "2022-03-26 22:56:06.685387 Epoch 300, Training Loss 0.4175719006744492\n",
      "2022-03-26 22:56:06.700814 Epoch 300, Training Loss 0.4180267737497149\n",
      "2022-03-26 22:56:06.718694 Epoch 300, Training Loss 0.41867762194265185\n",
      "2022-03-26 22:56:06.734705 Epoch 300, Training Loss 0.41937885900287675\n",
      "2022-03-26 22:56:06.751221 Epoch 300, Training Loss 0.4200019834520262\n",
      "2022-03-26 22:56:06.767765 Epoch 300, Training Loss 0.42047261406698494\n",
      "2022-03-26 22:56:06.784787 Epoch 300, Training Loss 0.42101018130779266\n",
      "2022-03-26 22:56:06.801834 Epoch 300, Training Loss 0.4216047412218035\n",
      "2022-03-26 22:56:06.817694 Epoch 300, Training Loss 0.42222815877793696\n",
      "2022-03-26 22:56:06.835711 Epoch 300, Training Loss 0.4229725212468516\n",
      "2022-03-26 22:56:06.851592 Epoch 300, Training Loss 0.42353847523784394\n",
      "2022-03-26 22:56:06.867626 Epoch 300, Training Loss 0.42426229620833533\n",
      "2022-03-26 22:56:06.885503 Epoch 300, Training Loss 0.42488852352894785\n",
      "2022-03-26 22:56:06.902209 Epoch 300, Training Loss 0.42585800633863413\n",
      "2022-03-26 22:56:06.918245 Epoch 300, Training Loss 0.4265008996743375\n",
      "2022-03-26 22:56:06.933823 Epoch 300, Training Loss 0.42738395818816427\n",
      "2022-03-26 22:56:06.950855 Epoch 300, Training Loss 0.4282271457678827\n",
      "2022-03-26 22:56:06.968832 Epoch 300, Training Loss 0.42917980505224995\n",
      "2022-03-26 22:56:06.984607 Epoch 300, Training Loss 0.4297794966060487\n",
      "2022-03-26 22:56:07.008608 Epoch 300, Training Loss 0.4303949114764133\n",
      "2022-03-26 22:56:07.026617 Epoch 300, Training Loss 0.4309155771037197\n",
      "2022-03-26 22:56:07.041841 Epoch 300, Training Loss 0.4316137562627378\n",
      "2022-03-26 22:56:07.057824 Epoch 300, Training Loss 0.4323041003073573\n",
      "2022-03-26 22:56:07.072955 Epoch 300, Training Loss 0.4332660646999584\n",
      "2022-03-26 22:56:07.089960 Epoch 300, Training Loss 0.4339559866339349\n",
      "2022-03-26 22:56:07.113973 Epoch 300, Training Loss 0.4344769262368112\n",
      "2022-03-26 22:56:07.134989 Epoch 300, Training Loss 0.43548762619190506\n",
      "2022-03-26 22:56:07.151947 Epoch 300, Training Loss 0.43614946115199865\n",
      "2022-03-26 22:56:07.171978 Epoch 300, Training Loss 0.4368471194944723\n",
      "2022-03-26 22:56:07.218028 Epoch 300, Training Loss 0.4373475861214006\n",
      "2022-03-26 22:56:07.235091 Epoch 300, Training Loss 0.4377764208085092\n",
      "2022-03-26 22:56:07.252809 Epoch 300, Training Loss 0.43848117820137295\n",
      "2022-03-26 22:56:07.267814 Epoch 300, Training Loss 0.43929170639923465\n",
      "2022-03-26 22:56:07.284826 Epoch 300, Training Loss 0.4399500816222042\n",
      "2022-03-26 22:56:07.303433 Epoch 300, Training Loss 0.4405851137760045\n",
      "2022-03-26 22:56:07.319138 Epoch 300, Training Loss 0.4413603871984555\n",
      "2022-03-26 22:56:07.333939 Epoch 300, Training Loss 0.44191203900920156\n",
      "2022-03-26 22:56:07.352941 Epoch 300, Training Loss 0.44247028246864945\n",
      "2022-03-26 22:56:07.368970 Epoch 300, Training Loss 0.4433328571069576\n",
      "2022-03-26 22:56:07.385949 Epoch 300, Training Loss 0.4439610080874484\n",
      "2022-03-26 22:56:07.400823 Epoch 300, Training Loss 0.4446644097414163\n",
      "2022-03-26 22:56:07.416836 Epoch 300, Training Loss 0.4452978436980406\n",
      "2022-03-26 22:56:07.434854 Epoch 300, Training Loss 0.4456234264861592\n",
      "2022-03-26 22:56:07.451858 Epoch 300, Training Loss 0.44646533942588457\n",
      "2022-03-26 22:56:07.471871 Epoch 300, Training Loss 0.4470246161722466\n",
      "2022-03-26 22:56:07.487880 Epoch 300, Training Loss 0.44796367721332003\n",
      "2022-03-26 22:56:07.503821 Epoch 300, Training Loss 0.4489800492897058\n",
      "2022-03-26 22:56:07.522832 Epoch 300, Training Loss 0.4497825017441874\n",
      "2022-03-26 22:56:07.542841 Epoch 300, Training Loss 0.4505214574163222\n",
      "2022-03-26 22:56:07.565955 Epoch 300, Training Loss 0.45125311411097835\n",
      "2022-03-26 22:56:07.583969 Epoch 300, Training Loss 0.4518779089002658\n",
      "2022-03-26 22:56:07.601828 Epoch 300, Training Loss 0.4530241588116302\n",
      "2022-03-26 22:56:07.617730 Epoch 300, Training Loss 0.45410191749825196\n",
      "2022-03-26 22:56:07.634737 Epoch 300, Training Loss 0.4548191891987915\n",
      "2022-03-26 22:56:07.652833 Epoch 300, Training Loss 0.45560504122615775\n",
      "2022-03-26 22:56:07.667848 Epoch 300, Training Loss 0.45621919037435976\n",
      "2022-03-26 22:56:07.683240 Epoch 300, Training Loss 0.4569356215884314\n",
      "2022-03-26 22:56:07.701887 Epoch 300, Training Loss 0.45769127906131013\n",
      "2022-03-26 22:56:07.718084 Epoch 300, Training Loss 0.4584475063606906\n",
      "2022-03-26 22:56:07.734136 Epoch 300, Training Loss 0.4593509674224707\n",
      "2022-03-26 22:56:07.750063 Epoch 300, Training Loss 0.46029611438741463\n",
      "2022-03-26 22:56:07.769085 Epoch 300, Training Loss 0.4608279380499554\n",
      "2022-03-26 22:56:07.784022 Epoch 300, Training Loss 0.46130855168070634\n",
      "2022-03-26 22:56:07.801391 Epoch 300, Training Loss 0.4618999413822008\n",
      "2022-03-26 22:56:07.817836 Epoch 300, Training Loss 0.46270782692962903\n",
      "2022-03-26 22:56:07.834841 Epoch 300, Training Loss 0.463395512850998\n",
      "2022-03-26 22:56:07.851754 Epoch 300, Training Loss 0.46436017584008027\n",
      "2022-03-26 22:56:07.867747 Epoch 300, Training Loss 0.46505439159510387\n",
      "2022-03-26 22:56:07.884781 Epoch 300, Training Loss 0.46598389767624837\n",
      "2022-03-26 22:56:07.901012 Epoch 300, Training Loss 0.4665484221085258\n",
      "2022-03-26 22:56:07.918096 Epoch 300, Training Loss 0.4670523294554952\n",
      "2022-03-26 22:56:07.934423 Epoch 300, Training Loss 0.4674389831855169\n",
      "2022-03-26 22:56:07.950435 Epoch 300, Training Loss 0.46825406016291254\n",
      "2022-03-26 22:56:07.970455 Epoch 300, Training Loss 0.46912152390650774\n",
      "2022-03-26 22:56:07.985944 Epoch 300, Training Loss 0.46990922017170644\n",
      "2022-03-26 22:56:08.001819 Epoch 300, Training Loss 0.4704551635419621\n",
      "2022-03-26 22:56:08.016831 Epoch 300, Training Loss 0.4712694960710643\n",
      "2022-03-26 22:56:08.033852 Epoch 300, Training Loss 0.4721770061327673\n",
      "2022-03-26 22:56:08.050970 Epoch 300, Training Loss 0.4727871157705327\n",
      "2022-03-26 22:56:08.066976 Epoch 300, Training Loss 0.47354835489064534\n",
      "2022-03-26 22:56:08.083931 Epoch 300, Training Loss 0.47417997094371434\n",
      "2022-03-26 22:56:08.100952 Epoch 300, Training Loss 0.47457204282741106\n",
      "2022-03-26 22:56:08.116764 Epoch 300, Training Loss 0.47534692477997004\n",
      "2022-03-26 22:56:08.134780 Epoch 300, Training Loss 0.47632921999677674\n",
      "2022-03-26 22:56:08.153782 Epoch 300, Training Loss 0.47720810854831314\n",
      "2022-03-26 22:56:08.169698 Epoch 300, Training Loss 0.47776846488570923\n",
      "2022-03-26 22:56:08.184724 Epoch 300, Training Loss 0.47872593179535683\n",
      "2022-03-26 22:56:08.200601 Epoch 300, Training Loss 0.4794905076322653\n",
      "2022-03-26 22:56:08.217805 Epoch 300, Training Loss 0.48008896837301573\n",
      "2022-03-26 22:56:08.234771 Epoch 300, Training Loss 0.48059243912739524\n",
      "2022-03-26 22:56:08.251779 Epoch 300, Training Loss 0.4811579863281201\n",
      "2022-03-26 22:56:08.268585 Epoch 300, Training Loss 0.48178404885942067\n",
      "2022-03-26 22:56:08.283152 Epoch 300, Training Loss 0.4824540884522221\n",
      "2022-03-26 22:56:08.301184 Epoch 300, Training Loss 0.4832338639308729\n",
      "2022-03-26 22:56:08.316062 Epoch 300, Training Loss 0.48381827112353976\n",
      "2022-03-26 22:56:08.333958 Epoch 300, Training Loss 0.48462858064400266\n",
      "2022-03-26 22:56:08.349960 Epoch 300, Training Loss 0.48545014271345893\n",
      "2022-03-26 22:56:08.366983 Epoch 300, Training Loss 0.48614132198531307\n",
      "2022-03-26 22:56:08.383808 Epoch 300, Training Loss 0.487210817501673\n",
      "2022-03-26 22:56:08.401368 Epoch 300, Training Loss 0.48783766133401096\n",
      "2022-03-26 22:56:08.418233 Epoch 300, Training Loss 0.48842364423872564\n",
      "2022-03-26 22:56:08.434399 Epoch 300, Training Loss 0.4889070533425607\n",
      "2022-03-26 22:56:08.451347 Epoch 300, Training Loss 0.48966093929222476\n",
      "2022-03-26 22:56:08.466121 Epoch 300, Training Loss 0.4904396624668785\n",
      "2022-03-26 22:56:08.483486 Epoch 300, Training Loss 0.4910786910282681\n",
      "2022-03-26 22:56:08.499503 Epoch 300, Training Loss 0.49197808601667203\n",
      "2022-03-26 22:56:08.516971 Epoch 300, Training Loss 0.49267263828641006\n",
      "2022-03-26 22:56:08.533941 Epoch 300, Training Loss 0.4933977641565416\n",
      "2022-03-26 22:56:08.549956 Epoch 300, Training Loss 0.49413424753167134\n",
      "2022-03-26 22:56:08.569967 Epoch 300, Training Loss 0.4948122080634622\n",
      "2022-03-26 22:56:08.584995 Epoch 300, Training Loss 0.49526006627418195\n",
      "2022-03-26 22:56:08.601307 Epoch 300, Training Loss 0.49572688489771255\n",
      "2022-03-26 22:56:08.616310 Epoch 300, Training Loss 0.49635209741494846\n",
      "2022-03-26 22:56:08.633268 Epoch 300, Training Loss 0.49694756847208416\n",
      "2022-03-26 22:56:08.650710 Epoch 300, Training Loss 0.4975368707533688\n",
      "2022-03-26 22:56:08.666620 Epoch 300, Training Loss 0.49806744393790164\n",
      "2022-03-26 22:56:08.683824 Epoch 300, Training Loss 0.49863540165869474\n",
      "2022-03-26 22:56:08.700757 Epoch 300, Training Loss 0.49927570691803835\n",
      "2022-03-26 22:56:08.717760 Epoch 300, Training Loss 0.500102467091797\n",
      "2022-03-26 22:56:08.733775 Epoch 300, Training Loss 0.5006678188243485\n",
      "2022-03-26 22:56:08.751346 Epoch 300, Training Loss 0.5011198753514863\n",
      "2022-03-26 22:56:08.767222 Epoch 300, Training Loss 0.5020439576386185\n",
      "2022-03-26 22:56:08.782740 Epoch 300, Training Loss 0.5025618017253364\n",
      "2022-03-26 22:56:08.800745 Epoch 300, Training Loss 0.5032238845553849\n",
      "2022-03-26 22:56:08.814676 Epoch 300, Training Loss 0.5038701409040509\n",
      "2022-03-26 22:56:08.834325 Epoch 300, Training Loss 0.5045331091908238\n",
      "2022-03-26 22:56:08.848242 Epoch 300, Training Loss 0.5050379680017071\n",
      "2022-03-26 22:56:08.867838 Epoch 300, Training Loss 0.5058311458362643\n",
      "2022-03-26 22:56:08.883722 Epoch 300, Training Loss 0.5065246850557035\n",
      "2022-03-26 22:56:08.901740 Epoch 300, Training Loss 0.507316206384193\n",
      "2022-03-26 22:56:08.920737 Epoch 300, Training Loss 0.5083305312468268\n",
      "2022-03-26 22:56:08.934745 Epoch 300, Training Loss 0.5091324240121695\n",
      "2022-03-26 22:56:08.952835 Epoch 300, Training Loss 0.5096739970357217\n",
      "2022-03-26 22:56:08.968577 Epoch 300, Training Loss 0.5105278018642875\n",
      "2022-03-26 22:56:08.983876 Epoch 300, Training Loss 0.511285613564884\n",
      "2022-03-26 22:56:09.001940 Epoch 300, Training Loss 0.5118679414548533\n",
      "2022-03-26 22:56:09.018885 Epoch 300, Training Loss 0.5127781088775991\n",
      "2022-03-26 22:56:09.035920 Epoch 300, Training Loss 0.5133906802557924\n",
      "2022-03-26 22:56:09.052923 Epoch 300, Training Loss 0.5143620730818385\n",
      "2022-03-26 22:56:09.067841 Epoch 300, Training Loss 0.5152508941147943\n",
      "2022-03-26 22:56:09.086829 Epoch 300, Training Loss 0.515972563525295\n",
      "2022-03-26 22:56:09.100862 Epoch 300, Training Loss 0.5165478652700439\n",
      "2022-03-26 22:56:09.115835 Epoch 300, Training Loss 0.5171343120162749\n",
      "2022-03-26 22:56:09.142847 Epoch 300, Training Loss 0.5177636375588834\n",
      "2022-03-26 22:56:09.162849 Epoch 300, Training Loss 0.518261312599987\n",
      "2022-03-26 22:56:09.184829 Epoch 300, Training Loss 0.5187396205523435\n",
      "2022-03-26 22:56:09.201841 Epoch 300, Training Loss 0.519514008594291\n",
      "2022-03-26 22:56:09.218847 Epoch 300, Training Loss 0.5202293878092485\n",
      "2022-03-26 22:56:09.234811 Epoch 300, Training Loss 0.5207974221319189\n",
      "2022-03-26 22:56:09.252250 Epoch 300, Training Loss 0.5214453363586264\n",
      "2022-03-26 22:56:09.268096 Epoch 300, Training Loss 0.5221504051895702\n",
      "2022-03-26 22:56:09.283999 Epoch 300, Training Loss 0.5228563840965481\n",
      "2022-03-26 22:56:09.300975 Epoch 300, Training Loss 0.5234273047093541\n",
      "2022-03-26 22:56:09.318954 Epoch 300, Training Loss 0.5238206508138296\n",
      "2022-03-26 22:56:09.333853 Epoch 300, Training Loss 0.5245671631658778\n",
      "2022-03-26 22:56:09.351934 Epoch 300, Training Loss 0.5251660178536954\n",
      "2022-03-26 22:56:09.366838 Epoch 300, Training Loss 0.5259227137583906\n",
      "2022-03-26 22:56:09.384943 Epoch 300, Training Loss 0.5265113354643898\n",
      "2022-03-26 22:56:09.401150 Epoch 300, Training Loss 0.5271085688982473\n",
      "2022-03-26 22:56:09.416763 Epoch 300, Training Loss 0.5277321962139491\n",
      "2022-03-26 22:56:09.433784 Epoch 300, Training Loss 0.5285111822740501\n",
      "2022-03-26 22:56:09.449787 Epoch 300, Training Loss 0.5291813080725463\n",
      "2022-03-26 22:56:09.468792 Epoch 300, Training Loss 0.5298194536543868\n",
      "2022-03-26 22:56:09.484317 Epoch 300, Training Loss 0.5304641471723156\n",
      "2022-03-26 22:56:09.500862 Epoch 300, Training Loss 0.5311788089973543\n",
      "2022-03-26 22:56:09.518777 Epoch 300, Training Loss 0.5321435106303686\n",
      "2022-03-26 22:56:09.532799 Epoch 300, Training Loss 0.5327651111213753\n",
      "2022-03-26 22:56:09.551741 Epoch 300, Training Loss 0.5334318938005306\n",
      "2022-03-26 22:56:09.566740 Epoch 300, Training Loss 0.5342923812854016\n",
      "2022-03-26 22:56:09.584487 Epoch 300, Training Loss 0.5349380283252053\n",
      "2022-03-26 22:56:09.601149 Epoch 300, Training Loss 0.5356238223707585\n",
      "2022-03-26 22:56:09.617012 Epoch 300, Training Loss 0.5362450587170203\n",
      "2022-03-26 22:56:09.634442 Epoch 300, Training Loss 0.5370358942872118\n",
      "2022-03-26 22:56:09.649483 Epoch 300, Training Loss 0.5377742249490051\n",
      "2022-03-26 22:56:09.668848 Epoch 300, Training Loss 0.5384514975883162\n",
      "2022-03-26 22:56:09.684353 Epoch 300, Training Loss 0.5391403906942939\n",
      "2022-03-26 22:56:09.701690 Epoch 300, Training Loss 0.5395990663858325\n",
      "2022-03-26 22:56:09.717700 Epoch 300, Training Loss 0.5400763725304543\n",
      "2022-03-26 22:56:09.733688 Epoch 300, Training Loss 0.5409651103303256\n",
      "2022-03-26 22:56:09.741700 Epoch 300, Training Loss 0.5413468230487136\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ja1-PxhTbu5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.82\n",
      "Accuracy val: 0.61\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0COWXnIccUmu"
   },
   "source": [
    "Problem 2 Part 2 Subsection  B: Using Dropout with p=0.3\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1gGTBumvbu4W"
   },
   "outputs": [],
   "source": [
    "class ResNet10(nn.Module):\n",
    "  def __init__(self, n_chans1=32, n_blocks=10):\n",
    "    super().__init__()\n",
    "    self.n_chans1 = n_chans1\n",
    "    self.conv1 = nn.Sequential(\n",
    "        *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "    self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
    "    self.fc1 = nn.Linear(8*8*n_chans1 // 2, 32)\n",
    "    self.fc2 = nn.Linear(32,2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "    out = self.conv1_dropout(out)\n",
    "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "    out = self.conv2_dropout(out)\n",
    "    out = out.view(-1, 8*8*self.n_chans1 // 2)\n",
    "    out = torch.tanh(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9A4g03bjlaUt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet10(\n",
       "  (conv1): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv1_dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet10()\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DLLkORm8laOJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 22:56:24.232976 Epoch 1, Training Loss 0.0029549324299063522\n",
      "2022-03-26 22:56:24.253956 Epoch 1, Training Loss 0.005928043819144559\n",
      "2022-03-26 22:56:24.273961 Epoch 1, Training Loss 0.008855945618866046\n",
      "2022-03-26 22:56:24.292974 Epoch 1, Training Loss 0.011795571393064221\n",
      "2022-03-26 22:56:24.311983 Epoch 1, Training Loss 0.014728807122506144\n",
      "2022-03-26 22:56:24.327724 Epoch 1, Training Loss 0.017657711377838992\n",
      "2022-03-26 22:56:24.345704 Epoch 1, Training Loss 0.02061652496952535\n",
      "2022-03-26 22:56:24.370150 Epoch 1, Training Loss 0.023545584105469686\n",
      "2022-03-26 22:56:24.389968 Epoch 1, Training Loss 0.02649913937844279\n",
      "2022-03-26 22:56:24.406972 Epoch 1, Training Loss 0.029452876056856512\n",
      "2022-03-26 22:56:24.424994 Epoch 1, Training Loss 0.03237599820432151\n",
      "2022-03-26 22:56:24.441188 Epoch 1, Training Loss 0.03529697580410696\n",
      "2022-03-26 22:56:24.457202 Epoch 1, Training Loss 0.038219732396742874\n",
      "2022-03-26 22:56:24.475289 Epoch 1, Training Loss 0.041148569272912064\n",
      "2022-03-26 22:56:24.491378 Epoch 1, Training Loss 0.04410650083780898\n",
      "2022-03-26 22:56:24.506947 Epoch 1, Training Loss 0.04703884325978701\n",
      "2022-03-26 22:56:24.522961 Epoch 1, Training Loss 0.049970722869229135\n",
      "2022-03-26 22:56:24.539963 Epoch 1, Training Loss 0.05292002837676221\n",
      "2022-03-26 22:56:24.561059 Epoch 1, Training Loss 0.05583504978043344\n",
      "2022-03-26 22:56:24.578017 Epoch 1, Training Loss 0.058773918529910506\n",
      "2022-03-26 22:56:24.592955 Epoch 1, Training Loss 0.06170076665366092\n",
      "2022-03-26 22:56:24.609957 Epoch 1, Training Loss 0.06460753121339452\n",
      "2022-03-26 22:56:24.625672 Epoch 1, Training Loss 0.06751561469739051\n",
      "2022-03-26 22:56:24.641945 Epoch 1, Training Loss 0.07048726112336454\n",
      "2022-03-26 22:56:24.658922 Epoch 1, Training Loss 0.07342385849379518\n",
      "2022-03-26 22:56:24.676311 Epoch 1, Training Loss 0.07632724464396991\n",
      "2022-03-26 22:56:24.694844 Epoch 1, Training Loss 0.07927929166027957\n",
      "2022-03-26 22:56:24.709806 Epoch 1, Training Loss 0.08221489053857906\n",
      "2022-03-26 22:56:24.726467 Epoch 1, Training Loss 0.08513847915717708\n",
      "2022-03-26 22:56:24.741150 Epoch 1, Training Loss 0.08806608643983026\n",
      "2022-03-26 22:56:24.756036 Epoch 1, Training Loss 0.09096702651294601\n",
      "2022-03-26 22:56:24.772070 Epoch 1, Training Loss 0.09389696950497835\n",
      "2022-03-26 22:56:24.789947 Epoch 1, Training Loss 0.09682494234246061\n",
      "2022-03-26 22:56:24.804968 Epoch 1, Training Loss 0.09972222991611647\n",
      "2022-03-26 22:56:24.822816 Epoch 1, Training Loss 0.10264270933692718\n",
      "2022-03-26 22:56:24.838810 Epoch 1, Training Loss 0.1055403676484247\n",
      "2022-03-26 22:56:24.858272 Epoch 1, Training Loss 0.10844811667566714\n",
      "2022-03-26 22:56:24.877963 Epoch 1, Training Loss 0.1113851015525096\n",
      "2022-03-26 22:56:24.894393 Epoch 1, Training Loss 0.11427696860964646\n",
      "2022-03-26 22:56:24.910560 Epoch 1, Training Loss 0.11719225251766117\n",
      "2022-03-26 22:56:24.926801 Epoch 1, Training Loss 0.12009633898430164\n",
      "2022-03-26 22:56:24.940872 Epoch 1, Training Loss 0.12302101786484194\n",
      "2022-03-26 22:56:24.955574 Epoch 1, Training Loss 0.12591904538976567\n",
      "2022-03-26 22:56:24.971572 Epoch 1, Training Loss 0.12885708729629322\n",
      "2022-03-26 22:56:24.990514 Epoch 1, Training Loss 0.1317477052473961\n",
      "2022-03-26 22:56:25.005850 Epoch 1, Training Loss 0.13462911572907588\n",
      "2022-03-26 22:56:25.022854 Epoch 1, Training Loss 0.1375183751211142\n",
      "2022-03-26 22:56:25.040868 Epoch 1, Training Loss 0.1404098186956342\n",
      "2022-03-26 22:56:25.057865 Epoch 1, Training Loss 0.14332443277549256\n",
      "2022-03-26 22:56:25.074791 Epoch 1, Training Loss 0.14620589905077844\n",
      "2022-03-26 22:56:25.090911 Epoch 1, Training Loss 0.1490957142446962\n",
      "2022-03-26 22:56:25.105915 Epoch 1, Training Loss 0.15199080940402682\n",
      "2022-03-26 22:56:25.123859 Epoch 1, Training Loss 0.15487194549092245\n",
      "2022-03-26 22:56:25.141860 Epoch 1, Training Loss 0.15775125075484175\n",
      "2022-03-26 22:56:25.156796 Epoch 1, Training Loss 0.16063349661619766\n",
      "2022-03-26 22:56:25.174302 Epoch 1, Training Loss 0.16351354701439744\n",
      "2022-03-26 22:56:25.188866 Epoch 1, Training Loss 0.16638847385221125\n",
      "2022-03-26 22:56:25.205875 Epoch 1, Training Loss 0.16926254732224644\n",
      "2022-03-26 22:56:25.223092 Epoch 1, Training Loss 0.17215681929722468\n",
      "2022-03-26 22:56:25.239915 Epoch 1, Training Loss 0.17505977647688686\n",
      "2022-03-26 22:56:25.256857 Epoch 1, Training Loss 0.17796157449102767\n",
      "2022-03-26 22:56:25.273287 Epoch 1, Training Loss 0.18089855967275323\n",
      "2022-03-26 22:56:25.290300 Epoch 1, Training Loss 0.18379082704139182\n",
      "2022-03-26 22:56:25.304870 Epoch 1, Training Loss 0.18665660952058288\n",
      "2022-03-26 22:56:25.322822 Epoch 1, Training Loss 0.18960216313676762\n",
      "2022-03-26 22:56:25.339817 Epoch 1, Training Loss 0.19245875430533954\n",
      "2022-03-26 22:56:25.356773 Epoch 1, Training Loss 0.19538774545235402\n",
      "2022-03-26 22:56:25.373795 Epoch 1, Training Loss 0.19827778077186525\n",
      "2022-03-26 22:56:25.391814 Epoch 1, Training Loss 0.20113983087222595\n",
      "2022-03-26 22:56:25.413844 Epoch 1, Training Loss 0.20401041373572387\n",
      "2022-03-26 22:56:25.437851 Epoch 1, Training Loss 0.2068814252648512\n",
      "2022-03-26 22:56:25.458858 Epoch 1, Training Loss 0.20975413651722472\n",
      "2022-03-26 22:56:25.476870 Epoch 1, Training Loss 0.2126496124755391\n",
      "2022-03-26 22:56:25.492847 Epoch 1, Training Loss 0.21549567206741294\n",
      "2022-03-26 22:56:25.506846 Epoch 1, Training Loss 0.21835742673605604\n",
      "2022-03-26 22:56:25.522855 Epoch 1, Training Loss 0.2212144410823617\n",
      "2022-03-26 22:56:25.539828 Epoch 1, Training Loss 0.2241274487332005\n",
      "2022-03-26 22:56:25.556832 Epoch 1, Training Loss 0.22699789348465707\n",
      "2022-03-26 22:56:25.572798 Epoch 1, Training Loss 0.22988724830510365\n",
      "2022-03-26 22:56:25.590804 Epoch 1, Training Loss 0.23272053817349017\n",
      "2022-03-26 22:56:25.605861 Epoch 1, Training Loss 0.23557579090528172\n",
      "2022-03-26 22:56:25.622865 Epoch 1, Training Loss 0.23846527438639376\n",
      "2022-03-26 22:56:25.638868 Epoch 1, Training Loss 0.24134339792344273\n",
      "2022-03-26 22:56:25.655881 Epoch 1, Training Loss 0.24418728186956148\n",
      "2022-03-26 22:56:25.673872 Epoch 1, Training Loss 0.24701105389753572\n",
      "2022-03-26 22:56:25.689884 Epoch 1, Training Loss 0.24987141159184448\n",
      "2022-03-26 22:56:25.706812 Epoch 1, Training Loss 0.25271424765477096\n",
      "2022-03-26 22:56:25.722969 Epoch 1, Training Loss 0.25557848346202877\n",
      "2022-03-26 22:56:25.739812 Epoch 1, Training Loss 0.25848672304616865\n",
      "2022-03-26 22:56:25.756809 Epoch 1, Training Loss 0.261354101283471\n",
      "2022-03-26 22:56:25.773815 Epoch 1, Training Loss 0.26420859516124284\n",
      "2022-03-26 22:56:25.789817 Epoch 1, Training Loss 0.26710437752706623\n",
      "2022-03-26 22:56:25.804808 Epoch 1, Training Loss 0.2699434235882576\n",
      "2022-03-26 22:56:25.822861 Epoch 1, Training Loss 0.27282058217031574\n",
      "2022-03-26 22:56:25.837862 Epoch 1, Training Loss 0.2756820816518096\n",
      "2022-03-26 22:56:25.854787 Epoch 1, Training Loss 0.2785436814398412\n",
      "2022-03-26 22:56:25.870776 Epoch 1, Training Loss 0.28141554969046123\n",
      "2022-03-26 22:56:25.889311 Epoch 1, Training Loss 0.28422993711193506\n",
      "2022-03-26 22:56:25.905048 Epoch 1, Training Loss 0.2870379759527533\n",
      "2022-03-26 22:56:25.920954 Epoch 1, Training Loss 0.2899140412240382\n",
      "2022-03-26 22:56:25.937957 Epoch 1, Training Loss 0.29274466946301864\n",
      "2022-03-26 22:56:25.954049 Epoch 1, Training Loss 0.2955970114759167\n",
      "2022-03-26 22:56:25.971948 Epoch 1, Training Loss 0.2983850219365581\n",
      "2022-03-26 22:56:25.987955 Epoch 1, Training Loss 0.301214278811384\n",
      "2022-03-26 22:56:26.005183 Epoch 1, Training Loss 0.30408239669507114\n",
      "2022-03-26 22:56:26.021982 Epoch 1, Training Loss 0.30694449374742827\n",
      "2022-03-26 22:56:26.038756 Epoch 1, Training Loss 0.3098419737023161\n",
      "2022-03-26 22:56:26.053787 Epoch 1, Training Loss 0.3126421596692956\n",
      "2022-03-26 22:56:26.073802 Epoch 1, Training Loss 0.31554962789920893\n",
      "2022-03-26 22:56:26.089799 Epoch 1, Training Loss 0.31838310039256845\n",
      "2022-03-26 22:56:26.104572 Epoch 1, Training Loss 0.3212057367310195\n",
      "2022-03-26 22:56:26.120560 Epoch 1, Training Loss 0.32404174188823653\n",
      "2022-03-26 22:56:26.137152 Epoch 1, Training Loss 0.32682397938750285\n",
      "2022-03-26 22:56:26.153968 Epoch 1, Training Loss 0.3296323475020621\n",
      "2022-03-26 22:56:26.170989 Epoch 1, Training Loss 0.33247059659884715\n",
      "2022-03-26 22:56:26.188966 Epoch 1, Training Loss 0.3352683776479853\n",
      "2022-03-26 22:56:26.205858 Epoch 1, Training Loss 0.33810456329599364\n",
      "2022-03-26 22:56:26.221851 Epoch 1, Training Loss 0.34093092408631465\n",
      "2022-03-26 22:56:26.239455 Epoch 1, Training Loss 0.3436937399227601\n",
      "2022-03-26 22:56:26.254069 Epoch 1, Training Loss 0.34655999527562914\n",
      "2022-03-26 22:56:26.271480 Epoch 1, Training Loss 0.34933739428020194\n",
      "2022-03-26 22:56:26.287019 Epoch 1, Training Loss 0.35206873490072577\n",
      "2022-03-26 22:56:26.304954 Epoch 1, Training Loss 0.35485515265208684\n",
      "2022-03-26 22:56:26.320863 Epoch 1, Training Loss 0.3576734108693155\n",
      "2022-03-26 22:56:26.338974 Epoch 1, Training Loss 0.3604574258370168\n",
      "2022-03-26 22:56:26.354988 Epoch 1, Training Loss 0.36335947416017733\n",
      "2022-03-26 22:56:26.370981 Epoch 1, Training Loss 0.36621001309446055\n",
      "2022-03-26 22:56:26.387981 Epoch 1, Training Loss 0.36900078549104576\n",
      "2022-03-26 22:56:26.403876 Epoch 1, Training Loss 0.37174336257797985\n",
      "2022-03-26 22:56:26.421845 Epoch 1, Training Loss 0.3745342624156981\n",
      "2022-03-26 22:56:26.437848 Epoch 1, Training Loss 0.3773464461421723\n",
      "2022-03-26 22:56:26.453852 Epoch 1, Training Loss 0.3800551293756041\n",
      "2022-03-26 22:56:26.470811 Epoch 1, Training Loss 0.38288276793096987\n",
      "2022-03-26 22:56:26.486948 Epoch 1, Training Loss 0.3856345707803126\n",
      "2022-03-26 22:56:26.503856 Epoch 1, Training Loss 0.38840763892054253\n",
      "2022-03-26 22:56:26.521852 Epoch 1, Training Loss 0.3911700428599287\n",
      "2022-03-26 22:56:26.537868 Epoch 1, Training Loss 0.39395591365102\n",
      "2022-03-26 22:56:26.557521 Epoch 1, Training Loss 0.3967221566783193\n",
      "2022-03-26 22:56:26.574968 Epoch 1, Training Loss 0.3994504527362716\n",
      "2022-03-26 22:56:26.589766 Epoch 1, Training Loss 0.40223790983409835\n",
      "2022-03-26 22:56:26.604758 Epoch 1, Training Loss 0.4050210625924113\n",
      "2022-03-26 22:56:26.621763 Epoch 1, Training Loss 0.4077812167994507\n",
      "2022-03-26 22:56:26.637747 Epoch 1, Training Loss 0.410598544513478\n",
      "2022-03-26 22:56:26.653404 Epoch 1, Training Loss 0.4133819622151992\n",
      "2022-03-26 22:56:26.670355 Epoch 1, Training Loss 0.4162745841628755\n",
      "2022-03-26 22:56:26.686904 Epoch 1, Training Loss 0.41908727795876505\n",
      "2022-03-26 22:56:26.703906 Epoch 1, Training Loss 0.4219121289680071\n",
      "2022-03-26 22:56:26.721970 Epoch 1, Training Loss 0.4246783930322398\n",
      "2022-03-26 22:56:26.737859 Epoch 1, Training Loss 0.4274533291911835\n",
      "2022-03-26 22:56:26.754906 Epoch 1, Training Loss 0.430294850903094\n",
      "2022-03-26 22:56:26.771855 Epoch 1, Training Loss 0.43310479831207743\n",
      "2022-03-26 22:56:26.787870 Epoch 1, Training Loss 0.43582923150123537\n",
      "2022-03-26 22:56:26.805877 Epoch 1, Training Loss 0.4385911860429417\n",
      "2022-03-26 22:56:26.820855 Epoch 1, Training Loss 0.44132935482522717\n",
      "2022-03-26 22:56:26.837864 Epoch 1, Training Loss 0.44413311798554245\n",
      "2022-03-26 22:56:26.855411 Epoch 1, Training Loss 0.446928402042145\n",
      "2022-03-26 22:56:26.870442 Epoch 1, Training Loss 0.4496669138179106\n",
      "2022-03-26 22:56:26.887275 Epoch 1, Training Loss 0.45249014681257554\n",
      "2022-03-26 22:56:26.903300 Epoch 1, Training Loss 0.4551876249825558\n",
      "2022-03-26 22:56:26.921306 Epoch 1, Training Loss 0.4578897160337404\n",
      "2022-03-26 22:56:26.936991 Epoch 1, Training Loss 0.46059855689173157\n",
      "2022-03-26 22:56:26.954934 Epoch 1, Training Loss 0.4632719642365985\n",
      "2022-03-26 22:56:26.971356 Epoch 1, Training Loss 0.4660813534046378\n",
      "2022-03-26 22:56:26.986745 Epoch 1, Training Loss 0.4687597815642881\n",
      "2022-03-26 22:56:27.004224 Epoch 1, Training Loss 0.4715137350589723\n",
      "2022-03-26 22:56:27.021181 Epoch 1, Training Loss 0.47427856586778255\n",
      "2022-03-26 22:56:27.037734 Epoch 1, Training Loss 0.47707015687547377\n",
      "2022-03-26 22:56:27.055713 Epoch 1, Training Loss 0.47985613346099854\n",
      "2022-03-26 22:56:27.070706 Epoch 1, Training Loss 0.4826718846245495\n",
      "2022-03-26 22:56:27.086741 Epoch 1, Training Loss 0.4853651438222822\n",
      "2022-03-26 22:56:27.103224 Epoch 1, Training Loss 0.4881253291273971\n",
      "2022-03-26 22:56:27.121237 Epoch 1, Training Loss 0.4908362578248124\n",
      "2022-03-26 22:56:27.137170 Epoch 1, Training Loss 0.4936593499634882\n",
      "2022-03-26 22:56:27.154212 Epoch 1, Training Loss 0.49647722341825284\n",
      "2022-03-26 22:56:27.171234 Epoch 1, Training Loss 0.4993268047147395\n",
      "2022-03-26 22:56:27.187877 Epoch 1, Training Loss 0.5020816036502419\n",
      "2022-03-26 22:56:27.204864 Epoch 1, Training Loss 0.5047765927546469\n",
      "2022-03-26 22:56:27.220886 Epoch 1, Training Loss 0.5075335240425052\n",
      "2022-03-26 22:56:27.238904 Epoch 1, Training Loss 0.5103087486208552\n",
      "2022-03-26 22:56:27.254899 Epoch 1, Training Loss 0.5130619057608993\n",
      "2022-03-26 22:56:27.278038 Epoch 1, Training Loss 0.5157486868026616\n",
      "2022-03-26 22:56:27.302163 Epoch 1, Training Loss 0.5185375420943551\n",
      "2022-03-26 22:56:27.323123 Epoch 1, Training Loss 0.5213266336704458\n",
      "2022-03-26 22:56:27.337990 Epoch 1, Training Loss 0.5241674459193979\n",
      "2022-03-26 22:56:27.353954 Epoch 1, Training Loss 0.5268725229956001\n",
      "2022-03-26 22:56:27.373989 Epoch 1, Training Loss 0.5295636458774967\n",
      "2022-03-26 22:56:27.397012 Epoch 1, Training Loss 0.532210871386711\n",
      "2022-03-26 22:56:27.425072 Epoch 1, Training Loss 0.5347633319132773\n",
      "2022-03-26 22:56:27.445132 Epoch 1, Training Loss 0.5375062074807598\n",
      "2022-03-26 22:56:27.461178 Epoch 1, Training Loss 0.5401748327343056\n",
      "2022-03-26 22:56:27.501098 Epoch 1, Training Loss 0.5428120980177389\n",
      "2022-03-26 22:56:27.524103 Epoch 1, Training Loss 0.5455440990150432\n",
      "2022-03-26 22:56:27.538744 Epoch 1, Training Loss 0.5482198438985878\n",
      "2022-03-26 22:56:27.555199 Epoch 1, Training Loss 0.550933997954249\n",
      "2022-03-26 22:56:27.570033 Epoch 1, Training Loss 0.553581867986323\n",
      "2022-03-26 22:56:27.587969 Epoch 1, Training Loss 0.5563673018799413\n",
      "2022-03-26 22:56:27.605901 Epoch 1, Training Loss 0.5590701356263417\n",
      "2022-03-26 22:56:27.620900 Epoch 1, Training Loss 0.5619093210191068\n",
      "2022-03-26 22:56:27.636863 Epoch 1, Training Loss 0.5646705962812809\n",
      "2022-03-26 22:56:27.658313 Epoch 1, Training Loss 0.5673731132541471\n",
      "2022-03-26 22:56:27.674147 Epoch 1, Training Loss 0.5700396278020367\n",
      "2022-03-26 22:56:27.689378 Epoch 1, Training Loss 0.5727167364276583\n",
      "2022-03-26 22:56:27.705071 Epoch 1, Training Loss 0.5756464202690612\n",
      "2022-03-26 22:56:27.721194 Epoch 1, Training Loss 0.5782982489032209\n",
      "2022-03-26 22:56:27.737824 Epoch 1, Training Loss 0.5810280471201748\n",
      "2022-03-26 22:56:27.755840 Epoch 1, Training Loss 0.5836714054922314\n",
      "2022-03-26 22:56:27.770804 Epoch 1, Training Loss 0.5863566499232026\n",
      "2022-03-26 22:56:27.787215 Epoch 1, Training Loss 0.5891605816838686\n",
      "2022-03-26 22:56:27.804863 Epoch 1, Training Loss 0.5918911155837271\n",
      "2022-03-26 22:56:27.820882 Epoch 1, Training Loss 0.5946173984986132\n",
      "2022-03-26 22:56:27.838798 Epoch 1, Training Loss 0.597383418046605\n",
      "2022-03-26 22:56:27.858110 Epoch 1, Training Loss 0.6001227505676582\n",
      "2022-03-26 22:56:27.873086 Epoch 1, Training Loss 0.6029010386113316\n",
      "2022-03-26 22:56:27.889059 Epoch 1, Training Loss 0.6055650659229445\n",
      "2022-03-26 22:56:27.905915 Epoch 1, Training Loss 0.6082775559266815\n",
      "2022-03-26 22:56:27.921909 Epoch 1, Training Loss 0.6109638342162227\n",
      "2022-03-26 22:56:27.937913 Epoch 1, Training Loss 0.6136589858233167\n",
      "2022-03-26 22:56:27.958922 Epoch 1, Training Loss 0.6163120474047064\n",
      "2022-03-26 22:56:27.974934 Epoch 1, Training Loss 0.618978575672335\n",
      "2022-03-26 22:56:27.989932 Epoch 1, Training Loss 0.6216267966248495\n",
      "2022-03-26 22:56:28.007841 Epoch 1, Training Loss 0.6244056419948177\n",
      "2022-03-26 22:56:28.023848 Epoch 1, Training Loss 0.6271442077348909\n",
      "2022-03-26 22:56:28.041506 Epoch 1, Training Loss 0.629922224127728\n",
      "2022-03-26 22:56:28.059737 Epoch 1, Training Loss 0.6326160540666117\n",
      "2022-03-26 22:56:28.077745 Epoch 1, Training Loss 0.6352969676332401\n",
      "2022-03-26 22:56:28.092745 Epoch 1, Training Loss 0.6380355952645812\n",
      "2022-03-26 22:56:28.108185 Epoch 1, Training Loss 0.6407944570721873\n",
      "2022-03-26 22:56:28.124846 Epoch 1, Training Loss 0.6434848817717999\n",
      "2022-03-26 22:56:28.139850 Epoch 1, Training Loss 0.6461348801927493\n",
      "2022-03-26 22:56:28.159804 Epoch 1, Training Loss 0.6487611233425872\n",
      "2022-03-26 22:56:28.175983 Epoch 1, Training Loss 0.6515605428334698\n",
      "2022-03-26 22:56:28.191364 Epoch 1, Training Loss 0.6543587696216905\n",
      "2022-03-26 22:56:28.207067 Epoch 1, Training Loss 0.6569628084407133\n",
      "2022-03-26 22:56:28.223144 Epoch 1, Training Loss 0.6596246253498985\n",
      "2022-03-26 22:56:28.237986 Epoch 1, Training Loss 0.6622832745237424\n",
      "2022-03-26 22:56:28.258993 Epoch 1, Training Loss 0.6650854285110903\n",
      "2022-03-26 22:56:28.275219 Epoch 1, Training Loss 0.6678467281639119\n",
      "2022-03-26 22:56:28.290489 Epoch 1, Training Loss 0.6705224129854871\n",
      "2022-03-26 22:56:28.308793 Epoch 1, Training Loss 0.6731252042228913\n",
      "2022-03-26 22:56:28.323675 Epoch 1, Training Loss 0.6757832073494602\n",
      "2022-03-26 22:56:28.340681 Epoch 1, Training Loss 0.6784472956376917\n",
      "2022-03-26 22:56:28.356682 Epoch 1, Training Loss 0.6810568061935932\n",
      "2022-03-26 22:56:28.372055 Epoch 1, Training Loss 0.6837467324093479\n",
      "2022-03-26 22:56:28.388050 Epoch 1, Training Loss 0.6865010133484746\n",
      "2022-03-26 22:56:28.404957 Epoch 1, Training Loss 0.6891528419826342\n",
      "2022-03-26 22:56:28.421355 Epoch 1, Training Loss 0.6917980043479549\n",
      "2022-03-26 22:56:28.438358 Epoch 1, Training Loss 0.6944680841987395\n",
      "2022-03-26 22:56:28.455362 Epoch 1, Training Loss 0.6970214279716277\n",
      "2022-03-26 22:56:28.476369 Epoch 1, Training Loss 0.6996601304739637\n",
      "2022-03-26 22:56:28.492269 Epoch 1, Training Loss 0.7022300175083872\n",
      "2022-03-26 22:56:28.510510 Epoch 1, Training Loss 0.7049198943330809\n",
      "2022-03-26 22:56:28.528114 Epoch 1, Training Loss 0.7076640677878924\n",
      "2022-03-26 22:56:28.546013 Epoch 1, Training Loss 0.7103224102493442\n",
      "2022-03-26 22:56:28.561220 Epoch 1, Training Loss 0.7130431848413804\n",
      "2022-03-26 22:56:28.578169 Epoch 1, Training Loss 0.7157071926404753\n",
      "2022-03-26 22:56:28.593155 Epoch 1, Training Loss 0.7183883312108267\n",
      "2022-03-26 22:56:28.608165 Epoch 1, Training Loss 0.7210568875607932\n",
      "2022-03-26 22:56:28.626519 Epoch 1, Training Loss 0.7237644177263655\n",
      "2022-03-26 22:56:28.643149 Epoch 1, Training Loss 0.726381866828255\n",
      "2022-03-26 22:56:28.661265 Epoch 1, Training Loss 0.7291480132076137\n",
      "2022-03-26 22:56:28.677144 Epoch 1, Training Loss 0.731870207335333\n",
      "2022-03-26 22:56:28.692050 Epoch 1, Training Loss 0.7344565144585221\n",
      "2022-03-26 22:56:28.709056 Epoch 1, Training Loss 0.7369819117324127\n",
      "2022-03-26 22:56:28.723972 Epoch 1, Training Loss 0.7395784406710768\n",
      "2022-03-26 22:56:28.738974 Epoch 1, Training Loss 0.7421674063748411\n",
      "2022-03-26 22:56:28.758859 Epoch 1, Training Loss 0.7448424649665423\n",
      "2022-03-26 22:56:28.773775 Epoch 1, Training Loss 0.7474665047262635\n",
      "2022-03-26 22:56:28.788732 Epoch 1, Training Loss 0.7500945719916498\n",
      "2022-03-26 22:56:28.804736 Epoch 1, Training Loss 0.7528711307384169\n",
      "2022-03-26 22:56:28.820753 Epoch 1, Training Loss 0.7554570338915071\n",
      "2022-03-26 22:56:28.837732 Epoch 1, Training Loss 0.7581650046131495\n",
      "2022-03-26 22:56:28.854622 Epoch 1, Training Loss 0.7607789097539605\n",
      "2022-03-26 22:56:28.870626 Epoch 1, Training Loss 0.7634059516975032\n",
      "2022-03-26 22:56:28.887082 Epoch 1, Training Loss 0.7661261616460503\n",
      "2022-03-26 22:56:28.906094 Epoch 1, Training Loss 0.7687404186219511\n",
      "2022-03-26 22:56:28.922078 Epoch 1, Training Loss 0.7713209107098982\n",
      "2022-03-26 22:56:28.938064 Epoch 1, Training Loss 0.7739926613200351\n",
      "2022-03-26 22:56:28.955709 Epoch 1, Training Loss 0.7765771693280895\n",
      "2022-03-26 22:56:28.971712 Epoch 1, Training Loss 0.7792406923630658\n",
      "2022-03-26 22:56:28.987819 Epoch 1, Training Loss 0.7819553281340148\n",
      "2022-03-26 22:56:29.003872 Epoch 1, Training Loss 0.7846396871844826\n",
      "2022-03-26 22:56:29.021816 Epoch 1, Training Loss 0.787224188485109\n",
      "2022-03-26 22:56:29.037251 Epoch 1, Training Loss 0.7899323351242963\n",
      "2022-03-26 22:56:29.056855 Epoch 1, Training Loss 0.7926761320484873\n",
      "2022-03-26 22:56:29.072815 Epoch 1, Training Loss 0.7952573610388715\n",
      "2022-03-26 22:56:29.087808 Epoch 1, Training Loss 0.797845630084767\n",
      "2022-03-26 22:56:29.104769 Epoch 1, Training Loss 0.8005987201505305\n",
      "2022-03-26 22:56:29.121764 Epoch 1, Training Loss 0.8032261974671308\n",
      "2022-03-26 22:56:29.138687 Epoch 1, Training Loss 0.8058532461181016\n",
      "2022-03-26 22:56:29.155704 Epoch 1, Training Loss 0.8085171990382397\n",
      "2022-03-26 22:56:29.170715 Epoch 1, Training Loss 0.8112068411029513\n",
      "2022-03-26 22:56:29.187718 Epoch 1, Training Loss 0.8138269088457307\n",
      "2022-03-26 22:56:29.204043 Epoch 1, Training Loss 0.8164873580493586\n",
      "2022-03-26 22:56:29.221714 Epoch 1, Training Loss 0.8191028126060506\n",
      "2022-03-26 22:56:29.236694 Epoch 1, Training Loss 0.8218911928898843\n",
      "2022-03-26 22:56:29.255692 Epoch 1, Training Loss 0.8245925482581643\n",
      "2022-03-26 22:56:29.271709 Epoch 1, Training Loss 0.827254017905506\n",
      "2022-03-26 22:56:29.287714 Epoch 1, Training Loss 0.8300047917744083\n",
      "2022-03-26 22:56:29.304745 Epoch 1, Training Loss 0.8325869085843606\n",
      "2022-03-26 22:56:29.320754 Epoch 1, Training Loss 0.835216887771626\n",
      "2022-03-26 22:56:29.336829 Epoch 1, Training Loss 0.8380294221136576\n",
      "2022-03-26 22:56:29.353844 Epoch 1, Training Loss 0.8406121316163436\n",
      "2022-03-26 22:56:29.369747 Epoch 1, Training Loss 0.8432093334319951\n",
      "2022-03-26 22:56:29.388182 Epoch 1, Training Loss 0.8458516268474062\n",
      "2022-03-26 22:56:29.403992 Epoch 1, Training Loss 0.848479458133278\n",
      "2022-03-26 22:56:29.421034 Epoch 1, Training Loss 0.8512764849016429\n",
      "2022-03-26 22:56:29.436881 Epoch 1, Training Loss 0.8539045997287916\n",
      "2022-03-26 22:56:29.455859 Epoch 1, Training Loss 0.8565542018017196\n",
      "2022-03-26 22:56:29.471799 Epoch 1, Training Loss 0.8591715884025749\n",
      "2022-03-26 22:56:29.489801 Epoch 1, Training Loss 0.8618101457805585\n",
      "2022-03-26 22:56:29.504801 Epoch 1, Training Loss 0.8644107586282599\n",
      "2022-03-26 22:56:29.520823 Epoch 1, Training Loss 0.8670129108307002\n",
      "2022-03-26 22:56:29.536862 Epoch 1, Training Loss 0.8695261377812652\n",
      "2022-03-26 22:56:29.555857 Epoch 1, Training Loss 0.8721828811308917\n",
      "2022-03-26 22:56:29.571801 Epoch 1, Training Loss 0.8748022860578258\n",
      "2022-03-26 22:56:29.587740 Epoch 1, Training Loss 0.877376927134326\n",
      "2022-03-26 22:56:29.603744 Epoch 1, Training Loss 0.8799311311348624\n",
      "2022-03-26 22:56:29.622737 Epoch 1, Training Loss 0.8825883763220609\n",
      "2022-03-26 22:56:29.637739 Epoch 1, Training Loss 0.8852756927385355\n",
      "2022-03-26 22:56:29.655701 Epoch 1, Training Loss 0.8880344085071398\n",
      "2022-03-26 22:56:29.670704 Epoch 1, Training Loss 0.890700990739076\n",
      "2022-03-26 22:56:29.688020 Epoch 1, Training Loss 0.8933816982047332\n",
      "2022-03-26 22:56:29.704956 Epoch 1, Training Loss 0.8961949618271244\n",
      "2022-03-26 22:56:29.720881 Epoch 1, Training Loss 0.8988500051486218\n",
      "2022-03-26 22:56:29.737884 Epoch 1, Training Loss 0.9013987832971851\n",
      "2022-03-26 22:56:29.755862 Epoch 1, Training Loss 0.9040405660334145\n",
      "2022-03-26 22:56:29.770881 Epoch 1, Training Loss 0.9066933699885903\n",
      "2022-03-26 22:56:29.786879 Epoch 1, Training Loss 0.9092646061307024\n",
      "2022-03-26 22:56:29.803807 Epoch 1, Training Loss 0.9119022357494325\n",
      "2022-03-26 22:56:29.820804 Epoch 1, Training Loss 0.9146037511813366\n",
      "2022-03-26 22:56:29.836195 Epoch 1, Training Loss 0.9173145277420883\n",
      "2022-03-26 22:56:29.856970 Epoch 1, Training Loss 0.9199838103235835\n",
      "2022-03-26 22:56:29.873063 Epoch 1, Training Loss 0.922657829568819\n",
      "2022-03-26 22:56:29.887033 Epoch 1, Training Loss 0.9253482905495197\n",
      "2022-03-26 22:56:29.903981 Epoch 1, Training Loss 0.9279603612087571\n",
      "2022-03-26 22:56:29.920991 Epoch 1, Training Loss 0.9307168958437108\n",
      "2022-03-26 22:56:29.938002 Epoch 1, Training Loss 0.9332958287595178\n",
      "2022-03-26 22:56:29.955828 Epoch 1, Training Loss 0.93594948882642\n",
      "2022-03-26 22:56:29.970812 Epoch 1, Training Loss 0.93860629559173\n",
      "2022-03-26 22:56:29.986761 Epoch 1, Training Loss 0.9411792104201548\n",
      "2022-03-26 22:56:30.004612 Epoch 1, Training Loss 0.9437288071798242\n",
      "2022-03-26 22:56:30.021632 Epoch 1, Training Loss 0.9463623693532042\n",
      "2022-03-26 22:56:30.038065 Epoch 1, Training Loss 0.948959758367075\n",
      "2022-03-26 22:56:30.056989 Epoch 1, Training Loss 0.9515531854251461\n",
      "2022-03-26 22:56:30.072871 Epoch 1, Training Loss 0.9540750254755435\n",
      "2022-03-26 22:56:30.087864 Epoch 1, Training Loss 0.9566283299185127\n",
      "2022-03-26 22:56:30.103840 Epoch 1, Training Loss 0.959183929673851\n",
      "2022-03-26 22:56:30.122828 Epoch 1, Training Loss 0.9617574082311157\n",
      "2022-03-26 22:56:30.138752 Epoch 1, Training Loss 0.9644308091734376\n",
      "2022-03-26 22:56:30.154771 Epoch 1, Training Loss 0.966890873811434\n",
      "2022-03-26 22:56:30.171397 Epoch 1, Training Loss 0.9694693606832753\n",
      "2022-03-26 22:56:30.187094 Epoch 1, Training Loss 0.9720960690847138\n",
      "2022-03-26 22:56:30.204013 Epoch 1, Training Loss 0.9747494209148085\n",
      "2022-03-26 22:56:30.221002 Epoch 1, Training Loss 0.9773002416276566\n",
      "2022-03-26 22:56:30.237972 Epoch 1, Training Loss 0.9799360062765039\n",
      "2022-03-26 22:56:30.253971 Epoch 1, Training Loss 0.9823298100620279\n",
      "2022-03-26 22:56:30.270975 Epoch 1, Training Loss 0.9849375413201958\n",
      "2022-03-26 22:56:30.291800 Epoch 1, Training Loss 0.9875342016634734\n",
      "2022-03-26 22:56:30.309982 Epoch 1, Training Loss 0.9901716550597754\n",
      "2022-03-26 22:56:30.324150 Epoch 1, Training Loss 0.9927539151647816\n",
      "2022-03-26 22:56:30.342821 Epoch 1, Training Loss 0.9952463862841087\n",
      "2022-03-26 22:56:30.359278 Epoch 1, Training Loss 0.9978758950367608\n",
      "2022-03-26 22:56:30.373732 Epoch 1, Training Loss 1.0005577686802505\n",
      "2022-03-26 22:56:30.390689 Epoch 1, Training Loss 1.0030200597270371\n",
      "2022-03-26 22:56:30.406693 Epoch 1, Training Loss 1.0055826929829004\n",
      "2022-03-26 22:56:30.423635 Epoch 1, Training Loss 1.0080313345660334\n",
      "2022-03-26 22:56:30.439242 Epoch 1, Training Loss 1.0106066088847188\n",
      "2022-03-26 22:56:30.456275 Epoch 1, Training Loss 1.0132482653993475\n",
      "2022-03-26 22:56:30.473131 Epoch 1, Training Loss 1.0157499127375804\n",
      "2022-03-26 22:56:30.488150 Epoch 1, Training Loss 1.018390429904089\n",
      "2022-03-26 22:56:30.504329 Epoch 1, Training Loss 1.0209379307449322\n",
      "2022-03-26 22:56:30.522284 Epoch 1, Training Loss 1.0236037873551058\n",
      "2022-03-26 22:56:30.537727 Epoch 1, Training Loss 1.026245461248071\n",
      "2022-03-26 22:56:30.555827 Epoch 1, Training Loss 1.02887362241745\n",
      "2022-03-26 22:56:30.571830 Epoch 1, Training Loss 1.0315346667528762\n",
      "2022-03-26 22:56:30.587855 Epoch 1, Training Loss 1.0340416219533253\n",
      "2022-03-26 22:56:30.604859 Epoch 1, Training Loss 1.0367172497617618\n",
      "2022-03-26 22:56:30.620845 Epoch 1, Training Loss 1.039322920315101\n",
      "2022-03-26 22:56:30.637851 Epoch 1, Training Loss 1.0418841047665042\n",
      "2022-03-26 22:56:30.654345 Epoch 1, Training Loss 1.0444874310737375\n",
      "2022-03-26 22:56:30.671365 Epoch 1, Training Loss 1.0471725023311118\n",
      "2022-03-26 22:56:30.687225 Epoch 1, Training Loss 1.0498556165439088\n",
      "2022-03-26 22:56:30.703865 Epoch 1, Training Loss 1.052377094088308\n",
      "2022-03-26 22:56:30.720881 Epoch 1, Training Loss 1.0550052909290089\n",
      "2022-03-26 22:56:30.736906 Epoch 1, Training Loss 1.0576389332866425\n",
      "2022-03-26 22:56:30.754889 Epoch 1, Training Loss 1.0601921400145802\n",
      "2022-03-26 22:56:30.771957 Epoch 1, Training Loss 1.062716245346362\n",
      "2022-03-26 22:56:30.787984 Epoch 1, Training Loss 1.0654380120279845\n",
      "2022-03-26 22:56:30.804980 Epoch 1, Training Loss 1.0679793548401055\n",
      "2022-03-26 22:56:30.821870 Epoch 1, Training Loss 1.0706175951396717\n",
      "2022-03-26 22:56:30.836887 Epoch 1, Training Loss 1.0732330381108062\n",
      "2022-03-26 22:56:30.855892 Epoch 1, Training Loss 1.0758804572207847\n",
      "2022-03-26 22:56:30.871864 Epoch 1, Training Loss 1.078307170849627\n",
      "2022-03-26 22:56:30.888880 Epoch 1, Training Loss 1.0808166863058535\n",
      "2022-03-26 22:56:30.905081 Epoch 1, Training Loss 1.0834354464050449\n",
      "2022-03-26 22:56:30.921061 Epoch 1, Training Loss 1.0859742060951565\n",
      "2022-03-26 22:56:30.935461 Epoch 1, Training Loss 1.0886262710137136\n",
      "2022-03-26 22:56:30.954880 Epoch 1, Training Loss 1.0912277372291936\n",
      "2022-03-26 22:56:30.969824 Epoch 1, Training Loss 1.0938308196299522\n",
      "2022-03-26 22:56:30.988814 Epoch 1, Training Loss 1.0964198338101283\n",
      "2022-03-26 22:56:31.003249 Epoch 1, Training Loss 1.0989347870087685\n",
      "2022-03-26 22:56:31.021834 Epoch 1, Training Loss 1.1014933421483735\n",
      "2022-03-26 22:56:31.036820 Epoch 1, Training Loss 1.1040282071094074\n",
      "2022-03-26 22:56:31.055811 Epoch 1, Training Loss 1.1065378039694198\n",
      "2022-03-26 22:56:31.091394 Epoch 1, Training Loss 1.1091951470240913\n",
      "2022-03-26 22:56:31.124386 Epoch 1, Training Loss 1.1116137525919454\n",
      "2022-03-26 22:56:31.155015 Epoch 1, Training Loss 1.1141779733740764\n",
      "2022-03-26 22:56:31.171964 Epoch 1, Training Loss 1.1167399886319094\n",
      "2022-03-26 22:56:31.187967 Epoch 1, Training Loss 1.1194124441317586\n",
      "2022-03-26 22:56:31.204994 Epoch 1, Training Loss 1.1221304814833815\n",
      "2022-03-26 22:56:31.226006 Epoch 1, Training Loss 1.1247194413943669\n",
      "2022-03-26 22:56:31.247717 Epoch 1, Training Loss 1.1273592061094007\n",
      "2022-03-26 22:56:31.265958 Epoch 1, Training Loss 1.1299164005557594\n",
      "2022-03-26 22:56:31.285967 Epoch 1, Training Loss 1.1324782246518927\n",
      "2022-03-26 22:56:31.303970 Epoch 1, Training Loss 1.135190033546799\n",
      "2022-03-26 22:56:31.321100 Epoch 1, Training Loss 1.137730384574217\n",
      "2022-03-26 22:56:31.337958 Epoch 1, Training Loss 1.1403149245949962\n",
      "2022-03-26 22:56:31.354005 Epoch 1, Training Loss 1.1428548690608091\n",
      "2022-03-26 22:56:31.370979 Epoch 1, Training Loss 1.1456017919513575\n",
      "2022-03-26 22:56:31.386868 Epoch 1, Training Loss 1.148266776595884\n",
      "2022-03-26 22:56:31.404873 Epoch 1, Training Loss 1.1506733706845043\n",
      "2022-03-26 22:56:31.420538 Epoch 1, Training Loss 1.1532158186978392\n",
      "2022-03-26 22:56:31.437350 Epoch 1, Training Loss 1.155785236822065\n",
      "2022-03-26 22:56:31.455007 Epoch 1, Training Loss 1.1582410248649089\n",
      "2022-03-26 22:56:31.469996 Epoch 1, Training Loss 1.1608227108750502\n",
      "2022-03-26 22:56:31.487988 Epoch 1, Training Loss 1.1634801021012504\n",
      "2022-03-26 22:56:31.502999 Epoch 1, Training Loss 1.1661348804793394\n",
      "2022-03-26 22:56:31.520982 Epoch 1, Training Loss 1.1687401995024718\n",
      "2022-03-26 22:56:31.537985 Epoch 1, Training Loss 1.1713297600331514\n",
      "2022-03-26 22:56:31.555077 Epoch 1, Training Loss 1.173829263128588\n",
      "2022-03-26 22:56:31.571097 Epoch 1, Training Loss 1.1763082196950303\n",
      "2022-03-26 22:56:31.586981 Epoch 1, Training Loss 1.1788385858011368\n",
      "2022-03-26 22:56:31.604995 Epoch 1, Training Loss 1.1813555576307389\n",
      "2022-03-26 22:56:31.621438 Epoch 1, Training Loss 1.1840158717711564\n",
      "2022-03-26 22:56:31.637204 Epoch 1, Training Loss 1.1866634500301099\n",
      "2022-03-26 22:56:31.653885 Epoch 1, Training Loss 1.1891500644976525\n",
      "2022-03-26 22:56:31.671024 Epoch 1, Training Loss 1.191557324146066\n",
      "2022-03-26 22:56:31.687875 Epoch 1, Training Loss 1.1942263460525162\n",
      "2022-03-26 22:56:31.703833 Epoch 1, Training Loss 1.1967893601073634\n",
      "2022-03-26 22:56:31.720874 Epoch 1, Training Loss 1.1991128505343367\n",
      "2022-03-26 22:56:31.736812 Epoch 1, Training Loss 1.2015764473954125\n",
      "2022-03-26 22:56:31.754807 Epoch 1, Training Loss 1.2041203475669218\n",
      "2022-03-26 22:56:31.770772 Epoch 1, Training Loss 1.2067986293826871\n",
      "2022-03-26 22:56:31.787780 Epoch 1, Training Loss 1.2093640940878398\n",
      "2022-03-26 22:56:31.803984 Epoch 1, Training Loss 1.2120587688577755\n",
      "2022-03-26 22:56:31.820984 Epoch 1, Training Loss 1.2148050771040075\n",
      "2022-03-26 22:56:31.836876 Epoch 1, Training Loss 1.2173870865951109\n",
      "2022-03-26 22:56:31.854865 Epoch 1, Training Loss 1.2200965869152331\n",
      "2022-03-26 22:56:31.870871 Epoch 1, Training Loss 1.2226776927328475\n",
      "2022-03-26 22:56:31.887875 Epoch 1, Training Loss 1.225168273576995\n",
      "2022-03-26 22:56:31.903900 Epoch 1, Training Loss 1.2276983248913074\n",
      "2022-03-26 22:56:31.921867 Epoch 1, Training Loss 1.2301846542931578\n",
      "2022-03-26 22:56:31.938456 Epoch 1, Training Loss 1.2326609208760664\n",
      "2022-03-26 22:56:31.954101 Epoch 1, Training Loss 1.2350176435602291\n",
      "2022-03-26 22:56:31.970058 Epoch 1, Training Loss 1.2375362135870072\n",
      "2022-03-26 22:56:31.988057 Epoch 1, Training Loss 1.239846212632211\n",
      "2022-03-26 22:56:32.004075 Epoch 1, Training Loss 1.242530793637571\n",
      "2022-03-26 22:56:32.021085 Epoch 1, Training Loss 1.2450561578316457\n",
      "2022-03-26 22:56:32.036882 Epoch 1, Training Loss 1.2475637477986954\n",
      "2022-03-26 22:56:32.054564 Epoch 1, Training Loss 1.2500079807722966\n",
      "2022-03-26 22:56:32.069562 Epoch 1, Training Loss 1.2526246475441682\n",
      "2022-03-26 22:56:32.087064 Epoch 1, Training Loss 1.2551931577265416\n",
      "2022-03-26 22:56:32.104069 Epoch 1, Training Loss 1.257751378409393\n",
      "2022-03-26 22:56:32.121094 Epoch 1, Training Loss 1.260477710105574\n",
      "2022-03-26 22:56:32.138112 Epoch 1, Training Loss 1.2632508694058489\n",
      "2022-03-26 22:56:32.154116 Epoch 1, Training Loss 1.2657277724322151\n",
      "2022-03-26 22:56:32.172082 Epoch 1, Training Loss 1.268230641284562\n",
      "2022-03-26 22:56:32.188393 Epoch 1, Training Loss 1.2708569490696158\n",
      "2022-03-26 22:56:32.204063 Epoch 1, Training Loss 1.273479181482359\n",
      "2022-03-26 22:56:32.221993 Epoch 1, Training Loss 1.2758849180872789\n",
      "2022-03-26 22:56:32.237071 Epoch 1, Training Loss 1.278305481309476\n",
      "2022-03-26 22:56:32.254864 Epoch 1, Training Loss 1.2808711969334146\n",
      "2022-03-26 22:56:32.269894 Epoch 1, Training Loss 1.2835304256900193\n",
      "2022-03-26 22:56:32.287883 Epoch 1, Training Loss 1.2860629698809456\n",
      "2022-03-26 22:56:32.303896 Epoch 1, Training Loss 1.2886503992788017\n",
      "2022-03-26 22:56:32.321870 Epoch 1, Training Loss 1.2913185135482828\n",
      "2022-03-26 22:56:32.337873 Epoch 1, Training Loss 1.2939145723572167\n",
      "2022-03-26 22:56:32.363299 Epoch 1, Training Loss 1.296449782598354\n",
      "2022-03-26 22:56:32.383289 Epoch 1, Training Loss 1.2988392895140002\n",
      "2022-03-26 22:56:32.402956 Epoch 1, Training Loss 1.3014662930422731\n",
      "2022-03-26 22:56:32.419997 Epoch 1, Training Loss 1.3039997402969223\n",
      "2022-03-26 22:56:32.437859 Epoch 1, Training Loss 1.3064167051364088\n",
      "2022-03-26 22:56:32.458048 Epoch 1, Training Loss 1.3091944968304061\n",
      "2022-03-26 22:56:32.478911 Epoch 1, Training Loss 1.3118809132319886\n",
      "2022-03-26 22:56:32.504901 Epoch 1, Training Loss 1.314458320512796\n",
      "2022-03-26 22:56:32.520859 Epoch 1, Training Loss 1.3169540442774057\n",
      "2022-03-26 22:56:32.537864 Epoch 1, Training Loss 1.3194464945122408\n",
      "2022-03-26 22:56:32.569873 Epoch 1, Training Loss 1.3219326277218206\n",
      "2022-03-26 22:56:32.592071 Epoch 1, Training Loss 1.3244480025737793\n",
      "2022-03-26 22:56:32.606960 Epoch 1, Training Loss 1.3270262650516638\n",
      "2022-03-26 22:56:32.623972 Epoch 1, Training Loss 1.3295353423908849\n",
      "2022-03-26 22:56:32.639045 Epoch 1, Training Loss 1.3319610034108467\n",
      "2022-03-26 22:56:32.655092 Epoch 1, Training Loss 1.3345662925554358\n",
      "2022-03-26 22:56:32.670818 Epoch 1, Training Loss 1.3370075187719692\n",
      "2022-03-26 22:56:32.687852 Epoch 1, Training Loss 1.3395449795076608\n",
      "2022-03-26 22:56:32.705298 Epoch 1, Training Loss 1.3419612313780334\n",
      "2022-03-26 22:56:32.720718 Epoch 1, Training Loss 1.344375349524076\n",
      "2022-03-26 22:56:32.740243 Epoch 1, Training Loss 1.3470604110251911\n",
      "2022-03-26 22:56:32.755268 Epoch 1, Training Loss 1.349451250889722\n",
      "2022-03-26 22:56:32.770245 Epoch 1, Training Loss 1.3520774577584718\n",
      "2022-03-26 22:56:32.787889 Epoch 1, Training Loss 1.3546301759112522\n",
      "2022-03-26 22:56:32.803912 Epoch 1, Training Loss 1.3570707164457083\n",
      "2022-03-26 22:56:32.821906 Epoch 1, Training Loss 1.3596614202879884\n",
      "2022-03-26 22:56:32.836915 Epoch 1, Training Loss 1.362175337188994\n",
      "2022-03-26 22:56:32.855224 Epoch 1, Training Loss 1.3647080164431307\n",
      "2022-03-26 22:56:32.871123 Epoch 1, Training Loss 1.367232395407489\n",
      "2022-03-26 22:56:32.887947 Epoch 1, Training Loss 1.3696969427416086\n",
      "2022-03-26 22:56:32.905004 Epoch 1, Training Loss 1.3721503340984549\n",
      "2022-03-26 22:56:32.922008 Epoch 1, Training Loss 1.3747915738378949\n",
      "2022-03-26 22:56:32.938025 Epoch 1, Training Loss 1.3772990814865094\n",
      "2022-03-26 22:56:32.954007 Epoch 1, Training Loss 1.3797842142222179\n",
      "2022-03-26 22:56:32.971149 Epoch 1, Training Loss 1.3822941600209306\n",
      "2022-03-26 22:56:32.988162 Epoch 1, Training Loss 1.3847928214865877\n",
      "2022-03-26 22:56:33.003817 Epoch 1, Training Loss 1.3875375272672805\n",
      "2022-03-26 22:56:33.021823 Epoch 1, Training Loss 1.3900729465057782\n",
      "2022-03-26 22:56:33.036763 Epoch 1, Training Loss 1.3926540742749753\n",
      "2022-03-26 22:56:33.054769 Epoch 1, Training Loss 1.3952801162019715\n",
      "2022-03-26 22:56:33.070338 Epoch 1, Training Loss 1.3978406350935817\n",
      "2022-03-26 22:56:33.087967 Epoch 1, Training Loss 1.4003481808525826\n",
      "2022-03-26 22:56:33.104004 Epoch 1, Training Loss 1.4028172018887746\n",
      "2022-03-26 22:56:33.122290 Epoch 1, Training Loss 1.405263183366917\n",
      "2022-03-26 22:56:33.137996 Epoch 1, Training Loss 1.407933600418403\n",
      "2022-03-26 22:56:33.154028 Epoch 1, Training Loss 1.410566826610614\n",
      "2022-03-26 22:56:33.170611 Epoch 1, Training Loss 1.4128220418225164\n",
      "2022-03-26 22:56:33.186582 Epoch 1, Training Loss 1.415180772009408\n",
      "2022-03-26 22:56:33.203713 Epoch 1, Training Loss 1.417664998937446\n",
      "2022-03-26 22:56:33.220727 Epoch 1, Training Loss 1.420220492136143\n",
      "2022-03-26 22:56:33.238729 Epoch 1, Training Loss 1.4227804271766291\n",
      "2022-03-26 22:56:33.254186 Epoch 1, Training Loss 1.4251883214392016\n",
      "2022-03-26 22:56:33.270753 Epoch 1, Training Loss 1.4277605591222757\n",
      "2022-03-26 22:56:33.287460 Epoch 1, Training Loss 1.4304037620039547\n",
      "2022-03-26 22:56:33.304073 Epoch 1, Training Loss 1.4328947652636281\n",
      "2022-03-26 22:56:33.322212 Epoch 1, Training Loss 1.4353635356859173\n",
      "2022-03-26 22:56:33.337947 Epoch 1, Training Loss 1.4379458485356986\n",
      "2022-03-26 22:56:33.353950 Epoch 1, Training Loss 1.4404366778595674\n",
      "2022-03-26 22:56:33.370954 Epoch 1, Training Loss 1.4429873482650504\n",
      "2022-03-26 22:56:33.386852 Epoch 1, Training Loss 1.4453698024725365\n",
      "2022-03-26 22:56:33.403877 Epoch 1, Training Loss 1.4478844170985015\n",
      "2022-03-26 22:56:33.421885 Epoch 1, Training Loss 1.450406144342154\n",
      "2022-03-26 22:56:33.437883 Epoch 1, Training Loss 1.4529796314361456\n",
      "2022-03-26 22:56:33.453811 Epoch 1, Training Loss 1.4555273100238322\n",
      "2022-03-26 22:56:33.471826 Epoch 1, Training Loss 1.4581663680198553\n",
      "2022-03-26 22:56:33.487738 Epoch 1, Training Loss 1.4606907180203197\n",
      "2022-03-26 22:56:33.505384 Epoch 1, Training Loss 1.4630627507139045\n",
      "2022-03-26 22:56:33.522307 Epoch 1, Training Loss 1.4655133066579813\n",
      "2022-03-26 22:56:33.541233 Epoch 1, Training Loss 1.4679645565159791\n",
      "2022-03-26 22:56:33.556825 Epoch 1, Training Loss 1.4704224817893083\n",
      "2022-03-26 22:56:33.571826 Epoch 1, Training Loss 1.4728251445628797\n",
      "2022-03-26 22:56:33.588749 Epoch 1, Training Loss 1.475330355832034\n",
      "2022-03-26 22:56:33.603641 Epoch 1, Training Loss 1.4777339344744183\n",
      "2022-03-26 22:56:33.623354 Epoch 1, Training Loss 1.4802575896463126\n",
      "2022-03-26 22:56:33.637114 Epoch 1, Training Loss 1.4829159555837625\n",
      "2022-03-26 22:56:33.655034 Epoch 1, Training Loss 1.4855078280429401\n",
      "2022-03-26 22:56:33.671003 Epoch 1, Training Loss 1.4881295808745771\n",
      "2022-03-26 22:56:33.688758 Epoch 1, Training Loss 1.4907018551436226\n",
      "2022-03-26 22:56:33.703865 Epoch 1, Training Loss 1.4931316166887503\n",
      "2022-03-26 22:56:33.720977 Epoch 1, Training Loss 1.4956253687743946\n",
      "2022-03-26 22:56:33.738856 Epoch 1, Training Loss 1.4979884254048244\n",
      "2022-03-26 22:56:33.754858 Epoch 1, Training Loss 1.5005695153685177\n",
      "2022-03-26 22:56:33.770889 Epoch 1, Training Loss 1.5031729764340784\n",
      "2022-03-26 22:56:33.788884 Epoch 1, Training Loss 1.5057297268182115\n",
      "2022-03-26 22:56:33.804001 Epoch 1, Training Loss 1.508282233382125\n",
      "2022-03-26 22:56:33.823474 Epoch 1, Training Loss 1.5108083469788436\n",
      "2022-03-26 22:56:33.838397 Epoch 1, Training Loss 1.5132431499183636\n",
      "2022-03-26 22:56:33.854181 Epoch 1, Training Loss 1.5156820465231795\n",
      "2022-03-26 22:56:33.871067 Epoch 1, Training Loss 1.5182724000547854\n",
      "2022-03-26 22:56:33.886745 Epoch 1, Training Loss 1.5208641868418136\n",
      "2022-03-26 22:56:33.903826 Epoch 1, Training Loss 1.5232876150504402\n",
      "2022-03-26 22:56:33.921866 Epoch 1, Training Loss 1.5257131792700198\n",
      "2022-03-26 22:56:33.937805 Epoch 1, Training Loss 1.5279974295660053\n",
      "2022-03-26 22:56:33.953425 Epoch 1, Training Loss 1.5305974939290214\n",
      "2022-03-26 22:56:33.971442 Epoch 1, Training Loss 1.533135758184106\n",
      "2022-03-26 22:56:33.988002 Epoch 1, Training Loss 1.5354761394393412\n",
      "2022-03-26 22:56:34.004974 Epoch 1, Training Loss 1.5379636839527608\n",
      "2022-03-26 22:56:34.020977 Epoch 1, Training Loss 1.5403872749689596\n",
      "2022-03-26 22:56:34.038861 Epoch 1, Training Loss 1.5429320274411564\n",
      "2022-03-26 22:56:34.053825 Epoch 1, Training Loss 1.5452788802973754\n",
      "2022-03-26 22:56:34.071778 Epoch 1, Training Loss 1.5477723361890945\n",
      "2022-03-26 22:56:34.088469 Epoch 1, Training Loss 1.5502832640162514\n",
      "2022-03-26 22:56:34.104407 Epoch 1, Training Loss 1.5527384207986505\n",
      "2022-03-26 22:56:34.122526 Epoch 1, Training Loss 1.5553386019318916\n",
      "2022-03-26 22:56:34.138080 Epoch 1, Training Loss 1.557798559555922\n",
      "2022-03-26 22:56:34.155210 Epoch 1, Training Loss 1.5603857941334816\n",
      "2022-03-26 22:56:34.171095 Epoch 1, Training Loss 1.562870114965512\n",
      "2022-03-26 22:56:34.188031 Epoch 1, Training Loss 1.5652964814849522\n",
      "2022-03-26 22:56:34.205051 Epoch 1, Training Loss 1.567607899456073\n",
      "2022-03-26 22:56:34.222289 Epoch 1, Training Loss 1.5699405496382652\n",
      "2022-03-26 22:56:34.236863 Epoch 1, Training Loss 1.5724155395232198\n",
      "2022-03-26 22:56:34.253884 Epoch 1, Training Loss 1.5749101588488235\n",
      "2022-03-26 22:56:34.270894 Epoch 1, Training Loss 1.5774061062451823\n",
      "2022-03-26 22:56:34.287864 Epoch 1, Training Loss 1.5798782044664368\n",
      "2022-03-26 22:56:34.303875 Epoch 1, Training Loss 1.5823855159227804\n",
      "2022-03-26 22:56:34.320893 Epoch 1, Training Loss 1.584970050150781\n",
      "2022-03-26 22:56:34.338902 Epoch 1, Training Loss 1.5874232928771193\n",
      "2022-03-26 22:56:34.354915 Epoch 1, Training Loss 1.5898124895742178\n",
      "2022-03-26 22:56:34.369918 Epoch 1, Training Loss 1.592460745283405\n",
      "2022-03-26 22:56:34.387930 Epoch 1, Training Loss 1.5949968631615115\n",
      "2022-03-26 22:56:34.403881 Epoch 1, Training Loss 1.5975307866435526\n",
      "2022-03-26 22:56:34.421860 Epoch 1, Training Loss 1.6001242475436472\n",
      "2022-03-26 22:56:34.437798 Epoch 1, Training Loss 1.602773862116782\n",
      "2022-03-26 22:56:34.455805 Epoch 1, Training Loss 1.6052265670293433\n",
      "2022-03-26 22:56:34.471805 Epoch 1, Training Loss 1.6075814995924225\n",
      "2022-03-26 22:56:34.487822 Epoch 1, Training Loss 1.6101384746753955\n",
      "2022-03-26 22:56:34.504474 Epoch 1, Training Loss 1.6125549855439558\n",
      "2022-03-26 22:56:34.521487 Epoch 1, Training Loss 1.615216292993492\n",
      "2022-03-26 22:56:34.538111 Epoch 1, Training Loss 1.617745273253497\n",
      "2022-03-26 22:56:34.554908 Epoch 1, Training Loss 1.6203802229498354\n",
      "2022-03-26 22:56:34.571925 Epoch 1, Training Loss 1.6230343818054784\n",
      "2022-03-26 22:56:34.588930 Epoch 1, Training Loss 1.6254694178281233\n",
      "2022-03-26 22:56:34.604857 Epoch 1, Training Loss 1.6278520552703486\n",
      "2022-03-26 22:56:34.622779 Epoch 1, Training Loss 1.6303252746991794\n",
      "2022-03-26 22:56:34.637682 Epoch 1, Training Loss 1.6327689180288778\n",
      "2022-03-26 22:56:34.654685 Epoch 1, Training Loss 1.6350842435341661\n",
      "2022-03-26 22:56:34.671716 Epoch 1, Training Loss 1.6375700755192495\n",
      "2022-03-26 22:56:34.687720 Epoch 1, Training Loss 1.6401236624364048\n",
      "2022-03-26 22:56:34.704638 Epoch 1, Training Loss 1.6426841105951373\n",
      "2022-03-26 22:56:34.719675 Epoch 1, Training Loss 1.6451458857797296\n",
      "2022-03-26 22:56:34.739017 Epoch 1, Training Loss 1.6476215047909475\n",
      "2022-03-26 22:56:34.754917 Epoch 1, Training Loss 1.6501177877111508\n",
      "2022-03-26 22:56:34.770937 Epoch 1, Training Loss 1.652762753121993\n",
      "2022-03-26 22:56:34.787925 Epoch 1, Training Loss 1.6552607664061934\n",
      "2022-03-26 22:56:34.803860 Epoch 1, Training Loss 1.6576558947563171\n",
      "2022-03-26 22:56:34.821865 Epoch 1, Training Loss 1.66029252527315\n",
      "2022-03-26 22:56:34.838752 Epoch 1, Training Loss 1.6628774287145767\n",
      "2022-03-26 22:56:34.855422 Epoch 1, Training Loss 1.6651808537180772\n",
      "2022-03-26 22:56:34.870441 Epoch 1, Training Loss 1.6675702524002252\n",
      "2022-03-26 22:56:34.888100 Epoch 1, Training Loss 1.6700619895135045\n",
      "2022-03-26 22:56:34.904099 Epoch 1, Training Loss 1.6727695519966848\n",
      "2022-03-26 22:56:34.921852 Epoch 1, Training Loss 1.675293608089847\n",
      "2022-03-26 22:56:34.937744 Epoch 1, Training Loss 1.6779181286502067\n",
      "2022-03-26 22:56:34.954315 Epoch 1, Training Loss 1.6804411499701497\n",
      "2022-03-26 22:56:34.972211 Epoch 1, Training Loss 1.6828807633551186\n",
      "2022-03-26 22:56:34.988120 Epoch 1, Training Loss 1.6852852066459558\n",
      "2022-03-26 22:56:35.005010 Epoch 1, Training Loss 1.687729938713181\n",
      "2022-03-26 22:56:35.021761 Epoch 1, Training Loss 1.6902009718253483\n",
      "2022-03-26 22:56:35.038739 Epoch 1, Training Loss 1.6927634916647012\n",
      "2022-03-26 22:56:35.054292 Epoch 1, Training Loss 1.6954391709983807\n",
      "2022-03-26 22:56:35.071224 Epoch 1, Training Loss 1.6978179977068206\n",
      "2022-03-26 22:56:35.088180 Epoch 1, Training Loss 1.70020256429682\n",
      "2022-03-26 22:56:35.103817 Epoch 1, Training Loss 1.7026757164989286\n",
      "2022-03-26 22:56:35.122021 Epoch 1, Training Loss 1.7052734417988515\n",
      "2022-03-26 22:56:35.138938 Epoch 1, Training Loss 1.7079208322498194\n",
      "2022-03-26 22:56:35.153888 Epoch 1, Training Loss 1.7103600752018298\n",
      "2022-03-26 22:56:35.170883 Epoch 1, Training Loss 1.7127812759345755\n",
      "2022-03-26 22:56:35.187794 Epoch 1, Training Loss 1.7151663766797547\n",
      "2022-03-26 22:56:35.204799 Epoch 1, Training Loss 1.717680206536637\n",
      "2022-03-26 22:56:35.220805 Epoch 1, Training Loss 1.720236193493504\n",
      "2022-03-26 22:56:35.237800 Epoch 1, Training Loss 1.7228847339635005\n",
      "2022-03-26 22:56:35.253995 Epoch 1, Training Loss 1.7252144087915835\n",
      "2022-03-26 22:56:35.271000 Epoch 1, Training Loss 1.727860431536994\n",
      "2022-03-26 22:56:35.287201 Epoch 1, Training Loss 1.7302658106664868\n",
      "2022-03-26 22:56:35.303869 Epoch 1, Training Loss 1.732658207873859\n",
      "2022-03-26 22:56:35.320862 Epoch 1, Training Loss 1.7349978369825028\n",
      "2022-03-26 22:56:35.337876 Epoch 1, Training Loss 1.7375966702275873\n",
      "2022-03-26 22:56:35.354861 Epoch 1, Training Loss 1.7399081961273233\n",
      "2022-03-26 22:56:35.371819 Epoch 1, Training Loss 1.7423281989743948\n",
      "2022-03-26 22:56:35.389773 Epoch 1, Training Loss 1.7448945219254555\n",
      "2022-03-26 22:56:35.404792 Epoch 1, Training Loss 1.7474519258264996\n",
      "2022-03-26 22:56:35.422820 Epoch 1, Training Loss 1.7500200268557615\n",
      "2022-03-26 22:56:35.437588 Epoch 1, Training Loss 1.7524394540835524\n",
      "2022-03-26 22:56:35.454134 Epoch 1, Training Loss 1.754951487874131\n",
      "2022-03-26 22:56:35.470693 Epoch 1, Training Loss 1.7576061580187219\n",
      "2022-03-26 22:56:35.487588 Epoch 1, Training Loss 1.7601106009824807\n",
      "2022-03-26 22:56:35.504958 Epoch 1, Training Loss 1.7624709778429601\n",
      "2022-03-26 22:56:35.521902 Epoch 1, Training Loss 1.7649698792516118\n",
      "2022-03-26 22:56:35.538907 Epoch 1, Training Loss 1.7674936722306644\n",
      "2022-03-26 22:56:35.554919 Epoch 1, Training Loss 1.7699751419484462\n",
      "2022-03-26 22:56:35.571872 Epoch 1, Training Loss 1.7723986210725498\n",
      "2022-03-26 22:56:35.587877 Epoch 1, Training Loss 1.774688974670742\n",
      "2022-03-26 22:56:35.604798 Epoch 1, Training Loss 1.7770217158605375\n",
      "2022-03-26 22:56:35.621821 Epoch 1, Training Loss 1.779456483281177\n",
      "2022-03-26 22:56:35.638648 Epoch 1, Training Loss 1.781888654469834\n",
      "2022-03-26 22:56:35.655670 Epoch 1, Training Loss 1.784389243406408\n",
      "2022-03-26 22:56:35.671145 Epoch 1, Training Loss 1.7868414356580475\n",
      "2022-03-26 22:56:35.687983 Epoch 1, Training Loss 1.7892744702756251\n",
      "2022-03-26 22:56:35.703976 Epoch 1, Training Loss 1.7916422240874346\n",
      "2022-03-26 22:56:35.720006 Epoch 1, Training Loss 1.7939623133910587\n",
      "2022-03-26 22:56:35.738811 Epoch 1, Training Loss 1.7966183012403796\n",
      "2022-03-26 22:56:35.754477 Epoch 1, Training Loss 1.7990289100295747\n",
      "2022-03-26 22:56:35.771500 Epoch 1, Training Loss 1.801467600076095\n",
      "2022-03-26 22:56:35.789358 Epoch 1, Training Loss 1.8040409935709765\n",
      "2022-03-26 22:56:35.804247 Epoch 1, Training Loss 1.806599018823765\n",
      "2022-03-26 22:56:35.823059 Epoch 1, Training Loss 1.8089799054748261\n",
      "2022-03-26 22:56:35.836960 Epoch 1, Training Loss 1.8113075517632466\n",
      "2022-03-26 22:56:35.854887 Epoch 1, Training Loss 1.813755827639109\n",
      "2022-03-26 22:56:35.871966 Epoch 1, Training Loss 1.816190856497001\n",
      "2022-03-26 22:56:35.888978 Epoch 1, Training Loss 1.8186474745840673\n",
      "2022-03-26 22:56:35.903980 Epoch 1, Training Loss 1.820993000436622\n",
      "2022-03-26 22:56:35.921986 Epoch 1, Training Loss 1.8234075820049667\n",
      "2022-03-26 22:56:35.937878 Epoch 1, Training Loss 1.8258621322224513\n",
      "2022-03-26 22:56:35.953895 Epoch 1, Training Loss 1.8282766674485658\n",
      "2022-03-26 22:56:35.970915 Epoch 1, Training Loss 1.8308261517063735\n",
      "2022-03-26 22:56:35.987897 Epoch 1, Training Loss 1.8333979034057968\n",
      "2022-03-26 22:56:36.003901 Epoch 1, Training Loss 1.8357565785612902\n",
      "2022-03-26 22:56:36.019939 Epoch 1, Training Loss 1.838380228680418\n",
      "2022-03-26 22:56:36.038240 Epoch 1, Training Loss 1.84081313265559\n",
      "2022-03-26 22:56:36.053750 Epoch 1, Training Loss 1.8432537747161162\n",
      "2022-03-26 22:56:36.070770 Epoch 1, Training Loss 1.8456005513515619\n",
      "2022-03-26 22:56:36.086797 Epoch 1, Training Loss 1.8482144168575707\n",
      "2022-03-26 22:56:36.104808 Epoch 1, Training Loss 1.8506613777726508\n",
      "2022-03-26 22:56:36.120076 Epoch 1, Training Loss 1.8531560124948507\n",
      "2022-03-26 22:56:36.137528 Epoch 1, Training Loss 1.855655043478817\n",
      "2022-03-26 22:56:36.155116 Epoch 1, Training Loss 1.858116230543922\n",
      "2022-03-26 22:56:36.170215 Epoch 1, Training Loss 1.8604663749180181\n",
      "2022-03-26 22:56:36.187635 Epoch 1, Training Loss 1.863092497029268\n",
      "2022-03-26 22:56:36.204205 Epoch 1, Training Loss 1.8654546586753766\n",
      "2022-03-26 22:56:36.221686 Epoch 1, Training Loss 1.8679817866181474\n",
      "2022-03-26 22:56:36.237578 Epoch 1, Training Loss 1.8703843679879328\n",
      "2022-03-26 22:56:36.253716 Epoch 1, Training Loss 1.8730207826475354\n",
      "2022-03-26 22:56:36.270705 Epoch 1, Training Loss 1.875482653565419\n",
      "2022-03-26 22:56:36.288629 Epoch 1, Training Loss 1.8780167168363586\n",
      "2022-03-26 22:56:36.304426 Epoch 1, Training Loss 1.8803869351706541\n",
      "2022-03-26 22:56:36.320456 Epoch 1, Training Loss 1.8830992483421969\n",
      "2022-03-26 22:56:36.339054 Epoch 1, Training Loss 1.8854779354141802\n",
      "2022-03-26 22:56:36.355058 Epoch 1, Training Loss 1.8879231891363784\n",
      "2022-03-26 22:56:36.370007 Epoch 1, Training Loss 1.8903607265723636\n",
      "2022-03-26 22:56:36.388003 Epoch 1, Training Loss 1.8928614809080158\n",
      "2022-03-26 22:56:36.403992 Epoch 1, Training Loss 1.8951118466494334\n",
      "2022-03-26 22:56:36.421112 Epoch 1, Training Loss 1.8975593249511231\n",
      "2022-03-26 22:56:36.437471 Epoch 1, Training Loss 1.9001845616818693\n",
      "2022-03-26 22:56:36.453396 Epoch 1, Training Loss 1.902734888179223\n",
      "2022-03-26 22:56:36.471412 Epoch 1, Training Loss 1.9051184366121316\n",
      "2022-03-26 22:56:36.487263 Epoch 1, Training Loss 1.9075241312956261\n",
      "2022-03-26 22:56:36.504866 Epoch 1, Training Loss 1.9101142707993002\n",
      "2022-03-26 22:56:36.520885 Epoch 1, Training Loss 1.9125574987257838\n",
      "2022-03-26 22:56:36.537814 Epoch 1, Training Loss 1.9148602302726883\n",
      "2022-03-26 22:56:36.554818 Epoch 1, Training Loss 1.9172621630036923\n",
      "2022-03-26 22:56:36.570854 Epoch 1, Training Loss 1.9198803602886931\n",
      "2022-03-26 22:56:36.587860 Epoch 1, Training Loss 1.922481450583319\n",
      "2022-03-26 22:56:36.603867 Epoch 1, Training Loss 1.9248833871253617\n",
      "2022-03-26 22:56:36.620862 Epoch 1, Training Loss 1.9272903655191211\n",
      "2022-03-26 22:56:36.636886 Epoch 1, Training Loss 1.9296090287320755\n",
      "2022-03-26 22:56:36.654914 Epoch 1, Training Loss 1.9320997391515375\n",
      "2022-03-26 22:56:36.670880 Epoch 1, Training Loss 1.9345207769242698\n",
      "2022-03-26 22:56:36.687884 Epoch 1, Training Loss 1.936779554237795\n",
      "2022-03-26 22:56:36.705878 Epoch 1, Training Loss 1.9393193683660854\n",
      "2022-03-26 22:56:36.720881 Epoch 1, Training Loss 1.9418145794697734\n",
      "2022-03-26 22:56:36.737881 Epoch 1, Training Loss 1.9443096312720451\n",
      "2022-03-26 22:56:36.753502 Epoch 1, Training Loss 1.946757690376028\n",
      "2022-03-26 22:56:36.771509 Epoch 1, Training Loss 1.949297177059876\n",
      "2022-03-26 22:56:36.786474 Epoch 1, Training Loss 1.95192579799296\n",
      "2022-03-26 22:56:36.804442 Epoch 1, Training Loss 1.9543484760367351\n",
      "2022-03-26 22:56:36.821168 Epoch 1, Training Loss 1.9566655326682283\n",
      "2022-03-26 22:56:36.836983 Epoch 1, Training Loss 1.959184981368082\n",
      "2022-03-26 22:56:36.852995 Epoch 1, Training Loss 1.9614637286766716\n",
      "2022-03-26 22:56:36.870978 Epoch 1, Training Loss 1.9638765397888924\n",
      "2022-03-26 22:56:36.887973 Epoch 1, Training Loss 1.9661494920320828\n",
      "2022-03-26 22:56:36.903976 Epoch 1, Training Loss 1.9686449784452043\n",
      "2022-03-26 22:56:36.920907 Epoch 1, Training Loss 1.9710746448668068\n",
      "2022-03-26 22:56:36.937896 Epoch 1, Training Loss 1.973537007256237\n",
      "2022-03-26 22:56:36.952832 Epoch 1, Training Loss 1.975979042480059\n",
      "2022-03-26 22:56:36.970842 Epoch 1, Training Loss 1.9783818595244755\n",
      "2022-03-26 22:56:36.986705 Epoch 1, Training Loss 1.9806859224958493\n",
      "2022-03-26 22:56:37.004694 Epoch 1, Training Loss 1.9832204208349633\n",
      "2022-03-26 22:56:37.020666 Epoch 1, Training Loss 1.9856827211806842\n",
      "2022-03-26 22:56:37.038389 Epoch 1, Training Loss 1.9882426351842368\n",
      "2022-03-26 22:56:37.054454 Epoch 1, Training Loss 1.9905685402853104\n",
      "2022-03-26 22:56:37.071470 Epoch 1, Training Loss 1.9931170992229297\n",
      "2022-03-26 22:56:37.087399 Epoch 1, Training Loss 1.99537008391012\n",
      "2022-03-26 22:56:37.104141 Epoch 1, Training Loss 1.997885793218832\n",
      "2022-03-26 22:56:37.121089 Epoch 1, Training Loss 2.0003447072280336\n",
      "2022-03-26 22:56:37.136792 Epoch 1, Training Loss 2.002758359665151\n",
      "2022-03-26 22:56:37.153913 Epoch 1, Training Loss 2.0053780252671305\n",
      "2022-03-26 22:56:37.170909 Epoch 1, Training Loss 2.0077266241888254\n",
      "2022-03-26 22:56:37.186865 Epoch 1, Training Loss 2.0100891466640753\n",
      "2022-03-26 22:56:37.204810 Epoch 1, Training Loss 2.01260062221371\n",
      "2022-03-26 22:56:37.219891 Epoch 1, Training Loss 2.0151101061145362\n",
      "2022-03-26 22:56:37.237874 Epoch 1, Training Loss 2.017312778841199\n",
      "2022-03-26 22:56:37.253886 Epoch 1, Training Loss 2.019910931282336\n",
      "2022-03-26 22:56:37.271000 Epoch 1, Training Loss 2.02234679704432\n",
      "2022-03-26 22:56:37.287970 Epoch 1, Training Loss 2.024812766505629\n",
      "2022-03-26 22:56:37.304866 Epoch 1, Training Loss 2.0270280115439765\n",
      "2022-03-26 22:56:37.320805 Epoch 1, Training Loss 2.029576141815966\n",
      "2022-03-26 22:56:37.338406 Epoch 1, Training Loss 2.0322211053975097\n",
      "2022-03-26 22:56:37.355086 Epoch 1, Training Loss 2.034570103563616\n",
      "2022-03-26 22:56:37.371101 Epoch 1, Training Loss 2.03694555216738\n",
      "2022-03-26 22:56:37.387771 Epoch 1, Training Loss 2.039523356101092\n",
      "2022-03-26 22:56:37.404064 Epoch 1, Training Loss 2.041770087483594\n",
      "2022-03-26 22:56:37.422101 Epoch 1, Training Loss 2.0444094881682138\n",
      "2022-03-26 22:56:37.444274 Epoch 1, Training Loss 2.0468472356686505\n",
      "2022-03-26 22:56:37.461278 Epoch 1, Training Loss 2.04943263119139\n",
      "2022-03-26 22:56:37.475280 Epoch 1, Training Loss 2.052157272005935\n",
      "2022-03-26 23:07:20.183474 Epoch 50, Training Loss 0.0011943670947228552\n",
      "2022-03-26 23:07:20.203479 Epoch 50, Training Loss 0.002183534528898156\n",
      "2022-03-26 23:07:20.222483 Epoch 50, Training Loss 0.0031881102972933094\n",
      "2022-03-26 23:07:20.241930 Epoch 50, Training Loss 0.004237491227781681\n",
      "2022-03-26 23:07:20.261687 Epoch 50, Training Loss 0.005506334402372161\n",
      "2022-03-26 23:07:20.282690 Epoch 50, Training Loss 0.006628507695844411\n",
      "2022-03-26 23:07:20.301638 Epoch 50, Training Loss 0.007563650608062744\n",
      "2022-03-26 23:07:20.321713 Epoch 50, Training Loss 0.008607237159138751\n",
      "2022-03-26 23:07:20.341940 Epoch 50, Training Loss 0.009582257026906514\n",
      "2022-03-26 23:07:20.361944 Epoch 50, Training Loss 0.010678855964289906\n",
      "2022-03-26 23:07:20.380949 Epoch 50, Training Loss 0.011526109998488365\n",
      "2022-03-26 23:07:20.400992 Epoch 50, Training Loss 0.012479729817041655\n",
      "2022-03-26 23:07:20.420588 Epoch 50, Training Loss 0.01342013348703799\n",
      "2022-03-26 23:07:20.440598 Epoch 50, Training Loss 0.014491808688854012\n",
      "2022-03-26 23:07:20.459920 Epoch 50, Training Loss 0.015663342555160716\n",
      "2022-03-26 23:07:20.480838 Epoch 50, Training Loss 0.016723226479557166\n",
      "2022-03-26 23:07:20.500480 Epoch 50, Training Loss 0.01779887606115902\n",
      "2022-03-26 23:07:20.520465 Epoch 50, Training Loss 0.018773284638324356\n",
      "2022-03-26 23:07:20.540407 Epoch 50, Training Loss 0.019751765691410857\n",
      "2022-03-26 23:07:20.560518 Epoch 50, Training Loss 0.020861828418643882\n",
      "2022-03-26 23:07:20.581534 Epoch 50, Training Loss 0.02179380420528714\n",
      "2022-03-26 23:07:20.600487 Epoch 50, Training Loss 0.02287331314952782\n",
      "2022-03-26 23:07:20.620533 Epoch 50, Training Loss 0.023705381185502347\n",
      "2022-03-26 23:07:20.640464 Epoch 50, Training Loss 0.02477254792857353\n",
      "2022-03-26 23:07:20.659614 Epoch 50, Training Loss 0.025875647049730697\n",
      "2022-03-26 23:07:20.679558 Epoch 50, Training Loss 0.026952692843459147\n",
      "2022-03-26 23:07:20.699392 Epoch 50, Training Loss 0.02794671576956044\n",
      "2022-03-26 23:07:20.718427 Epoch 50, Training Loss 0.02935087421666021\n",
      "2022-03-26 23:07:20.738143 Epoch 50, Training Loss 0.030197377323799426\n",
      "2022-03-26 23:07:20.756711 Epoch 50, Training Loss 0.0313170046147788\n",
      "2022-03-26 23:07:20.776957 Epoch 50, Training Loss 0.032583368480053096\n",
      "2022-03-26 23:07:20.796973 Epoch 50, Training Loss 0.033675654541196115\n",
      "2022-03-26 23:07:20.815127 Epoch 50, Training Loss 0.03489520612275204\n",
      "2022-03-26 23:07:20.835156 Epoch 50, Training Loss 0.03613472137304828\n",
      "2022-03-26 23:07:20.855268 Epoch 50, Training Loss 0.03711915260080791\n",
      "2022-03-26 23:07:20.875292 Epoch 50, Training Loss 0.038159003724222595\n",
      "2022-03-26 23:07:20.896255 Epoch 50, Training Loss 0.03905905222953738\n",
      "2022-03-26 23:07:20.915325 Epoch 50, Training Loss 0.04000957580783483\n",
      "2022-03-26 23:07:20.934267 Epoch 50, Training Loss 0.041175178173557876\n",
      "2022-03-26 23:07:20.955161 Epoch 50, Training Loss 0.04227127313918775\n",
      "2022-03-26 23:07:20.974557 Epoch 50, Training Loss 0.043253295180742696\n",
      "2022-03-26 23:07:20.995598 Epoch 50, Training Loss 0.044352449419553325\n",
      "2022-03-26 23:07:21.015324 Epoch 50, Training Loss 0.04540858076661444\n",
      "2022-03-26 23:07:21.035357 Epoch 50, Training Loss 0.04654176171173525\n",
      "2022-03-26 23:07:21.056116 Epoch 50, Training Loss 0.04762964061154124\n",
      "2022-03-26 23:07:21.076118 Epoch 50, Training Loss 0.048802682582069844\n",
      "2022-03-26 23:07:21.096114 Epoch 50, Training Loss 0.04984570441343595\n",
      "2022-03-26 23:07:21.116465 Epoch 50, Training Loss 0.05074831896730701\n",
      "2022-03-26 23:07:21.135267 Epoch 50, Training Loss 0.05195673377922429\n",
      "2022-03-26 23:07:21.156143 Epoch 50, Training Loss 0.053138293878501636\n",
      "2022-03-26 23:07:21.176002 Epoch 50, Training Loss 0.05436083964069786\n",
      "2022-03-26 23:07:21.195990 Epoch 50, Training Loss 0.05515531932606416\n",
      "2022-03-26 23:07:21.216031 Epoch 50, Training Loss 0.056241760244759756\n",
      "2022-03-26 23:07:21.235964 Epoch 50, Training Loss 0.05715599480797263\n",
      "2022-03-26 23:07:21.254980 Epoch 50, Training Loss 0.0582395512277208\n",
      "2022-03-26 23:07:21.276016 Epoch 50, Training Loss 0.05945427536659533\n",
      "2022-03-26 23:07:21.296054 Epoch 50, Training Loss 0.0604569754941994\n",
      "2022-03-26 23:07:21.315070 Epoch 50, Training Loss 0.06152158518276556\n",
      "2022-03-26 23:07:21.335054 Epoch 50, Training Loss 0.06245352293524291\n",
      "2022-03-26 23:07:21.359053 Epoch 50, Training Loss 0.06334911244909477\n",
      "2022-03-26 23:07:21.379042 Epoch 50, Training Loss 0.06419341277588358\n",
      "2022-03-26 23:07:21.399082 Epoch 50, Training Loss 0.06532749465054563\n",
      "2022-03-26 23:07:21.418925 Epoch 50, Training Loss 0.06624779402447478\n",
      "2022-03-26 23:07:21.439961 Epoch 50, Training Loss 0.06746822168759982\n",
      "2022-03-26 23:07:21.460975 Epoch 50, Training Loss 0.06853597449219745\n",
      "2022-03-26 23:07:21.480991 Epoch 50, Training Loss 0.06953927066624926\n",
      "2022-03-26 23:07:21.500997 Epoch 50, Training Loss 0.07069293998391427\n",
      "2022-03-26 23:07:21.520418 Epoch 50, Training Loss 0.0716515545497465\n",
      "2022-03-26 23:07:21.540741 Epoch 50, Training Loss 0.0723144831254964\n",
      "2022-03-26 23:07:21.562686 Epoch 50, Training Loss 0.07333908635941917\n",
      "2022-03-26 23:07:21.582542 Epoch 50, Training Loss 0.07431318441315381\n",
      "2022-03-26 23:07:21.601687 Epoch 50, Training Loss 0.07525426980174715\n",
      "2022-03-26 23:07:21.621686 Epoch 50, Training Loss 0.07628192880269512\n",
      "2022-03-26 23:07:21.641727 Epoch 50, Training Loss 0.07727285762272222\n",
      "2022-03-26 23:07:21.661750 Epoch 50, Training Loss 0.07825305143280713\n",
      "2022-03-26 23:07:21.682065 Epoch 50, Training Loss 0.07940684018842399\n",
      "2022-03-26 23:07:21.701067 Epoch 50, Training Loss 0.08020254428429371\n",
      "2022-03-26 23:07:21.721142 Epoch 50, Training Loss 0.08134135878299509\n",
      "2022-03-26 23:07:21.740851 Epoch 50, Training Loss 0.08234810318483417\n",
      "2022-03-26 23:07:21.759975 Epoch 50, Training Loss 0.08341192025357805\n",
      "2022-03-26 23:07:21.780986 Epoch 50, Training Loss 0.08411701774353261\n",
      "2022-03-26 23:07:21.800853 Epoch 50, Training Loss 0.0851441631689096\n",
      "2022-03-26 23:07:21.819888 Epoch 50, Training Loss 0.08630531386036397\n",
      "2022-03-26 23:07:21.839728 Epoch 50, Training Loss 0.0872687137950107\n",
      "2022-03-26 23:07:21.859745 Epoch 50, Training Loss 0.08819145291967465\n",
      "2022-03-26 23:07:21.879614 Epoch 50, Training Loss 0.08927542878233868\n",
      "2022-03-26 23:07:21.899589 Epoch 50, Training Loss 0.09035480754149844\n",
      "2022-03-26 23:07:21.919570 Epoch 50, Training Loss 0.09146250536679612\n",
      "2022-03-26 23:07:21.938997 Epoch 50, Training Loss 0.09211657609781036\n",
      "2022-03-26 23:07:21.960003 Epoch 50, Training Loss 0.09303921834587137\n",
      "2022-03-26 23:07:21.980041 Epoch 50, Training Loss 0.0942491791437349\n",
      "2022-03-26 23:07:21.999983 Epoch 50, Training Loss 0.09551300096999654\n",
      "2022-03-26 23:07:22.019018 Epoch 50, Training Loss 0.09679621595250981\n",
      "2022-03-26 23:07:22.048106 Epoch 50, Training Loss 0.09795848838508586\n",
      "2022-03-26 23:07:22.076870 Epoch 50, Training Loss 0.09880538311455865\n",
      "2022-03-26 23:07:22.109813 Epoch 50, Training Loss 0.09981363645904814\n",
      "2022-03-26 23:07:22.139833 Epoch 50, Training Loss 0.10084173533007922\n",
      "2022-03-26 23:07:22.168842 Epoch 50, Training Loss 0.10193932437530868\n",
      "2022-03-26 23:07:22.196687 Epoch 50, Training Loss 0.10300855418605268\n",
      "2022-03-26 23:07:22.222625 Epoch 50, Training Loss 0.10428718845252796\n",
      "2022-03-26 23:07:22.250090 Epoch 50, Training Loss 0.10534176375250072\n",
      "2022-03-26 23:07:22.276984 Epoch 50, Training Loss 0.10641845176591898\n",
      "2022-03-26 23:07:22.302912 Epoch 50, Training Loss 0.10735931809600967\n",
      "2022-03-26 23:07:22.331823 Epoch 50, Training Loss 0.10862755897404898\n",
      "2022-03-26 23:07:22.356828 Epoch 50, Training Loss 0.1096601095955695\n",
      "2022-03-26 23:07:22.383446 Epoch 50, Training Loss 0.11085085475536259\n",
      "2022-03-26 23:07:22.410195 Epoch 50, Training Loss 0.11195786918520623\n",
      "2022-03-26 23:07:22.436848 Epoch 50, Training Loss 0.11305851224438308\n",
      "2022-03-26 23:07:22.464087 Epoch 50, Training Loss 0.11399768959835667\n",
      "2022-03-26 23:07:22.490049 Epoch 50, Training Loss 0.11500587884117575\n",
      "2022-03-26 23:07:22.508972 Epoch 50, Training Loss 0.11613230967460692\n",
      "2022-03-26 23:07:22.527971 Epoch 50, Training Loss 0.1171002128087651\n",
      "2022-03-26 23:07:22.547723 Epoch 50, Training Loss 0.11827031494406483\n",
      "2022-03-26 23:07:22.566760 Epoch 50, Training Loss 0.11909652922464453\n",
      "2022-03-26 23:07:22.584722 Epoch 50, Training Loss 0.12033510170019496\n",
      "2022-03-26 23:07:22.604613 Epoch 50, Training Loss 0.12128867220390788\n",
      "2022-03-26 23:07:22.623201 Epoch 50, Training Loss 0.12247917909756341\n",
      "2022-03-26 23:07:22.642080 Epoch 50, Training Loss 0.12393443229253334\n",
      "2022-03-26 23:07:22.661016 Epoch 50, Training Loss 0.1248577653294634\n",
      "2022-03-26 23:07:22.680883 Epoch 50, Training Loss 0.1259694821999201\n",
      "2022-03-26 23:07:22.698833 Epoch 50, Training Loss 0.12689244084041137\n",
      "2022-03-26 23:07:22.717753 Epoch 50, Training Loss 0.127994340475258\n",
      "2022-03-26 23:07:22.736947 Epoch 50, Training Loss 0.12927636168801876\n",
      "2022-03-26 23:07:22.755996 Epoch 50, Training Loss 0.1304772426862546\n",
      "2022-03-26 23:07:22.774967 Epoch 50, Training Loss 0.1313534932368247\n",
      "2022-03-26 23:07:22.793829 Epoch 50, Training Loss 0.13268742872321088\n",
      "2022-03-26 23:07:22.812821 Epoch 50, Training Loss 0.13347422360154368\n",
      "2022-03-26 23:07:22.832485 Epoch 50, Training Loss 0.1345988550911779\n",
      "2022-03-26 23:07:22.851469 Epoch 50, Training Loss 0.1357041462455564\n",
      "2022-03-26 23:07:22.870462 Epoch 50, Training Loss 0.13696910673395143\n",
      "2022-03-26 23:07:22.889192 Epoch 50, Training Loss 0.1378378562457726\n",
      "2022-03-26 23:07:22.908192 Epoch 50, Training Loss 0.139330161273327\n",
      "2022-03-26 23:07:22.926034 Epoch 50, Training Loss 0.14068145634573134\n",
      "2022-03-26 23:07:22.945962 Epoch 50, Training Loss 0.14146955444684725\n",
      "2022-03-26 23:07:22.964879 Epoch 50, Training Loss 0.14260979038675117\n",
      "2022-03-26 23:07:22.983917 Epoch 50, Training Loss 0.1435067965398969\n",
      "2022-03-26 23:07:23.001839 Epoch 50, Training Loss 0.1447513648463637\n",
      "2022-03-26 23:07:23.021878 Epoch 50, Training Loss 0.14584001830166868\n",
      "2022-03-26 23:07:23.040187 Epoch 50, Training Loss 0.14674528800618009\n",
      "2022-03-26 23:07:23.059970 Epoch 50, Training Loss 0.14793859090646513\n",
      "2022-03-26 23:07:23.078889 Epoch 50, Training Loss 0.1492689748096954\n",
      "2022-03-26 23:07:23.097893 Epoch 50, Training Loss 0.1502406563600311\n",
      "2022-03-26 23:07:23.117876 Epoch 50, Training Loss 0.15098379015007898\n",
      "2022-03-26 23:07:23.135664 Epoch 50, Training Loss 0.15211228740489696\n",
      "2022-03-26 23:07:23.155543 Epoch 50, Training Loss 0.15298648685445565\n",
      "2022-03-26 23:07:23.175473 Epoch 50, Training Loss 0.15410418103418083\n",
      "2022-03-26 23:07:23.194481 Epoch 50, Training Loss 0.15532773992289667\n",
      "2022-03-26 23:07:23.213442 Epoch 50, Training Loss 0.15653724545408088\n",
      "2022-03-26 23:07:23.232305 Epoch 50, Training Loss 0.15746652043384055\n",
      "2022-03-26 23:07:23.250356 Epoch 50, Training Loss 0.15842541976048208\n",
      "2022-03-26 23:07:23.268384 Epoch 50, Training Loss 0.1595871770930717\n",
      "2022-03-26 23:07:23.287330 Epoch 50, Training Loss 0.16089939956774796\n",
      "2022-03-26 23:07:23.306189 Epoch 50, Training Loss 0.16196278576046\n",
      "2022-03-26 23:07:23.325194 Epoch 50, Training Loss 0.16320822206909394\n",
      "2022-03-26 23:07:23.344816 Epoch 50, Training Loss 0.16437379326052068\n",
      "2022-03-26 23:07:23.364728 Epoch 50, Training Loss 0.16573946509519807\n",
      "2022-03-26 23:07:23.383768 Epoch 50, Training Loss 0.16659998344948224\n",
      "2022-03-26 23:07:23.401850 Epoch 50, Training Loss 0.16782970029069944\n",
      "2022-03-26 23:07:23.420852 Epoch 50, Training Loss 0.1691605793240735\n",
      "2022-03-26 23:07:23.439868 Epoch 50, Training Loss 0.17036688922311338\n",
      "2022-03-26 23:07:23.460921 Epoch 50, Training Loss 0.17123652205747716\n",
      "2022-03-26 23:07:23.479946 Epoch 50, Training Loss 0.17217374495838\n",
      "2022-03-26 23:07:23.499727 Epoch 50, Training Loss 0.17331832075667808\n",
      "2022-03-26 23:07:23.517710 Epoch 50, Training Loss 0.1744940021763677\n",
      "2022-03-26 23:07:23.535734 Epoch 50, Training Loss 0.17557393742339386\n",
      "2022-03-26 23:07:23.555680 Epoch 50, Training Loss 0.17663962327305924\n",
      "2022-03-26 23:07:23.575008 Epoch 50, Training Loss 0.17800164276071825\n",
      "2022-03-26 23:07:23.594041 Epoch 50, Training Loss 0.17888355217016566\n",
      "2022-03-26 23:07:23.613311 Epoch 50, Training Loss 0.18006580458272753\n",
      "2022-03-26 23:07:23.631825 Epoch 50, Training Loss 0.18091793057253902\n",
      "2022-03-26 23:07:23.650829 Epoch 50, Training Loss 0.18217003840924528\n",
      "2022-03-26 23:07:23.669471 Epoch 50, Training Loss 0.1830973217401968\n",
      "2022-03-26 23:07:23.688495 Epoch 50, Training Loss 0.18424577016354826\n",
      "2022-03-26 23:07:23.707566 Epoch 50, Training Loss 0.18570466762613458\n",
      "2022-03-26 23:07:23.726613 Epoch 50, Training Loss 0.18701255039485823\n",
      "2022-03-26 23:07:23.745607 Epoch 50, Training Loss 0.1881645234192119\n",
      "2022-03-26 23:07:23.765338 Epoch 50, Training Loss 0.18925704782271324\n",
      "2022-03-26 23:07:23.783890 Epoch 50, Training Loss 0.19071503902030418\n",
      "2022-03-26 23:07:23.801929 Epoch 50, Training Loss 0.19161877501041383\n",
      "2022-03-26 23:07:23.822422 Epoch 50, Training Loss 0.19254624698777942\n",
      "2022-03-26 23:07:23.840273 Epoch 50, Training Loss 0.19370698258090202\n",
      "2022-03-26 23:07:23.860364 Epoch 50, Training Loss 0.19467667484527354\n",
      "2022-03-26 23:07:23.881042 Epoch 50, Training Loss 0.19575675323491207\n",
      "2022-03-26 23:07:23.900072 Epoch 50, Training Loss 0.19708793669405494\n",
      "2022-03-26 23:07:23.918102 Epoch 50, Training Loss 0.1984088010800159\n",
      "2022-03-26 23:07:23.937840 Epoch 50, Training Loss 0.19941981628422847\n",
      "2022-03-26 23:07:23.957857 Epoch 50, Training Loss 0.20017877022933472\n",
      "2022-03-26 23:07:23.977350 Epoch 50, Training Loss 0.2013210007144362\n",
      "2022-03-26 23:07:23.997362 Epoch 50, Training Loss 0.2023139156953758\n",
      "2022-03-26 23:07:24.016293 Epoch 50, Training Loss 0.20319210118649866\n",
      "2022-03-26 23:07:24.034775 Epoch 50, Training Loss 0.2040383279933344\n",
      "2022-03-26 23:07:24.054657 Epoch 50, Training Loss 0.20516125899751472\n",
      "2022-03-26 23:07:24.074744 Epoch 50, Training Loss 0.20632733598999356\n",
      "2022-03-26 23:07:24.094551 Epoch 50, Training Loss 0.20713978510378572\n",
      "2022-03-26 23:07:24.114591 Epoch 50, Training Loss 0.2083743156679451\n",
      "2022-03-26 23:07:24.133425 Epoch 50, Training Loss 0.20940033637959024\n",
      "2022-03-26 23:07:24.153388 Epoch 50, Training Loss 0.21038720972092864\n",
      "2022-03-26 23:07:24.172276 Epoch 50, Training Loss 0.21154289774577636\n",
      "2022-03-26 23:07:24.192012 Epoch 50, Training Loss 0.21243875014507557\n",
      "2022-03-26 23:07:24.212038 Epoch 50, Training Loss 0.2134803224859945\n",
      "2022-03-26 23:07:24.231046 Epoch 50, Training Loss 0.214369586895189\n",
      "2022-03-26 23:07:24.250047 Epoch 50, Training Loss 0.21551288934924717\n",
      "2022-03-26 23:07:24.269012 Epoch 50, Training Loss 0.21640556258008914\n",
      "2022-03-26 23:07:24.288029 Epoch 50, Training Loss 0.21734866164529415\n",
      "2022-03-26 23:07:24.307915 Epoch 50, Training Loss 0.21803827351316465\n",
      "2022-03-26 23:07:24.334731 Epoch 50, Training Loss 0.21920906674221655\n",
      "2022-03-26 23:07:24.361703 Epoch 50, Training Loss 0.22022515367668913\n",
      "2022-03-26 23:07:24.388512 Epoch 50, Training Loss 0.22149603507098028\n",
      "2022-03-26 23:07:24.416365 Epoch 50, Training Loss 0.22238986967774607\n",
      "2022-03-26 23:07:24.443377 Epoch 50, Training Loss 0.22355752421157135\n",
      "2022-03-26 23:07:24.471311 Epoch 50, Training Loss 0.22431859755150194\n",
      "2022-03-26 23:07:24.499741 Epoch 50, Training Loss 0.22522509303849067\n",
      "2022-03-26 23:07:24.526678 Epoch 50, Training Loss 0.22602086702880958\n",
      "2022-03-26 23:07:24.548140 Epoch 50, Training Loss 0.22739099686407982\n",
      "2022-03-26 23:07:24.566992 Epoch 50, Training Loss 0.2286913685329125\n",
      "2022-03-26 23:07:24.586027 Epoch 50, Training Loss 0.2301980494080907\n",
      "2022-03-26 23:07:24.605037 Epoch 50, Training Loss 0.23122368230844093\n",
      "2022-03-26 23:07:24.626976 Epoch 50, Training Loss 0.23212792432826498\n",
      "2022-03-26 23:07:24.645980 Epoch 50, Training Loss 0.23304767773279447\n",
      "2022-03-26 23:07:24.664992 Epoch 50, Training Loss 0.23417937054353602\n",
      "2022-03-26 23:07:24.683012 Epoch 50, Training Loss 0.2353244372036146\n",
      "2022-03-26 23:07:24.701181 Epoch 50, Training Loss 0.23639172879631257\n",
      "2022-03-26 23:07:24.720875 Epoch 50, Training Loss 0.23728797060754292\n",
      "2022-03-26 23:07:24.738882 Epoch 50, Training Loss 0.2383833934583932\n",
      "2022-03-26 23:07:24.757903 Epoch 50, Training Loss 0.23925599615897059\n",
      "2022-03-26 23:07:24.776517 Epoch 50, Training Loss 0.24034284089532348\n",
      "2022-03-26 23:07:24.795718 Epoch 50, Training Loss 0.2416287124766718\n",
      "2022-03-26 23:07:24.814726 Epoch 50, Training Loss 0.24293488012555312\n",
      "2022-03-26 23:07:24.833759 Epoch 50, Training Loss 0.2442216110199004\n",
      "2022-03-26 23:07:24.854769 Epoch 50, Training Loss 0.24541560448039218\n",
      "2022-03-26 23:07:24.873790 Epoch 50, Training Loss 0.2465924746392633\n",
      "2022-03-26 23:07:24.892759 Epoch 50, Training Loss 0.24764969502873432\n",
      "2022-03-26 23:07:24.910878 Epoch 50, Training Loss 0.24910438662904608\n",
      "2022-03-26 23:07:24.929326 Epoch 50, Training Loss 0.25022525616618985\n",
      "2022-03-26 23:07:24.949156 Epoch 50, Training Loss 0.2513717485358343\n",
      "2022-03-26 23:07:24.967043 Epoch 50, Training Loss 0.2524585923575379\n",
      "2022-03-26 23:07:24.986052 Epoch 50, Training Loss 0.25338829440229077\n",
      "2022-03-26 23:07:25.004767 Epoch 50, Training Loss 0.2548062904259128\n",
      "2022-03-26 23:07:25.023771 Epoch 50, Training Loss 0.25595059472581616\n",
      "2022-03-26 23:07:25.043788 Epoch 50, Training Loss 0.2571518065222084\n",
      "2022-03-26 23:07:25.063787 Epoch 50, Training Loss 0.25823442535022334\n",
      "2022-03-26 23:07:25.082670 Epoch 50, Training Loss 0.2594806347661616\n",
      "2022-03-26 23:07:25.100759 Epoch 50, Training Loss 0.2607284634162093\n",
      "2022-03-26 23:07:25.118861 Epoch 50, Training Loss 0.2617203371451639\n",
      "2022-03-26 23:07:25.137875 Epoch 50, Training Loss 0.2626033131881138\n",
      "2022-03-26 23:07:25.156922 Epoch 50, Training Loss 0.26371480383531515\n",
      "2022-03-26 23:07:25.175938 Epoch 50, Training Loss 0.26475529559433003\n",
      "2022-03-26 23:07:25.193965 Epoch 50, Training Loss 0.266057161800087\n",
      "2022-03-26 23:07:25.212975 Epoch 50, Training Loss 0.26729359544451586\n",
      "2022-03-26 23:07:25.232483 Epoch 50, Training Loss 0.26828302241042445\n",
      "2022-03-26 23:07:25.250601 Epoch 50, Training Loss 0.2693782614167694\n",
      "2022-03-26 23:07:25.269674 Epoch 50, Training Loss 0.270604312877216\n",
      "2022-03-26 23:07:25.289603 Epoch 50, Training Loss 0.27164053772111685\n",
      "2022-03-26 23:07:25.308680 Epoch 50, Training Loss 0.2727842660206358\n",
      "2022-03-26 23:07:25.326135 Epoch 50, Training Loss 0.2739303637191158\n",
      "2022-03-26 23:07:25.345328 Epoch 50, Training Loss 0.27470613958890483\n",
      "2022-03-26 23:07:25.365425 Epoch 50, Training Loss 0.2755920800863934\n",
      "2022-03-26 23:07:25.383386 Epoch 50, Training Loss 0.2764998308532988\n",
      "2022-03-26 23:07:25.402367 Epoch 50, Training Loss 0.277861956897599\n",
      "2022-03-26 23:07:25.421369 Epoch 50, Training Loss 0.2788422452214429\n",
      "2022-03-26 23:07:25.440929 Epoch 50, Training Loss 0.28016301600829413\n",
      "2022-03-26 23:07:25.459997 Epoch 50, Training Loss 0.281368503942514\n",
      "2022-03-26 23:07:25.479014 Epoch 50, Training Loss 0.282536083246436\n",
      "2022-03-26 23:07:25.498037 Epoch 50, Training Loss 0.2837285900207432\n",
      "2022-03-26 23:07:25.516460 Epoch 50, Training Loss 0.28465133531928977\n",
      "2022-03-26 23:07:25.534391 Epoch 50, Training Loss 0.2853189829517813\n",
      "2022-03-26 23:07:25.553342 Epoch 50, Training Loss 0.28630902379979867\n",
      "2022-03-26 23:07:25.572311 Epoch 50, Training Loss 0.2871267687329246\n",
      "2022-03-26 23:07:25.591330 Epoch 50, Training Loss 0.2882298064963592\n",
      "2022-03-26 23:07:25.611158 Epoch 50, Training Loss 0.2892237903974245\n",
      "2022-03-26 23:07:25.630001 Epoch 50, Training Loss 0.2901419372967137\n",
      "2022-03-26 23:07:25.649033 Epoch 50, Training Loss 0.2912957085977735\n",
      "2022-03-26 23:07:25.667059 Epoch 50, Training Loss 0.2922825413896605\n",
      "2022-03-26 23:07:25.686952 Epoch 50, Training Loss 0.2935473101065897\n",
      "2022-03-26 23:07:25.704980 Epoch 50, Training Loss 0.2946435594192856\n",
      "2022-03-26 23:07:25.724846 Epoch 50, Training Loss 0.29567371891892474\n",
      "2022-03-26 23:07:25.743734 Epoch 50, Training Loss 0.29656946941104995\n",
      "2022-03-26 23:07:25.763606 Epoch 50, Training Loss 0.2976971247312053\n",
      "2022-03-26 23:07:25.781205 Epoch 50, Training Loss 0.2987215411480126\n",
      "2022-03-26 23:07:25.800272 Epoch 50, Training Loss 0.2999862562817381\n",
      "2022-03-26 23:07:25.818360 Epoch 50, Training Loss 0.3011844679522697\n",
      "2022-03-26 23:07:25.836338 Epoch 50, Training Loss 0.30211463059915605\n",
      "2022-03-26 23:07:25.855352 Epoch 50, Training Loss 0.30314801621924886\n",
      "2022-03-26 23:07:25.874515 Epoch 50, Training Loss 0.30418492552569454\n",
      "2022-03-26 23:07:25.893030 Epoch 50, Training Loss 0.3052843584276526\n",
      "2022-03-26 23:07:25.912066 Epoch 50, Training Loss 0.30615626462280293\n",
      "2022-03-26 23:07:25.931088 Epoch 50, Training Loss 0.30731006610728895\n",
      "2022-03-26 23:07:25.949134 Epoch 50, Training Loss 0.3084455765879063\n",
      "2022-03-26 23:07:25.967848 Epoch 50, Training Loss 0.30971220890274437\n",
      "2022-03-26 23:07:25.985799 Epoch 50, Training Loss 0.3109299653326459\n",
      "2022-03-26 23:07:26.004816 Epoch 50, Training Loss 0.3122183944258239\n",
      "2022-03-26 23:07:26.024828 Epoch 50, Training Loss 0.3130622468031276\n",
      "2022-03-26 23:07:26.042713 Epoch 50, Training Loss 0.31412061538232866\n",
      "2022-03-26 23:07:26.061857 Epoch 50, Training Loss 0.3150610882607872\n",
      "2022-03-26 23:07:26.080855 Epoch 50, Training Loss 0.31611023656547527\n",
      "2022-03-26 23:07:26.100148 Epoch 50, Training Loss 0.31743476229250583\n",
      "2022-03-26 23:07:26.118200 Epoch 50, Training Loss 0.3188205747043385\n",
      "2022-03-26 23:07:26.136833 Epoch 50, Training Loss 0.32015958481737417\n",
      "2022-03-26 23:07:26.156850 Epoch 50, Training Loss 0.3211252881437921\n",
      "2022-03-26 23:07:26.175516 Epoch 50, Training Loss 0.3220252924410583\n",
      "2022-03-26 23:07:26.194917 Epoch 50, Training Loss 0.3231389751977018\n",
      "2022-03-26 23:07:26.213846 Epoch 50, Training Loss 0.32455188272249363\n",
      "2022-03-26 23:07:26.232724 Epoch 50, Training Loss 0.32557875443907347\n",
      "2022-03-26 23:07:26.250765 Epoch 50, Training Loss 0.3267703852080323\n",
      "2022-03-26 23:07:26.269485 Epoch 50, Training Loss 0.32800756170011847\n",
      "2022-03-26 23:07:26.289450 Epoch 50, Training Loss 0.3291709877340995\n",
      "2022-03-26 23:07:26.308505 Epoch 50, Training Loss 0.33030090239041904\n",
      "2022-03-26 23:07:26.327034 Epoch 50, Training Loss 0.3312955497933166\n",
      "2022-03-26 23:07:26.355073 Epoch 50, Training Loss 0.3324238566486427\n",
      "2022-03-26 23:07:26.380545 Epoch 50, Training Loss 0.3333778643547117\n",
      "2022-03-26 23:07:26.407114 Epoch 50, Training Loss 0.3345084683517056\n",
      "2022-03-26 23:07:26.434050 Epoch 50, Training Loss 0.3354869442980003\n",
      "2022-03-26 23:07:26.461083 Epoch 50, Training Loss 0.3365322870519155\n",
      "2022-03-26 23:07:26.487727 Epoch 50, Training Loss 0.3376625426437544\n",
      "2022-03-26 23:07:26.514339 Epoch 50, Training Loss 0.33885261530766403\n",
      "2022-03-26 23:07:26.541392 Epoch 50, Training Loss 0.3401612988518327\n",
      "2022-03-26 23:07:26.560508 Epoch 50, Training Loss 0.3414432231117697\n",
      "2022-03-26 23:07:26.579577 Epoch 50, Training Loss 0.3421625046778823\n",
      "2022-03-26 23:07:26.597615 Epoch 50, Training Loss 0.34334878619674525\n",
      "2022-03-26 23:07:26.616651 Epoch 50, Training Loss 0.34432530029655417\n",
      "2022-03-26 23:07:26.635845 Epoch 50, Training Loss 0.34539238639804715\n",
      "2022-03-26 23:07:26.654369 Epoch 50, Training Loss 0.34634589196166116\n",
      "2022-03-26 23:07:26.672453 Epoch 50, Training Loss 0.3476020056573326\n",
      "2022-03-26 23:07:26.690990 Epoch 50, Training Loss 0.34857379338320565\n",
      "2022-03-26 23:07:26.710019 Epoch 50, Training Loss 0.3498027830020241\n",
      "2022-03-26 23:07:26.730037 Epoch 50, Training Loss 0.3508037247926073\n",
      "2022-03-26 23:07:26.748058 Epoch 50, Training Loss 0.3517675690943628\n",
      "2022-03-26 23:07:26.767089 Epoch 50, Training Loss 0.35279163619136567\n",
      "2022-03-26 23:07:26.784099 Epoch 50, Training Loss 0.3538015771399983\n",
      "2022-03-26 23:07:26.803125 Epoch 50, Training Loss 0.35479947886503566\n",
      "2022-03-26 23:07:26.823012 Epoch 50, Training Loss 0.35598364944958016\n",
      "2022-03-26 23:07:26.842029 Epoch 50, Training Loss 0.3573418159771453\n",
      "2022-03-26 23:07:26.860057 Epoch 50, Training Loss 0.3584551390479593\n",
      "2022-03-26 23:07:26.879054 Epoch 50, Training Loss 0.3595800640637917\n",
      "2022-03-26 23:07:26.897093 Epoch 50, Training Loss 0.36035360567405095\n",
      "2022-03-26 23:07:26.915941 Epoch 50, Training Loss 0.36164560906417537\n",
      "2022-03-26 23:07:26.934712 Epoch 50, Training Loss 0.3627244372044683\n",
      "2022-03-26 23:07:26.953777 Epoch 50, Training Loss 0.3635702435775181\n",
      "2022-03-26 23:07:26.972792 Epoch 50, Training Loss 0.3646353673752007\n",
      "2022-03-26 23:07:26.991824 Epoch 50, Training Loss 0.36568848944990834\n",
      "2022-03-26 23:07:27.010825 Epoch 50, Training Loss 0.36685432268835394\n",
      "2022-03-26 23:07:27.028927 Epoch 50, Training Loss 0.3680947336851788\n",
      "2022-03-26 23:07:27.046941 Epoch 50, Training Loss 0.3689700572387032\n",
      "2022-03-26 23:07:27.065983 Epoch 50, Training Loss 0.36972792038832175\n",
      "2022-03-26 23:07:27.083855 Epoch 50, Training Loss 0.37080937700198435\n",
      "2022-03-26 23:07:27.102884 Epoch 50, Training Loss 0.3718788736616559\n",
      "2022-03-26 23:07:27.122271 Epoch 50, Training Loss 0.3728597403487281\n",
      "2022-03-26 23:07:27.140300 Epoch 50, Training Loss 0.3739772635652586\n",
      "2022-03-26 23:07:27.159474 Epoch 50, Training Loss 0.3751569493957188\n",
      "2022-03-26 23:07:27.178413 Epoch 50, Training Loss 0.3760831347664299\n",
      "2022-03-26 23:07:27.196362 Epoch 50, Training Loss 0.37707793003762774\n",
      "2022-03-26 23:07:27.216663 Epoch 50, Training Loss 0.377991153151178\n",
      "2022-03-26 23:07:27.234045 Epoch 50, Training Loss 0.3789819090262703\n",
      "2022-03-26 23:07:27.252067 Epoch 50, Training Loss 0.38008351589712647\n",
      "2022-03-26 23:07:27.272094 Epoch 50, Training Loss 0.38095032536160306\n",
      "2022-03-26 23:07:27.291137 Epoch 50, Training Loss 0.3817796210956086\n",
      "2022-03-26 23:07:27.310020 Epoch 50, Training Loss 0.38280574622971325\n",
      "2022-03-26 23:07:27.330156 Epoch 50, Training Loss 0.38390990558182797\n",
      "2022-03-26 23:07:27.349024 Epoch 50, Training Loss 0.38488721291122535\n",
      "2022-03-26 23:07:27.367073 Epoch 50, Training Loss 0.38607434901739934\n",
      "2022-03-26 23:07:27.384712 Epoch 50, Training Loss 0.3871447373076778\n",
      "2022-03-26 23:07:27.404848 Epoch 50, Training Loss 0.38835662001234184\n",
      "2022-03-26 23:07:27.423425 Epoch 50, Training Loss 0.38926996546023335\n",
      "2022-03-26 23:07:27.442351 Epoch 50, Training Loss 0.390225425858022\n",
      "2022-03-26 23:07:27.462276 Epoch 50, Training Loss 0.39113897839775474\n",
      "2022-03-26 23:07:27.481296 Epoch 50, Training Loss 0.3921423694666694\n",
      "2022-03-26 23:07:27.499926 Epoch 50, Training Loss 0.3930199019744268\n",
      "2022-03-26 23:07:27.517939 Epoch 50, Training Loss 0.39402984391392953\n",
      "2022-03-26 23:07:27.536503 Epoch 50, Training Loss 0.39491029636329394\n",
      "2022-03-26 23:07:27.556465 Epoch 50, Training Loss 0.3962042295109585\n",
      "2022-03-26 23:07:27.574338 Epoch 50, Training Loss 0.39725945809917984\n",
      "2022-03-26 23:07:27.594338 Epoch 50, Training Loss 0.39863538665844656\n",
      "2022-03-26 23:07:27.613361 Epoch 50, Training Loss 0.3999957577956607\n",
      "2022-03-26 23:07:27.633316 Epoch 50, Training Loss 0.40125450933985696\n",
      "2022-03-26 23:07:27.651376 Epoch 50, Training Loss 0.4027274533763261\n",
      "2022-03-26 23:07:27.670518 Epoch 50, Training Loss 0.40390176434651054\n",
      "2022-03-26 23:07:27.689510 Epoch 50, Training Loss 0.40529267486099085\n",
      "2022-03-26 23:07:27.707480 Epoch 50, Training Loss 0.40628408608229266\n",
      "2022-03-26 23:07:27.726511 Epoch 50, Training Loss 0.40730506821971413\n",
      "2022-03-26 23:07:27.748454 Epoch 50, Training Loss 0.40805902436871055\n",
      "2022-03-26 23:07:27.766411 Epoch 50, Training Loss 0.40898186776339246\n",
      "2022-03-26 23:07:27.784342 Epoch 50, Training Loss 0.4102152930501172\n",
      "2022-03-26 23:07:27.802768 Epoch 50, Training Loss 0.41137389552867626\n",
      "2022-03-26 23:07:27.822666 Epoch 50, Training Loss 0.4125226436521086\n",
      "2022-03-26 23:07:27.842644 Epoch 50, Training Loss 0.41360733111191283\n",
      "2022-03-26 23:07:27.861357 Epoch 50, Training Loss 0.4146759481076389\n",
      "2022-03-26 23:07:27.879567 Epoch 50, Training Loss 0.41542484571256905\n",
      "2022-03-26 23:07:27.898566 Epoch 50, Training Loss 0.41638983119174344\n",
      "2022-03-26 23:07:27.917628 Epoch 50, Training Loss 0.41751390535508276\n",
      "2022-03-26 23:07:27.935461 Epoch 50, Training Loss 0.41881739643528637\n",
      "2022-03-26 23:07:27.955494 Epoch 50, Training Loss 0.41957627278764537\n",
      "2022-03-26 23:07:27.974456 Epoch 50, Training Loss 0.4207330082383607\n",
      "2022-03-26 23:07:27.992508 Epoch 50, Training Loss 0.42138134739587985\n",
      "2022-03-26 23:07:28.011818 Epoch 50, Training Loss 0.42224219075554165\n",
      "2022-03-26 23:07:28.030790 Epoch 50, Training Loss 0.4234380468230723\n",
      "2022-03-26 23:07:28.048824 Epoch 50, Training Loss 0.42440214631197704\n",
      "2022-03-26 23:07:28.066891 Epoch 50, Training Loss 0.4256303331736104\n",
      "2022-03-26 23:07:28.085738 Epoch 50, Training Loss 0.42649848602921764\n",
      "2022-03-26 23:07:28.103675 Epoch 50, Training Loss 0.42756927821337415\n",
      "2022-03-26 23:07:28.123687 Epoch 50, Training Loss 0.4287261270043795\n",
      "2022-03-26 23:07:28.141653 Epoch 50, Training Loss 0.4298095044577518\n",
      "2022-03-26 23:07:28.161704 Epoch 50, Training Loss 0.4307878941983518\n",
      "2022-03-26 23:07:28.180619 Epoch 50, Training Loss 0.43203071217097894\n",
      "2022-03-26 23:07:28.198643 Epoch 50, Training Loss 0.43322770545244826\n",
      "2022-03-26 23:07:28.216720 Epoch 50, Training Loss 0.4342658118823605\n",
      "2022-03-26 23:07:28.235631 Epoch 50, Training Loss 0.4354295091098532\n",
      "2022-03-26 23:07:28.255193 Epoch 50, Training Loss 0.4366223036938006\n",
      "2022-03-26 23:07:28.274186 Epoch 50, Training Loss 0.43771506148531003\n",
      "2022-03-26 23:07:28.292053 Epoch 50, Training Loss 0.43905669252585877\n",
      "2022-03-26 23:07:28.311085 Epoch 50, Training Loss 0.44011540539429317\n",
      "2022-03-26 23:07:28.329091 Epoch 50, Training Loss 0.44090396600306186\n",
      "2022-03-26 23:07:28.348017 Epoch 50, Training Loss 0.44198599526339477\n",
      "2022-03-26 23:07:28.367028 Epoch 50, Training Loss 0.4430585284062359\n",
      "2022-03-26 23:07:28.386582 Epoch 50, Training Loss 0.4440555795074424\n",
      "2022-03-26 23:07:28.405337 Epoch 50, Training Loss 0.4451576400443416\n",
      "2022-03-26 23:07:28.425073 Epoch 50, Training Loss 0.44638056180361285\n",
      "2022-03-26 23:07:28.444972 Epoch 50, Training Loss 0.44752893690258033\n",
      "2022-03-26 23:07:28.463882 Epoch 50, Training Loss 0.44833437240946933\n",
      "2022-03-26 23:07:28.482845 Epoch 50, Training Loss 0.44955870402438564\n",
      "2022-03-26 23:07:28.501899 Epoch 50, Training Loss 0.4507689417323188\n",
      "2022-03-26 23:07:28.520905 Epoch 50, Training Loss 0.451766125457671\n",
      "2022-03-26 23:07:28.539855 Epoch 50, Training Loss 0.45268599342202287\n",
      "2022-03-26 23:07:28.558850 Epoch 50, Training Loss 0.4538936868043202\n",
      "2022-03-26 23:07:28.578843 Epoch 50, Training Loss 0.4550783346833475\n",
      "2022-03-26 23:07:28.596885 Epoch 50, Training Loss 0.4559984559293293\n",
      "2022-03-26 23:07:28.615914 Epoch 50, Training Loss 0.45689911686855816\n",
      "2022-03-26 23:07:28.634188 Epoch 50, Training Loss 0.45776324709663\n",
      "2022-03-26 23:07:28.654166 Epoch 50, Training Loss 0.45880711208219116\n",
      "2022-03-26 23:07:28.672195 Epoch 50, Training Loss 0.46001234582013184\n",
      "2022-03-26 23:07:28.692102 Epoch 50, Training Loss 0.4611938316803759\n",
      "2022-03-26 23:07:28.710107 Epoch 50, Training Loss 0.4622493733835342\n",
      "2022-03-26 23:07:28.729593 Epoch 50, Training Loss 0.46320073485679336\n",
      "2022-03-26 23:07:28.748191 Epoch 50, Training Loss 0.4643914746811323\n",
      "2022-03-26 23:07:28.767182 Epoch 50, Training Loss 0.46564507949382755\n",
      "2022-03-26 23:07:28.785860 Epoch 50, Training Loss 0.46696611476676236\n",
      "2022-03-26 23:07:28.804881 Epoch 50, Training Loss 0.4681805753342026\n",
      "2022-03-26 23:07:28.824447 Epoch 50, Training Loss 0.46928447157220765\n",
      "2022-03-26 23:07:28.843386 Epoch 50, Training Loss 0.4705428999403249\n",
      "2022-03-26 23:07:28.862448 Epoch 50, Training Loss 0.47140438111541827\n",
      "2022-03-26 23:07:28.880447 Epoch 50, Training Loss 0.4726511125674333\n",
      "2022-03-26 23:07:28.899322 Epoch 50, Training Loss 0.4735496444317996\n",
      "2022-03-26 23:07:28.917575 Epoch 50, Training Loss 0.4748839041613557\n",
      "2022-03-26 23:07:28.936713 Epoch 50, Training Loss 0.4763002475661695\n",
      "2022-03-26 23:07:28.955899 Epoch 50, Training Loss 0.4772619113440404\n",
      "2022-03-26 23:07:28.982746 Epoch 50, Training Loss 0.4781998653546014\n",
      "2022-03-26 23:07:29.009777 Epoch 50, Training Loss 0.4791391814303825\n",
      "2022-03-26 23:07:29.035739 Epoch 50, Training Loss 0.4800668769632764\n",
      "2022-03-26 23:07:29.062884 Epoch 50, Training Loss 0.4811006710504937\n",
      "2022-03-26 23:07:29.088767 Epoch 50, Training Loss 0.481971708176386\n",
      "2022-03-26 23:07:29.115162 Epoch 50, Training Loss 0.48275230973577865\n",
      "2022-03-26 23:07:29.141679 Epoch 50, Training Loss 0.4837835828971375\n",
      "2022-03-26 23:07:29.168606 Epoch 50, Training Loss 0.484713857817223\n",
      "2022-03-26 23:07:29.184084 Epoch 50, Training Loss 0.48588029388576515\n",
      "2022-03-26 23:07:29.198073 Epoch 50, Training Loss 0.4870875674440428\n",
      "2022-03-26 23:07:29.213920 Epoch 50, Training Loss 0.4881457077420276\n",
      "2022-03-26 23:07:29.228925 Epoch 50, Training Loss 0.48930917844138183\n",
      "2022-03-26 23:07:29.242962 Epoch 50, Training Loss 0.49041221132668694\n",
      "2022-03-26 23:07:29.257967 Epoch 50, Training Loss 0.491683744561032\n",
      "2022-03-26 23:07:29.273419 Epoch 50, Training Loss 0.49269800661774854\n",
      "2022-03-26 23:07:29.288434 Epoch 50, Training Loss 0.4937109182710233\n",
      "2022-03-26 23:07:29.303660 Epoch 50, Training Loss 0.49482786304810467\n",
      "2022-03-26 23:07:29.318663 Epoch 50, Training Loss 0.4959881720335587\n",
      "2022-03-26 23:07:29.332650 Epoch 50, Training Loss 0.4970738720101164\n",
      "2022-03-26 23:07:29.347765 Epoch 50, Training Loss 0.4981434225578747\n",
      "2022-03-26 23:07:29.362451 Epoch 50, Training Loss 0.499137710503605\n",
      "2022-03-26 23:07:29.377520 Epoch 50, Training Loss 0.5004375894813586\n",
      "2022-03-26 23:07:29.393164 Epoch 50, Training Loss 0.5014445535514666\n",
      "2022-03-26 23:07:29.408431 Epoch 50, Training Loss 0.5027628404557553\n",
      "2022-03-26 23:07:29.423563 Epoch 50, Training Loss 0.5035679986714707\n",
      "2022-03-26 23:07:29.438572 Epoch 50, Training Loss 0.50462517454801\n",
      "2022-03-26 23:07:29.453107 Epoch 50, Training Loss 0.5059731715475507\n",
      "2022-03-26 23:07:29.468635 Epoch 50, Training Loss 0.5070413423468695\n",
      "2022-03-26 23:07:29.482564 Epoch 50, Training Loss 0.5080479343833826\n",
      "2022-03-26 23:07:29.496587 Epoch 50, Training Loss 0.509213762286374\n",
      "2022-03-26 23:07:29.511492 Epoch 50, Training Loss 0.5105386701843623\n",
      "2022-03-26 23:07:29.525671 Epoch 50, Training Loss 0.5117115013282317\n",
      "2022-03-26 23:07:29.540638 Epoch 50, Training Loss 0.5126644916394177\n",
      "2022-03-26 23:07:29.556088 Epoch 50, Training Loss 0.513761836244627\n",
      "2022-03-26 23:07:29.571085 Epoch 50, Training Loss 0.5146650380033362\n",
      "2022-03-26 23:07:29.586099 Epoch 50, Training Loss 0.5159909538448314\n",
      "2022-03-26 23:07:29.600753 Epoch 50, Training Loss 0.5170259867482783\n",
      "2022-03-26 23:07:29.615733 Epoch 50, Training Loss 0.5181182186926723\n",
      "2022-03-26 23:07:29.631782 Epoch 50, Training Loss 0.5191410825685467\n",
      "2022-03-26 23:07:29.646781 Epoch 50, Training Loss 0.5206223153092367\n",
      "2022-03-26 23:07:29.661827 Epoch 50, Training Loss 0.5219308174479648\n",
      "2022-03-26 23:07:29.676692 Epoch 50, Training Loss 0.5231019148741232\n",
      "2022-03-26 23:07:29.690745 Epoch 50, Training Loss 0.5244474295155167\n",
      "2022-03-26 23:07:29.705767 Epoch 50, Training Loss 0.5255959341897989\n",
      "2022-03-26 23:07:29.720752 Epoch 50, Training Loss 0.5266124929308587\n",
      "2022-03-26 23:07:29.735710 Epoch 50, Training Loss 0.5276749459526423\n",
      "2022-03-26 23:07:29.749756 Epoch 50, Training Loss 0.5286638325894885\n",
      "2022-03-26 23:07:29.764777 Epoch 50, Training Loss 0.5300411381532469\n",
      "2022-03-26 23:07:29.778759 Epoch 50, Training Loss 0.5310792938979996\n",
      "2022-03-26 23:07:29.793840 Epoch 50, Training Loss 0.5321111427548596\n",
      "2022-03-26 23:07:29.808843 Epoch 50, Training Loss 0.5329050511655296\n",
      "2022-03-26 23:07:29.824079 Epoch 50, Training Loss 0.5339591673115636\n",
      "2022-03-26 23:07:29.838082 Epoch 50, Training Loss 0.5351370270447353\n",
      "2022-03-26 23:07:29.852843 Epoch 50, Training Loss 0.5361553124149742\n",
      "2022-03-26 23:07:29.867846 Epoch 50, Training Loss 0.5373009311421143\n",
      "2022-03-26 23:07:29.881887 Epoch 50, Training Loss 0.5383913070344559\n",
      "2022-03-26 23:07:29.896886 Epoch 50, Training Loss 0.5391022157486137\n",
      "2022-03-26 23:07:29.911898 Epoch 50, Training Loss 0.5402629871654998\n",
      "2022-03-26 23:07:29.925384 Epoch 50, Training Loss 0.541502649064564\n",
      "2022-03-26 23:07:29.940992 Epoch 50, Training Loss 0.5425335396738613\n",
      "2022-03-26 23:07:29.955018 Epoch 50, Training Loss 0.543909968698726\n",
      "2022-03-26 23:07:29.970015 Epoch 50, Training Loss 0.5448230084250955\n",
      "2022-03-26 23:07:29.983894 Epoch 50, Training Loss 0.5458644227603512\n",
      "2022-03-26 23:07:29.998899 Epoch 50, Training Loss 0.5470808014997741\n",
      "2022-03-26 23:07:30.013760 Epoch 50, Training Loss 0.5481142823196128\n",
      "2022-03-26 23:07:30.029758 Epoch 50, Training Loss 0.5490088611460098\n",
      "2022-03-26 23:07:30.043953 Epoch 50, Training Loss 0.5502963295525602\n",
      "2022-03-26 23:07:30.058957 Epoch 50, Training Loss 0.5512094390971581\n",
      "2022-03-26 23:07:30.073845 Epoch 50, Training Loss 0.5520441292801781\n",
      "2022-03-26 23:07:30.088721 Epoch 50, Training Loss 0.5528444172933583\n",
      "2022-03-26 23:07:30.103710 Epoch 50, Training Loss 0.5535936071287335\n",
      "2022-03-26 23:07:30.118884 Epoch 50, Training Loss 0.554683824329425\n",
      "2022-03-26 23:07:30.132732 Epoch 50, Training Loss 0.555672019872519\n",
      "2022-03-26 23:07:30.146746 Epoch 50, Training Loss 0.5569033981741541\n",
      "2022-03-26 23:07:30.161480 Epoch 50, Training Loss 0.557949726843773\n",
      "2022-03-26 23:07:30.176331 Epoch 50, Training Loss 0.5590211326051551\n",
      "2022-03-26 23:07:30.191352 Epoch 50, Training Loss 0.5600429229114366\n",
      "2022-03-26 23:07:30.205285 Epoch 50, Training Loss 0.5610200031791501\n",
      "2022-03-26 23:07:30.219399 Epoch 50, Training Loss 0.5620137215270411\n",
      "2022-03-26 23:07:30.233732 Epoch 50, Training Loss 0.5631393344353532\n",
      "2022-03-26 23:07:30.248746 Epoch 50, Training Loss 0.5642830384204455\n",
      "2022-03-26 23:07:30.263392 Epoch 50, Training Loss 0.5652215176683557\n",
      "2022-03-26 23:07:30.277419 Epoch 50, Training Loss 0.5661432499166035\n",
      "2022-03-26 23:07:30.292714 Epoch 50, Training Loss 0.566995416608308\n",
      "2022-03-26 23:07:30.306751 Epoch 50, Training Loss 0.5681768912640984\n",
      "2022-03-26 23:07:30.322608 Epoch 50, Training Loss 0.5692116563277476\n",
      "2022-03-26 23:07:30.336679 Epoch 50, Training Loss 0.5705447565868992\n",
      "2022-03-26 23:07:30.351804 Epoch 50, Training Loss 0.5714205320533889\n",
      "2022-03-26 23:07:30.365866 Epoch 50, Training Loss 0.572218639390243\n",
      "2022-03-26 23:07:30.380889 Epoch 50, Training Loss 0.5732490620802125\n",
      "2022-03-26 23:07:30.394900 Epoch 50, Training Loss 0.5743667992484539\n",
      "2022-03-26 23:07:30.409846 Epoch 50, Training Loss 0.5756126232159412\n",
      "2022-03-26 23:07:30.425740 Epoch 50, Training Loss 0.5766725851904095\n",
      "2022-03-26 23:07:30.440684 Epoch 50, Training Loss 0.5777547586604458\n",
      "2022-03-26 23:07:30.455573 Epoch 50, Training Loss 0.5786647797393067\n",
      "2022-03-26 23:07:30.469696 Epoch 50, Training Loss 0.5798653137805821\n",
      "2022-03-26 23:07:30.485113 Epoch 50, Training Loss 0.5810301869421663\n",
      "2022-03-26 23:07:30.499120 Epoch 50, Training Loss 0.5822279416691617\n",
      "2022-03-26 23:07:30.514115 Epoch 50, Training Loss 0.5833012332849186\n",
      "2022-03-26 23:07:30.528124 Epoch 50, Training Loss 0.5846322768027216\n",
      "2022-03-26 23:07:30.543678 Epoch 50, Training Loss 0.5859977891835411\n",
      "2022-03-26 23:07:30.559684 Epoch 50, Training Loss 0.5870283124087107\n",
      "2022-03-26 23:07:30.573856 Epoch 50, Training Loss 0.5881702692612357\n",
      "2022-03-26 23:07:30.588742 Epoch 50, Training Loss 0.5890678353321827\n",
      "2022-03-26 23:07:30.603755 Epoch 50, Training Loss 0.5903785352206901\n",
      "2022-03-26 23:07:30.618736 Epoch 50, Training Loss 0.5914696769793625\n",
      "2022-03-26 23:07:30.632757 Epoch 50, Training Loss 0.5923364193695585\n",
      "2022-03-26 23:07:30.648731 Epoch 50, Training Loss 0.5934870103588494\n",
      "2022-03-26 23:07:30.663738 Epoch 50, Training Loss 0.5944800440917539\n",
      "2022-03-26 23:07:30.678738 Epoch 50, Training Loss 0.5955002676800388\n",
      "2022-03-26 23:07:30.691894 Epoch 50, Training Loss 0.5963884999837412\n",
      "2022-03-26 23:07:30.706900 Epoch 50, Training Loss 0.597361770965864\n",
      "2022-03-26 23:07:30.721540 Epoch 50, Training Loss 0.5984349271373066\n",
      "2022-03-26 23:07:30.735144 Epoch 50, Training Loss 0.5994605234516855\n",
      "2022-03-26 23:07:30.749961 Epoch 50, Training Loss 0.6005012031711275\n",
      "2022-03-26 23:07:30.764106 Epoch 50, Training Loss 0.60171990976919\n",
      "2022-03-26 23:07:30.777969 Epoch 50, Training Loss 0.6030016639043608\n",
      "2022-03-26 23:07:30.792846 Epoch 50, Training Loss 0.603952621407521\n",
      "2022-03-26 23:07:30.807868 Epoch 50, Training Loss 0.605145292361374\n",
      "2022-03-26 23:07:30.821792 Epoch 50, Training Loss 0.6060154987570575\n",
      "2022-03-26 23:07:30.837010 Epoch 50, Training Loss 0.6071118749773411\n",
      "2022-03-26 23:07:30.850934 Epoch 50, Training Loss 0.6081255858816454\n",
      "2022-03-26 23:07:30.864931 Epoch 50, Training Loss 0.6092355279513942\n",
      "2022-03-26 23:07:30.879945 Epoch 50, Training Loss 0.6106182965628631\n",
      "2022-03-26 23:07:30.895042 Epoch 50, Training Loss 0.6120059347671011\n",
      "2022-03-26 23:07:30.909877 Epoch 50, Training Loss 0.6133182887988322\n",
      "2022-03-26 23:07:30.923871 Epoch 50, Training Loss 0.6142265620591391\n",
      "2022-03-26 23:07:30.938920 Epoch 50, Training Loss 0.615285275080015\n",
      "2022-03-26 23:07:30.953967 Epoch 50, Training Loss 0.6163025505250067\n",
      "2022-03-26 23:07:30.968999 Epoch 50, Training Loss 0.6174999294073685\n",
      "2022-03-26 23:07:30.987932 Epoch 50, Training Loss 0.6185887647254388\n",
      "2022-03-26 23:07:31.006959 Epoch 50, Training Loss 0.6197061964770412\n",
      "2022-03-26 23:07:31.027002 Epoch 50, Training Loss 0.6210156487839301\n",
      "2022-03-26 23:07:31.046950 Epoch 50, Training Loss 0.6218803772688521\n",
      "2022-03-26 23:07:31.066123 Epoch 50, Training Loss 0.6229350818392566\n",
      "2022-03-26 23:07:31.084974 Epoch 50, Training Loss 0.6241026763873332\n",
      "2022-03-26 23:07:31.103751 Epoch 50, Training Loss 0.6254251985751149\n",
      "2022-03-26 23:07:31.123783 Epoch 50, Training Loss 0.6265061230153379\n",
      "2022-03-26 23:07:31.142875 Epoch 50, Training Loss 0.627550242959386\n",
      "2022-03-26 23:07:31.161916 Epoch 50, Training Loss 0.6286165881949617\n",
      "2022-03-26 23:07:31.180834 Epoch 50, Training Loss 0.6296419261209191\n",
      "2022-03-26 23:07:31.200819 Epoch 50, Training Loss 0.6306208820294237\n",
      "2022-03-26 23:07:31.219714 Epoch 50, Training Loss 0.6316245367459934\n",
      "2022-03-26 23:07:31.240476 Epoch 50, Training Loss 0.6326855529299782\n",
      "2022-03-26 23:07:31.259394 Epoch 50, Training Loss 0.6337375765871209\n",
      "2022-03-26 23:07:31.278933 Epoch 50, Training Loss 0.6351156914630509\n",
      "2022-03-26 23:07:31.297857 Epoch 50, Training Loss 0.636269696075898\n",
      "2022-03-26 23:07:31.316731 Epoch 50, Training Loss 0.6369933242840535\n",
      "2022-03-26 23:07:31.335732 Epoch 50, Training Loss 0.6381634094983416\n",
      "2022-03-26 23:07:31.355568 Epoch 50, Training Loss 0.6392731461531062\n",
      "2022-03-26 23:07:31.374831 Epoch 50, Training Loss 0.6401850803733786\n",
      "2022-03-26 23:07:31.394637 Epoch 50, Training Loss 0.6411178180628725\n",
      "2022-03-26 23:07:31.414280 Epoch 50, Training Loss 0.6424209720948163\n",
      "2022-03-26 23:07:31.433344 Epoch 50, Training Loss 0.6435661027803445\n",
      "2022-03-26 23:07:31.453021 Epoch 50, Training Loss 0.6445250396838273\n",
      "2022-03-26 23:07:31.473027 Epoch 50, Training Loss 0.6455583086099161\n",
      "2022-03-26 23:07:31.491885 Epoch 50, Training Loss 0.6469427923412274\n",
      "2022-03-26 23:07:31.511361 Epoch 50, Training Loss 0.6482695135314142\n",
      "2022-03-26 23:07:31.530347 Epoch 50, Training Loss 0.6495601710151223\n",
      "2022-03-26 23:07:31.550303 Epoch 50, Training Loss 0.6505303417172883\n",
      "2022-03-26 23:07:31.569198 Epoch 50, Training Loss 0.6516664801046367\n",
      "2022-03-26 23:07:31.589865 Epoch 50, Training Loss 0.6524364691408698\n",
      "2022-03-26 23:07:31.608891 Epoch 50, Training Loss 0.6534801617912624\n",
      "2022-03-26 23:07:31.627837 Epoch 50, Training Loss 0.6546003269722395\n",
      "2022-03-26 23:07:31.646732 Epoch 50, Training Loss 0.6554999065673565\n",
      "2022-03-26 23:07:31.666755 Epoch 50, Training Loss 0.6568486852871488\n",
      "2022-03-26 23:07:31.685905 Epoch 50, Training Loss 0.657830621808996\n",
      "2022-03-26 23:07:31.704953 Epoch 50, Training Loss 0.6591100784976159\n",
      "2022-03-26 23:07:31.723696 Epoch 50, Training Loss 0.6601074809003669\n",
      "2022-03-26 23:07:31.744626 Epoch 50, Training Loss 0.6608560256031163\n",
      "2022-03-26 23:07:31.764207 Epoch 50, Training Loss 0.6622410152879212\n",
      "2022-03-26 23:07:31.783038 Epoch 50, Training Loss 0.6632697654654608\n",
      "2022-03-26 23:07:31.802009 Epoch 50, Training Loss 0.664252453981458\n",
      "2022-03-26 23:07:31.821214 Epoch 50, Training Loss 0.6651197352525219\n",
      "2022-03-26 23:07:31.840134 Epoch 50, Training Loss 0.6662285439955914\n",
      "2022-03-26 23:07:31.859734 Epoch 50, Training Loss 0.667199851225709\n",
      "2022-03-26 23:07:31.879616 Epoch 50, Training Loss 0.6682894173485544\n",
      "2022-03-26 23:07:31.899574 Epoch 50, Training Loss 0.669369462429715\n",
      "2022-03-26 23:07:31.918056 Epoch 50, Training Loss 0.6703534819128568\n",
      "2022-03-26 23:07:31.937978 Epoch 50, Training Loss 0.6719058205556991\n",
      "2022-03-26 23:07:31.956888 Epoch 50, Training Loss 0.6727914079985655\n",
      "2022-03-26 23:07:31.976910 Epoch 50, Training Loss 0.6737607701507675\n",
      "2022-03-26 23:07:31.996904 Epoch 50, Training Loss 0.6751924575594686\n",
      "2022-03-26 23:07:32.016178 Epoch 50, Training Loss 0.6765113688643326\n",
      "2022-03-26 23:07:32.035044 Epoch 50, Training Loss 0.6774738967571112\n",
      "2022-03-26 23:07:32.055106 Epoch 50, Training Loss 0.6784506733612636\n",
      "2022-03-26 23:07:32.075336 Epoch 50, Training Loss 0.6794751187419648\n",
      "2022-03-26 23:07:32.095270 Epoch 50, Training Loss 0.680613650080493\n",
      "2022-03-26 23:07:32.114988 Epoch 50, Training Loss 0.6816950348941871\n",
      "2022-03-26 23:07:32.134002 Epoch 50, Training Loss 0.6826323920960926\n",
      "2022-03-26 23:07:32.152888 Epoch 50, Training Loss 0.6838554247565891\n",
      "2022-03-26 23:07:32.171950 Epoch 50, Training Loss 0.6850354794955924\n",
      "2022-03-26 23:07:32.191968 Epoch 50, Training Loss 0.6862180399925203\n",
      "2022-03-26 23:07:32.211886 Epoch 50, Training Loss 0.6873876954740881\n",
      "2022-03-26 23:07:32.230749 Epoch 50, Training Loss 0.6881406285878643\n",
      "2022-03-26 23:07:32.249327 Epoch 50, Training Loss 0.6891771675375722\n",
      "2022-03-26 23:07:32.267916 Epoch 50, Training Loss 0.6904530452797785\n",
      "2022-03-26 23:07:32.287749 Epoch 50, Training Loss 0.691529440605427\n",
      "2022-03-26 23:07:32.306851 Epoch 50, Training Loss 0.6924203513071056\n",
      "2022-03-26 23:07:32.326784 Epoch 50, Training Loss 0.6935760061759169\n",
      "2022-03-26 23:07:32.346812 Epoch 50, Training Loss 0.6945801732485252\n",
      "2022-03-26 23:07:32.365749 Epoch 50, Training Loss 0.6958886801129411\n",
      "2022-03-26 23:07:32.385690 Epoch 50, Training Loss 0.6966513631593846\n",
      "2022-03-26 23:07:32.404715 Epoch 50, Training Loss 0.6977748762616112\n",
      "2022-03-26 23:07:32.424315 Epoch 50, Training Loss 0.6987074186734836\n",
      "2022-03-26 23:07:32.443172 Epoch 50, Training Loss 0.6995910342087222\n",
      "2022-03-26 23:07:32.462912 Epoch 50, Training Loss 0.7007814300487109\n",
      "2022-03-26 23:07:32.482840 Epoch 50, Training Loss 0.7019633834471788\n",
      "2022-03-26 23:07:32.502861 Epoch 50, Training Loss 0.7032303454930825\n",
      "2022-03-26 23:07:32.522547 Epoch 50, Training Loss 0.704374255884029\n",
      "2022-03-26 23:07:32.541659 Epoch 50, Training Loss 0.7054385625188003\n",
      "2022-03-26 23:07:32.561525 Epoch 50, Training Loss 0.7065353840970627\n",
      "2022-03-26 23:07:32.581863 Epoch 50, Training Loss 0.7079189600389632\n",
      "2022-03-26 23:07:32.600982 Epoch 50, Training Loss 0.7089276765770924\n",
      "2022-03-26 23:07:32.619457 Epoch 50, Training Loss 0.7097970372270745\n",
      "2022-03-26 23:07:32.639590 Epoch 50, Training Loss 0.7109244838547524\n",
      "2022-03-26 23:07:32.658891 Epoch 50, Training Loss 0.7120020625841282\n",
      "2022-03-26 23:07:32.679112 Epoch 50, Training Loss 0.7129554345327265\n",
      "2022-03-26 23:07:32.698063 Epoch 50, Training Loss 0.7139383201556437\n",
      "2022-03-26 23:07:32.717009 Epoch 50, Training Loss 0.7149745537649335\n",
      "2022-03-26 23:07:32.736901 Epoch 50, Training Loss 0.7155592277684175\n",
      "2022-03-26 23:07:32.757954 Epoch 50, Training Loss 0.7164697153184115\n",
      "2022-03-26 23:07:32.776963 Epoch 50, Training Loss 0.7177184160103274\n",
      "2022-03-26 23:07:32.796430 Epoch 50, Training Loss 0.718541121467605\n",
      "2022-03-26 23:07:32.815460 Epoch 50, Training Loss 0.7194726751436054\n",
      "2022-03-26 23:07:32.834996 Epoch 50, Training Loss 0.7207045027666994\n",
      "2022-03-26 23:07:32.854809 Epoch 50, Training Loss 0.7216345491006856\n",
      "2022-03-26 23:07:32.875283 Epoch 50, Training Loss 0.7225183038150563\n",
      "2022-03-26 23:07:32.894161 Epoch 50, Training Loss 0.723611425408317\n",
      "2022-03-26 23:07:32.914917 Epoch 50, Training Loss 0.7246731840588553\n",
      "2022-03-26 23:07:32.934833 Epoch 50, Training Loss 0.7258281471479274\n",
      "2022-03-26 23:07:32.955204 Epoch 50, Training Loss 0.7268539831766387\n",
      "2022-03-26 23:07:32.974870 Epoch 50, Training Loss 0.7280268649311017\n",
      "2022-03-26 23:07:32.994833 Epoch 50, Training Loss 0.7292796925205709\n",
      "2022-03-26 23:07:33.013842 Epoch 50, Training Loss 0.7303067784937446\n",
      "2022-03-26 23:07:33.033130 Epoch 50, Training Loss 0.7314534215518581\n",
      "2022-03-26 23:07:33.052868 Epoch 50, Training Loss 0.7323986723294953\n",
      "2022-03-26 23:07:33.073863 Epoch 50, Training Loss 0.7334228868374739\n",
      "2022-03-26 23:07:33.093733 Epoch 50, Training Loss 0.734210471972785\n",
      "2022-03-26 23:07:33.113763 Epoch 50, Training Loss 0.7350340089224794\n",
      "2022-03-26 23:07:33.133677 Epoch 50, Training Loss 0.7361677020711972\n",
      "2022-03-26 23:07:33.153781 Epoch 50, Training Loss 0.7372017119394239\n",
      "2022-03-26 23:07:33.172785 Epoch 50, Training Loss 0.7384460203330535\n",
      "2022-03-26 23:07:33.192796 Epoch 50, Training Loss 0.7394296170957863\n",
      "2022-03-26 23:07:33.211828 Epoch 50, Training Loss 0.7405405614687048\n",
      "2022-03-26 23:07:33.231719 Epoch 50, Training Loss 0.7417431399797845\n",
      "2022-03-26 23:07:33.250744 Epoch 50, Training Loss 0.7430756687355773\n",
      "2022-03-26 23:07:33.270509 Epoch 50, Training Loss 0.744183948049155\n",
      "2022-03-26 23:07:33.290490 Epoch 50, Training Loss 0.7451400589150237\n",
      "2022-03-26 23:07:33.310130 Epoch 50, Training Loss 0.7463764867666737\n",
      "2022-03-26 23:07:33.330180 Epoch 50, Training Loss 0.7475034193614559\n",
      "2022-03-26 23:07:33.350876 Epoch 50, Training Loss 0.7486116855650606\n",
      "2022-03-26 23:07:33.369739 Epoch 50, Training Loss 0.7495935204083962\n",
      "2022-03-26 23:07:33.389182 Epoch 50, Training Loss 0.7507336304315826\n",
      "2022-03-26 23:07:33.409208 Epoch 50, Training Loss 0.75188949681304\n",
      "2022-03-26 23:07:33.429908 Epoch 50, Training Loss 0.7528882035056649\n",
      "2022-03-26 23:07:33.448889 Epoch 50, Training Loss 0.7541012558180963\n",
      "2022-03-26 23:07:33.467910 Epoch 50, Training Loss 0.7550463748862372\n",
      "2022-03-26 23:07:33.487883 Epoch 50, Training Loss 0.7561872592362602\n",
      "2022-03-26 23:07:33.507945 Epoch 50, Training Loss 0.7571446009151771\n",
      "2022-03-26 23:07:33.528729 Epoch 50, Training Loss 0.7580062156290654\n",
      "2022-03-26 23:07:33.547677 Epoch 50, Training Loss 0.7592150348684062\n",
      "2022-03-26 23:07:33.567697 Epoch 50, Training Loss 0.7604366666673089\n",
      "2022-03-26 23:07:33.587669 Epoch 50, Training Loss 0.7615115109764402\n",
      "2022-03-26 23:07:33.606901 Epoch 50, Training Loss 0.7625812618323909\n",
      "2022-03-26 23:07:33.627889 Epoch 50, Training Loss 0.7635243940536324\n",
      "2022-03-26 23:07:33.647870 Epoch 50, Training Loss 0.764705447894533\n",
      "2022-03-26 23:07:33.666779 Epoch 50, Training Loss 0.765626241712619\n",
      "2022-03-26 23:07:33.685890 Epoch 50, Training Loss 0.7665192519155\n",
      "2022-03-26 23:07:33.706534 Epoch 50, Training Loss 0.7677677958975058\n",
      "2022-03-26 23:07:33.725688 Epoch 50, Training Loss 0.7686677184861029\n",
      "2022-03-26 23:07:33.745685 Epoch 50, Training Loss 0.7697697669038992\n",
      "2022-03-26 23:07:33.764466 Epoch 50, Training Loss 0.7705706877781607\n",
      "2022-03-26 23:07:33.784541 Epoch 50, Training Loss 0.7715655678068586\n",
      "2022-03-26 23:07:33.804284 Epoch 50, Training Loss 0.7726515041440344\n",
      "2022-03-26 23:07:33.823964 Epoch 50, Training Loss 0.7737313986891676\n",
      "2022-03-26 23:07:33.842992 Epoch 50, Training Loss 0.7748126083475244\n",
      "2022-03-26 23:07:33.863858 Epoch 50, Training Loss 0.7755293248559508\n",
      "2022-03-26 23:07:33.884158 Epoch 50, Training Loss 0.7765962633178057\n",
      "2022-03-26 23:07:33.903718 Epoch 50, Training Loss 0.7777094730483297\n",
      "2022-03-26 23:07:33.923077 Epoch 50, Training Loss 0.7788182141835732\n",
      "2022-03-26 23:07:33.942945 Epoch 50, Training Loss 0.7800387257657697\n",
      "2022-03-26 23:07:33.963347 Epoch 50, Training Loss 0.7811345932123911\n",
      "2022-03-26 23:07:33.983399 Epoch 50, Training Loss 0.7822923975832322\n",
      "2022-03-26 23:07:34.004125 Epoch 50, Training Loss 0.7831544539202815\n",
      "2022-03-26 23:07:34.023851 Epoch 50, Training Loss 0.784145965448121\n",
      "2022-03-26 23:07:34.043854 Epoch 50, Training Loss 0.7851599609607931\n",
      "2022-03-26 23:07:34.063903 Epoch 50, Training Loss 0.7863614109471021\n",
      "2022-03-26 23:07:34.083855 Epoch 50, Training Loss 0.7874849979834788\n",
      "2022-03-26 23:07:34.103857 Epoch 50, Training Loss 0.7886423834449495\n",
      "2022-03-26 23:07:34.123391 Epoch 50, Training Loss 0.7896081389063765\n",
      "2022-03-26 23:07:34.144390 Epoch 50, Training Loss 0.7905741019169693\n",
      "2022-03-26 23:07:34.163947 Epoch 50, Training Loss 0.7917561120236926\n",
      "2022-03-26 23:07:34.183856 Epoch 50, Training Loss 0.7928124696702299\n",
      "2022-03-26 23:07:34.204936 Epoch 50, Training Loss 0.793716084393089\n",
      "2022-03-26 23:07:34.224951 Epoch 50, Training Loss 0.7948359423281287\n",
      "2022-03-26 23:07:34.243982 Epoch 50, Training Loss 0.7959290367105732\n",
      "2022-03-26 23:07:34.263850 Epoch 50, Training Loss 0.7969821668646829\n",
      "2022-03-26 23:07:34.283728 Epoch 50, Training Loss 0.7978052377243481\n",
      "2022-03-26 23:07:34.304773 Epoch 50, Training Loss 0.7988405207081524\n",
      "2022-03-26 23:07:34.325742 Epoch 50, Training Loss 0.7999335144791762\n",
      "2022-03-26 23:07:34.346029 Epoch 50, Training Loss 0.8010795655305428\n",
      "2022-03-26 23:07:34.365906 Epoch 50, Training Loss 0.8018698724334502\n",
      "2022-03-26 23:07:34.385864 Epoch 50, Training Loss 0.8032159488219435\n",
      "2022-03-26 23:07:34.405879 Epoch 50, Training Loss 0.804512021639158\n",
      "2022-03-26 23:07:34.425868 Epoch 50, Training Loss 0.8057453498206175\n",
      "2022-03-26 23:07:34.445874 Epoch 50, Training Loss 0.8074087273434299\n",
      "2022-03-26 23:07:34.465908 Epoch 50, Training Loss 0.8084653984860081\n",
      "2022-03-26 23:07:34.485195 Epoch 50, Training Loss 0.8096652862513462\n",
      "2022-03-26 23:07:34.504881 Epoch 50, Training Loss 0.8107792699276029\n",
      "2022-03-26 23:07:34.525912 Epoch 50, Training Loss 0.8117013867096523\n",
      "2022-03-26 23:07:34.545868 Epoch 50, Training Loss 0.8131610817464111\n",
      "2022-03-26 23:07:34.565890 Epoch 50, Training Loss 0.814389352146012\n",
      "2022-03-26 23:07:34.586259 Epoch 50, Training Loss 0.8155867115158559\n",
      "2022-03-26 23:07:34.607321 Epoch 50, Training Loss 0.816882963756771\n",
      "2022-03-26 23:07:34.627494 Epoch 50, Training Loss 0.8179751413557536\n",
      "2022-03-26 23:07:34.647419 Epoch 50, Training Loss 0.8189395217182082\n",
      "2022-03-26 23:07:34.666761 Epoch 50, Training Loss 0.8199679238716965\n",
      "2022-03-26 23:07:34.685791 Epoch 50, Training Loss 0.821118528428285\n",
      "2022-03-26 23:07:34.705750 Epoch 50, Training Loss 0.8219322678073288\n",
      "2022-03-26 23:07:34.724861 Epoch 50, Training Loss 0.8228496290991069\n",
      "2022-03-26 23:07:34.745799 Epoch 50, Training Loss 0.8241687937618216\n",
      "2022-03-26 23:07:34.765352 Epoch 50, Training Loss 0.8253723107792837\n",
      "2022-03-26 23:07:34.784816 Epoch 50, Training Loss 0.826509977469359\n",
      "2022-03-26 23:07:34.804877 Epoch 50, Training Loss 0.8274802075475073\n",
      "2022-03-26 23:07:34.824876 Epoch 50, Training Loss 0.8287057327035138\n",
      "2022-03-26 23:07:34.844765 Epoch 50, Training Loss 0.82993732991121\n",
      "2022-03-26 23:07:34.864611 Epoch 50, Training Loss 0.830951425044433\n",
      "2022-03-26 23:07:34.884527 Epoch 50, Training Loss 0.8320060226770923\n",
      "2022-03-26 23:07:34.905519 Epoch 50, Training Loss 0.8329964322812112\n",
      "2022-03-26 23:07:34.924543 Epoch 50, Training Loss 0.8339928700338544\n",
      "2022-03-26 23:07:34.944165 Epoch 50, Training Loss 0.8351187066501363\n",
      "2022-03-26 23:07:34.963687 Epoch 50, Training Loss 0.8362795994104937\n",
      "2022-03-26 23:07:34.983083 Epoch 50, Training Loss 0.8370366354885004\n",
      "2022-03-26 23:07:35.003977 Epoch 50, Training Loss 0.8380904667975043\n",
      "2022-03-26 23:07:35.024902 Epoch 50, Training Loss 0.8392714065359071\n",
      "2022-03-26 23:07:35.044933 Epoch 50, Training Loss 0.8406170876434697\n",
      "2022-03-26 23:07:35.052967 Epoch 50, Training Loss 0.8418035850195629\n",
      "2022-03-26 23:18:58.518426 Epoch 100, Training Loss 0.0009013950977179095\n",
      "2022-03-26 23:18:58.537812 Epoch 100, Training Loss 0.0017297527064447818\n",
      "2022-03-26 23:18:58.556818 Epoch 100, Training Loss 0.002679147150205529\n",
      "2022-03-26 23:18:58.575820 Epoch 100, Training Loss 0.0037183311894116804\n",
      "2022-03-26 23:18:58.593824 Epoch 100, Training Loss 0.00462434648552819\n",
      "2022-03-26 23:18:58.612829 Epoch 100, Training Loss 0.0056520314777598665\n",
      "2022-03-26 23:18:58.632833 Epoch 100, Training Loss 0.00666173477002117\n",
      "2022-03-26 23:18:58.652715 Epoch 100, Training Loss 0.007527862134796884\n",
      "2022-03-26 23:18:58.671016 Epoch 100, Training Loss 0.00841178438242744\n",
      "2022-03-26 23:18:58.692017 Epoch 100, Training Loss 0.009255240716592734\n",
      "2022-03-26 23:18:58.712028 Epoch 100, Training Loss 0.010273722431543843\n",
      "2022-03-26 23:18:58.730949 Epoch 100, Training Loss 0.011001996600719365\n",
      "2022-03-26 23:18:58.749958 Epoch 100, Training Loss 0.012044060946730396\n",
      "2022-03-26 23:18:58.769992 Epoch 100, Training Loss 0.012842072397851578\n",
      "2022-03-26 23:18:58.789025 Epoch 100, Training Loss 0.01400202245968382\n",
      "2022-03-26 23:18:58.808980 Epoch 100, Training Loss 0.014922587310566622\n",
      "2022-03-26 23:18:58.827992 Epoch 100, Training Loss 0.015748585733916142\n",
      "2022-03-26 23:18:58.847035 Epoch 100, Training Loss 0.016534867341561087\n",
      "2022-03-26 23:18:58.866042 Epoch 100, Training Loss 0.01746375557711667\n",
      "2022-03-26 23:18:58.885088 Epoch 100, Training Loss 0.018336878560693062\n",
      "2022-03-26 23:18:58.904786 Epoch 100, Training Loss 0.0189773217034157\n",
      "2022-03-26 23:18:58.925791 Epoch 100, Training Loss 0.020146055904495745\n",
      "2022-03-26 23:18:58.943824 Epoch 100, Training Loss 0.02102017997170958\n",
      "2022-03-26 23:18:58.962872 Epoch 100, Training Loss 0.021943823989394984\n",
      "2022-03-26 23:18:58.981907 Epoch 100, Training Loss 0.02279432144616266\n",
      "2022-03-26 23:18:59.001813 Epoch 100, Training Loss 0.0238133277124761\n",
      "2022-03-26 23:18:59.021244 Epoch 100, Training Loss 0.024505916839975224\n",
      "2022-03-26 23:18:59.041189 Epoch 100, Training Loss 0.02548991338066433\n",
      "2022-03-26 23:18:59.059295 Epoch 100, Training Loss 0.026279794361890124\n",
      "2022-03-26 23:18:59.077940 Epoch 100, Training Loss 0.027288632319711358\n",
      "2022-03-26 23:18:59.097872 Epoch 100, Training Loss 0.02793680905076244\n",
      "2022-03-26 23:18:59.117880 Epoch 100, Training Loss 0.028946767453952214\n",
      "2022-03-26 23:18:59.137792 Epoch 100, Training Loss 0.029650872320775182\n",
      "2022-03-26 23:18:59.157899 Epoch 100, Training Loss 0.030582634841694552\n",
      "2022-03-26 23:18:59.176902 Epoch 100, Training Loss 0.03116053765844506\n",
      "2022-03-26 23:18:59.195773 Epoch 100, Training Loss 0.032239406043306336\n",
      "2022-03-26 23:18:59.215770 Epoch 100, Training Loss 0.03295978023420514\n",
      "2022-03-26 23:18:59.235776 Epoch 100, Training Loss 0.033918505091496444\n",
      "2022-03-26 23:18:59.255527 Epoch 100, Training Loss 0.03491366546019874\n",
      "2022-03-26 23:18:59.274533 Epoch 100, Training Loss 0.03578084901623104\n",
      "2022-03-26 23:18:59.294677 Epoch 100, Training Loss 0.03640552737828716\n",
      "2022-03-26 23:18:59.313531 Epoch 100, Training Loss 0.03748278704750568\n",
      "2022-03-26 23:18:59.332583 Epoch 100, Training Loss 0.03849420294432384\n",
      "2022-03-26 23:18:59.352640 Epoch 100, Training Loss 0.039176296471329905\n",
      "2022-03-26 23:18:59.372338 Epoch 100, Training Loss 0.04006139564392207\n",
      "2022-03-26 23:18:59.392743 Epoch 100, Training Loss 0.040708990810472336\n",
      "2022-03-26 23:18:59.410659 Epoch 100, Training Loss 0.04180750822472146\n",
      "2022-03-26 23:18:59.429618 Epoch 100, Training Loss 0.04279780799470594\n",
      "2022-03-26 23:18:59.449581 Epoch 100, Training Loss 0.04360183059711895\n",
      "2022-03-26 23:18:59.468470 Epoch 100, Training Loss 0.04455397699190223\n",
      "2022-03-26 23:18:59.488386 Epoch 100, Training Loss 0.04561059965806849\n",
      "2022-03-26 23:18:59.507578 Epoch 100, Training Loss 0.04643990187083974\n",
      "2022-03-26 23:18:59.526588 Epoch 100, Training Loss 0.04728995130190154\n",
      "2022-03-26 23:18:59.545474 Epoch 100, Training Loss 0.048207130959576655\n",
      "2022-03-26 23:18:59.566536 Epoch 100, Training Loss 0.04928516731847583\n",
      "2022-03-26 23:18:59.585981 Epoch 100, Training Loss 0.0501712793889253\n",
      "2022-03-26 23:18:59.605986 Epoch 100, Training Loss 0.051122971431678516\n",
      "2022-03-26 23:18:59.626021 Epoch 100, Training Loss 0.05193382783619034\n",
      "2022-03-26 23:18:59.644934 Epoch 100, Training Loss 0.05282733774246157\n",
      "2022-03-26 23:18:59.663781 Epoch 100, Training Loss 0.05380683329404163\n",
      "2022-03-26 23:18:59.684780 Epoch 100, Training Loss 0.05501706101705351\n",
      "2022-03-26 23:18:59.703673 Epoch 100, Training Loss 0.05578690096545402\n",
      "2022-03-26 23:18:59.722666 Epoch 100, Training Loss 0.05674010583811709\n",
      "2022-03-26 23:18:59.742520 Epoch 100, Training Loss 0.05760035566661669\n",
      "2022-03-26 23:18:59.761510 Epoch 100, Training Loss 0.058402998673031704\n",
      "2022-03-26 23:18:59.780365 Epoch 100, Training Loss 0.059344930264651014\n",
      "2022-03-26 23:18:59.800249 Epoch 100, Training Loss 0.060286694475452006\n",
      "2022-03-26 23:18:59.820203 Epoch 100, Training Loss 0.061029767593764286\n",
      "2022-03-26 23:18:59.839927 Epoch 100, Training Loss 0.06203400982005517\n",
      "2022-03-26 23:18:59.859995 Epoch 100, Training Loss 0.06308840066575638\n",
      "2022-03-26 23:18:59.877836 Epoch 100, Training Loss 0.06391344983559435\n",
      "2022-03-26 23:18:59.897805 Epoch 100, Training Loss 0.06507770538025195\n",
      "2022-03-26 23:18:59.916826 Epoch 100, Training Loss 0.06617584382481587\n",
      "2022-03-26 23:18:59.936798 Epoch 100, Training Loss 0.06719986419848469\n",
      "2022-03-26 23:18:59.955810 Epoch 100, Training Loss 0.06796314832194687\n",
      "2022-03-26 23:18:59.975803 Epoch 100, Training Loss 0.06882086190421258\n",
      "2022-03-26 23:18:59.994825 Epoch 100, Training Loss 0.0694612014629042\n",
      "2022-03-26 23:19:00.014754 Epoch 100, Training Loss 0.0704983297516318\n",
      "2022-03-26 23:19:00.033787 Epoch 100, Training Loss 0.07122875342283712\n",
      "2022-03-26 23:19:00.053617 Epoch 100, Training Loss 0.07213405003328152\n",
      "2022-03-26 23:19:00.073654 Epoch 100, Training Loss 0.07310771942138672\n",
      "2022-03-26 23:19:00.092642 Epoch 100, Training Loss 0.07443496211410483\n",
      "2022-03-26 23:19:00.111691 Epoch 100, Training Loss 0.07544499544231482\n",
      "2022-03-26 23:19:00.131663 Epoch 100, Training Loss 0.07646667088389092\n",
      "2022-03-26 23:19:00.150235 Epoch 100, Training Loss 0.07751035278715442\n",
      "2022-03-26 23:19:00.169323 Epoch 100, Training Loss 0.07850592871151311\n",
      "2022-03-26 23:19:00.188455 Epoch 100, Training Loss 0.07929866202651997\n",
      "2022-03-26 23:19:00.209185 Epoch 100, Training Loss 0.08022691945895515\n",
      "2022-03-26 23:19:00.228129 Epoch 100, Training Loss 0.08120043015541017\n",
      "2022-03-26 23:19:00.247961 Epoch 100, Training Loss 0.08210961387285491\n",
      "2022-03-26 23:19:00.267010 Epoch 100, Training Loss 0.0827482064133105\n",
      "2022-03-26 23:19:00.287046 Epoch 100, Training Loss 0.08368001641977169\n",
      "2022-03-26 23:19:00.306034 Epoch 100, Training Loss 0.08451553512259823\n",
      "2022-03-26 23:19:00.324519 Epoch 100, Training Loss 0.08540047522243636\n",
      "2022-03-26 23:19:00.344403 Epoch 100, Training Loss 0.08642071230179818\n",
      "2022-03-26 23:19:00.363440 Epoch 100, Training Loss 0.08763205894576315\n",
      "2022-03-26 23:19:00.382779 Epoch 100, Training Loss 0.08835384840398189\n",
      "2022-03-26 23:19:00.402752 Epoch 100, Training Loss 0.08929300944670997\n",
      "2022-03-26 23:19:00.423572 Epoch 100, Training Loss 0.09048292867820282\n",
      "2022-03-26 23:19:00.443575 Epoch 100, Training Loss 0.09131673599600487\n",
      "2022-03-26 23:19:00.462659 Epoch 100, Training Loss 0.09225586091008638\n",
      "2022-03-26 23:19:00.481695 Epoch 100, Training Loss 0.0932870239324277\n",
      "2022-03-26 23:19:00.501590 Epoch 100, Training Loss 0.09418562188020448\n",
      "2022-03-26 23:19:00.520143 Epoch 100, Training Loss 0.09517067842318884\n",
      "2022-03-26 23:19:00.539128 Epoch 100, Training Loss 0.09611711832111144\n",
      "2022-03-26 23:19:00.557969 Epoch 100, Training Loss 0.09723845935995927\n",
      "2022-03-26 23:19:00.577603 Epoch 100, Training Loss 0.09780143468123872\n",
      "2022-03-26 23:19:00.596644 Epoch 100, Training Loss 0.09851449762311433\n",
      "2022-03-26 23:19:00.616684 Epoch 100, Training Loss 0.09937929798422566\n",
      "2022-03-26 23:19:00.636693 Epoch 100, Training Loss 0.10026359257033414\n",
      "2022-03-26 23:19:00.656992 Epoch 100, Training Loss 0.10111772179451135\n",
      "2022-03-26 23:19:00.677020 Epoch 100, Training Loss 0.10209144324140476\n",
      "2022-03-26 23:19:00.696032 Epoch 100, Training Loss 0.10326744166329084\n",
      "2022-03-26 23:19:00.714920 Epoch 100, Training Loss 0.10405683780417722\n",
      "2022-03-26 23:19:00.733622 Epoch 100, Training Loss 0.10469556876155726\n",
      "2022-03-26 23:19:00.753630 Epoch 100, Training Loss 0.10577801822701378\n",
      "2022-03-26 23:19:00.772502 Epoch 100, Training Loss 0.10675250797930276\n",
      "2022-03-26 23:19:00.791531 Epoch 100, Training Loss 0.10753195243113486\n",
      "2022-03-26 23:19:00.811517 Epoch 100, Training Loss 0.1083765346223436\n",
      "2022-03-26 23:19:00.831041 Epoch 100, Training Loss 0.10937543041870722\n",
      "2022-03-26 23:19:00.849990 Epoch 100, Training Loss 0.11041796504688994\n",
      "2022-03-26 23:19:00.868890 Epoch 100, Training Loss 0.1111571510582019\n",
      "2022-03-26 23:19:00.888905 Epoch 100, Training Loss 0.11230046768932392\n",
      "2022-03-26 23:19:00.908928 Epoch 100, Training Loss 0.11312597837594464\n",
      "2022-03-26 23:19:00.927601 Epoch 100, Training Loss 0.1137472356829192\n",
      "2022-03-26 23:19:00.946640 Epoch 100, Training Loss 0.11470461996924847\n",
      "2022-03-26 23:19:00.966680 Epoch 100, Training Loss 0.11570277314661714\n",
      "2022-03-26 23:19:00.985772 Epoch 100, Training Loss 0.11659121742029019\n",
      "2022-03-26 23:19:01.005655 Epoch 100, Training Loss 0.11759784192685276\n",
      "2022-03-26 23:19:01.024646 Epoch 100, Training Loss 0.1184552354580911\n",
      "2022-03-26 23:19:01.044522 Epoch 100, Training Loss 0.11932962698399868\n",
      "2022-03-26 23:19:01.064488 Epoch 100, Training Loss 0.12040468006182814\n",
      "2022-03-26 23:19:01.083589 Epoch 100, Training Loss 0.12145949736275637\n",
      "2022-03-26 23:19:01.103639 Epoch 100, Training Loss 0.12218732884168015\n",
      "2022-03-26 23:19:01.121679 Epoch 100, Training Loss 0.12315842654089183\n",
      "2022-03-26 23:19:01.141676 Epoch 100, Training Loss 0.12423031668528876\n",
      "2022-03-26 23:19:01.160640 Epoch 100, Training Loss 0.12518201970383333\n",
      "2022-03-26 23:19:01.180657 Epoch 100, Training Loss 0.1259923113124145\n",
      "2022-03-26 23:19:01.199561 Epoch 100, Training Loss 0.1268151256129565\n",
      "2022-03-26 23:19:01.219571 Epoch 100, Training Loss 0.1275845247766246\n",
      "2022-03-26 23:19:01.238575 Epoch 100, Training Loss 0.1287904044856196\n",
      "2022-03-26 23:19:01.258600 Epoch 100, Training Loss 0.1297818195941808\n",
      "2022-03-26 23:19:01.278228 Epoch 100, Training Loss 0.13058303201290042\n",
      "2022-03-26 23:19:01.297025 Epoch 100, Training Loss 0.13152667361757028\n",
      "2022-03-26 23:19:01.316865 Epoch 100, Training Loss 0.13276100021494014\n",
      "2022-03-26 23:19:01.335863 Epoch 100, Training Loss 0.13373894581709372\n",
      "2022-03-26 23:19:01.355867 Epoch 100, Training Loss 0.13490146970200112\n",
      "2022-03-26 23:19:01.374907 Epoch 100, Training Loss 0.13554602091574608\n",
      "2022-03-26 23:19:01.393769 Epoch 100, Training Loss 0.13635709409213737\n",
      "2022-03-26 23:19:01.412791 Epoch 100, Training Loss 0.13719711096390433\n",
      "2022-03-26 23:19:01.431833 Epoch 100, Training Loss 0.13818008797552883\n",
      "2022-03-26 23:19:01.451761 Epoch 100, Training Loss 0.13900330655105278\n",
      "2022-03-26 23:19:01.471673 Epoch 100, Training Loss 0.1400227096227124\n",
      "2022-03-26 23:19:01.490673 Epoch 100, Training Loss 0.1409562017454211\n",
      "2022-03-26 23:19:01.509754 Epoch 100, Training Loss 0.14196466423971268\n",
      "2022-03-26 23:19:01.528758 Epoch 100, Training Loss 0.14258043010673865\n",
      "2022-03-26 23:19:01.548775 Epoch 100, Training Loss 0.14332641745009994\n",
      "2022-03-26 23:19:01.568683 Epoch 100, Training Loss 0.14428272664242084\n",
      "2022-03-26 23:19:01.587756 Epoch 100, Training Loss 0.14507243757510124\n",
      "2022-03-26 23:19:01.608588 Epoch 100, Training Loss 0.14587279571139294\n",
      "2022-03-26 23:19:01.627623 Epoch 100, Training Loss 0.14688661705959788\n",
      "2022-03-26 23:19:01.646102 Epoch 100, Training Loss 0.1478326699084333\n",
      "2022-03-26 23:19:01.668143 Epoch 100, Training Loss 0.1485091694404402\n",
      "2022-03-26 23:19:01.687160 Epoch 100, Training Loss 0.14938215034849503\n",
      "2022-03-26 23:19:01.707177 Epoch 100, Training Loss 0.1501016313081507\n",
      "2022-03-26 23:19:01.727047 Epoch 100, Training Loss 0.15115608069140588\n",
      "2022-03-26 23:19:01.745448 Epoch 100, Training Loss 0.1521998612243501\n",
      "2022-03-26 23:19:01.765565 Epoch 100, Training Loss 0.15284133395727942\n",
      "2022-03-26 23:19:01.784521 Epoch 100, Training Loss 0.15372257209037576\n",
      "2022-03-26 23:19:01.805482 Epoch 100, Training Loss 0.15464441988931593\n",
      "2022-03-26 23:19:01.824436 Epoch 100, Training Loss 0.1558900470547664\n",
      "2022-03-26 23:19:01.843362 Epoch 100, Training Loss 0.15698942347713138\n",
      "2022-03-26 23:19:01.862452 Epoch 100, Training Loss 0.15798007340534873\n",
      "2022-03-26 23:19:01.882355 Epoch 100, Training Loss 0.1588976802042378\n",
      "2022-03-26 23:19:01.902328 Epoch 100, Training Loss 0.15987222567391213\n",
      "2022-03-26 23:19:01.921498 Epoch 100, Training Loss 0.16061100474251505\n",
      "2022-03-26 23:19:01.940684 Epoch 100, Training Loss 0.16164768881657543\n",
      "2022-03-26 23:19:01.960562 Epoch 100, Training Loss 0.16262565263549386\n",
      "2022-03-26 23:19:01.979757 Epoch 100, Training Loss 0.16362034218847904\n",
      "2022-03-26 23:19:01.998773 Epoch 100, Training Loss 0.16451707181265895\n",
      "2022-03-26 23:19:02.018707 Epoch 100, Training Loss 0.16542825388633992\n",
      "2022-03-26 23:19:02.037710 Epoch 100, Training Loss 0.16658244714560105\n",
      "2022-03-26 23:19:02.057604 Epoch 100, Training Loss 0.16745657715803522\n",
      "2022-03-26 23:19:02.076649 Epoch 100, Training Loss 0.16818893653200107\n",
      "2022-03-26 23:19:02.096432 Epoch 100, Training Loss 0.16905773848371433\n",
      "2022-03-26 23:19:02.116814 Epoch 100, Training Loss 0.16992188006868142\n",
      "2022-03-26 23:19:02.141755 Epoch 100, Training Loss 0.17065789369518494\n",
      "2022-03-26 23:19:02.168644 Epoch 100, Training Loss 0.1717907098476844\n",
      "2022-03-26 23:19:02.195608 Epoch 100, Training Loss 0.1726157500996919\n",
      "2022-03-26 23:19:02.221637 Epoch 100, Training Loss 0.17339326212625675\n",
      "2022-03-26 23:19:02.247583 Epoch 100, Training Loss 0.17435875805595036\n",
      "2022-03-26 23:19:02.275790 Epoch 100, Training Loss 0.17539297836973233\n",
      "2022-03-26 23:19:02.301770 Epoch 100, Training Loss 0.17612892332131905\n",
      "2022-03-26 23:19:02.328658 Epoch 100, Training Loss 0.17682414069352553\n",
      "2022-03-26 23:19:02.355537 Epoch 100, Training Loss 0.17794575025815793\n",
      "2022-03-26 23:19:02.380678 Epoch 100, Training Loss 0.17885433602363557\n",
      "2022-03-26 23:19:02.407580 Epoch 100, Training Loss 0.17968590816725855\n",
      "2022-03-26 23:19:02.432527 Epoch 100, Training Loss 0.18050783441957\n",
      "2022-03-26 23:19:02.458593 Epoch 100, Training Loss 0.1815869308569852\n",
      "2022-03-26 23:19:02.485455 Epoch 100, Training Loss 0.18257637146641226\n",
      "2022-03-26 23:19:02.510420 Epoch 100, Training Loss 0.18351465772332437\n",
      "2022-03-26 23:19:02.532756 Epoch 100, Training Loss 0.18432574561032491\n",
      "2022-03-26 23:19:02.551791 Epoch 100, Training Loss 0.18519836355505698\n",
      "2022-03-26 23:19:02.569773 Epoch 100, Training Loss 0.18611761557933926\n",
      "2022-03-26 23:19:02.588774 Epoch 100, Training Loss 0.18683188890709596\n",
      "2022-03-26 23:19:02.607799 Epoch 100, Training Loss 0.18776951402501987\n",
      "2022-03-26 23:19:02.625821 Epoch 100, Training Loss 0.18838102677289179\n",
      "2022-03-26 23:19:02.644876 Epoch 100, Training Loss 0.18954492712874546\n",
      "2022-03-26 23:19:02.663800 Epoch 100, Training Loss 0.19038433354833853\n",
      "2022-03-26 23:19:02.682792 Epoch 100, Training Loss 0.19122918373178643\n",
      "2022-03-26 23:19:02.701817 Epoch 100, Training Loss 0.1923680691920278\n",
      "2022-03-26 23:19:02.719755 Epoch 100, Training Loss 0.1932792994372375\n",
      "2022-03-26 23:19:02.739833 Epoch 100, Training Loss 0.19418628075543573\n",
      "2022-03-26 23:19:02.758641 Epoch 100, Training Loss 0.19508191958412796\n",
      "2022-03-26 23:19:02.776659 Epoch 100, Training Loss 0.19585656388031553\n",
      "2022-03-26 23:19:02.794643 Epoch 100, Training Loss 0.1966491545862554\n",
      "2022-03-26 23:19:02.812583 Epoch 100, Training Loss 0.19747211751730545\n",
      "2022-03-26 23:19:02.831778 Epoch 100, Training Loss 0.19852889514030397\n",
      "2022-03-26 23:19:02.849660 Epoch 100, Training Loss 0.1996057919224205\n",
      "2022-03-26 23:19:02.867609 Epoch 100, Training Loss 0.2007297510686128\n",
      "2022-03-26 23:19:02.886637 Epoch 100, Training Loss 0.20163397799672372\n",
      "2022-03-26 23:19:02.905674 Epoch 100, Training Loss 0.20283717252409367\n",
      "2022-03-26 23:19:02.924151 Epoch 100, Training Loss 0.2037682716956224\n",
      "2022-03-26 23:19:02.942034 Epoch 100, Training Loss 0.2045545422512552\n",
      "2022-03-26 23:19:02.961057 Epoch 100, Training Loss 0.20552757336660418\n",
      "2022-03-26 23:19:02.979071 Epoch 100, Training Loss 0.20654465848832484\n",
      "2022-03-26 23:19:02.998088 Epoch 100, Training Loss 0.20775155422022884\n",
      "2022-03-26 23:19:03.017221 Epoch 100, Training Loss 0.20870256736455367\n",
      "2022-03-26 23:19:03.035244 Epoch 100, Training Loss 0.20968948277975896\n",
      "2022-03-26 23:19:03.054453 Epoch 100, Training Loss 0.21062942866779044\n",
      "2022-03-26 23:19:03.072417 Epoch 100, Training Loss 0.21175875566194735\n",
      "2022-03-26 23:19:03.091364 Epoch 100, Training Loss 0.21270735481815875\n",
      "2022-03-26 23:19:03.109426 Epoch 100, Training Loss 0.21354899054293133\n",
      "2022-03-26 23:19:03.127423 Epoch 100, Training Loss 0.2145374550691346\n",
      "2022-03-26 23:19:03.147074 Epoch 100, Training Loss 0.2152709207876259\n",
      "2022-03-26 23:19:03.166078 Epoch 100, Training Loss 0.2164762884454654\n",
      "2022-03-26 23:19:03.186083 Epoch 100, Training Loss 0.21724331234117297\n",
      "2022-03-26 23:19:03.206080 Epoch 100, Training Loss 0.21812070262096728\n",
      "2022-03-26 23:19:03.225114 Epoch 100, Training Loss 0.21920979548903072\n",
      "2022-03-26 23:19:03.243090 Epoch 100, Training Loss 0.22019835224236978\n",
      "2022-03-26 23:19:03.261082 Epoch 100, Training Loss 0.22107018648511004\n",
      "2022-03-26 23:19:03.280430 Epoch 100, Training Loss 0.2217527416813404\n",
      "2022-03-26 23:19:03.299179 Epoch 100, Training Loss 0.22250952859363898\n",
      "2022-03-26 23:19:03.318286 Epoch 100, Training Loss 0.2237602752035536\n",
      "2022-03-26 23:19:03.336760 Epoch 100, Training Loss 0.22486922914719643\n",
      "2022-03-26 23:19:03.355628 Epoch 100, Training Loss 0.22569739826195076\n",
      "2022-03-26 23:19:03.375731 Epoch 100, Training Loss 0.22627387716032354\n",
      "2022-03-26 23:19:03.394633 Epoch 100, Training Loss 0.22728622516097924\n",
      "2022-03-26 23:19:03.412544 Epoch 100, Training Loss 0.2282085953771001\n",
      "2022-03-26 23:19:03.431578 Epoch 100, Training Loss 0.22913883043372113\n",
      "2022-03-26 23:19:03.450183 Epoch 100, Training Loss 0.22992203157881033\n",
      "2022-03-26 23:19:03.469116 Epoch 100, Training Loss 0.23072926208491215\n",
      "2022-03-26 23:19:03.487090 Epoch 100, Training Loss 0.23158640965171481\n",
      "2022-03-26 23:19:03.506973 Epoch 100, Training Loss 0.23244128195221161\n",
      "2022-03-26 23:19:03.525922 Epoch 100, Training Loss 0.23332932614304525\n",
      "2022-03-26 23:19:03.543805 Epoch 100, Training Loss 0.23432660743098735\n",
      "2022-03-26 23:19:03.560765 Epoch 100, Training Loss 0.2352865113474219\n",
      "2022-03-26 23:19:03.580634 Epoch 100, Training Loss 0.23626004986445923\n",
      "2022-03-26 23:19:03.598555 Epoch 100, Training Loss 0.2370852180149244\n",
      "2022-03-26 23:19:03.617589 Epoch 100, Training Loss 0.23777264432833933\n",
      "2022-03-26 23:19:03.635546 Epoch 100, Training Loss 0.238938045699883\n",
      "2022-03-26 23:19:03.655189 Epoch 100, Training Loss 0.23974514480137155\n",
      "2022-03-26 23:19:03.672873 Epoch 100, Training Loss 0.240913931671006\n",
      "2022-03-26 23:19:03.691656 Epoch 100, Training Loss 0.2416517710899148\n",
      "2022-03-26 23:19:03.709662 Epoch 100, Training Loss 0.24248932976551982\n",
      "2022-03-26 23:19:03.728755 Epoch 100, Training Loss 0.24331237615831672\n",
      "2022-03-26 23:19:03.747641 Epoch 100, Training Loss 0.24432680986421493\n",
      "2022-03-26 23:19:03.766623 Epoch 100, Training Loss 0.24529001680786347\n",
      "2022-03-26 23:19:03.785754 Epoch 100, Training Loss 0.2461587962744486\n",
      "2022-03-26 23:19:03.804695 Epoch 100, Training Loss 0.24714129865931733\n",
      "2022-03-26 23:19:03.822776 Epoch 100, Training Loss 0.24781619214340853\n",
      "2022-03-26 23:19:03.841804 Epoch 100, Training Loss 0.24869075501361465\n",
      "2022-03-26 23:19:03.859945 Epoch 100, Training Loss 0.24967500354018052\n",
      "2022-03-26 23:19:03.877937 Epoch 100, Training Loss 0.2505617328464528\n",
      "2022-03-26 23:19:03.895774 Epoch 100, Training Loss 0.25116802584332276\n",
      "2022-03-26 23:19:03.914594 Epoch 100, Training Loss 0.25187552001927516\n",
      "2022-03-26 23:19:03.933461 Epoch 100, Training Loss 0.25263690853210363\n",
      "2022-03-26 23:19:03.952523 Epoch 100, Training Loss 0.25333991414293305\n",
      "2022-03-26 23:19:03.970562 Epoch 100, Training Loss 0.25423259530073544\n",
      "2022-03-26 23:19:03.989638 Epoch 100, Training Loss 0.25506576122072955\n",
      "2022-03-26 23:19:04.008658 Epoch 100, Training Loss 0.2559027788050644\n",
      "2022-03-26 23:19:04.026681 Epoch 100, Training Loss 0.25671039613159113\n",
      "2022-03-26 23:19:04.045249 Epoch 100, Training Loss 0.25777783909874497\n",
      "2022-03-26 23:19:04.063226 Epoch 100, Training Loss 0.2588095953473655\n",
      "2022-03-26 23:19:04.081217 Epoch 100, Training Loss 0.25977510903649925\n",
      "2022-03-26 23:19:04.100085 Epoch 100, Training Loss 0.2608746364522163\n",
      "2022-03-26 23:19:04.119189 Epoch 100, Training Loss 0.261685709171283\n",
      "2022-03-26 23:19:04.136683 Epoch 100, Training Loss 0.2624042091695854\n",
      "2022-03-26 23:19:04.155693 Epoch 100, Training Loss 0.2634395248521015\n",
      "2022-03-26 23:19:04.173583 Epoch 100, Training Loss 0.2640518517140537\n",
      "2022-03-26 23:19:04.191635 Epoch 100, Training Loss 0.2650699153580629\n",
      "2022-03-26 23:19:04.210656 Epoch 100, Training Loss 0.2660022332997578\n",
      "2022-03-26 23:19:04.228256 Epoch 100, Training Loss 0.26698320562882194\n",
      "2022-03-26 23:19:04.247952 Epoch 100, Training Loss 0.2678937516401491\n",
      "2022-03-26 23:19:04.265962 Epoch 100, Training Loss 0.26865828723248925\n",
      "2022-03-26 23:19:04.283968 Epoch 100, Training Loss 0.26950178701249533\n",
      "2022-03-26 23:19:04.302755 Epoch 100, Training Loss 0.2704974014283446\n",
      "2022-03-26 23:19:04.321305 Epoch 100, Training Loss 0.2713947063669219\n",
      "2022-03-26 23:19:04.340187 Epoch 100, Training Loss 0.27206166877466087\n",
      "2022-03-26 23:19:04.366435 Epoch 100, Training Loss 0.27292605373255735\n",
      "2022-03-26 23:19:04.392256 Epoch 100, Training Loss 0.273686027282949\n",
      "2022-03-26 23:19:04.418321 Epoch 100, Training Loss 0.27474379486135203\n",
      "2022-03-26 23:19:04.444891 Epoch 100, Training Loss 0.275737828991907\n",
      "2022-03-26 23:19:04.470921 Epoch 100, Training Loss 0.2765604902411361\n",
      "2022-03-26 23:19:04.496860 Epoch 100, Training Loss 0.2773677872117523\n",
      "2022-03-26 23:19:04.523652 Epoch 100, Training Loss 0.2784236289198746\n",
      "2022-03-26 23:19:04.549701 Epoch 100, Training Loss 0.2791671357343874\n",
      "2022-03-26 23:19:04.568837 Epoch 100, Training Loss 0.28009611619707875\n",
      "2022-03-26 23:19:04.587757 Epoch 100, Training Loss 0.2809199097058962\n",
      "2022-03-26 23:19:04.605771 Epoch 100, Training Loss 0.281639390970435\n",
      "2022-03-26 23:19:04.623831 Epoch 100, Training Loss 0.28270622493360964\n",
      "2022-03-26 23:19:04.642859 Epoch 100, Training Loss 0.283542084724397\n",
      "2022-03-26 23:19:04.660880 Epoch 100, Training Loss 0.28438410856534757\n",
      "2022-03-26 23:19:04.678817 Epoch 100, Training Loss 0.28508909553518075\n",
      "2022-03-26 23:19:04.696784 Epoch 100, Training Loss 0.2863039840060427\n",
      "2022-03-26 23:19:04.715783 Epoch 100, Training Loss 0.2871448069887088\n",
      "2022-03-26 23:19:04.734828 Epoch 100, Training Loss 0.2879573836960756\n",
      "2022-03-26 23:19:04.752768 Epoch 100, Training Loss 0.28882241729275343\n",
      "2022-03-26 23:19:04.770766 Epoch 100, Training Loss 0.2898000804207209\n",
      "2022-03-26 23:19:04.789789 Epoch 100, Training Loss 0.2907184293812803\n",
      "2022-03-26 23:19:04.807646 Epoch 100, Training Loss 0.2918401604418255\n",
      "2022-03-26 23:19:04.826609 Epoch 100, Training Loss 0.29274308391849097\n",
      "2022-03-26 23:19:04.843589 Epoch 100, Training Loss 0.29364045006234935\n",
      "2022-03-26 23:19:04.862686 Epoch 100, Training Loss 0.2945258198949077\n",
      "2022-03-26 23:19:04.881658 Epoch 100, Training Loss 0.2954336485594435\n",
      "2022-03-26 23:19:04.899640 Epoch 100, Training Loss 0.29640334516839906\n",
      "2022-03-26 23:19:04.917656 Epoch 100, Training Loss 0.29718959522064387\n",
      "2022-03-26 23:19:04.936696 Epoch 100, Training Loss 0.298095280709474\n",
      "2022-03-26 23:19:04.954998 Epoch 100, Training Loss 0.29902200419884506\n",
      "2022-03-26 23:19:04.973905 Epoch 100, Training Loss 0.29969030359516974\n",
      "2022-03-26 23:19:04.991931 Epoch 100, Training Loss 0.3007593129754371\n",
      "2022-03-26 23:19:05.009942 Epoch 100, Training Loss 0.3014506866102633\n",
      "2022-03-26 23:19:05.027984 Epoch 100, Training Loss 0.30222433050880043\n",
      "2022-03-26 23:19:05.045821 Epoch 100, Training Loss 0.30319861591319597\n",
      "2022-03-26 23:19:05.063983 Epoch 100, Training Loss 0.3044641053737582\n",
      "2022-03-26 23:19:05.083015 Epoch 100, Training Loss 0.30559514039922553\n",
      "2022-03-26 23:19:05.102041 Epoch 100, Training Loss 0.3065786873135725\n",
      "2022-03-26 23:19:05.121041 Epoch 100, Training Loss 0.3073623454021981\n",
      "2022-03-26 23:19:05.139073 Epoch 100, Training Loss 0.3082174991097901\n",
      "2022-03-26 23:19:05.157131 Epoch 100, Training Loss 0.3090219144016276\n",
      "2022-03-26 23:19:05.175205 Epoch 100, Training Loss 0.3100689330979077\n",
      "2022-03-26 23:19:05.193209 Epoch 100, Training Loss 0.31089130059227615\n",
      "2022-03-26 23:19:05.211784 Epoch 100, Training Loss 0.3115596505229735\n",
      "2022-03-26 23:19:05.229768 Epoch 100, Training Loss 0.31233760013299827\n",
      "2022-03-26 23:19:05.248789 Epoch 100, Training Loss 0.313051380083689\n",
      "2022-03-26 23:19:05.266834 Epoch 100, Training Loss 0.31391170079750785\n",
      "2022-03-26 23:19:05.284985 Epoch 100, Training Loss 0.31514168060039316\n",
      "2022-03-26 23:19:05.303250 Epoch 100, Training Loss 0.3160669039125028\n",
      "2022-03-26 23:19:05.322290 Epoch 100, Training Loss 0.3168868901174697\n",
      "2022-03-26 23:19:05.340497 Epoch 100, Training Loss 0.31761506443743204\n",
      "2022-03-26 23:19:05.358558 Epoch 100, Training Loss 0.31875136799519627\n",
      "2022-03-26 23:19:05.376622 Epoch 100, Training Loss 0.31975612234886347\n",
      "2022-03-26 23:19:05.394756 Epoch 100, Training Loss 0.32072563168337886\n",
      "2022-03-26 23:19:05.413666 Epoch 100, Training Loss 0.321683759960677\n",
      "2022-03-26 23:19:05.432131 Epoch 100, Training Loss 0.32233596823709393\n",
      "2022-03-26 23:19:05.451173 Epoch 100, Training Loss 0.32324017809175165\n",
      "2022-03-26 23:19:05.470205 Epoch 100, Training Loss 0.32427242794610045\n",
      "2022-03-26 23:19:05.489334 Epoch 100, Training Loss 0.32504223473846455\n",
      "2022-03-26 23:19:05.506793 Epoch 100, Training Loss 0.32596548233190764\n",
      "2022-03-26 23:19:05.525334 Epoch 100, Training Loss 0.3270936533618156\n",
      "2022-03-26 23:19:05.543806 Epoch 100, Training Loss 0.32799357351134806\n",
      "2022-03-26 23:19:05.561842 Epoch 100, Training Loss 0.3288342862025551\n",
      "2022-03-26 23:19:05.580800 Epoch 100, Training Loss 0.33004881285340587\n",
      "2022-03-26 23:19:05.599775 Epoch 100, Training Loss 0.33086554351670056\n",
      "2022-03-26 23:19:05.617765 Epoch 100, Training Loss 0.3316479025746855\n",
      "2022-03-26 23:19:05.635774 Epoch 100, Training Loss 0.332538115780067\n",
      "2022-03-26 23:19:05.654754 Epoch 100, Training Loss 0.33345186001504473\n",
      "2022-03-26 23:19:05.673708 Epoch 100, Training Loss 0.33451168136218623\n",
      "2022-03-26 23:19:05.691649 Epoch 100, Training Loss 0.3354099333438727\n",
      "2022-03-26 23:19:05.710589 Epoch 100, Training Loss 0.33630738028175083\n",
      "2022-03-26 23:19:05.727992 Epoch 100, Training Loss 0.3378920793685767\n",
      "2022-03-26 23:19:05.746324 Epoch 100, Training Loss 0.3387318553826998\n",
      "2022-03-26 23:19:05.765146 Epoch 100, Training Loss 0.33967796890327084\n",
      "2022-03-26 23:19:05.783189 Epoch 100, Training Loss 0.34061752705622816\n",
      "2022-03-26 23:19:05.802072 Epoch 100, Training Loss 0.3414176781768994\n",
      "2022-03-26 23:19:05.819227 Epoch 100, Training Loss 0.3425324666683021\n",
      "2022-03-26 23:19:05.837337 Epoch 100, Training Loss 0.3434885569545619\n",
      "2022-03-26 23:19:05.856660 Epoch 100, Training Loss 0.34444726732990627\n",
      "2022-03-26 23:19:05.874558 Epoch 100, Training Loss 0.3452974121893763\n",
      "2022-03-26 23:19:05.892535 Epoch 100, Training Loss 0.3462479222765969\n",
      "2022-03-26 23:19:05.910648 Epoch 100, Training Loss 0.3471651225138808\n",
      "2022-03-26 23:19:05.929682 Epoch 100, Training Loss 0.348139224759758\n",
      "2022-03-26 23:19:05.948467 Epoch 100, Training Loss 0.3490218677179283\n",
      "2022-03-26 23:19:05.966584 Epoch 100, Training Loss 0.3500546931153368\n",
      "2022-03-26 23:19:05.984703 Epoch 100, Training Loss 0.35081907763810416\n",
      "2022-03-26 23:19:06.003528 Epoch 100, Training Loss 0.35165520458270216\n",
      "2022-03-26 23:19:06.022513 Epoch 100, Training Loss 0.3526203221524768\n",
      "2022-03-26 23:19:06.040430 Epoch 100, Training Loss 0.3534256232059215\n",
      "2022-03-26 23:19:06.058462 Epoch 100, Training Loss 0.35450099785919387\n",
      "2022-03-26 23:19:06.076539 Epoch 100, Training Loss 0.35557995976694406\n",
      "2022-03-26 23:19:06.095198 Epoch 100, Training Loss 0.3565198704409782\n",
      "2022-03-26 23:19:06.114259 Epoch 100, Training Loss 0.3576749282724717\n",
      "2022-03-26 23:19:06.132361 Epoch 100, Training Loss 0.3585682544866791\n",
      "2022-03-26 23:19:06.151394 Epoch 100, Training Loss 0.35935111004678183\n",
      "2022-03-26 23:19:06.169409 Epoch 100, Training Loss 0.36018011797114713\n",
      "2022-03-26 23:19:06.187452 Epoch 100, Training Loss 0.36095913772083\n",
      "2022-03-26 23:19:06.205842 Epoch 100, Training Loss 0.36147199845527445\n",
      "2022-03-26 23:19:06.223792 Epoch 100, Training Loss 0.3623161996188371\n",
      "2022-03-26 23:19:06.241793 Epoch 100, Training Loss 0.3634307182887021\n",
      "2022-03-26 23:19:06.259664 Epoch 100, Training Loss 0.3644363130526165\n",
      "2022-03-26 23:19:06.276977 Epoch 100, Training Loss 0.3652431222865039\n",
      "2022-03-26 23:19:06.294871 Epoch 100, Training Loss 0.3661177238006421\n",
      "2022-03-26 23:19:06.313763 Epoch 100, Training Loss 0.36708838063890065\n",
      "2022-03-26 23:19:06.332640 Epoch 100, Training Loss 0.3681781790064424\n",
      "2022-03-26 23:19:06.352667 Epoch 100, Training Loss 0.36917790850562515\n",
      "2022-03-26 23:19:06.371241 Epoch 100, Training Loss 0.370164283591768\n",
      "2022-03-26 23:19:06.389060 Epoch 100, Training Loss 0.37096355421006527\n",
      "2022-03-26 23:19:06.407926 Epoch 100, Training Loss 0.37170384214509783\n",
      "2022-03-26 23:19:06.426371 Epoch 100, Training Loss 0.3725136259708868\n",
      "2022-03-26 23:19:06.444301 Epoch 100, Training Loss 0.37346016701377566\n",
      "2022-03-26 23:19:06.462946 Epoch 100, Training Loss 0.3741496428275657\n",
      "2022-03-26 23:19:06.481863 Epoch 100, Training Loss 0.3752087008998827\n",
      "2022-03-26 23:19:06.500766 Epoch 100, Training Loss 0.37605877502647506\n",
      "2022-03-26 23:19:06.518832 Epoch 100, Training Loss 0.3772076151483809\n",
      "2022-03-26 23:19:06.538839 Epoch 100, Training Loss 0.3783360406032304\n",
      "2022-03-26 23:19:06.557781 Epoch 100, Training Loss 0.3791190496719707\n",
      "2022-03-26 23:19:06.576251 Epoch 100, Training Loss 0.38008691904032627\n",
      "2022-03-26 23:19:06.594238 Epoch 100, Training Loss 0.3809742219841389\n",
      "2022-03-26 23:19:06.612265 Epoch 100, Training Loss 0.3820537787950252\n",
      "2022-03-26 23:19:06.631282 Epoch 100, Training Loss 0.38321578620797225\n",
      "2022-03-26 23:19:06.649492 Epoch 100, Training Loss 0.38411654661531036\n",
      "2022-03-26 23:19:06.668384 Epoch 100, Training Loss 0.38491051706968976\n",
      "2022-03-26 23:19:06.687446 Epoch 100, Training Loss 0.38580692488976454\n",
      "2022-03-26 23:19:06.706070 Epoch 100, Training Loss 0.3867734863096491\n",
      "2022-03-26 23:19:06.724680 Epoch 100, Training Loss 0.3877434101327301\n",
      "2022-03-26 23:19:06.744635 Epoch 100, Training Loss 0.3885753157422366\n",
      "2022-03-26 23:19:06.763589 Epoch 100, Training Loss 0.3894348459704148\n",
      "2022-03-26 23:19:06.781005 Epoch 100, Training Loss 0.39021402746057876\n",
      "2022-03-26 23:19:06.800035 Epoch 100, Training Loss 0.39084425759132563\n",
      "2022-03-26 23:19:06.819094 Epoch 100, Training Loss 0.3918158005722953\n",
      "2022-03-26 23:19:06.836979 Epoch 100, Training Loss 0.3924979997412933\n",
      "2022-03-26 23:19:06.855961 Epoch 100, Training Loss 0.393676165881974\n",
      "2022-03-26 23:19:06.873849 Epoch 100, Training Loss 0.3946590093548036\n",
      "2022-03-26 23:19:06.892741 Epoch 100, Training Loss 0.3954747455656681\n",
      "2022-03-26 23:19:06.910763 Epoch 100, Training Loss 0.39657118146681725\n",
      "2022-03-26 23:19:06.930031 Epoch 100, Training Loss 0.39750044371770776\n",
      "2022-03-26 23:19:06.947930 Epoch 100, Training Loss 0.3981804901834034\n",
      "2022-03-26 23:19:06.967812 Epoch 100, Training Loss 0.39902862075649564\n",
      "2022-03-26 23:19:06.985760 Epoch 100, Training Loss 0.3999182390587409\n",
      "2022-03-26 23:19:07.004794 Epoch 100, Training Loss 0.400996492723065\n",
      "2022-03-26 23:19:07.022650 Epoch 100, Training Loss 0.4018801104687059\n",
      "2022-03-26 23:19:07.041823 Epoch 100, Training Loss 0.4027402157063984\n",
      "2022-03-26 23:19:07.059660 Epoch 100, Training Loss 0.40341623512375385\n",
      "2022-03-26 23:19:07.077560 Epoch 100, Training Loss 0.404361946351083\n",
      "2022-03-26 23:19:07.095870 Epoch 100, Training Loss 0.40550716842531853\n",
      "2022-03-26 23:19:07.115125 Epoch 100, Training Loss 0.4065750528631918\n",
      "2022-03-26 23:19:07.133640 Epoch 100, Training Loss 0.4074468138577688\n",
      "2022-03-26 23:19:07.151683 Epoch 100, Training Loss 0.40849086253539374\n",
      "2022-03-26 23:19:07.170701 Epoch 100, Training Loss 0.40935885235476677\n",
      "2022-03-26 23:19:07.190659 Epoch 100, Training Loss 0.41016715559203304\n",
      "2022-03-26 23:19:07.207622 Epoch 100, Training Loss 0.41134995961433174\n",
      "2022-03-26 23:19:07.226557 Epoch 100, Training Loss 0.41208210785675536\n",
      "2022-03-26 23:19:07.245110 Epoch 100, Training Loss 0.4126800202652621\n",
      "2022-03-26 23:19:07.263146 Epoch 100, Training Loss 0.4136674748662183\n",
      "2022-03-26 23:19:07.282229 Epoch 100, Training Loss 0.4146804847680699\n",
      "2022-03-26 23:19:07.300100 Epoch 100, Training Loss 0.4158234241825845\n",
      "2022-03-26 23:19:07.318617 Epoch 100, Training Loss 0.41679157366228226\n",
      "2022-03-26 23:19:07.336717 Epoch 100, Training Loss 0.4177842449654094\n",
      "2022-03-26 23:19:07.355199 Epoch 100, Training Loss 0.4187276185778401\n",
      "2022-03-26 23:19:07.374108 Epoch 100, Training Loss 0.4197045033392699\n",
      "2022-03-26 23:19:07.400824 Epoch 100, Training Loss 0.4206407407817938\n",
      "2022-03-26 23:19:07.427024 Epoch 100, Training Loss 0.4216114646943329\n",
      "2022-03-26 23:19:07.452128 Epoch 100, Training Loss 0.42248088060437566\n",
      "2022-03-26 23:19:07.478013 Epoch 100, Training Loss 0.42350694300878383\n",
      "2022-03-26 23:19:07.504788 Epoch 100, Training Loss 0.4243518118663212\n",
      "2022-03-26 23:19:07.529796 Epoch 100, Training Loss 0.42526249187376797\n",
      "2022-03-26 23:19:07.555766 Epoch 100, Training Loss 0.42646668580791836\n",
      "2022-03-26 23:19:07.581685 Epoch 100, Training Loss 0.42739840191038675\n",
      "2022-03-26 23:19:07.595596 Epoch 100, Training Loss 0.42840971795799176\n",
      "2022-03-26 23:19:07.610615 Epoch 100, Training Loss 0.4292620513445276\n",
      "2022-03-26 23:19:07.624476 Epoch 100, Training Loss 0.4301156486239275\n",
      "2022-03-26 23:19:07.638376 Epoch 100, Training Loss 0.43111014617678456\n",
      "2022-03-26 23:19:07.652808 Epoch 100, Training Loss 0.4318785434183867\n",
      "2022-03-26 23:19:07.667812 Epoch 100, Training Loss 0.43263206609984495\n",
      "2022-03-26 23:19:07.681829 Epoch 100, Training Loss 0.4337689329291244\n",
      "2022-03-26 23:19:07.696823 Epoch 100, Training Loss 0.434861000846414\n",
      "2022-03-26 23:19:07.710743 Epoch 100, Training Loss 0.4355570192989486\n",
      "2022-03-26 23:19:07.725265 Epoch 100, Training Loss 0.43652454567382404\n",
      "2022-03-26 23:19:07.739216 Epoch 100, Training Loss 0.4373186063735991\n",
      "2022-03-26 23:19:07.754093 Epoch 100, Training Loss 0.43843433542934523\n",
      "2022-03-26 23:19:07.768666 Epoch 100, Training Loss 0.4392573392909506\n",
      "2022-03-26 23:19:07.783226 Epoch 100, Training Loss 0.4401583067901299\n",
      "2022-03-26 23:19:07.798066 Epoch 100, Training Loss 0.4411256320183844\n",
      "2022-03-26 23:19:07.813444 Epoch 100, Training Loss 0.44197242788951413\n",
      "2022-03-26 23:19:07.827371 Epoch 100, Training Loss 0.4430672847249014\n",
      "2022-03-26 23:19:07.842367 Epoch 100, Training Loss 0.443947228934149\n",
      "2022-03-26 23:19:07.857390 Epoch 100, Training Loss 0.44480771558059146\n",
      "2022-03-26 23:19:07.871084 Epoch 100, Training Loss 0.44564873193535964\n",
      "2022-03-26 23:19:07.884961 Epoch 100, Training Loss 0.44672368043828803\n",
      "2022-03-26 23:19:07.899983 Epoch 100, Training Loss 0.4473568978898056\n",
      "2022-03-26 23:19:07.915004 Epoch 100, Training Loss 0.4483071480641889\n",
      "2022-03-26 23:19:07.928886 Epoch 100, Training Loss 0.4493073400710245\n",
      "2022-03-26 23:19:07.943898 Epoch 100, Training Loss 0.45013498013739084\n",
      "2022-03-26 23:19:07.957928 Epoch 100, Training Loss 0.4508313865536619\n",
      "2022-03-26 23:19:07.972819 Epoch 100, Training Loss 0.45182838315701546\n",
      "2022-03-26 23:19:07.987817 Epoch 100, Training Loss 0.4528165772519148\n",
      "2022-03-26 23:19:08.002772 Epoch 100, Training Loss 0.4534844280889882\n",
      "2022-03-26 23:19:08.016777 Epoch 100, Training Loss 0.4544579757525183\n",
      "2022-03-26 23:19:08.030967 Epoch 100, Training Loss 0.4552679427368257\n",
      "2022-03-26 23:19:08.045970 Epoch 100, Training Loss 0.45624540548037995\n",
      "2022-03-26 23:19:08.059995 Epoch 100, Training Loss 0.4570953766327075\n",
      "2022-03-26 23:19:08.074888 Epoch 100, Training Loss 0.4579802036971387\n",
      "2022-03-26 23:19:08.088901 Epoch 100, Training Loss 0.45912583751599195\n",
      "2022-03-26 23:19:08.103934 Epoch 100, Training Loss 0.4601362059107217\n",
      "2022-03-26 23:19:08.117798 Epoch 100, Training Loss 0.4610371862912117\n",
      "2022-03-26 23:19:08.132834 Epoch 100, Training Loss 0.46184396564655594\n",
      "2022-03-26 23:19:08.146837 Epoch 100, Training Loss 0.4628293362953474\n",
      "2022-03-26 23:19:08.161865 Epoch 100, Training Loss 0.4636214010017302\n",
      "2022-03-26 23:19:08.176871 Epoch 100, Training Loss 0.46474509451852736\n",
      "2022-03-26 23:19:08.190764 Epoch 100, Training Loss 0.4656114663233233\n",
      "2022-03-26 23:19:08.205772 Epoch 100, Training Loss 0.46655319532012696\n",
      "2022-03-26 23:19:08.220646 Epoch 100, Training Loss 0.46749048270380406\n",
      "2022-03-26 23:19:08.234728 Epoch 100, Training Loss 0.46835926704851866\n",
      "2022-03-26 23:19:08.250575 Epoch 100, Training Loss 0.46926541131018373\n",
      "2022-03-26 23:19:08.264393 Epoch 100, Training Loss 0.47015163519650777\n",
      "2022-03-26 23:19:08.278963 Epoch 100, Training Loss 0.4715179477430061\n",
      "2022-03-26 23:19:08.293786 Epoch 100, Training Loss 0.4725593408888868\n",
      "2022-03-26 23:19:08.307791 Epoch 100, Training Loss 0.4735343102985026\n",
      "2022-03-26 23:19:08.322760 Epoch 100, Training Loss 0.47459959789462713\n",
      "2022-03-26 23:19:08.336775 Epoch 100, Training Loss 0.47562757664171934\n",
      "2022-03-26 23:19:08.351657 Epoch 100, Training Loss 0.47671558412597004\n",
      "2022-03-26 23:19:08.366253 Epoch 100, Training Loss 0.4777255132603828\n",
      "2022-03-26 23:19:08.380016 Epoch 100, Training Loss 0.47860131090711755\n",
      "2022-03-26 23:19:08.394763 Epoch 100, Training Loss 0.4797143511226415\n",
      "2022-03-26 23:19:08.408647 Epoch 100, Training Loss 0.4805510685495708\n",
      "2022-03-26 23:19:08.423527 Epoch 100, Training Loss 0.48130893017477394\n",
      "2022-03-26 23:19:08.437081 Epoch 100, Training Loss 0.48230039124446145\n",
      "2022-03-26 23:19:08.451937 Epoch 100, Training Loss 0.48314036893875095\n",
      "2022-03-26 23:19:08.465954 Epoch 100, Training Loss 0.4836793552198069\n",
      "2022-03-26 23:19:08.480887 Epoch 100, Training Loss 0.4846010880016\n",
      "2022-03-26 23:19:08.494903 Epoch 100, Training Loss 0.48541811333440454\n",
      "2022-03-26 23:19:08.509928 Epoch 100, Training Loss 0.48627725033961294\n",
      "2022-03-26 23:19:08.523783 Epoch 100, Training Loss 0.48730387875948417\n",
      "2022-03-26 23:19:08.537654 Epoch 100, Training Loss 0.48826459770464836\n",
      "2022-03-26 23:19:08.551231 Epoch 100, Training Loss 0.4891301408371962\n",
      "2022-03-26 23:19:08.566189 Epoch 100, Training Loss 0.49013551707615327\n",
      "2022-03-26 23:19:08.581829 Epoch 100, Training Loss 0.4910532152637496\n",
      "2022-03-26 23:19:08.596839 Epoch 100, Training Loss 0.49192792935597013\n",
      "2022-03-26 23:19:08.610857 Epoch 100, Training Loss 0.49281137819637727\n",
      "2022-03-26 23:19:08.624877 Epoch 100, Training Loss 0.49388507675484317\n",
      "2022-03-26 23:19:08.639843 Epoch 100, Training Loss 0.4951446762551432\n",
      "2022-03-26 23:19:08.653847 Epoch 100, Training Loss 0.4959622764450205\n",
      "2022-03-26 23:19:08.667871 Epoch 100, Training Loss 0.4969734783901278\n",
      "2022-03-26 23:19:08.682769 Epoch 100, Training Loss 0.49798922034938015\n",
      "2022-03-26 23:19:08.697795 Epoch 100, Training Loss 0.4988068136793878\n",
      "2022-03-26 23:19:08.712842 Epoch 100, Training Loss 0.49986581580565714\n",
      "2022-03-26 23:19:08.727846 Epoch 100, Training Loss 0.5006757849241461\n",
      "2022-03-26 23:19:08.741896 Epoch 100, Training Loss 0.5014059985309001\n",
      "2022-03-26 23:19:08.756900 Epoch 100, Training Loss 0.5025657222170354\n",
      "2022-03-26 23:19:08.770960 Epoch 100, Training Loss 0.5034214242568711\n",
      "2022-03-26 23:19:08.784867 Epoch 100, Training Loss 0.5044584474371522\n",
      "2022-03-26 23:19:08.799772 Epoch 100, Training Loss 0.5055713304092208\n",
      "2022-03-26 23:19:08.813801 Epoch 100, Training Loss 0.5064930603708453\n",
      "2022-03-26 23:19:08.828764 Epoch 100, Training Loss 0.5074974053427387\n",
      "2022-03-26 23:19:08.843650 Epoch 100, Training Loss 0.5087099304360807\n",
      "2022-03-26 23:19:08.857589 Epoch 100, Training Loss 0.5097548850738179\n",
      "2022-03-26 23:19:08.871520 Epoch 100, Training Loss 0.5107377336534393\n",
      "2022-03-26 23:19:08.885474 Epoch 100, Training Loss 0.5116457147595218\n",
      "2022-03-26 23:19:08.900589 Epoch 100, Training Loss 0.5125850006518766\n",
      "2022-03-26 23:19:08.914263 Epoch 100, Training Loss 0.5134897600583104\n",
      "2022-03-26 23:19:08.928377 Epoch 100, Training Loss 0.5142461393419129\n",
      "2022-03-26 23:19:08.943608 Epoch 100, Training Loss 0.51552584115654\n",
      "2022-03-26 23:19:08.958565 Epoch 100, Training Loss 0.51632368987631\n",
      "2022-03-26 23:19:08.972530 Epoch 100, Training Loss 0.5174694778897878\n",
      "2022-03-26 23:19:08.987568 Epoch 100, Training Loss 0.5183622839734378\n",
      "2022-03-26 23:19:09.001412 Epoch 100, Training Loss 0.51925952336215\n",
      "2022-03-26 23:19:09.015510 Epoch 100, Training Loss 0.5201095712687963\n",
      "2022-03-26 23:19:09.030418 Epoch 100, Training Loss 0.5212509037969667\n",
      "2022-03-26 23:19:09.044472 Epoch 100, Training Loss 0.5220059420522827\n",
      "2022-03-26 23:19:09.058419 Epoch 100, Training Loss 0.5227959035988659\n",
      "2022-03-26 23:19:09.073270 Epoch 100, Training Loss 0.5236995561272287\n",
      "2022-03-26 23:19:09.087310 Epoch 100, Training Loss 0.5242245063528685\n",
      "2022-03-26 23:19:09.101390 Epoch 100, Training Loss 0.5250177219167085\n",
      "2022-03-26 23:19:09.116309 Epoch 100, Training Loss 0.5258351378047558\n",
      "2022-03-26 23:19:09.130430 Epoch 100, Training Loss 0.5269295881547587\n",
      "2022-03-26 23:19:09.145473 Epoch 100, Training Loss 0.5280255466089834\n",
      "2022-03-26 23:19:09.161466 Epoch 100, Training Loss 0.5289410369856583\n",
      "2022-03-26 23:19:09.175391 Epoch 100, Training Loss 0.5300272583885266\n",
      "2022-03-26 23:19:09.189444 Epoch 100, Training Loss 0.530900165438652\n",
      "2022-03-26 23:19:09.204447 Epoch 100, Training Loss 0.5317281161046699\n",
      "2022-03-26 23:19:09.217499 Epoch 100, Training Loss 0.5326078534507386\n",
      "2022-03-26 23:19:09.232433 Epoch 100, Training Loss 0.5333816410254335\n",
      "2022-03-26 23:19:09.246498 Epoch 100, Training Loss 0.5345268156141272\n",
      "2022-03-26 23:19:09.261488 Epoch 100, Training Loss 0.5355706371538475\n",
      "2022-03-26 23:19:09.275576 Epoch 100, Training Loss 0.5365634500751715\n",
      "2022-03-26 23:19:09.289680 Epoch 100, Training Loss 0.5374019487053537\n",
      "2022-03-26 23:19:09.302893 Epoch 100, Training Loss 0.5385618563122152\n",
      "2022-03-26 23:19:09.317926 Epoch 100, Training Loss 0.5395425521123135\n",
      "2022-03-26 23:19:09.332946 Epoch 100, Training Loss 0.5405866969043337\n",
      "2022-03-26 23:19:09.346969 Epoch 100, Training Loss 0.5415747339082191\n",
      "2022-03-26 23:19:09.362981 Epoch 100, Training Loss 0.5423753623233731\n",
      "2022-03-26 23:19:09.377865 Epoch 100, Training Loss 0.5435968145461338\n",
      "2022-03-26 23:19:09.395907 Epoch 100, Training Loss 0.5446772216759679\n",
      "2022-03-26 23:19:09.415936 Epoch 100, Training Loss 0.5455849527779137\n",
      "2022-03-26 23:19:09.435953 Epoch 100, Training Loss 0.5464362090124804\n",
      "2022-03-26 23:19:09.455963 Epoch 100, Training Loss 0.5472832689885898\n",
      "2022-03-26 23:19:09.474854 Epoch 100, Training Loss 0.5481586727263678\n",
      "2022-03-26 23:19:09.494862 Epoch 100, Training Loss 0.5491265478875021\n",
      "2022-03-26 23:19:09.513908 Epoch 100, Training Loss 0.5503248552151043\n",
      "2022-03-26 23:19:09.533923 Epoch 100, Training Loss 0.5513183448244544\n",
      "2022-03-26 23:19:09.552940 Epoch 100, Training Loss 0.552573740749103\n",
      "2022-03-26 23:19:09.573949 Epoch 100, Training Loss 0.5533709878583088\n",
      "2022-03-26 23:19:09.592981 Epoch 100, Training Loss 0.554260619346748\n",
      "2022-03-26 23:19:09.612011 Epoch 100, Training Loss 0.5553876473699384\n",
      "2022-03-26 23:19:09.632051 Epoch 100, Training Loss 0.5562438233505429\n",
      "2022-03-26 23:19:09.651110 Epoch 100, Training Loss 0.5569352166503286\n",
      "2022-03-26 23:19:09.670234 Epoch 100, Training Loss 0.5578751401294528\n",
      "2022-03-26 23:19:09.690015 Epoch 100, Training Loss 0.5588907887182577\n",
      "2022-03-26 23:19:09.709061 Epoch 100, Training Loss 0.5598048940872598\n",
      "2022-03-26 23:19:09.727931 Epoch 100, Training Loss 0.5606643000755773\n",
      "2022-03-26 23:19:09.748029 Epoch 100, Training Loss 0.5615458187773404\n",
      "2022-03-26 23:19:09.767810 Epoch 100, Training Loss 0.5629180722757984\n",
      "2022-03-26 23:19:09.786792 Epoch 100, Training Loss 0.5639906256171443\n",
      "2022-03-26 23:19:09.807684 Epoch 100, Training Loss 0.5649572809410217\n",
      "2022-03-26 23:19:09.826599 Epoch 100, Training Loss 0.5659744406828795\n",
      "2022-03-26 23:19:09.845566 Epoch 100, Training Loss 0.5666958974374224\n",
      "2022-03-26 23:19:09.864590 Epoch 100, Training Loss 0.5673728976636896\n",
      "2022-03-26 23:19:09.884116 Epoch 100, Training Loss 0.5682954334313303\n",
      "2022-03-26 23:19:09.902977 Epoch 100, Training Loss 0.5694138107778471\n",
      "2022-03-26 23:19:09.922871 Epoch 100, Training Loss 0.5704747796287317\n",
      "2022-03-26 23:19:09.941898 Epoch 100, Training Loss 0.5716497047096872\n",
      "2022-03-26 23:19:09.960936 Epoch 100, Training Loss 0.5725680926190618\n",
      "2022-03-26 23:19:09.980270 Epoch 100, Training Loss 0.5734033474455709\n",
      "2022-03-26 23:19:10.000621 Epoch 100, Training Loss 0.574611118268174\n",
      "2022-03-26 23:19:10.019955 Epoch 100, Training Loss 0.5755358939356816\n",
      "2022-03-26 23:19:10.040636 Epoch 100, Training Loss 0.5764353822945328\n",
      "2022-03-26 23:19:10.059657 Epoch 100, Training Loss 0.5772913732110997\n",
      "2022-03-26 23:19:10.078568 Epoch 100, Training Loss 0.5782742680567304\n",
      "2022-03-26 23:19:10.098601 Epoch 100, Training Loss 0.5790318072680622\n",
      "2022-03-26 23:19:10.117608 Epoch 100, Training Loss 0.5798434872380303\n",
      "2022-03-26 23:19:10.136789 Epoch 100, Training Loss 0.5807146305394599\n",
      "2022-03-26 23:19:10.156767 Epoch 100, Training Loss 0.5818350011354212\n",
      "2022-03-26 23:19:10.176537 Epoch 100, Training Loss 0.5827133531689339\n",
      "2022-03-26 23:19:10.196453 Epoch 100, Training Loss 0.5838441211168114\n",
      "2022-03-26 23:19:10.215546 Epoch 100, Training Loss 0.5846223817075915\n",
      "2022-03-26 23:19:10.235022 Epoch 100, Training Loss 0.58564644049653\n",
      "2022-03-26 23:19:10.254038 Epoch 100, Training Loss 0.5865719779144467\n",
      "2022-03-26 23:19:10.273887 Epoch 100, Training Loss 0.5878561182171488\n",
      "2022-03-26 23:19:10.293916 Epoch 100, Training Loss 0.589090024609395\n",
      "2022-03-26 23:19:10.312948 Epoch 100, Training Loss 0.5901042876569816\n",
      "2022-03-26 23:19:10.332956 Epoch 100, Training Loss 0.5909570377424854\n",
      "2022-03-26 23:19:10.351983 Epoch 100, Training Loss 0.5920821471363688\n",
      "2022-03-26 23:19:10.371928 Epoch 100, Training Loss 0.5929680153765642\n",
      "2022-03-26 23:19:10.390783 Epoch 100, Training Loss 0.5937438644945164\n",
      "2022-03-26 23:19:10.409795 Epoch 100, Training Loss 0.5945170261442204\n",
      "2022-03-26 23:19:10.429071 Epoch 100, Training Loss 0.5953024411216721\n",
      "2022-03-26 23:19:10.448838 Epoch 100, Training Loss 0.5963274153983197\n",
      "2022-03-26 23:19:10.469765 Epoch 100, Training Loss 0.5974802154942852\n",
      "2022-03-26 23:19:10.489809 Epoch 100, Training Loss 0.5986348795692634\n",
      "2022-03-26 23:19:10.508560 Epoch 100, Training Loss 0.5997352576088113\n",
      "2022-03-26 23:19:10.527059 Epoch 100, Training Loss 0.600387775722672\n",
      "2022-03-26 23:19:10.546447 Epoch 100, Training Loss 0.6010814089985455\n",
      "2022-03-26 23:19:10.566031 Epoch 100, Training Loss 0.6020724988730667\n",
      "2022-03-26 23:19:10.585792 Epoch 100, Training Loss 0.6031217435208123\n",
      "2022-03-26 23:19:10.606729 Epoch 100, Training Loss 0.603886502897343\n",
      "2022-03-26 23:19:10.626771 Epoch 100, Training Loss 0.6050829959418768\n",
      "2022-03-26 23:19:10.646799 Epoch 100, Training Loss 0.6058396132629545\n",
      "2022-03-26 23:19:10.665779 Epoch 100, Training Loss 0.6067950523189266\n",
      "2022-03-26 23:19:10.685859 Epoch 100, Training Loss 0.6074668964385377\n",
      "2022-03-26 23:19:10.705872 Epoch 100, Training Loss 0.6083160017228797\n",
      "2022-03-26 23:19:10.725932 Epoch 100, Training Loss 0.6090215841293944\n",
      "2022-03-26 23:19:10.744959 Epoch 100, Training Loss 0.610255844567133\n",
      "2022-03-26 23:19:10.765311 Epoch 100, Training Loss 0.6110610170361331\n",
      "2022-03-26 23:19:10.785357 Epoch 100, Training Loss 0.6119383455008802\n",
      "2022-03-26 23:19:10.805254 Epoch 100, Training Loss 0.6127869877440241\n",
      "2022-03-26 23:19:10.825002 Epoch 100, Training Loss 0.6136358488170083\n",
      "2022-03-26 23:19:10.843938 Epoch 100, Training Loss 0.6145559654897436\n",
      "2022-03-26 23:19:10.864547 Epoch 100, Training Loss 0.6154067470975544\n",
      "2022-03-26 23:19:10.883482 Epoch 100, Training Loss 0.6161431632459621\n",
      "2022-03-26 23:19:10.903643 Epoch 100, Training Loss 0.6170097910763358\n",
      "2022-03-26 23:19:10.922318 Epoch 100, Training Loss 0.6179181397952083\n",
      "2022-03-26 23:19:10.942366 Epoch 100, Training Loss 0.6186362830802913\n",
      "2022-03-26 23:19:10.961331 Epoch 100, Training Loss 0.6199799783318244\n",
      "2022-03-26 23:19:10.981277 Epoch 100, Training Loss 0.6209767509223251\n",
      "2022-03-26 23:19:11.001286 Epoch 100, Training Loss 0.622020197913165\n",
      "2022-03-26 23:19:11.021338 Epoch 100, Training Loss 0.622851586745828\n",
      "2022-03-26 23:19:11.041354 Epoch 100, Training Loss 0.6236119910960307\n",
      "2022-03-26 23:19:11.060487 Epoch 100, Training Loss 0.6245643797966526\n",
      "2022-03-26 23:19:11.079896 Epoch 100, Training Loss 0.6254271721977103\n",
      "2022-03-26 23:19:11.100225 Epoch 100, Training Loss 0.6262807584250979\n",
      "2022-03-26 23:19:11.119262 Epoch 100, Training Loss 0.6272938848685121\n",
      "2022-03-26 23:19:11.140277 Epoch 100, Training Loss 0.6279496481580198\n",
      "2022-03-26 23:19:11.159143 Epoch 100, Training Loss 0.6288670160047843\n",
      "2022-03-26 23:19:11.179944 Epoch 100, Training Loss 0.6298055362594707\n",
      "2022-03-26 23:19:11.198974 Epoch 100, Training Loss 0.6306449739295809\n",
      "2022-03-26 23:19:11.218994 Epoch 100, Training Loss 0.6314758638210614\n",
      "2022-03-26 23:19:11.239036 Epoch 100, Training Loss 0.6323057730560717\n",
      "2022-03-26 23:19:11.258896 Epoch 100, Training Loss 0.6331441520958605\n",
      "2022-03-26 23:19:11.277776 Epoch 100, Training Loss 0.6338938071828364\n",
      "2022-03-26 23:19:11.297791 Epoch 100, Training Loss 0.6349004721626297\n",
      "2022-03-26 23:19:11.316810 Epoch 100, Training Loss 0.6359440353139282\n",
      "2022-03-26 23:19:11.337015 Epoch 100, Training Loss 0.637070494196604\n",
      "2022-03-26 23:19:11.356034 Epoch 100, Training Loss 0.6377240787915257\n",
      "2022-03-26 23:19:11.376211 Epoch 100, Training Loss 0.6385108834261175\n",
      "2022-03-26 23:19:11.396380 Epoch 100, Training Loss 0.6394272838026056\n",
      "2022-03-26 23:19:11.416599 Epoch 100, Training Loss 0.6404955199231273\n",
      "2022-03-26 23:19:11.435965 Epoch 100, Training Loss 0.6412836008745691\n",
      "2022-03-26 23:19:11.455989 Epoch 100, Training Loss 0.6423969946782607\n",
      "2022-03-26 23:19:11.475805 Epoch 100, Training Loss 0.643253142152296\n",
      "2022-03-26 23:19:11.494777 Epoch 100, Training Loss 0.6441330804163233\n",
      "2022-03-26 23:19:11.514797 Epoch 100, Training Loss 0.6449660400066839\n",
      "2022-03-26 23:19:11.534799 Epoch 100, Training Loss 0.6459169380576409\n",
      "2022-03-26 23:19:11.552971 Epoch 100, Training Loss 0.6466263696132108\n",
      "2022-03-26 23:19:11.572990 Epoch 100, Training Loss 0.6475475542914227\n",
      "2022-03-26 23:19:11.593011 Epoch 100, Training Loss 0.6484538682205293\n",
      "2022-03-26 23:19:11.612042 Epoch 100, Training Loss 0.6494069054074909\n",
      "2022-03-26 23:19:11.630940 Epoch 100, Training Loss 0.650358276332126\n",
      "2022-03-26 23:19:11.650953 Epoch 100, Training Loss 0.6513561584684245\n",
      "2022-03-26 23:19:11.671055 Epoch 100, Training Loss 0.6521690610195975\n",
      "2022-03-26 23:19:11.690912 Epoch 100, Training Loss 0.6530486984402323\n",
      "2022-03-26 23:19:11.709952 Epoch 100, Training Loss 0.6538801036603615\n",
      "2022-03-26 23:19:11.729881 Epoch 100, Training Loss 0.6549306371251641\n",
      "2022-03-26 23:19:11.748902 Epoch 100, Training Loss 0.6558717371481458\n",
      "2022-03-26 23:19:11.768920 Epoch 100, Training Loss 0.6567968580957568\n",
      "2022-03-26 23:19:11.787259 Epoch 100, Training Loss 0.6576048536297611\n",
      "2022-03-26 23:19:11.807233 Epoch 100, Training Loss 0.6588416334689425\n",
      "2022-03-26 23:19:11.826400 Epoch 100, Training Loss 0.6596854474309766\n",
      "2022-03-26 23:19:11.846394 Epoch 100, Training Loss 0.6606379509963038\n",
      "2022-03-26 23:19:11.865181 Epoch 100, Training Loss 0.6617576642643155\n",
      "2022-03-26 23:19:11.885554 Epoch 100, Training Loss 0.6628380220030885\n",
      "2022-03-26 23:19:11.905581 Epoch 100, Training Loss 0.6636372907158664\n",
      "2022-03-26 23:19:11.925419 Epoch 100, Training Loss 0.6644750820172717\n",
      "2022-03-26 23:19:11.943389 Epoch 100, Training Loss 0.6653918313324604\n",
      "2022-03-26 23:19:11.963429 Epoch 100, Training Loss 0.666370470086327\n",
      "2022-03-26 23:19:11.982467 Epoch 100, Training Loss 0.6671413890922161\n",
      "2022-03-26 23:19:12.002661 Epoch 100, Training Loss 0.6679644023289766\n",
      "2022-03-26 23:19:12.022679 Epoch 100, Training Loss 0.6688069533890165\n",
      "2022-03-26 23:19:12.041232 Epoch 100, Training Loss 0.6696141280252915\n",
      "2022-03-26 23:19:12.060081 Epoch 100, Training Loss 0.670347141678376\n",
      "2022-03-26 23:19:12.081065 Epoch 100, Training Loss 0.6711084572479243\n",
      "2022-03-26 23:19:12.100655 Epoch 100, Training Loss 0.6721745400553774\n",
      "2022-03-26 23:19:12.119972 Epoch 100, Training Loss 0.6735089263114173\n",
      "2022-03-26 23:19:12.139068 Epoch 100, Training Loss 0.6744300433055824\n",
      "2022-03-26 23:19:12.158206 Epoch 100, Training Loss 0.6753335632860203\n",
      "2022-03-26 23:19:12.178079 Epoch 100, Training Loss 0.6762619118785005\n",
      "2022-03-26 23:19:12.197969 Epoch 100, Training Loss 0.6771622319584307\n",
      "2022-03-26 23:19:12.216964 Epoch 100, Training Loss 0.6778712393453968\n",
      "2022-03-26 23:19:12.237043 Epoch 100, Training Loss 0.6788929475619055\n",
      "2022-03-26 23:19:12.255828 Epoch 100, Training Loss 0.6799796791866307\n",
      "2022-03-26 23:19:12.275782 Epoch 100, Training Loss 0.6809633044559328\n",
      "2022-03-26 23:19:12.294783 Epoch 100, Training Loss 0.6817916305854802\n",
      "2022-03-26 23:19:12.314820 Epoch 100, Training Loss 0.6828138804268045\n",
      "2022-03-26 23:19:12.333661 Epoch 100, Training Loss 0.683582127513483\n",
      "2022-03-26 23:19:12.354048 Epoch 100, Training Loss 0.6843685651450511\n",
      "2022-03-26 23:19:12.372887 Epoch 100, Training Loss 0.6854256489468963\n",
      "2022-03-26 23:19:12.391921 Epoch 100, Training Loss 0.6862820089244477\n",
      "2022-03-26 23:19:12.410956 Epoch 100, Training Loss 0.6874191790361843\n",
      "2022-03-26 23:19:12.430981 Epoch 100, Training Loss 0.6883116944138047\n",
      "2022-03-26 23:19:12.450068 Epoch 100, Training Loss 0.689403389459071\n",
      "2022-03-26 23:19:12.469098 Epoch 100, Training Loss 0.6900496598323593\n",
      "2022-03-26 23:19:12.488701 Epoch 100, Training Loss 0.6910643372922907\n",
      "2022-03-26 23:19:12.509736 Epoch 100, Training Loss 0.6919109879628472\n",
      "2022-03-26 23:19:12.529024 Epoch 100, Training Loss 0.6927767709621688\n",
      "2022-03-26 23:19:12.549042 Epoch 100, Training Loss 0.6935910213252773\n",
      "2022-03-26 23:19:12.568967 Epoch 100, Training Loss 0.6946740089093938\n",
      "2022-03-26 23:19:12.587984 Epoch 100, Training Loss 0.6958514261047554\n",
      "2022-03-26 23:19:12.607943 Epoch 100, Training Loss 0.6967353123761809\n",
      "2022-03-26 23:19:12.626819 Epoch 100, Training Loss 0.6975220965073846\n",
      "2022-03-26 23:19:12.646868 Epoch 100, Training Loss 0.698667382530849\n",
      "2022-03-26 23:19:12.666348 Epoch 100, Training Loss 0.699508089886602\n",
      "2022-03-26 23:19:12.686267 Epoch 100, Training Loss 0.7003074580674891\n",
      "2022-03-26 23:19:12.706330 Epoch 100, Training Loss 0.7010868512989615\n",
      "2022-03-26 23:19:12.725790 Epoch 100, Training Loss 0.7021672960818576\n",
      "2022-03-26 23:19:12.746176 Epoch 100, Training Loss 0.7030130102277716\n",
      "2022-03-26 23:19:12.765060 Epoch 100, Training Loss 0.7037923279244577\n",
      "2022-03-26 23:19:12.784958 Epoch 100, Training Loss 0.7045047634169269\n",
      "2022-03-26 23:19:12.803987 Epoch 100, Training Loss 0.7054331922698813\n",
      "2022-03-26 23:19:12.823938 Epoch 100, Training Loss 0.7064930342728525\n",
      "2022-03-26 23:19:12.843764 Epoch 100, Training Loss 0.7072983783910342\n",
      "2022-03-26 23:19:12.862792 Epoch 100, Training Loss 0.7079179694356821\n",
      "2022-03-26 23:19:12.882557 Epoch 100, Training Loss 0.7089225548078947\n",
      "2022-03-26 23:19:12.903099 Epoch 100, Training Loss 0.7100206862401475\n",
      "2022-03-26 23:19:12.921984 Epoch 100, Training Loss 0.7112740081213319\n",
      "2022-03-26 23:19:12.941861 Epoch 100, Training Loss 0.7123036974912409\n",
      "2022-03-26 23:19:12.960892 Epoch 100, Training Loss 0.7133384535608389\n",
      "2022-03-26 23:19:12.980897 Epoch 100, Training Loss 0.7141797143556273\n",
      "2022-03-26 23:19:12.999915 Epoch 100, Training Loss 0.7152069805147093\n",
      "2022-03-26 23:19:13.019951 Epoch 100, Training Loss 0.7161862724043829\n",
      "2022-03-26 23:19:13.040339 Epoch 100, Training Loss 0.7171398073892155\n",
      "2022-03-26 23:19:13.059278 Epoch 100, Training Loss 0.718467136363849\n",
      "2022-03-26 23:19:13.078265 Epoch 100, Training Loss 0.7192913048026507\n",
      "2022-03-26 23:19:13.087345 Epoch 100, Training Loss 0.7202370959855712\n",
      "2022-03-26 23:30:32.581190 Epoch 150, Training Loss 0.0008308235794077139\n",
      "2022-03-26 23:30:32.600195 Epoch 150, Training Loss 0.001375445624446625\n",
      "2022-03-26 23:30:32.619199 Epoch 150, Training Loss 0.002077328655725855\n",
      "2022-03-26 23:30:32.638205 Epoch 150, Training Loss 0.00278661051369689\n",
      "2022-03-26 23:30:32.657207 Epoch 150, Training Loss 0.0034643970334621342\n",
      "2022-03-26 23:30:32.677212 Epoch 150, Training Loss 0.004250681613717238\n",
      "2022-03-26 23:30:32.696216 Epoch 150, Training Loss 0.005103603348402721\n",
      "2022-03-26 23:30:32.717221 Epoch 150, Training Loss 0.006262264745619596\n",
      "2022-03-26 23:30:32.736225 Epoch 150, Training Loss 0.007034554582117768\n",
      "2022-03-26 23:30:32.756229 Epoch 150, Training Loss 0.007848449375318445\n",
      "2022-03-26 23:30:32.776236 Epoch 150, Training Loss 0.008469875549416408\n",
      "2022-03-26 23:30:32.796259 Epoch 150, Training Loss 0.009416786034393798\n",
      "2022-03-26 23:30:32.814498 Epoch 150, Training Loss 0.01019604366911037\n",
      "2022-03-26 23:30:32.834299 Epoch 150, Training Loss 0.010854064846587607\n",
      "2022-03-26 23:30:32.853489 Epoch 150, Training Loss 0.011619605562266181\n",
      "2022-03-26 23:30:32.872464 Epoch 150, Training Loss 0.012661098252476938\n",
      "2022-03-26 23:30:32.890775 Epoch 150, Training Loss 0.013442883444259235\n",
      "2022-03-26 23:30:32.910968 Epoch 150, Training Loss 0.014036473715701676\n",
      "2022-03-26 23:30:32.929980 Epoch 150, Training Loss 0.014815453525699313\n",
      "2022-03-26 23:30:32.948685 Epoch 150, Training Loss 0.015620297178283067\n",
      "2022-03-26 23:30:32.967692 Epoch 150, Training Loss 0.01633059658357859\n",
      "2022-03-26 23:30:32.986697 Epoch 150, Training Loss 0.017187693692229288\n",
      "2022-03-26 23:30:33.006707 Epoch 150, Training Loss 0.017888556660898506\n",
      "2022-03-26 23:30:33.025705 Epoch 150, Training Loss 0.018646681476432038\n",
      "2022-03-26 23:30:33.044710 Epoch 150, Training Loss 0.01935352708982385\n",
      "2022-03-26 23:30:33.064708 Epoch 150, Training Loss 0.02019607212842273\n",
      "2022-03-26 23:30:33.083712 Epoch 150, Training Loss 0.020951383863873494\n",
      "2022-03-26 23:30:33.102723 Epoch 150, Training Loss 0.021708135562174765\n",
      "2022-03-26 23:30:33.121727 Epoch 150, Training Loss 0.02247121175536719\n",
      "2022-03-26 23:30:33.140725 Epoch 150, Training Loss 0.023171783332019816\n",
      "2022-03-26 23:30:33.159895 Epoch 150, Training Loss 0.0239334983746414\n",
      "2022-03-26 23:30:33.178895 Epoch 150, Training Loss 0.024677008314205863\n",
      "2022-03-26 23:30:33.198833 Epoch 150, Training Loss 0.02538901331174709\n",
      "2022-03-26 23:30:33.218161 Epoch 150, Training Loss 0.026154006076285905\n",
      "2022-03-26 23:30:33.236553 Epoch 150, Training Loss 0.02693184996809801\n",
      "2022-03-26 23:30:33.256111 Epoch 150, Training Loss 0.02795004478805815\n",
      "2022-03-26 23:30:33.274108 Epoch 150, Training Loss 0.028742577032664852\n",
      "2022-03-26 23:30:33.294125 Epoch 150, Training Loss 0.02955284699454637\n",
      "2022-03-26 23:30:33.312129 Epoch 150, Training Loss 0.03045393499876837\n",
      "2022-03-26 23:30:33.332128 Epoch 150, Training Loss 0.03120762551836955\n",
      "2022-03-26 23:30:33.351127 Epoch 150, Training Loss 0.03179568326686654\n",
      "2022-03-26 23:30:33.370142 Epoch 150, Training Loss 0.03283023697031123\n",
      "2022-03-26 23:30:33.389147 Epoch 150, Training Loss 0.03354269601499943\n",
      "2022-03-26 23:30:33.409145 Epoch 150, Training Loss 0.034184613115037496\n",
      "2022-03-26 23:30:33.429136 Epoch 150, Training Loss 0.03496473943790816\n",
      "2022-03-26 23:30:33.448148 Epoch 150, Training Loss 0.035750671573307205\n",
      "2022-03-26 23:30:33.467158 Epoch 150, Training Loss 0.0364401088956067\n",
      "2022-03-26 23:30:33.487169 Epoch 150, Training Loss 0.03725546865207155\n",
      "2022-03-26 23:30:33.506173 Epoch 150, Training Loss 0.03829213832040577\n",
      "2022-03-26 23:30:33.525175 Epoch 150, Training Loss 0.03937591486574744\n",
      "2022-03-26 23:30:33.544182 Epoch 150, Training Loss 0.04009603089688684\n",
      "2022-03-26 23:30:33.563376 Epoch 150, Training Loss 0.04076544037255485\n",
      "2022-03-26 23:30:33.582370 Epoch 150, Training Loss 0.04169045171469374\n",
      "2022-03-26 23:30:33.602373 Epoch 150, Training Loss 0.042498217595507724\n",
      "2022-03-26 23:30:33.620389 Epoch 150, Training Loss 0.04330397269609944\n",
      "2022-03-26 23:30:33.640394 Epoch 150, Training Loss 0.04414976794091637\n",
      "2022-03-26 23:30:33.660392 Epoch 150, Training Loss 0.045213883261546454\n",
      "2022-03-26 23:30:33.679391 Epoch 150, Training Loss 0.04623462949567439\n",
      "2022-03-26 23:30:33.698462 Epoch 150, Training Loss 0.04705133210972447\n",
      "2022-03-26 23:30:33.718399 Epoch 150, Training Loss 0.04785583078708795\n",
      "2022-03-26 23:30:33.737410 Epoch 150, Training Loss 0.04870026274715238\n",
      "2022-03-26 23:30:33.757414 Epoch 150, Training Loss 0.04967779439428578\n",
      "2022-03-26 23:30:33.776408 Epoch 150, Training Loss 0.050404348565489435\n",
      "2022-03-26 23:30:33.795429 Epoch 150, Training Loss 0.050943933332057864\n",
      "2022-03-26 23:30:33.816428 Epoch 150, Training Loss 0.05174858963398068\n",
      "2022-03-26 23:30:33.836432 Epoch 150, Training Loss 0.05251955886935944\n",
      "2022-03-26 23:30:33.855436 Epoch 150, Training Loss 0.053312651184208866\n",
      "2022-03-26 23:30:33.875441 Epoch 150, Training Loss 0.054099082032128064\n",
      "2022-03-26 23:30:33.893451 Epoch 150, Training Loss 0.05494102172534484\n",
      "2022-03-26 23:30:33.914456 Epoch 150, Training Loss 0.05580540904608529\n",
      "2022-03-26 23:30:33.933441 Epoch 150, Training Loss 0.05693395889323691\n",
      "2022-03-26 23:30:33.952459 Epoch 150, Training Loss 0.05791360307532503\n",
      "2022-03-26 23:30:33.971469 Epoch 150, Training Loss 0.05848351444887078\n",
      "2022-03-26 23:30:33.991473 Epoch 150, Training Loss 0.0597122225843732\n",
      "2022-03-26 23:30:34.011478 Epoch 150, Training Loss 0.06062927480091524\n",
      "2022-03-26 23:30:34.031476 Epoch 150, Training Loss 0.06135190817553674\n",
      "2022-03-26 23:30:34.050469 Epoch 150, Training Loss 0.06198396486089663\n",
      "2022-03-26 23:30:34.069485 Epoch 150, Training Loss 0.0627028064044845\n",
      "2022-03-26 23:30:34.088495 Epoch 150, Training Loss 0.06356227748534259\n",
      "2022-03-26 23:30:34.107500 Epoch 150, Training Loss 0.06423266365400056\n",
      "2022-03-26 23:30:34.127498 Epoch 150, Training Loss 0.0650348646561508\n",
      "2022-03-26 23:30:34.146509 Epoch 150, Training Loss 0.06580748765364937\n",
      "2022-03-26 23:30:34.165923 Epoch 150, Training Loss 0.06657319727456174\n",
      "2022-03-26 23:30:34.185293 Epoch 150, Training Loss 0.06723258120324606\n",
      "2022-03-26 23:30:34.204103 Epoch 150, Training Loss 0.06797361678784461\n",
      "2022-03-26 23:30:34.224567 Epoch 150, Training Loss 0.06862223956286145\n",
      "2022-03-26 23:30:34.243577 Epoch 150, Training Loss 0.0695236691885897\n",
      "2022-03-26 23:30:34.264092 Epoch 150, Training Loss 0.07029774045700307\n",
      "2022-03-26 23:30:34.282333 Epoch 150, Training Loss 0.07122672518805774\n",
      "2022-03-26 23:30:34.301736 Epoch 150, Training Loss 0.07201280465821171\n",
      "2022-03-26 23:30:34.320295 Epoch 150, Training Loss 0.07286434862619776\n",
      "2022-03-26 23:30:34.339179 Epoch 150, Training Loss 0.07339440343325096\n",
      "2022-03-26 23:30:34.358590 Epoch 150, Training Loss 0.07435477946115576\n",
      "2022-03-26 23:30:34.378957 Epoch 150, Training Loss 0.07535983351490382\n",
      "2022-03-26 23:30:34.397638 Epoch 150, Training Loss 0.07627352271848323\n",
      "2022-03-26 23:30:34.417374 Epoch 150, Training Loss 0.07722571438840588\n",
      "2022-03-26 23:30:34.436942 Epoch 150, Training Loss 0.07809939073479694\n",
      "2022-03-26 23:30:34.455584 Epoch 150, Training Loss 0.07905539176653108\n",
      "2022-03-26 23:30:34.474470 Epoch 150, Training Loss 0.07965147186575643\n",
      "2022-03-26 23:30:34.494054 Epoch 150, Training Loss 0.08029990088756737\n",
      "2022-03-26 23:30:34.513256 Epoch 150, Training Loss 0.08087035186607819\n",
      "2022-03-26 23:30:34.532636 Epoch 150, Training Loss 0.08174494423372361\n",
      "2022-03-26 23:30:34.551829 Epoch 150, Training Loss 0.08244068089805906\n",
      "2022-03-26 23:30:34.570833 Epoch 150, Training Loss 0.08306967091682317\n",
      "2022-03-26 23:30:34.589590 Epoch 150, Training Loss 0.0843986967183135\n",
      "2022-03-26 23:30:34.608884 Epoch 150, Training Loss 0.08526591762252476\n",
      "2022-03-26 23:30:34.627232 Epoch 150, Training Loss 0.08613640641617348\n",
      "2022-03-26 23:30:34.646801 Epoch 150, Training Loss 0.08680759953415912\n",
      "2022-03-26 23:30:34.667800 Epoch 150, Training Loss 0.08751571986376477\n",
      "2022-03-26 23:30:34.686805 Epoch 150, Training Loss 0.0883634917418975\n",
      "2022-03-26 23:30:34.705802 Epoch 150, Training Loss 0.08909584783837009\n",
      "2022-03-26 23:30:34.725819 Epoch 150, Training Loss 0.09004080348917286\n",
      "2022-03-26 23:30:34.744823 Epoch 150, Training Loss 0.09107637024291641\n",
      "2022-03-26 23:30:34.763822 Epoch 150, Training Loss 0.09201934758354635\n",
      "2022-03-26 23:30:34.783820 Epoch 150, Training Loss 0.09274800826826364\n",
      "2022-03-26 23:30:34.802837 Epoch 150, Training Loss 0.09333396510547384\n",
      "2022-03-26 23:30:34.822835 Epoch 150, Training Loss 0.09430041417593846\n",
      "2022-03-26 23:30:34.841846 Epoch 150, Training Loss 0.09497653359494855\n",
      "2022-03-26 23:30:34.861844 Epoch 150, Training Loss 0.09570046295137967\n",
      "2022-03-26 23:30:34.888850 Epoch 150, Training Loss 0.09650820931967567\n",
      "2022-03-26 23:30:34.914863 Epoch 150, Training Loss 0.0974690315439878\n",
      "2022-03-26 23:30:34.941868 Epoch 150, Training Loss 0.0982157848299007\n",
      "2022-03-26 23:30:34.967862 Epoch 150, Training Loss 0.09881865974429928\n",
      "2022-03-26 23:30:34.993880 Epoch 150, Training Loss 0.09970651704179662\n",
      "2022-03-26 23:30:35.020867 Epoch 150, Training Loss 0.10043997772971687\n",
      "2022-03-26 23:30:35.046892 Epoch 150, Training Loss 0.10116968058106844\n",
      "2022-03-26 23:30:35.074898 Epoch 150, Training Loss 0.10205665691886716\n",
      "2022-03-26 23:30:35.100892 Epoch 150, Training Loss 0.10262745965624709\n",
      "2022-03-26 23:30:35.127904 Epoch 150, Training Loss 0.10344706788239881\n",
      "2022-03-26 23:30:35.153902 Epoch 150, Training Loss 0.10456280250226141\n",
      "2022-03-26 23:30:35.180922 Epoch 150, Training Loss 0.10541863556560653\n",
      "2022-03-26 23:30:35.206928 Epoch 150, Training Loss 0.10643375784997135\n",
      "2022-03-26 23:30:35.233922 Epoch 150, Training Loss 0.10705447177905256\n",
      "2022-03-26 23:30:35.258940 Epoch 150, Training Loss 0.10770862421873585\n",
      "2022-03-26 23:30:35.279939 Epoch 150, Training Loss 0.10885901962552229\n",
      "2022-03-26 23:30:35.299944 Epoch 150, Training Loss 0.1098184153780608\n",
      "2022-03-26 23:30:35.317941 Epoch 150, Training Loss 0.11074152219173548\n",
      "2022-03-26 23:30:35.337958 Epoch 150, Training Loss 0.1117379418800554\n",
      "2022-03-26 23:30:35.355956 Epoch 150, Training Loss 0.11240552480111037\n",
      "2022-03-26 23:30:35.374966 Epoch 150, Training Loss 0.11347405223742775\n",
      "2022-03-26 23:30:35.392971 Epoch 150, Training Loss 0.11462357628833303\n",
      "2022-03-26 23:30:35.411969 Epoch 150, Training Loss 0.1155648510855482\n",
      "2022-03-26 23:30:35.429973 Epoch 150, Training Loss 0.11633692780876403\n",
      "2022-03-26 23:30:35.448978 Epoch 150, Training Loss 0.11725299395716099\n",
      "2022-03-26 23:30:35.467982 Epoch 150, Training Loss 0.11806626656018865\n",
      "2022-03-26 23:30:35.485987 Epoch 150, Training Loss 0.11907822774041948\n",
      "2022-03-26 23:30:35.503997 Epoch 150, Training Loss 0.11996926946560744\n",
      "2022-03-26 23:30:35.523000 Epoch 150, Training Loss 0.1209162685191235\n",
      "2022-03-26 23:30:35.540998 Epoch 150, Training Loss 0.12182881524953086\n",
      "2022-03-26 23:30:35.561009 Epoch 150, Training Loss 0.12268086257950424\n",
      "2022-03-26 23:30:35.579012 Epoch 150, Training Loss 0.12341643923231403\n",
      "2022-03-26 23:30:35.598999 Epoch 150, Training Loss 0.12421695987129455\n",
      "2022-03-26 23:30:35.617009 Epoch 150, Training Loss 0.12502488734014808\n",
      "2022-03-26 23:30:35.635013 Epoch 150, Training Loss 0.1260679936241311\n",
      "2022-03-26 23:30:35.654030 Epoch 150, Training Loss 0.12695748517123026\n",
      "2022-03-26 23:30:35.672033 Epoch 150, Training Loss 0.1279125511265167\n",
      "2022-03-26 23:30:35.691032 Epoch 150, Training Loss 0.12889857620686826\n",
      "2022-03-26 23:30:35.711043 Epoch 150, Training Loss 0.12971316872502836\n",
      "2022-03-26 23:30:35.730035 Epoch 150, Training Loss 0.13054879124054825\n",
      "2022-03-26 23:30:35.749033 Epoch 150, Training Loss 0.1317705775389586\n",
      "2022-03-26 23:30:35.768043 Epoch 150, Training Loss 0.13252914336788685\n",
      "2022-03-26 23:30:35.787060 Epoch 150, Training Loss 0.13340071952708846\n",
      "2022-03-26 23:30:35.805058 Epoch 150, Training Loss 0.13394975140119147\n",
      "2022-03-26 23:30:35.824062 Epoch 150, Training Loss 0.13463035687003905\n",
      "2022-03-26 23:30:35.842072 Epoch 150, Training Loss 0.13547522001101842\n",
      "2022-03-26 23:30:35.862071 Epoch 150, Training Loss 0.1359538694324396\n",
      "2022-03-26 23:30:35.880075 Epoch 150, Training Loss 0.13658726950893013\n",
      "2022-03-26 23:30:35.900073 Epoch 150, Training Loss 0.1374177666347655\n",
      "2022-03-26 23:30:35.919084 Epoch 150, Training Loss 0.13811494943583408\n",
      "2022-03-26 23:30:35.937088 Epoch 150, Training Loss 0.13885508995989096\n",
      "2022-03-26 23:30:35.955092 Epoch 150, Training Loss 0.1396439979067239\n",
      "2022-03-26 23:30:35.973096 Epoch 150, Training Loss 0.14041764790292285\n",
      "2022-03-26 23:30:35.992094 Epoch 150, Training Loss 0.14120408515338703\n",
      "2022-03-26 23:30:36.010110 Epoch 150, Training Loss 0.14219471846547577\n",
      "2022-03-26 23:30:36.028108 Epoch 150, Training Loss 0.14273229039386107\n",
      "2022-03-26 23:30:36.046107 Epoch 150, Training Loss 0.1435562214049537\n",
      "2022-03-26 23:30:36.065117 Epoch 150, Training Loss 0.14443418753269077\n",
      "2022-03-26 23:30:36.083115 Epoch 150, Training Loss 0.14564786340726915\n",
      "2022-03-26 23:30:36.102115 Epoch 150, Training Loss 0.14668312947006176\n",
      "2022-03-26 23:30:36.121118 Epoch 150, Training Loss 0.14743549862633581\n",
      "2022-03-26 23:30:36.139119 Epoch 150, Training Loss 0.14819956507981585\n",
      "2022-03-26 23:30:36.158125 Epoch 150, Training Loss 0.14928093236273207\n",
      "2022-03-26 23:30:36.177142 Epoch 150, Training Loss 0.15037935572054684\n",
      "2022-03-26 23:30:36.196133 Epoch 150, Training Loss 0.15104825139198158\n",
      "2022-03-26 23:30:36.214137 Epoch 150, Training Loss 0.1518039937748019\n",
      "2022-03-26 23:30:36.233143 Epoch 150, Training Loss 0.15275770818333492\n",
      "2022-03-26 23:30:36.253146 Epoch 150, Training Loss 0.15348899200596772\n",
      "2022-03-26 23:30:36.271157 Epoch 150, Training Loss 0.15454984218110818\n",
      "2022-03-26 23:30:36.290174 Epoch 150, Training Loss 0.15528367055803918\n",
      "2022-03-26 23:30:36.309158 Epoch 150, Training Loss 0.15599642541554884\n",
      "2022-03-26 23:30:36.328163 Epoch 150, Training Loss 0.15667125754191746\n",
      "2022-03-26 23:30:36.346166 Epoch 150, Training Loss 0.15737006605586126\n",
      "2022-03-26 23:30:36.365173 Epoch 150, Training Loss 0.15825645084423787\n",
      "2022-03-26 23:30:36.384175 Epoch 150, Training Loss 0.15909102562900698\n",
      "2022-03-26 23:30:36.402180 Epoch 150, Training Loss 0.15975358255226593\n",
      "2022-03-26 23:30:36.421183 Epoch 150, Training Loss 0.1607206225623865\n",
      "2022-03-26 23:30:36.439188 Epoch 150, Training Loss 0.16148891911634702\n",
      "2022-03-26 23:30:36.458194 Epoch 150, Training Loss 0.16216931436830165\n",
      "2022-03-26 23:30:36.476196 Epoch 150, Training Loss 0.16310877640686378\n",
      "2022-03-26 23:30:36.496201 Epoch 150, Training Loss 0.16392673529170054\n",
      "2022-03-26 23:30:36.516207 Epoch 150, Training Loss 0.16470645017483654\n",
      "2022-03-26 23:30:36.534210 Epoch 150, Training Loss 0.16562770322308212\n",
      "2022-03-26 23:30:36.554214 Epoch 150, Training Loss 0.1663636464978118\n",
      "2022-03-26 23:30:36.573218 Epoch 150, Training Loss 0.1671314027989307\n",
      "2022-03-26 23:30:36.592224 Epoch 150, Training Loss 0.16788900984675073\n",
      "2022-03-26 23:30:36.611227 Epoch 150, Training Loss 0.16867164345196142\n",
      "2022-03-26 23:30:36.629231 Epoch 150, Training Loss 0.16941386526045593\n",
      "2022-03-26 23:30:36.648237 Epoch 150, Training Loss 0.1703384962609357\n",
      "2022-03-26 23:30:36.667241 Epoch 150, Training Loss 0.17100425986835108\n",
      "2022-03-26 23:30:36.685245 Epoch 150, Training Loss 0.17169326943967045\n",
      "2022-03-26 23:30:36.703247 Epoch 150, Training Loss 0.17264730710050333\n",
      "2022-03-26 23:30:36.724253 Epoch 150, Training Loss 0.17375452275318867\n",
      "2022-03-26 23:30:36.744257 Epoch 150, Training Loss 0.17491022068673692\n",
      "2022-03-26 23:30:36.765267 Epoch 150, Training Loss 0.17558559116042788\n",
      "2022-03-26 23:30:36.783277 Epoch 150, Training Loss 0.1762383513514648\n",
      "2022-03-26 23:30:36.803286 Epoch 150, Training Loss 0.1770294581913887\n",
      "2022-03-26 23:30:36.821294 Epoch 150, Training Loss 0.17792948619331544\n",
      "2022-03-26 23:30:36.840299 Epoch 150, Training Loss 0.17866824308167334\n",
      "2022-03-26 23:30:36.858297 Epoch 150, Training Loss 0.17917663560194128\n",
      "2022-03-26 23:30:36.877307 Epoch 150, Training Loss 0.1801612949584756\n",
      "2022-03-26 23:30:36.896306 Epoch 150, Training Loss 0.18097725099005052\n",
      "2022-03-26 23:30:36.915311 Epoch 150, Training Loss 0.182001335389169\n",
      "2022-03-26 23:30:36.934308 Epoch 150, Training Loss 0.18289339374703215\n",
      "2022-03-26 23:30:36.953318 Epoch 150, Training Loss 0.1835635965285094\n",
      "2022-03-26 23:30:36.972322 Epoch 150, Training Loss 0.18450856399353202\n",
      "2022-03-26 23:30:36.991436 Epoch 150, Training Loss 0.18535770052839118\n",
      "2022-03-26 23:30:37.010446 Epoch 150, Training Loss 0.18661277579224628\n",
      "2022-03-26 23:30:37.029451 Epoch 150, Training Loss 0.1875065888285332\n",
      "2022-03-26 23:30:37.047455 Epoch 150, Training Loss 0.1882983007851769\n",
      "2022-03-26 23:30:37.067447 Epoch 150, Training Loss 0.18898738474797105\n",
      "2022-03-26 23:30:37.085453 Epoch 150, Training Loss 0.18982147103380365\n",
      "2022-03-26 23:30:37.111463 Epoch 150, Training Loss 0.19074337561721996\n",
      "2022-03-26 23:30:37.137475 Epoch 150, Training Loss 0.19145616424053222\n",
      "2022-03-26 23:30:37.164481 Epoch 150, Training Loss 0.19206615283970943\n",
      "2022-03-26 23:30:37.190715 Epoch 150, Training Loss 0.19262905384573487\n",
      "2022-03-26 23:30:37.216709 Epoch 150, Training Loss 0.1934933850679861\n",
      "2022-03-26 23:30:37.242727 Epoch 150, Training Loss 0.19432410170964878\n",
      "2022-03-26 23:30:37.268733 Epoch 150, Training Loss 0.19498224964227212\n",
      "2022-03-26 23:30:37.294883 Epoch 150, Training Loss 0.1956570056240882\n",
      "2022-03-26 23:30:37.313880 Epoch 150, Training Loss 0.19663947729198525\n",
      "2022-03-26 23:30:37.331898 Epoch 150, Training Loss 0.19755035715029978\n",
      "2022-03-26 23:30:37.350890 Epoch 150, Training Loss 0.19822621467473256\n",
      "2022-03-26 23:30:37.369900 Epoch 150, Training Loss 0.1990322565940945\n",
      "2022-03-26 23:30:37.387904 Epoch 150, Training Loss 0.20000388158861634\n",
      "2022-03-26 23:30:37.405901 Epoch 150, Training Loss 0.20075454454287847\n",
      "2022-03-26 23:30:37.424905 Epoch 150, Training Loss 0.20179897272373404\n",
      "2022-03-26 23:30:37.442922 Epoch 150, Training Loss 0.20278862530313185\n",
      "2022-03-26 23:30:37.461920 Epoch 150, Training Loss 0.2036012865393363\n",
      "2022-03-26 23:30:37.480928 Epoch 150, Training Loss 0.2044991697649212\n",
      "2022-03-26 23:30:37.498924 Epoch 150, Training Loss 0.20518447500665474\n",
      "2022-03-26 23:30:37.517928 Epoch 150, Training Loss 0.20609360536955812\n",
      "2022-03-26 23:30:37.536941 Epoch 150, Training Loss 0.2067822879537597\n",
      "2022-03-26 23:30:37.554947 Epoch 150, Training Loss 0.20773808235097724\n",
      "2022-03-26 23:30:37.573952 Epoch 150, Training Loss 0.20878211631799293\n",
      "2022-03-26 23:30:37.594944 Epoch 150, Training Loss 0.2094817254549402\n",
      "2022-03-26 23:30:37.613961 Epoch 150, Training Loss 0.2106294141858435\n",
      "2022-03-26 23:30:37.632960 Epoch 150, Training Loss 0.21138303610674866\n",
      "2022-03-26 23:30:37.650957 Epoch 150, Training Loss 0.2122401885516808\n",
      "2022-03-26 23:30:37.668957 Epoch 150, Training Loss 0.21271629155139485\n",
      "2022-03-26 23:30:37.686972 Epoch 150, Training Loss 0.21342461211297215\n",
      "2022-03-26 23:30:37.705976 Epoch 150, Training Loss 0.21440164436159842\n",
      "2022-03-26 23:30:37.725075 Epoch 150, Training Loss 0.21524885281577438\n",
      "2022-03-26 23:30:37.743073 Epoch 150, Training Loss 0.21623045899678983\n",
      "2022-03-26 23:30:37.762078 Epoch 150, Training Loss 0.21689990543953294\n",
      "2022-03-26 23:30:37.780076 Epoch 150, Training Loss 0.21765439612481297\n",
      "2022-03-26 23:30:37.799082 Epoch 150, Training Loss 0.21844556767617346\n",
      "2022-03-26 23:30:37.817084 Epoch 150, Training Loss 0.21914570746214493\n",
      "2022-03-26 23:30:37.836100 Epoch 150, Training Loss 0.2197179889968594\n",
      "2022-03-26 23:30:37.853099 Epoch 150, Training Loss 0.22036117631608568\n",
      "2022-03-26 23:30:37.873103 Epoch 150, Training Loss 0.2212981682680452\n",
      "2022-03-26 23:30:37.891113 Epoch 150, Training Loss 0.22220874042309763\n",
      "2022-03-26 23:30:37.910117 Epoch 150, Training Loss 0.22326389911686978\n",
      "2022-03-26 23:30:37.928122 Epoch 150, Training Loss 0.2241639919064539\n",
      "2022-03-26 23:30:37.948202 Epoch 150, Training Loss 0.22483060133579136\n",
      "2022-03-26 23:30:37.967124 Epoch 150, Training Loss 0.2254811561549716\n",
      "2022-03-26 23:30:37.986129 Epoch 150, Training Loss 0.2261269812465019\n",
      "2022-03-26 23:30:38.005133 Epoch 150, Training Loss 0.22715254112735123\n",
      "2022-03-26 23:30:38.024143 Epoch 150, Training Loss 0.22783020554143754\n",
      "2022-03-26 23:30:38.042147 Epoch 150, Training Loss 0.22861885006927773\n",
      "2022-03-26 23:30:38.060363 Epoch 150, Training Loss 0.22937392719718805\n",
      "2022-03-26 23:30:38.077643 Epoch 150, Training Loss 0.23002614240969538\n",
      "2022-03-26 23:30:38.096526 Epoch 150, Training Loss 0.23110792989773518\n",
      "2022-03-26 23:30:38.114452 Epoch 150, Training Loss 0.232112118007277\n",
      "2022-03-26 23:30:38.133006 Epoch 150, Training Loss 0.23255188305816993\n",
      "2022-03-26 23:30:38.150434 Epoch 150, Training Loss 0.2335317456889945\n",
      "2022-03-26 23:30:38.168800 Epoch 150, Training Loss 0.23449745275023037\n",
      "2022-03-26 23:30:38.186114 Epoch 150, Training Loss 0.23526136546641055\n",
      "2022-03-26 23:30:38.205119 Epoch 150, Training Loss 0.2359658799436696\n",
      "2022-03-26 23:30:38.223860 Epoch 150, Training Loss 0.23700764420849588\n",
      "2022-03-26 23:30:38.241652 Epoch 150, Training Loss 0.2378851606336701\n",
      "2022-03-26 23:30:38.260650 Epoch 150, Training Loss 0.23841544261673833\n",
      "2022-03-26 23:30:38.278667 Epoch 150, Training Loss 0.2390361999154396\n",
      "2022-03-26 23:30:38.296665 Epoch 150, Training Loss 0.23992283973852388\n",
      "2022-03-26 23:30:38.315787 Epoch 150, Training Loss 0.24054849509845305\n",
      "2022-03-26 23:30:38.334063 Epoch 150, Training Loss 0.2413070007891911\n",
      "2022-03-26 23:30:38.352257 Epoch 150, Training Loss 0.24190455381675144\n",
      "2022-03-26 23:30:38.369268 Epoch 150, Training Loss 0.2424485439915791\n",
      "2022-03-26 23:30:38.389279 Epoch 150, Training Loss 0.24318363359364706\n",
      "2022-03-26 23:30:38.408453 Epoch 150, Training Loss 0.24385343461542788\n",
      "2022-03-26 23:30:38.426770 Epoch 150, Training Loss 0.24453221993220736\n",
      "2022-03-26 23:30:38.444607 Epoch 150, Training Loss 0.24541201619693384\n",
      "2022-03-26 23:30:38.463107 Epoch 150, Training Loss 0.24629751850119638\n",
      "2022-03-26 23:30:38.481497 Epoch 150, Training Loss 0.24753322629520044\n",
      "2022-03-26 23:30:38.499507 Epoch 150, Training Loss 0.24841790221383808\n",
      "2022-03-26 23:30:38.517929 Epoch 150, Training Loss 0.24918010415475997\n",
      "2022-03-26 23:30:38.535507 Epoch 150, Training Loss 0.24998744792493102\n",
      "2022-03-26 23:30:38.554238 Epoch 150, Training Loss 0.2506185146548864\n",
      "2022-03-26 23:30:38.571772 Epoch 150, Training Loss 0.25142132199328876\n",
      "2022-03-26 23:30:38.590938 Epoch 150, Training Loss 0.25251179849705124\n",
      "2022-03-26 23:30:38.609943 Epoch 150, Training Loss 0.2532406620052465\n",
      "2022-03-26 23:30:38.627953 Epoch 150, Training Loss 0.25398130108938194\n",
      "2022-03-26 23:30:38.646963 Epoch 150, Training Loss 0.2549113480331343\n",
      "2022-03-26 23:30:38.665962 Epoch 150, Training Loss 0.2556818963774025\n",
      "2022-03-26 23:30:38.683966 Epoch 150, Training Loss 0.2564562108663037\n",
      "2022-03-26 23:30:38.702956 Epoch 150, Training Loss 0.2573215780050858\n",
      "2022-03-26 23:30:38.720968 Epoch 150, Training Loss 0.25805522055577135\n",
      "2022-03-26 23:30:38.738972 Epoch 150, Training Loss 0.2589091173828105\n",
      "2022-03-26 23:30:38.757983 Epoch 150, Training Loss 0.2595717052517035\n",
      "2022-03-26 23:30:38.775992 Epoch 150, Training Loss 0.26017222033284815\n",
      "2022-03-26 23:30:38.794984 Epoch 150, Training Loss 0.2609966060770747\n",
      "2022-03-26 23:30:38.813989 Epoch 150, Training Loss 0.2616637447834625\n",
      "2022-03-26 23:30:38.832987 Epoch 150, Training Loss 0.26213204262354184\n",
      "2022-03-26 23:30:38.850997 Epoch 150, Training Loss 0.2627798538759846\n",
      "2022-03-26 23:30:38.868994 Epoch 150, Training Loss 0.2634928203985819\n",
      "2022-03-26 23:30:38.887018 Epoch 150, Training Loss 0.26430188523381565\n",
      "2022-03-26 23:30:38.905016 Epoch 150, Training Loss 0.2650183650767407\n",
      "2022-03-26 23:30:38.923008 Epoch 150, Training Loss 0.2658316916440759\n",
      "2022-03-26 23:30:38.941024 Epoch 150, Training Loss 0.26656276631690656\n",
      "2022-03-26 23:30:38.959022 Epoch 150, Training Loss 0.2672888063027731\n",
      "2022-03-26 23:30:38.978032 Epoch 150, Training Loss 0.2685490491064003\n",
      "2022-03-26 23:30:38.997037 Epoch 150, Training Loss 0.26937402266523114\n",
      "2022-03-26 23:30:39.017041 Epoch 150, Training Loss 0.2704366899436087\n",
      "2022-03-26 23:30:39.035045 Epoch 150, Training Loss 0.27115061471376883\n",
      "2022-03-26 23:30:39.053055 Epoch 150, Training Loss 0.2717419738888436\n",
      "2022-03-26 23:30:39.071054 Epoch 150, Training Loss 0.27243616677763516\n",
      "2022-03-26 23:30:39.090064 Epoch 150, Training Loss 0.2730330842763872\n",
      "2022-03-26 23:30:39.108062 Epoch 150, Training Loss 0.27404749900331277\n",
      "2022-03-26 23:30:39.127060 Epoch 150, Training Loss 0.2747978968617251\n",
      "2022-03-26 23:30:39.145076 Epoch 150, Training Loss 0.2754507496610017\n",
      "2022-03-26 23:30:39.165075 Epoch 150, Training Loss 0.27653963040665286\n",
      "2022-03-26 23:30:39.183073 Epoch 150, Training Loss 0.2772990092825707\n",
      "2022-03-26 23:30:39.202089 Epoch 150, Training Loss 0.27833177492289285\n",
      "2022-03-26 23:30:39.220087 Epoch 150, Training Loss 0.2791231159892533\n",
      "2022-03-26 23:30:39.240092 Epoch 150, Training Loss 0.28005850227440104\n",
      "2022-03-26 23:30:39.258102 Epoch 150, Training Loss 0.2812036450790322\n",
      "2022-03-26 23:30:39.276106 Epoch 150, Training Loss 0.281865466852932\n",
      "2022-03-26 23:30:39.294117 Epoch 150, Training Loss 0.28278975775632104\n",
      "2022-03-26 23:30:39.313121 Epoch 150, Training Loss 0.283509656329594\n",
      "2022-03-26 23:30:39.331788 Epoch 150, Training Loss 0.2843361529319183\n",
      "2022-03-26 23:30:39.350798 Epoch 150, Training Loss 0.2849463440115799\n",
      "2022-03-26 23:30:39.369808 Epoch 150, Training Loss 0.28590812570298724\n",
      "2022-03-26 23:30:39.388800 Epoch 150, Training Loss 0.2864677453666087\n",
      "2022-03-26 23:30:39.407804 Epoch 150, Training Loss 0.28739640524472726\n",
      "2022-03-26 23:30:39.426821 Epoch 150, Training Loss 0.2885183888246946\n",
      "2022-03-26 23:30:39.445819 Epoch 150, Training Loss 0.28936109590865766\n",
      "2022-03-26 23:30:39.464809 Epoch 150, Training Loss 0.290498213656723\n",
      "2022-03-26 23:30:39.482821 Epoch 150, Training Loss 0.29125520827062906\n",
      "2022-03-26 23:30:39.501819 Epoch 150, Training Loss 0.2920084359776943\n",
      "2022-03-26 23:30:39.519822 Epoch 150, Training Loss 0.2928528879076014\n",
      "2022-03-26 23:30:39.537834 Epoch 150, Training Loss 0.29359287073087814\n",
      "2022-03-26 23:30:39.556850 Epoch 150, Training Loss 0.2943523796394353\n",
      "2022-03-26 23:30:39.575842 Epoch 150, Training Loss 0.29515180056509766\n",
      "2022-03-26 23:30:39.594859 Epoch 150, Training Loss 0.29585839632679434\n",
      "2022-03-26 23:30:39.613859 Epoch 150, Training Loss 0.2967038076475758\n",
      "2022-03-26 23:30:39.634848 Epoch 150, Training Loss 0.297795609020821\n",
      "2022-03-26 23:30:39.652853 Epoch 150, Training Loss 0.2988643434727588\n",
      "2022-03-26 23:30:39.671856 Epoch 150, Training Loss 0.29957295313972954\n",
      "2022-03-26 23:30:39.691861 Epoch 150, Training Loss 0.3006632300212865\n",
      "2022-03-26 23:30:39.710865 Epoch 150, Training Loss 0.3017606719604234\n",
      "2022-03-26 23:30:39.728871 Epoch 150, Training Loss 0.3026367274620344\n",
      "2022-03-26 23:30:39.747877 Epoch 150, Training Loss 0.3033889539330207\n",
      "2022-03-26 23:30:39.767879 Epoch 150, Training Loss 0.3040034309829897\n",
      "2022-03-26 23:30:39.785886 Epoch 150, Training Loss 0.3047621954432534\n",
      "2022-03-26 23:30:39.803906 Epoch 150, Training Loss 0.30565117745448256\n",
      "2022-03-26 23:30:39.823899 Epoch 150, Training Loss 0.30640194216347716\n",
      "2022-03-26 23:30:39.842903 Epoch 150, Training Loss 0.307114488099847\n",
      "2022-03-26 23:30:39.860919 Epoch 150, Training Loss 0.30786500730173055\n",
      "2022-03-26 23:30:39.879911 Epoch 150, Training Loss 0.30882998843632087\n",
      "2022-03-26 23:30:39.897909 Epoch 150, Training Loss 0.30979312068361153\n",
      "2022-03-26 23:30:39.916932 Epoch 150, Training Loss 0.31068970747006214\n",
      "2022-03-26 23:30:39.935936 Epoch 150, Training Loss 0.3116177981314452\n",
      "2022-03-26 23:30:39.954928 Epoch 150, Training Loss 0.3124192384502772\n",
      "2022-03-26 23:30:39.972932 Epoch 150, Training Loss 0.3133153846806577\n",
      "2022-03-26 23:30:39.991949 Epoch 150, Training Loss 0.3141409622128967\n",
      "2022-03-26 23:30:40.009941 Epoch 150, Training Loss 0.31499887648445873\n",
      "2022-03-26 23:30:40.028957 Epoch 150, Training Loss 0.3159390206989425\n",
      "2022-03-26 23:30:40.047943 Epoch 150, Training Loss 0.3168860239446011\n",
      "2022-03-26 23:30:40.066954 Epoch 150, Training Loss 0.31760336942684925\n",
      "2022-03-26 23:30:40.085970 Epoch 150, Training Loss 0.31828904571130756\n",
      "2022-03-26 23:30:40.103974 Epoch 150, Training Loss 0.3191519535868369\n",
      "2022-03-26 23:30:40.121959 Epoch 150, Training Loss 0.3197687325041617\n",
      "2022-03-26 23:30:40.147965 Epoch 150, Training Loss 0.3207193612671264\n",
      "2022-03-26 23:30:40.173990 Epoch 150, Training Loss 0.3216750972030108\n",
      "2022-03-26 23:30:40.199987 Epoch 150, Training Loss 0.3225667590604109\n",
      "2022-03-26 23:30:40.226178 Epoch 150, Training Loss 0.32346995609343204\n",
      "2022-03-26 23:30:40.252189 Epoch 150, Training Loss 0.3241660278242872\n",
      "2022-03-26 23:30:40.278193 Epoch 150, Training Loss 0.3248447690092389\n",
      "2022-03-26 23:30:40.305207 Epoch 150, Training Loss 0.3258893420857847\n",
      "2022-03-26 23:30:40.331192 Epoch 150, Training Loss 0.32679253294492316\n",
      "2022-03-26 23:30:40.346206 Epoch 150, Training Loss 0.3275650192023543\n",
      "2022-03-26 23:30:40.360208 Epoch 150, Training Loss 0.32828758916129236\n",
      "2022-03-26 23:30:40.375211 Epoch 150, Training Loss 0.3292236887204373\n",
      "2022-03-26 23:30:40.389219 Epoch 150, Training Loss 0.33002660635029873\n",
      "2022-03-26 23:30:40.403216 Epoch 150, Training Loss 0.3308182189912747\n",
      "2022-03-26 23:30:40.417219 Epoch 150, Training Loss 0.3318469409671281\n",
      "2022-03-26 23:30:40.431223 Epoch 150, Training Loss 0.33258802487569694\n",
      "2022-03-26 23:30:40.445226 Epoch 150, Training Loss 0.33332515635606275\n",
      "2022-03-26 23:30:40.460229 Epoch 150, Training Loss 0.3342101318223397\n",
      "2022-03-26 23:30:40.474437 Epoch 150, Training Loss 0.3349248050804943\n",
      "2022-03-26 23:30:40.488425 Epoch 150, Training Loss 0.33572649197352816\n",
      "2022-03-26 23:30:40.502436 Epoch 150, Training Loss 0.3363149906210887\n",
      "2022-03-26 23:30:40.517440 Epoch 150, Training Loss 0.33711258948916367\n",
      "2022-03-26 23:30:40.531435 Epoch 150, Training Loss 0.3377964376183727\n",
      "2022-03-26 23:30:40.545446 Epoch 150, Training Loss 0.33874659716625655\n",
      "2022-03-26 23:30:40.559449 Epoch 150, Training Loss 0.33980559212777317\n",
      "2022-03-26 23:30:40.573452 Epoch 150, Training Loss 0.3404916925808353\n",
      "2022-03-26 23:30:40.587618 Epoch 150, Training Loss 0.3412840275660805\n",
      "2022-03-26 23:30:40.602095 Epoch 150, Training Loss 0.3422464812960466\n",
      "2022-03-26 23:30:40.616445 Epoch 150, Training Loss 0.34320035409134675\n",
      "2022-03-26 23:30:40.630679 Epoch 150, Training Loss 0.3441690373451204\n",
      "2022-03-26 23:30:40.644893 Epoch 150, Training Loss 0.3452659478730253\n",
      "2022-03-26 23:30:40.660403 Epoch 150, Training Loss 0.3464351447341997\n",
      "2022-03-26 23:30:40.673598 Epoch 150, Training Loss 0.34741925690180203\n",
      "2022-03-26 23:30:40.688601 Epoch 150, Training Loss 0.34843942439159775\n",
      "2022-03-26 23:30:40.702603 Epoch 150, Training Loss 0.34916840992925113\n",
      "2022-03-26 23:30:40.716824 Epoch 150, Training Loss 0.34984047920502664\n",
      "2022-03-26 23:30:40.731821 Epoch 150, Training Loss 0.35064772594615323\n",
      "2022-03-26 23:30:40.745823 Epoch 150, Training Loss 0.3515398777506845\n",
      "2022-03-26 23:30:40.759825 Epoch 150, Training Loss 0.3522643575735409\n",
      "2022-03-26 23:30:40.774829 Epoch 150, Training Loss 0.3532489410904058\n",
      "2022-03-26 23:30:40.788833 Epoch 150, Training Loss 0.3541086787153083\n",
      "2022-03-26 23:30:40.802835 Epoch 150, Training Loss 0.35488282826245593\n",
      "2022-03-26 23:30:40.817048 Epoch 150, Training Loss 0.355527531460423\n",
      "2022-03-26 23:30:40.831050 Epoch 150, Training Loss 0.35638851018817835\n",
      "2022-03-26 23:30:40.845055 Epoch 150, Training Loss 0.3572613461243222\n",
      "2022-03-26 23:30:40.860058 Epoch 150, Training Loss 0.35773503399261125\n",
      "2022-03-26 23:30:40.874060 Epoch 150, Training Loss 0.3586360431083328\n",
      "2022-03-26 23:30:40.888066 Epoch 150, Training Loss 0.3593426847549351\n",
      "2022-03-26 23:30:40.903066 Epoch 150, Training Loss 0.3599543157212265\n",
      "2022-03-26 23:30:40.917189 Epoch 150, Training Loss 0.36052039193222896\n",
      "2022-03-26 23:30:40.932193 Epoch 150, Training Loss 0.36103100967986507\n",
      "2022-03-26 23:30:40.947207 Epoch 150, Training Loss 0.36173966477441666\n",
      "2022-03-26 23:30:40.961212 Epoch 150, Training Loss 0.36279139909750363\n",
      "2022-03-26 23:30:40.975210 Epoch 150, Training Loss 0.363734757008455\n",
      "2022-03-26 23:30:40.989212 Epoch 150, Training Loss 0.36455429556881985\n",
      "2022-03-26 23:30:41.004216 Epoch 150, Training Loss 0.36544914993331257\n",
      "2022-03-26 23:30:41.018225 Epoch 150, Training Loss 0.36601732690316024\n",
      "2022-03-26 23:30:41.032229 Epoch 150, Training Loss 0.3669030546684704\n",
      "2022-03-26 23:30:41.046226 Epoch 150, Training Loss 0.3679600553896726\n",
      "2022-03-26 23:30:41.061229 Epoch 150, Training Loss 0.36855295635855106\n",
      "2022-03-26 23:30:41.075234 Epoch 150, Training Loss 0.3694137793672664\n",
      "2022-03-26 23:30:41.089377 Epoch 150, Training Loss 0.3704647572753984\n",
      "2022-03-26 23:30:41.104372 Epoch 150, Training Loss 0.3715345569888649\n",
      "2022-03-26 23:30:41.118374 Epoch 150, Training Loss 0.37213497949988034\n",
      "2022-03-26 23:30:41.132385 Epoch 150, Training Loss 0.37291512388707426\n",
      "2022-03-26 23:30:41.146771 Epoch 150, Training Loss 0.37364694986806807\n",
      "2022-03-26 23:30:41.160935 Epoch 150, Training Loss 0.37478432981559384\n",
      "2022-03-26 23:30:41.175450 Epoch 150, Training Loss 0.3755814816488329\n",
      "2022-03-26 23:30:41.189453 Epoch 150, Training Loss 0.3765684780104996\n",
      "2022-03-26 23:30:41.204451 Epoch 150, Training Loss 0.3773880953069233\n",
      "2022-03-26 23:30:41.218452 Epoch 150, Training Loss 0.378327341564476\n",
      "2022-03-26 23:30:41.233455 Epoch 150, Training Loss 0.37941725296742473\n",
      "2022-03-26 23:30:41.247459 Epoch 150, Training Loss 0.3803768112988728\n",
      "2022-03-26 23:30:41.262462 Epoch 150, Training Loss 0.38112643780305866\n",
      "2022-03-26 23:30:41.277465 Epoch 150, Training Loss 0.38167097467138333\n",
      "2022-03-26 23:30:41.291469 Epoch 150, Training Loss 0.3825060826204622\n",
      "2022-03-26 23:30:41.305473 Epoch 150, Training Loss 0.3832425351261788\n",
      "2022-03-26 23:30:41.319475 Epoch 150, Training Loss 0.3842355002603872\n",
      "2022-03-26 23:30:41.333478 Epoch 150, Training Loss 0.3850729210526132\n",
      "2022-03-26 23:30:41.348482 Epoch 150, Training Loss 0.38584045852389176\n",
      "2022-03-26 23:30:41.362484 Epoch 150, Training Loss 0.38667672960197225\n",
      "2022-03-26 23:30:41.377496 Epoch 150, Training Loss 0.38786758764473067\n",
      "2022-03-26 23:30:41.391499 Epoch 150, Training Loss 0.3888927277015603\n",
      "2022-03-26 23:30:41.405509 Epoch 150, Training Loss 0.38935599321751946\n",
      "2022-03-26 23:30:41.419497 Epoch 150, Training Loss 0.39044994180617126\n",
      "2022-03-26 23:30:41.434501 Epoch 150, Training Loss 0.3913961631791366\n",
      "2022-03-26 23:30:41.448518 Epoch 150, Training Loss 0.39240014960851205\n",
      "2022-03-26 23:30:41.463515 Epoch 150, Training Loss 0.3933945036376529\n",
      "2022-03-26 23:30:41.477898 Epoch 150, Training Loss 0.3940529324438261\n",
      "2022-03-26 23:30:41.492452 Epoch 150, Training Loss 0.39475109734955954\n",
      "2022-03-26 23:30:41.505837 Epoch 150, Training Loss 0.3955503723886617\n",
      "2022-03-26 23:30:41.521251 Epoch 150, Training Loss 0.3962821180329603\n",
      "2022-03-26 23:30:41.535160 Epoch 150, Training Loss 0.39689007531041687\n",
      "2022-03-26 23:30:41.549157 Epoch 150, Training Loss 0.39800584476317286\n",
      "2022-03-26 23:30:41.563160 Epoch 150, Training Loss 0.39891653673728106\n",
      "2022-03-26 23:30:41.578170 Epoch 150, Training Loss 0.3997606802017183\n",
      "2022-03-26 23:30:41.592174 Epoch 150, Training Loss 0.4006119196677147\n",
      "2022-03-26 23:30:41.607171 Epoch 150, Training Loss 0.4014386975246927\n",
      "2022-03-26 23:30:41.621179 Epoch 150, Training Loss 0.4023290379425449\n",
      "2022-03-26 23:30:41.636177 Epoch 150, Training Loss 0.4030847287239016\n",
      "2022-03-26 23:30:41.650180 Epoch 150, Training Loss 0.40409733610384907\n",
      "2022-03-26 23:30:41.665184 Epoch 150, Training Loss 0.40492582222079987\n",
      "2022-03-26 23:30:41.679186 Epoch 150, Training Loss 0.4057487789017465\n",
      "2022-03-26 23:30:41.694190 Epoch 150, Training Loss 0.4066220861871529\n",
      "2022-03-26 23:30:41.708199 Epoch 150, Training Loss 0.40752920394053543\n",
      "2022-03-26 23:30:41.722347 Epoch 150, Training Loss 0.4085555379957799\n",
      "2022-03-26 23:30:41.737336 Epoch 150, Training Loss 0.4093075344324722\n",
      "2022-03-26 23:30:41.751354 Epoch 150, Training Loss 0.4104470151769536\n",
      "2022-03-26 23:30:41.765357 Epoch 150, Training Loss 0.41131337425288034\n",
      "2022-03-26 23:30:41.780346 Epoch 150, Training Loss 0.4121478492646571\n",
      "2022-03-26 23:30:41.794862 Epoch 150, Training Loss 0.4130646635199447\n",
      "2022-03-26 23:30:41.809027 Epoch 150, Training Loss 0.41411574493588693\n",
      "2022-03-26 23:30:41.823024 Epoch 150, Training Loss 0.4149666265453524\n",
      "2022-03-26 23:30:41.837227 Epoch 150, Training Loss 0.41581054203345647\n",
      "2022-03-26 23:30:41.851351 Epoch 150, Training Loss 0.41635816325159636\n",
      "2022-03-26 23:30:41.866745 Epoch 150, Training Loss 0.4176593748733516\n",
      "2022-03-26 23:30:41.881232 Epoch 150, Training Loss 0.41838276245252554\n",
      "2022-03-26 23:30:41.894756 Epoch 150, Training Loss 0.4192304473627559\n",
      "2022-03-26 23:30:41.908920 Epoch 150, Training Loss 0.4200125890390952\n",
      "2022-03-26 23:30:41.923126 Epoch 150, Training Loss 0.4209205037568841\n",
      "2022-03-26 23:30:41.937420 Epoch 150, Training Loss 0.4217608919000382\n",
      "2022-03-26 23:30:41.951863 Epoch 150, Training Loss 0.422668087246168\n",
      "2022-03-26 23:30:41.965866 Epoch 150, Training Loss 0.4232701083354633\n",
      "2022-03-26 23:30:41.979869 Epoch 150, Training Loss 0.42402914116907\n",
      "2022-03-26 23:30:41.993872 Epoch 150, Training Loss 0.4251829077444418\n",
      "2022-03-26 23:30:42.008876 Epoch 150, Training Loss 0.42604113162478524\n",
      "2022-03-26 23:30:42.022879 Epoch 150, Training Loss 0.42698398312491836\n",
      "2022-03-26 23:30:42.037883 Epoch 150, Training Loss 0.427983538764517\n",
      "2022-03-26 23:30:42.051886 Epoch 150, Training Loss 0.4290345274197781\n",
      "2022-03-26 23:30:42.066881 Epoch 150, Training Loss 0.43009996745744933\n",
      "2022-03-26 23:30:42.080885 Epoch 150, Training Loss 0.4309649638583898\n",
      "2022-03-26 23:30:42.095889 Epoch 150, Training Loss 0.43186913697463475\n",
      "2022-03-26 23:30:42.109905 Epoch 150, Training Loss 0.432647791017047\n",
      "2022-03-26 23:30:42.124894 Epoch 150, Training Loss 0.4334245101189065\n",
      "2022-03-26 23:30:42.143080 Epoch 150, Training Loss 0.4343742159245264\n",
      "2022-03-26 23:30:42.161367 Epoch 150, Training Loss 0.4353554233184556\n",
      "2022-03-26 23:30:42.180672 Epoch 150, Training Loss 0.43606642753724245\n",
      "2022-03-26 23:30:42.199388 Epoch 150, Training Loss 0.43701597068773207\n",
      "2022-03-26 23:30:42.219015 Epoch 150, Training Loss 0.4381817065160293\n",
      "2022-03-26 23:30:42.237588 Epoch 150, Training Loss 0.4394178661467779\n",
      "2022-03-26 23:30:42.256983 Epoch 150, Training Loss 0.4402007880951742\n",
      "2022-03-26 23:30:42.276678 Epoch 150, Training Loss 0.44134442294802506\n",
      "2022-03-26 23:30:42.296639 Epoch 150, Training Loss 0.44224284955150334\n",
      "2022-03-26 23:30:42.315322 Epoch 150, Training Loss 0.4431605097811545\n",
      "2022-03-26 23:30:42.334315 Epoch 150, Training Loss 0.44385732961890034\n",
      "2022-03-26 23:30:42.353331 Epoch 150, Training Loss 0.4447649137671951\n",
      "2022-03-26 23:30:42.372549 Epoch 150, Training Loss 0.44548876019542477\n",
      "2022-03-26 23:30:42.391566 Epoch 150, Training Loss 0.44619186268285715\n",
      "2022-03-26 23:30:42.410570 Epoch 150, Training Loss 0.4467658505339147\n",
      "2022-03-26 23:30:42.431556 Epoch 150, Training Loss 0.44764795666917817\n",
      "2022-03-26 23:30:42.450568 Epoch 150, Training Loss 0.44844700266485626\n",
      "2022-03-26 23:30:42.469571 Epoch 150, Training Loss 0.4493399653821955\n",
      "2022-03-26 23:30:42.488569 Epoch 150, Training Loss 0.450375621619127\n",
      "2022-03-26 23:30:42.507592 Epoch 150, Training Loss 0.45145009442820877\n",
      "2022-03-26 23:30:42.527596 Epoch 150, Training Loss 0.4521990990471047\n",
      "2022-03-26 23:30:42.546588 Epoch 150, Training Loss 0.45295895902854405\n",
      "2022-03-26 23:30:42.565586 Epoch 150, Training Loss 0.45364417859812833\n",
      "2022-03-26 23:30:42.584609 Epoch 150, Training Loss 0.4543707138665802\n",
      "2022-03-26 23:30:42.603608 Epoch 150, Training Loss 0.4553441009329408\n",
      "2022-03-26 23:30:42.623559 Epoch 150, Training Loss 0.45620661726235734\n",
      "2022-03-26 23:30:42.642562 Epoch 150, Training Loss 0.45687257172658924\n",
      "2022-03-26 23:30:42.661721 Epoch 150, Training Loss 0.457575548297304\n",
      "2022-03-26 23:30:42.681073 Epoch 150, Training Loss 0.45851694626728895\n",
      "2022-03-26 23:30:42.700822 Epoch 150, Training Loss 0.4593578307601192\n",
      "2022-03-26 23:30:42.719403 Epoch 150, Training Loss 0.4604082532474757\n",
      "2022-03-26 23:30:42.739394 Epoch 150, Training Loss 0.461330286643999\n",
      "2022-03-26 23:30:42.758399 Epoch 150, Training Loss 0.46216116030045484\n",
      "2022-03-26 23:30:42.777403 Epoch 150, Training Loss 0.46312605233296106\n",
      "2022-03-26 23:30:42.796400 Epoch 150, Training Loss 0.46390196768676534\n",
      "2022-03-26 23:30:42.815822 Epoch 150, Training Loss 0.46472149580488425\n",
      "2022-03-26 23:30:42.834814 Epoch 150, Training Loss 0.4654507093188708\n",
      "2022-03-26 23:30:42.853817 Epoch 150, Training Loss 0.4663745124093102\n",
      "2022-03-26 23:30:42.871841 Epoch 150, Training Loss 0.4671799114826695\n",
      "2022-03-26 23:30:42.891062 Epoch 150, Training Loss 0.4678941192605611\n",
      "2022-03-26 23:30:42.911232 Epoch 150, Training Loss 0.46849181009527974\n",
      "2022-03-26 23:30:42.929813 Epoch 150, Training Loss 0.4691604228352037\n",
      "2022-03-26 23:30:42.948464 Epoch 150, Training Loss 0.4700969459531862\n",
      "2022-03-26 23:30:42.967077 Epoch 150, Training Loss 0.4708779062837591\n",
      "2022-03-26 23:30:42.985989 Epoch 150, Training Loss 0.4716767271232727\n",
      "2022-03-26 23:30:43.005762 Epoch 150, Training Loss 0.4727258130793681\n",
      "2022-03-26 23:30:43.024089 Epoch 150, Training Loss 0.47391665366756947\n",
      "2022-03-26 23:30:43.044271 Epoch 150, Training Loss 0.47472291929490124\n",
      "2022-03-26 23:30:43.062192 Epoch 150, Training Loss 0.47559026992686876\n",
      "2022-03-26 23:30:43.081607 Epoch 150, Training Loss 0.47654509891177077\n",
      "2022-03-26 23:30:43.100931 Epoch 150, Training Loss 0.47738377970960133\n",
      "2022-03-26 23:30:43.120083 Epoch 150, Training Loss 0.47809035214774137\n",
      "2022-03-26 23:30:43.141102 Epoch 150, Training Loss 0.47881328964324865\n",
      "2022-03-26 23:30:43.160092 Epoch 150, Training Loss 0.47971021995672486\n",
      "2022-03-26 23:30:43.178434 Epoch 150, Training Loss 0.4807854427782166\n",
      "2022-03-26 23:30:43.198181 Epoch 150, Training Loss 0.48187364546386785\n",
      "2022-03-26 23:30:43.216773 Epoch 150, Training Loss 0.4826631075357232\n",
      "2022-03-26 23:30:43.236336 Epoch 150, Training Loss 0.48331887334051643\n",
      "2022-03-26 23:30:43.255332 Epoch 150, Training Loss 0.4840415774480156\n",
      "2022-03-26 23:30:43.274260 Epoch 150, Training Loss 0.48514536019328913\n",
      "2022-03-26 23:30:43.293737 Epoch 150, Training Loss 0.486055848124387\n",
      "2022-03-26 23:30:43.312481 Epoch 150, Training Loss 0.48672379446608943\n",
      "2022-03-26 23:30:43.331866 Epoch 150, Training Loss 0.4875327878824585\n",
      "2022-03-26 23:30:43.351229 Epoch 150, Training Loss 0.4884195024781215\n",
      "2022-03-26 23:30:43.369897 Epoch 150, Training Loss 0.4892779107365157\n",
      "2022-03-26 23:30:43.389101 Epoch 150, Training Loss 0.49033754214148995\n",
      "2022-03-26 23:30:43.407706 Epoch 150, Training Loss 0.4911555163467022\n",
      "2022-03-26 23:30:43.427328 Epoch 150, Training Loss 0.4919973154125921\n",
      "2022-03-26 23:30:43.446680 Epoch 150, Training Loss 0.49273954880664417\n",
      "2022-03-26 23:30:43.465277 Epoch 150, Training Loss 0.49358909319886163\n",
      "2022-03-26 23:30:43.484121 Epoch 150, Training Loss 0.49451095997677436\n",
      "2022-03-26 23:30:43.503733 Epoch 150, Training Loss 0.4952781966808812\n",
      "2022-03-26 23:30:43.522087 Epoch 150, Training Loss 0.4960444797106716\n",
      "2022-03-26 23:30:43.542454 Epoch 150, Training Loss 0.49697427535453415\n",
      "2022-03-26 23:30:43.561764 Epoch 150, Training Loss 0.497919688711081\n",
      "2022-03-26 23:30:43.580132 Epoch 150, Training Loss 0.49872443076137385\n",
      "2022-03-26 23:30:43.599468 Epoch 150, Training Loss 0.4994593231803011\n",
      "2022-03-26 23:30:43.619057 Epoch 150, Training Loss 0.5006594925051759\n",
      "2022-03-26 23:30:43.638669 Epoch 150, Training Loss 0.5015386040024745\n",
      "2022-03-26 23:30:43.657241 Epoch 150, Training Loss 0.502282552821252\n",
      "2022-03-26 23:30:43.676425 Epoch 150, Training Loss 0.5030156859122884\n",
      "2022-03-26 23:30:43.695429 Epoch 150, Training Loss 0.5038400971142533\n",
      "2022-03-26 23:30:43.715414 Epoch 150, Training Loss 0.5049555460586572\n",
      "2022-03-26 23:30:43.735418 Epoch 150, Training Loss 0.5056792643597668\n",
      "2022-03-26 23:30:43.757423 Epoch 150, Training Loss 0.5065606865660309\n",
      "2022-03-26 23:30:43.776429 Epoch 150, Training Loss 0.5075444122943122\n",
      "2022-03-26 23:30:43.795434 Epoch 150, Training Loss 0.5083244668172143\n",
      "2022-03-26 23:30:43.816437 Epoch 150, Training Loss 0.5091873684426402\n",
      "2022-03-26 23:30:43.834441 Epoch 150, Training Loss 0.5097916867879345\n",
      "2022-03-26 23:30:43.854445 Epoch 150, Training Loss 0.5106894278617771\n",
      "2022-03-26 23:30:43.873457 Epoch 150, Training Loss 0.511826593857592\n",
      "2022-03-26 23:30:43.892462 Epoch 150, Training Loss 0.5127164005013682\n",
      "2022-03-26 23:30:43.911664 Epoch 150, Training Loss 0.5134136465656788\n",
      "2022-03-26 23:30:43.930813 Epoch 150, Training Loss 0.5143299606602515\n",
      "2022-03-26 23:30:43.949810 Epoch 150, Training Loss 0.515365622308858\n",
      "2022-03-26 23:30:43.969394 Epoch 150, Training Loss 0.5160678391109037\n",
      "2022-03-26 23:30:43.988751 Epoch 150, Training Loss 0.5169435839366425\n",
      "2022-03-26 23:30:44.008756 Epoch 150, Training Loss 0.517764310855085\n",
      "2022-03-26 23:30:44.027771 Epoch 150, Training Loss 0.5186059486378184\n",
      "2022-03-26 23:30:44.047044 Epoch 150, Training Loss 0.5193590219978177\n",
      "2022-03-26 23:30:44.066359 Epoch 150, Training Loss 0.5202274974959585\n",
      "2022-03-26 23:30:44.085630 Epoch 150, Training Loss 0.5210746342263868\n",
      "2022-03-26 23:30:44.104766 Epoch 150, Training Loss 0.5218268440812445\n",
      "2022-03-26 23:30:44.123902 Epoch 150, Training Loss 0.522792748706725\n",
      "2022-03-26 23:30:44.142900 Epoch 150, Training Loss 0.5233857771929573\n",
      "2022-03-26 23:30:44.162061 Epoch 150, Training Loss 0.5242549696999133\n",
      "2022-03-26 23:30:44.182065 Epoch 150, Training Loss 0.5247901285548344\n",
      "2022-03-26 23:30:44.201413 Epoch 150, Training Loss 0.5255227582838834\n",
      "2022-03-26 23:30:44.220732 Epoch 150, Training Loss 0.5264298217680753\n",
      "2022-03-26 23:30:44.240992 Epoch 150, Training Loss 0.5273536947529639\n",
      "2022-03-26 23:30:44.259996 Epoch 150, Training Loss 0.5283012887096161\n",
      "2022-03-26 23:30:44.279000 Epoch 150, Training Loss 0.5291541567086564\n",
      "2022-03-26 23:30:44.298374 Epoch 150, Training Loss 0.530133731072516\n",
      "2022-03-26 23:30:44.316775 Epoch 150, Training Loss 0.5310743791063118\n",
      "2022-03-26 23:30:44.336144 Epoch 150, Training Loss 0.5323636890067469\n",
      "2022-03-26 23:30:44.355498 Epoch 150, Training Loss 0.5331678941579121\n",
      "2022-03-26 23:30:44.374843 Epoch 150, Training Loss 0.5339636878131906\n",
      "2022-03-26 23:30:44.394125 Epoch 150, Training Loss 0.534602220520339\n",
      "2022-03-26 23:30:44.413343 Epoch 150, Training Loss 0.5354877092573039\n",
      "2022-03-26 23:30:44.431714 Epoch 150, Training Loss 0.5362103233480697\n",
      "2022-03-26 23:30:44.450998 Epoch 150, Training Loss 0.5369579803074718\n",
      "2022-03-26 23:30:44.469362 Epoch 150, Training Loss 0.5378004212284941\n",
      "2022-03-26 23:30:44.487615 Epoch 150, Training Loss 0.5386719983023451\n",
      "2022-03-26 23:30:44.506935 Epoch 150, Training Loss 0.5394886266011412\n",
      "2022-03-26 23:30:44.527220 Epoch 150, Training Loss 0.5401363612517066\n",
      "2022-03-26 23:30:44.546482 Epoch 150, Training Loss 0.5408611394788908\n",
      "2022-03-26 23:30:44.565487 Epoch 150, Training Loss 0.5415683793061224\n",
      "2022-03-26 23:30:44.584491 Epoch 150, Training Loss 0.5423100033150915\n",
      "2022-03-26 23:30:44.603495 Epoch 150, Training Loss 0.5430224006209532\n",
      "2022-03-26 23:30:44.622500 Epoch 150, Training Loss 0.5437131406324903\n",
      "2022-03-26 23:30:44.641823 Epoch 150, Training Loss 0.5447884725259088\n",
      "2022-03-26 23:30:44.660133 Epoch 150, Training Loss 0.5455614702247292\n",
      "2022-03-26 23:30:44.679410 Epoch 150, Training Loss 0.5462944117729621\n",
      "2022-03-26 23:30:44.698712 Epoch 150, Training Loss 0.5470576619781802\n",
      "2022-03-26 23:30:44.717008 Epoch 150, Training Loss 0.5477251451262428\n",
      "2022-03-26 23:30:44.736291 Epoch 150, Training Loss 0.5490774434164661\n",
      "2022-03-26 23:30:44.754632 Epoch 150, Training Loss 0.5500103864447236\n",
      "2022-03-26 23:30:44.773635 Epoch 150, Training Loss 0.5508520230079246\n",
      "2022-03-26 23:30:44.792639 Epoch 150, Training Loss 0.5517086522734683\n",
      "2022-03-26 23:30:44.812643 Epoch 150, Training Loss 0.5526732014649359\n",
      "2022-03-26 23:30:44.831796 Epoch 150, Training Loss 0.5535488968233928\n",
      "2022-03-26 23:30:44.850874 Epoch 150, Training Loss 0.5542586368444326\n",
      "2022-03-26 23:30:44.869877 Epoch 150, Training Loss 0.5551665582696496\n",
      "2022-03-26 23:30:44.889881 Epoch 150, Training Loss 0.5559631885241365\n",
      "2022-03-26 23:30:44.908885 Epoch 150, Training Loss 0.5569091822256518\n",
      "2022-03-26 23:30:44.927891 Epoch 150, Training Loss 0.5578300545511343\n",
      "2022-03-26 23:30:44.946894 Epoch 150, Training Loss 0.5587483471083214\n",
      "2022-03-26 23:30:44.966898 Epoch 150, Training Loss 0.5595735442989013\n",
      "2022-03-26 23:30:44.984903 Epoch 150, Training Loss 0.5601717597993133\n",
      "2022-03-26 23:30:45.003907 Epoch 150, Training Loss 0.5606021291535833\n",
      "2022-03-26 23:30:45.023913 Epoch 150, Training Loss 0.5615393005292434\n",
      "2022-03-26 23:30:45.043917 Epoch 150, Training Loss 0.5624864047674267\n",
      "2022-03-26 23:30:45.062920 Epoch 150, Training Loss 0.5634691958003641\n",
      "2022-03-26 23:30:45.081926 Epoch 150, Training Loss 0.5646302090657641\n",
      "2022-03-26 23:30:45.100931 Epoch 150, Training Loss 0.5656134540696278\n",
      "2022-03-26 23:30:45.119934 Epoch 150, Training Loss 0.5661593660369248\n",
      "2022-03-26 23:30:45.139938 Epoch 150, Training Loss 0.5668830504960112\n",
      "2022-03-26 23:30:45.159943 Epoch 150, Training Loss 0.5677156888920328\n",
      "2022-03-26 23:30:45.178947 Epoch 150, Training Loss 0.5685389585354749\n",
      "2022-03-26 23:30:45.197953 Epoch 150, Training Loss 0.5694754682386013\n",
      "2022-03-26 23:30:45.216956 Epoch 150, Training Loss 0.5704568436993357\n",
      "2022-03-26 23:30:45.236961 Epoch 150, Training Loss 0.5715763824979972\n",
      "2022-03-26 23:30:45.255966 Epoch 150, Training Loss 0.5725620712922967\n",
      "2022-03-26 23:30:45.275969 Epoch 150, Training Loss 0.5732472606784548\n",
      "2022-03-26 23:30:45.294973 Epoch 150, Training Loss 0.574310526061241\n",
      "2022-03-26 23:30:45.314980 Epoch 150, Training Loss 0.5752027301532229\n",
      "2022-03-26 23:30:45.333982 Epoch 150, Training Loss 0.5759187271375485\n",
      "2022-03-26 23:30:45.353987 Epoch 150, Training Loss 0.5769668110191365\n",
      "2022-03-26 23:30:45.372991 Epoch 150, Training Loss 0.5777861864670463\n",
      "2022-03-26 23:30:45.392995 Epoch 150, Training Loss 0.578612585735443\n",
      "2022-03-26 23:30:45.411999 Epoch 150, Training Loss 0.5796309378750794\n",
      "2022-03-26 23:30:45.433004 Epoch 150, Training Loss 0.580364106484996\n",
      "2022-03-26 23:30:45.452009 Epoch 150, Training Loss 0.5813100119990766\n",
      "2022-03-26 23:30:45.471013 Epoch 150, Training Loss 0.5822081344816691\n",
      "2022-03-26 23:30:45.491017 Epoch 150, Training Loss 0.5827673313867711\n",
      "2022-03-26 23:30:45.511022 Epoch 150, Training Loss 0.5837220339214101\n",
      "2022-03-26 23:30:45.531028 Epoch 150, Training Loss 0.5844804588181284\n",
      "2022-03-26 23:30:45.550031 Epoch 150, Training Loss 0.585441567099003\n",
      "2022-03-26 23:30:45.570035 Epoch 150, Training Loss 0.5863966123984597\n",
      "2022-03-26 23:30:45.589040 Epoch 150, Training Loss 0.587163878218902\n",
      "2022-03-26 23:30:45.608044 Epoch 150, Training Loss 0.5878214379558173\n",
      "2022-03-26 23:30:45.627050 Epoch 150, Training Loss 0.5884842200352408\n",
      "2022-03-26 23:30:45.647055 Epoch 150, Training Loss 0.5891858651052655\n",
      "2022-03-26 23:30:45.666057 Epoch 150, Training Loss 0.5902328276268357\n",
      "2022-03-26 23:30:45.687061 Epoch 150, Training Loss 0.5910365473278953\n",
      "2022-03-26 23:30:45.706067 Epoch 150, Training Loss 0.5921060700550713\n",
      "2022-03-26 23:30:45.726071 Epoch 150, Training Loss 0.5928241791932479\n",
      "2022-03-26 23:30:45.745076 Epoch 150, Training Loss 0.59409126624122\n",
      "2022-03-26 23:30:45.765081 Epoch 150, Training Loss 0.5950142218328803\n",
      "2022-03-26 23:30:45.785084 Epoch 150, Training Loss 0.5957545152558085\n",
      "2022-03-26 23:30:45.803088 Epoch 150, Training Loss 0.5964110616375419\n",
      "2022-03-26 23:30:45.823093 Epoch 150, Training Loss 0.5971058437891323\n",
      "2022-03-26 23:30:45.843097 Epoch 150, Training Loss 0.5982278519121887\n",
      "2022-03-26 23:30:45.862122 Epoch 150, Training Loss 0.5992465973510157\n",
      "2022-03-26 23:30:45.881121 Epoch 150, Training Loss 0.6002637099884355\n",
      "2022-03-26 23:30:45.901118 Epoch 150, Training Loss 0.6013469006246923\n",
      "2022-03-26 23:30:45.920129 Epoch 150, Training Loss 0.6023025780230227\n",
      "2022-03-26 23:30:45.939248 Epoch 150, Training Loss 0.6031760691529344\n",
      "2022-03-26 23:30:45.958842 Epoch 150, Training Loss 0.6036383443323853\n",
      "2022-03-26 23:30:45.978061 Epoch 150, Training Loss 0.6045173687093398\n",
      "2022-03-26 23:30:45.997060 Epoch 150, Training Loss 0.6052575241726683\n",
      "2022-03-26 23:30:46.016240 Epoch 150, Training Loss 0.6063290047828499\n",
      "2022-03-26 23:30:46.036250 Epoch 150, Training Loss 0.6072384033666547\n",
      "2022-03-26 23:30:46.056255 Epoch 150, Training Loss 0.6079460272703634\n",
      "2022-03-26 23:30:46.075345 Epoch 150, Training Loss 0.6086351754110487\n",
      "2022-03-26 23:30:46.094851 Epoch 150, Training Loss 0.6097997148780872\n",
      "2022-03-26 23:30:46.113872 Epoch 150, Training Loss 0.6104134046055777\n",
      "2022-03-26 23:30:46.132870 Epoch 150, Training Loss 0.6113334016116989\n",
      "2022-03-26 23:30:46.152868 Epoch 150, Training Loss 0.6120430170879949\n",
      "2022-03-26 23:30:46.172027 Epoch 150, Training Loss 0.6127566599937351\n",
      "2022-03-26 23:30:46.190958 Epoch 150, Training Loss 0.6134923313889662\n",
      "2022-03-26 23:30:46.209950 Epoch 150, Training Loss 0.6143322029077184\n",
      "2022-03-26 23:30:46.229379 Epoch 150, Training Loss 0.6149919143570658\n",
      "2022-03-26 23:30:46.249465 Epoch 150, Training Loss 0.6159434794922314\n",
      "2022-03-26 23:30:46.268320 Epoch 150, Training Loss 0.6167827313360961\n",
      "2022-03-26 23:30:46.288254 Epoch 150, Training Loss 0.6177008948515138\n",
      "2022-03-26 23:30:46.307602 Epoch 150, Training Loss 0.6182690465160648\n",
      "2022-03-26 23:30:46.326955 Epoch 150, Training Loss 0.6194454213161298\n",
      "2022-03-26 23:30:46.346408 Epoch 150, Training Loss 0.620385252263235\n",
      "2022-03-26 23:30:46.366275 Epoch 150, Training Loss 0.6212395834724617\n",
      "2022-03-26 23:30:46.384746 Epoch 150, Training Loss 0.6219410920310813\n",
      "2022-03-26 23:30:46.410758 Epoch 150, Training Loss 0.622790613480846\n",
      "2022-03-26 23:30:46.437333 Epoch 150, Training Loss 0.6237135738744151\n",
      "2022-03-26 23:30:46.463719 Epoch 150, Training Loss 0.6244601244130707\n",
      "2022-03-26 23:30:46.489500 Epoch 150, Training Loss 0.6252285182247381\n",
      "2022-03-26 23:30:46.516082 Epoch 150, Training Loss 0.6259482940063452\n",
      "2022-03-26 23:30:46.543020 Epoch 150, Training Loss 0.6267844255241897\n",
      "2022-03-26 23:30:46.569402 Epoch 150, Training Loss 0.6277544985113241\n",
      "2022-03-26 23:30:46.595408 Epoch 150, Training Loss 0.6284093165108006\n",
      "2022-03-26 23:30:46.620773 Epoch 150, Training Loss 0.6291354538305945\n",
      "2022-03-26 23:30:46.646236 Epoch 150, Training Loss 0.6298186259577646\n",
      "2022-03-26 23:30:46.673237 Epoch 150, Training Loss 0.6305618916097504\n",
      "2022-03-26 23:30:46.699237 Epoch 150, Training Loss 0.63132250655795\n",
      "2022-03-26 23:30:46.726249 Epoch 150, Training Loss 0.6319321656928343\n",
      "2022-03-26 23:30:46.752261 Epoch 150, Training Loss 0.6330822681069679\n",
      "2022-03-26 23:30:46.778260 Epoch 150, Training Loss 0.6336985627556091\n",
      "2022-03-26 23:30:46.796265 Epoch 150, Training Loss 0.6345318064970129\n",
      "2022-03-26 23:30:46.814275 Epoch 150, Training Loss 0.6354873981469732\n",
      "2022-03-26 23:30:46.832267 Epoch 150, Training Loss 0.6362395919192477\n",
      "2022-03-26 23:30:46.851277 Epoch 150, Training Loss 0.6369913702120866\n",
      "2022-03-26 23:30:46.869287 Epoch 150, Training Loss 0.6376378728300715\n",
      "2022-03-26 23:30:46.887504 Epoch 150, Training Loss 0.6383094075695633\n",
      "2022-03-26 23:30:46.906425 Epoch 150, Training Loss 0.6391094804115003\n",
      "2022-03-26 23:30:46.924228 Epoch 150, Training Loss 0.6398197711276277\n",
      "2022-03-26 23:30:46.942703 Epoch 150, Training Loss 0.6409319771830079\n",
      "2022-03-26 23:30:46.961545 Epoch 150, Training Loss 0.6416760129696878\n",
      "2022-03-26 23:30:46.978920 Epoch 150, Training Loss 0.6425901546197779\n",
      "2022-03-26 23:30:46.997266 Epoch 150, Training Loss 0.6435709049177292\n",
      "2022-03-26 23:30:47.015895 Epoch 150, Training Loss 0.6443463015129499\n",
      "2022-03-26 23:30:47.034574 Epoch 150, Training Loss 0.6449581702880542\n",
      "2022-03-26 23:30:47.051747 Epoch 150, Training Loss 0.6457124838362569\n",
      "2022-03-26 23:30:47.070149 Epoch 150, Training Loss 0.6464949523472725\n",
      "2022-03-26 23:30:47.088538 Epoch 150, Training Loss 0.6473096710870333\n",
      "2022-03-26 23:30:47.107065 Epoch 150, Training Loss 0.6481202665878378\n",
      "2022-03-26 23:30:47.124442 Epoch 150, Training Loss 0.6488592417724907\n",
      "2022-03-26 23:30:47.142940 Epoch 150, Training Loss 0.64982453030546\n",
      "2022-03-26 23:30:47.161296 Epoch 150, Training Loss 0.6507408937910939\n",
      "2022-03-26 23:30:47.169293 Epoch 150, Training Loss 0.6512963375472047\n",
      "2022-03-26 23:42:03.310723 Epoch 200, Training Loss 0.0006778102244257622\n",
      "2022-03-26 23:42:03.336729 Epoch 200, Training Loss 0.001198275467318952\n",
      "2022-03-26 23:42:03.363735 Epoch 200, Training Loss 0.0020145581049077653\n",
      "2022-03-26 23:42:03.388741 Epoch 200, Training Loss 0.002689761350221951\n",
      "2022-03-26 23:42:03.414749 Epoch 200, Training Loss 0.0037614413539466957\n",
      "2022-03-26 23:42:03.441753 Epoch 200, Training Loss 0.004550906779516079\n",
      "2022-03-26 23:42:03.467758 Epoch 200, Training Loss 0.005279861447756248\n",
      "2022-03-26 23:42:03.493767 Epoch 200, Training Loss 0.006117770205373349\n",
      "2022-03-26 23:42:03.519772 Epoch 200, Training Loss 0.006696492281106427\n",
      "2022-03-26 23:42:03.545778 Epoch 200, Training Loss 0.007385997516114998\n",
      "2022-03-26 23:42:03.571803 Epoch 200, Training Loss 0.008154809474945068\n",
      "2022-03-26 23:42:03.597808 Epoch 200, Training Loss 0.008838524903787677\n",
      "2022-03-26 23:42:03.623814 Epoch 200, Training Loss 0.009573944922908188\n",
      "2022-03-26 23:42:03.649820 Epoch 200, Training Loss 0.010406701689790887\n",
      "2022-03-26 23:42:03.675999 Epoch 200, Training Loss 0.011158054137168943\n",
      "2022-03-26 23:42:03.694824 Epoch 200, Training Loss 0.011916127427459677\n",
      "2022-03-26 23:42:03.712828 Epoch 200, Training Loss 0.01288939764737473\n",
      "2022-03-26 23:42:03.730839 Epoch 200, Training Loss 0.013708387494392103\n",
      "2022-03-26 23:42:03.748843 Epoch 200, Training Loss 0.014515607863131082\n",
      "2022-03-26 23:42:03.766841 Epoch 200, Training Loss 0.015398195256357607\n",
      "2022-03-26 23:42:03.784852 Epoch 200, Training Loss 0.016586673153026026\n",
      "2022-03-26 23:42:03.802849 Epoch 200, Training Loss 0.01733947120359182\n",
      "2022-03-26 23:42:03.820853 Epoch 200, Training Loss 0.0180816847802428\n",
      "2022-03-26 23:42:03.839857 Epoch 200, Training Loss 0.01877722860602162\n",
      "2022-03-26 23:42:03.857861 Epoch 200, Training Loss 0.019340762244466014\n",
      "2022-03-26 23:42:03.875871 Epoch 200, Training Loss 0.020117673346453615\n",
      "2022-03-26 23:42:03.893875 Epoch 200, Training Loss 0.020927005487939587\n",
      "2022-03-26 23:42:03.912874 Epoch 200, Training Loss 0.02172225096341594\n",
      "2022-03-26 23:42:03.929878 Epoch 200, Training Loss 0.022532778932615315\n",
      "2022-03-26 23:42:03.948888 Epoch 200, Training Loss 0.023419526836756244\n",
      "2022-03-26 23:42:03.966892 Epoch 200, Training Loss 0.024087824327561555\n",
      "2022-03-26 23:42:03.984896 Epoch 200, Training Loss 0.02499667137785031\n",
      "2022-03-26 23:42:04.002900 Epoch 200, Training Loss 0.025699029798093048\n",
      "2022-03-26 23:42:04.020904 Epoch 200, Training Loss 0.026557184012649615\n",
      "2022-03-26 23:42:04.038908 Epoch 200, Training Loss 0.027323790142298354\n",
      "2022-03-26 23:42:04.056913 Epoch 200, Training Loss 0.028144765692903563\n",
      "2022-03-26 23:42:04.074910 Epoch 200, Training Loss 0.0291342530256647\n",
      "2022-03-26 23:42:04.092914 Epoch 200, Training Loss 0.03004141910301755\n",
      "2022-03-26 23:42:04.110919 Epoch 200, Training Loss 0.030726431199656727\n",
      "2022-03-26 23:42:04.129929 Epoch 200, Training Loss 0.03146410575303275\n",
      "2022-03-26 23:42:04.147933 Epoch 200, Training Loss 0.03223991439775433\n",
      "2022-03-26 23:42:04.166937 Epoch 200, Training Loss 0.03287886852955879\n",
      "2022-03-26 23:42:04.184936 Epoch 200, Training Loss 0.033605059280115014\n",
      "2022-03-26 23:42:04.202927 Epoch 200, Training Loss 0.03439813406418656\n",
      "2022-03-26 23:42:04.220950 Epoch 200, Training Loss 0.03519519306052371\n",
      "2022-03-26 23:42:04.239950 Epoch 200, Training Loss 0.035925137226843776\n",
      "2022-03-26 23:42:04.257954 Epoch 200, Training Loss 0.03660786048987942\n",
      "2022-03-26 23:42:04.275958 Epoch 200, Training Loss 0.037379139334039614\n",
      "2022-03-26 23:42:04.293962 Epoch 200, Training Loss 0.03787903220909636\n",
      "2022-03-26 23:42:04.311965 Epoch 200, Training Loss 0.03843271496045925\n",
      "2022-03-26 23:42:04.329970 Epoch 200, Training Loss 0.038981559522011704\n",
      "2022-03-26 23:42:04.348973 Epoch 200, Training Loss 0.039784533860128556\n",
      "2022-03-26 23:42:04.366971 Epoch 200, Training Loss 0.0404660449842053\n",
      "2022-03-26 23:42:04.384983 Epoch 200, Training Loss 0.0412918957679168\n",
      "2022-03-26 23:42:04.402994 Epoch 200, Training Loss 0.04193703083278578\n",
      "2022-03-26 23:42:04.420991 Epoch 200, Training Loss 0.0425850712048733\n",
      "2022-03-26 23:42:04.439988 Epoch 200, Training Loss 0.043398794501333895\n",
      "2022-03-26 23:42:04.457999 Epoch 200, Training Loss 0.04440970802703477\n",
      "2022-03-26 23:42:04.475991 Epoch 200, Training Loss 0.04502340598636881\n",
      "2022-03-26 23:42:04.495006 Epoch 200, Training Loss 0.04552065598232972\n",
      "2022-03-26 23:42:04.513012 Epoch 200, Training Loss 0.0463110238618558\n",
      "2022-03-26 23:42:04.531016 Epoch 200, Training Loss 0.0471131239477021\n",
      "2022-03-26 23:42:04.548006 Epoch 200, Training Loss 0.04777778776557854\n",
      "2022-03-26 23:42:04.567019 Epoch 200, Training Loss 0.04845131865090421\n",
      "2022-03-26 23:42:04.585029 Epoch 200, Training Loss 0.04901585043848628\n",
      "2022-03-26 23:42:04.603030 Epoch 200, Training Loss 0.04985039808866008\n",
      "2022-03-26 23:42:04.621037 Epoch 200, Training Loss 0.0506710307220059\n",
      "2022-03-26 23:42:04.639034 Epoch 200, Training Loss 0.051427673881925894\n",
      "2022-03-26 23:42:04.656042 Epoch 200, Training Loss 0.05203388913360703\n",
      "2022-03-26 23:42:04.675049 Epoch 200, Training Loss 0.05266734793820345\n",
      "2022-03-26 23:42:04.693053 Epoch 200, Training Loss 0.05377762454092655\n",
      "2022-03-26 23:42:04.711057 Epoch 200, Training Loss 0.05474292412590798\n",
      "2022-03-26 23:42:04.729055 Epoch 200, Training Loss 0.055260827969712066\n",
      "2022-03-26 23:42:04.747063 Epoch 200, Training Loss 0.05583719429000259\n",
      "2022-03-26 23:42:04.765057 Epoch 200, Training Loss 0.05683471605448467\n",
      "2022-03-26 23:42:04.783061 Epoch 200, Training Loss 0.05740020978633705\n",
      "2022-03-26 23:42:04.802075 Epoch 200, Training Loss 0.05818398868488839\n",
      "2022-03-26 23:42:04.820081 Epoch 200, Training Loss 0.05888635007774129\n",
      "2022-03-26 23:42:04.838089 Epoch 200, Training Loss 0.05976631677211703\n",
      "2022-03-26 23:42:04.856095 Epoch 200, Training Loss 0.060361727195627546\n",
      "2022-03-26 23:42:04.874096 Epoch 200, Training Loss 0.06099811333524602\n",
      "2022-03-26 23:42:04.892103 Epoch 200, Training Loss 0.06152574413115411\n",
      "2022-03-26 23:42:04.910106 Epoch 200, Training Loss 0.062155011936526776\n",
      "2022-03-26 23:42:04.929110 Epoch 200, Training Loss 0.06280215107418997\n",
      "2022-03-26 23:42:04.946108 Epoch 200, Training Loss 0.06346065587247424\n",
      "2022-03-26 23:42:04.964118 Epoch 200, Training Loss 0.06411559422455175\n",
      "2022-03-26 23:42:04.983117 Epoch 200, Training Loss 0.06498419960289051\n",
      "2022-03-26 23:42:05.001114 Epoch 200, Training Loss 0.06574140344281941\n",
      "2022-03-26 23:42:05.019125 Epoch 200, Training Loss 0.06660209733354466\n",
      "2022-03-26 23:42:05.038135 Epoch 200, Training Loss 0.06723997233163975\n",
      "2022-03-26 23:42:05.056139 Epoch 200, Training Loss 0.06773564619633853\n",
      "2022-03-26 23:42:05.074137 Epoch 200, Training Loss 0.068365346745152\n",
      "2022-03-26 23:42:05.091141 Epoch 200, Training Loss 0.06918483187475473\n",
      "2022-03-26 23:42:05.110139 Epoch 200, Training Loss 0.0700361525158748\n",
      "2022-03-26 23:42:05.128149 Epoch 200, Training Loss 0.07105346454683777\n",
      "2022-03-26 23:42:05.146153 Epoch 200, Training Loss 0.07158917485905425\n",
      "2022-03-26 23:42:05.164163 Epoch 200, Training Loss 0.07257264989721196\n",
      "2022-03-26 23:42:05.183168 Epoch 200, Training Loss 0.0734685674652724\n",
      "2022-03-26 23:42:05.201172 Epoch 200, Training Loss 0.0740434346372819\n",
      "2022-03-26 23:42:05.220176 Epoch 200, Training Loss 0.0747614956039297\n",
      "2022-03-26 23:42:05.238174 Epoch 200, Training Loss 0.07567165304175423\n",
      "2022-03-26 23:42:05.256184 Epoch 200, Training Loss 0.0764413159292982\n",
      "2022-03-26 23:42:05.274188 Epoch 200, Training Loss 0.07721853008508073\n",
      "2022-03-26 23:42:05.292363 Epoch 200, Training Loss 0.07803582127594277\n",
      "2022-03-26 23:42:05.310197 Epoch 200, Training Loss 0.07877907351307246\n",
      "2022-03-26 23:42:05.328201 Epoch 200, Training Loss 0.07948490058826974\n",
      "2022-03-26 23:42:05.346199 Epoch 200, Training Loss 0.08044745218570885\n",
      "2022-03-26 23:42:05.364209 Epoch 200, Training Loss 0.08154761680709127\n",
      "2022-03-26 23:42:05.383213 Epoch 200, Training Loss 0.08224612851734356\n",
      "2022-03-26 23:42:05.402217 Epoch 200, Training Loss 0.08336068987084166\n",
      "2022-03-26 23:42:05.420222 Epoch 200, Training Loss 0.08415001505018806\n",
      "2022-03-26 23:42:05.438226 Epoch 200, Training Loss 0.08495640667045817\n",
      "2022-03-26 23:42:05.457224 Epoch 200, Training Loss 0.08567249534837425\n",
      "2022-03-26 23:42:05.475228 Epoch 200, Training Loss 0.08637987176323181\n",
      "2022-03-26 23:42:05.493225 Epoch 200, Training Loss 0.08707498715204351\n",
      "2022-03-26 23:42:05.511236 Epoch 200, Training Loss 0.08771069755639567\n",
      "2022-03-26 23:42:05.529240 Epoch 200, Training Loss 0.0883521041296937\n",
      "2022-03-26 23:42:05.547244 Epoch 200, Training Loss 0.08935997797095258\n",
      "2022-03-26 23:42:05.566255 Epoch 200, Training Loss 0.08991098331520929\n",
      "2022-03-26 23:42:05.584253 Epoch 200, Training Loss 0.09049301131454575\n",
      "2022-03-26 23:42:05.603257 Epoch 200, Training Loss 0.09120233052069573\n",
      "2022-03-26 23:42:05.621261 Epoch 200, Training Loss 0.09219819242539613\n",
      "2022-03-26 23:42:05.640271 Epoch 200, Training Loss 0.09278467499539066\n",
      "2022-03-26 23:42:05.658275 Epoch 200, Training Loss 0.09362028150454811\n",
      "2022-03-26 23:42:05.677274 Epoch 200, Training Loss 0.09442690125359293\n",
      "2022-03-26 23:42:05.695278 Epoch 200, Training Loss 0.09546870218061121\n",
      "2022-03-26 23:42:05.713282 Epoch 200, Training Loss 0.09605069603304119\n",
      "2022-03-26 23:42:05.731286 Epoch 200, Training Loss 0.09704531706355111\n",
      "2022-03-26 23:42:05.750296 Epoch 200, Training Loss 0.09766432966874994\n",
      "2022-03-26 23:42:05.768294 Epoch 200, Training Loss 0.09826465758978559\n",
      "2022-03-26 23:42:05.787305 Epoch 200, Training Loss 0.09891051034945661\n",
      "2022-03-26 23:42:05.806309 Epoch 200, Training Loss 0.09977323445670135\n",
      "2022-03-26 23:42:05.824313 Epoch 200, Training Loss 0.10068955975572777\n",
      "2022-03-26 23:42:05.843317 Epoch 200, Training Loss 0.10122971450123945\n",
      "2022-03-26 23:42:05.861315 Epoch 200, Training Loss 0.10207421963324632\n",
      "2022-03-26 23:42:05.880326 Epoch 200, Training Loss 0.10271342685612876\n",
      "2022-03-26 23:42:05.898330 Epoch 200, Training Loss 0.10347207614680386\n",
      "2022-03-26 23:42:05.916334 Epoch 200, Training Loss 0.10428512145948532\n",
      "2022-03-26 23:42:05.934338 Epoch 200, Training Loss 0.10515466260025873\n",
      "2022-03-26 23:42:05.952342 Epoch 200, Training Loss 0.10599088626901818\n",
      "2022-03-26 23:42:05.970346 Epoch 200, Training Loss 0.10691562222550287\n",
      "2022-03-26 23:42:05.988344 Epoch 200, Training Loss 0.1077307942502029\n",
      "2022-03-26 23:42:06.007355 Epoch 200, Training Loss 0.10855829483255401\n",
      "2022-03-26 23:42:06.026359 Epoch 200, Training Loss 0.10909289822858922\n",
      "2022-03-26 23:42:06.045357 Epoch 200, Training Loss 0.10985332514014086\n",
      "2022-03-26 23:42:06.063367 Epoch 200, Training Loss 0.11062535437781487\n",
      "2022-03-26 23:42:06.081371 Epoch 200, Training Loss 0.11132169981746723\n",
      "2022-03-26 23:42:06.100375 Epoch 200, Training Loss 0.11216232439746028\n",
      "2022-03-26 23:42:06.118367 Epoch 200, Training Loss 0.11289507348824035\n",
      "2022-03-26 23:42:06.136378 Epoch 200, Training Loss 0.11376343076796178\n",
      "2022-03-26 23:42:06.155388 Epoch 200, Training Loss 0.11472420520184899\n",
      "2022-03-26 23:42:06.173393 Epoch 200, Training Loss 0.11532173509640462\n",
      "2022-03-26 23:42:06.192397 Epoch 200, Training Loss 0.11606465805979337\n",
      "2022-03-26 23:42:06.211395 Epoch 200, Training Loss 0.1168493242443675\n",
      "2022-03-26 23:42:06.229399 Epoch 200, Training Loss 0.11784705851236572\n",
      "2022-03-26 23:42:06.247403 Epoch 200, Training Loss 0.11874976392139865\n",
      "2022-03-26 23:42:06.265407 Epoch 200, Training Loss 0.11936205671266521\n",
      "2022-03-26 23:42:06.284417 Epoch 200, Training Loss 0.12017647704809828\n",
      "2022-03-26 23:42:06.302421 Epoch 200, Training Loss 0.12110878050784625\n",
      "2022-03-26 23:42:06.320420 Epoch 200, Training Loss 0.12178371278831111\n",
      "2022-03-26 23:42:06.344414 Epoch 200, Training Loss 0.12241937658366035\n",
      "2022-03-26 23:42:06.369421 Epoch 200, Training Loss 0.12317562156626026\n",
      "2022-03-26 23:42:06.390422 Epoch 200, Training Loss 0.12392273918746988\n",
      "2022-03-26 23:42:06.409427 Epoch 200, Training Loss 0.12461714282669985\n",
      "2022-03-26 23:42:06.428431 Epoch 200, Training Loss 0.12508476375009092\n",
      "2022-03-26 23:42:06.446434 Epoch 200, Training Loss 0.12597030416474014\n",
      "2022-03-26 23:42:06.466454 Epoch 200, Training Loss 0.12684822646553254\n",
      "2022-03-26 23:42:06.488464 Epoch 200, Training Loss 0.12772910407437083\n",
      "2022-03-26 23:42:06.506462 Epoch 200, Training Loss 0.128666711280413\n",
      "2022-03-26 23:42:06.532474 Epoch 200, Training Loss 0.12938273196939923\n",
      "2022-03-26 23:42:06.558480 Epoch 200, Training Loss 0.13020962080382326\n",
      "2022-03-26 23:42:06.584485 Epoch 200, Training Loss 0.13087450390886468\n",
      "2022-03-26 23:42:06.610480 Epoch 200, Training Loss 0.13177577911130609\n",
      "2022-03-26 23:42:06.636497 Epoch 200, Training Loss 0.13294222280192558\n",
      "2022-03-26 23:42:06.662503 Epoch 200, Training Loss 0.1338728337031801\n",
      "2022-03-26 23:42:06.689497 Epoch 200, Training Loss 0.1345492115868327\n",
      "2022-03-26 23:42:06.714515 Epoch 200, Training Loss 0.1352711517335204\n",
      "2022-03-26 23:42:06.728506 Epoch 200, Training Loss 0.1359263087629967\n",
      "2022-03-26 23:42:06.742510 Epoch 200, Training Loss 0.1367697928415235\n",
      "2022-03-26 23:42:06.757512 Epoch 200, Training Loss 0.13775508864151548\n",
      "2022-03-26 23:42:06.771522 Epoch 200, Training Loss 0.13846879229521203\n",
      "2022-03-26 23:42:06.785519 Epoch 200, Training Loss 0.13966006596984765\n",
      "2022-03-26 23:42:06.799522 Epoch 200, Training Loss 0.14050037392874812\n",
      "2022-03-26 23:42:06.814526 Epoch 200, Training Loss 0.14126957994897651\n",
      "2022-03-26 23:42:06.828529 Epoch 200, Training Loss 0.1419502681173632\n",
      "2022-03-26 23:42:06.842532 Epoch 200, Training Loss 0.14255401377787674\n",
      "2022-03-26 23:42:06.856535 Epoch 200, Training Loss 0.14312374679481282\n",
      "2022-03-26 23:42:06.871538 Epoch 200, Training Loss 0.14381221855235526\n",
      "2022-03-26 23:42:06.885541 Epoch 200, Training Loss 0.14476940169206362\n",
      "2022-03-26 23:42:06.899545 Epoch 200, Training Loss 0.14551656778968508\n",
      "2022-03-26 23:42:06.913548 Epoch 200, Training Loss 0.14621817596885553\n",
      "2022-03-26 23:42:06.928558 Epoch 200, Training Loss 0.147227301736317\n",
      "2022-03-26 23:42:06.942554 Epoch 200, Training Loss 0.1481332890594097\n",
      "2022-03-26 23:42:06.956557 Epoch 200, Training Loss 0.148827629359177\n",
      "2022-03-26 23:42:06.970560 Epoch 200, Training Loss 0.1495562765528174\n",
      "2022-03-26 23:42:06.984564 Epoch 200, Training Loss 0.15021560141040236\n",
      "2022-03-26 23:42:06.998567 Epoch 200, Training Loss 0.15089492964775056\n",
      "2022-03-26 23:42:07.013571 Epoch 200, Training Loss 0.1518257943641804\n",
      "2022-03-26 23:42:07.027573 Epoch 200, Training Loss 0.15248699913091976\n",
      "2022-03-26 23:42:07.041577 Epoch 200, Training Loss 0.15325517556094148\n",
      "2022-03-26 23:42:07.055580 Epoch 200, Training Loss 0.15371606679981017\n",
      "2022-03-26 23:42:07.070583 Epoch 200, Training Loss 0.1546951259112419\n",
      "2022-03-26 23:42:07.084586 Epoch 200, Training Loss 0.15552157411337508\n",
      "2022-03-26 23:42:07.099590 Epoch 200, Training Loss 0.15612206968200176\n",
      "2022-03-26 23:42:07.113593 Epoch 200, Training Loss 0.1569518590217356\n",
      "2022-03-26 23:42:07.127596 Epoch 200, Training Loss 0.1576956591337843\n",
      "2022-03-26 23:42:07.141599 Epoch 200, Training Loss 0.15831352412090888\n",
      "2022-03-26 23:42:07.155602 Epoch 200, Training Loss 0.1589506199140378\n",
      "2022-03-26 23:42:07.169606 Epoch 200, Training Loss 0.15982613958361203\n",
      "2022-03-26 23:42:07.183609 Epoch 200, Training Loss 0.16064305531094447\n",
      "2022-03-26 23:42:07.197612 Epoch 200, Training Loss 0.16116566982720515\n",
      "2022-03-26 23:42:07.211615 Epoch 200, Training Loss 0.16179773020927254\n",
      "2022-03-26 23:42:07.225619 Epoch 200, Training Loss 0.16256874311915445\n",
      "2022-03-26 23:42:07.240622 Epoch 200, Training Loss 0.16335700951573795\n",
      "2022-03-26 23:42:07.254626 Epoch 200, Training Loss 0.16403131084064085\n",
      "2022-03-26 23:42:07.269629 Epoch 200, Training Loss 0.16481647063094332\n",
      "2022-03-26 23:42:07.284632 Epoch 200, Training Loss 0.16561750141556\n",
      "2022-03-26 23:42:07.298635 Epoch 200, Training Loss 0.16655591656179988\n",
      "2022-03-26 23:42:07.313639 Epoch 200, Training Loss 0.16734363698898375\n",
      "2022-03-26 23:42:07.327648 Epoch 200, Training Loss 0.16813634377916145\n",
      "2022-03-26 23:42:07.341651 Epoch 200, Training Loss 0.16899697410176173\n",
      "2022-03-26 23:42:07.355649 Epoch 200, Training Loss 0.16964417238674506\n",
      "2022-03-26 23:42:07.369651 Epoch 200, Training Loss 0.17031863347038895\n",
      "2022-03-26 23:42:07.384657 Epoch 200, Training Loss 0.17101293078164007\n",
      "2022-03-26 23:42:07.398658 Epoch 200, Training Loss 0.1717550073133405\n",
      "2022-03-26 23:42:07.413661 Epoch 200, Training Loss 0.17234790508094652\n",
      "2022-03-26 23:42:07.427664 Epoch 200, Training Loss 0.17312400206885376\n",
      "2022-03-26 23:42:07.441674 Epoch 200, Training Loss 0.17388366044634748\n",
      "2022-03-26 23:42:07.455671 Epoch 200, Training Loss 0.1748216429634777\n",
      "2022-03-26 23:42:07.469674 Epoch 200, Training Loss 0.17554989586705746\n",
      "2022-03-26 23:42:07.484677 Epoch 200, Training Loss 0.17619823586300512\n",
      "2022-03-26 23:42:07.499680 Epoch 200, Training Loss 0.1768427257952483\n",
      "2022-03-26 23:42:07.513684 Epoch 200, Training Loss 0.17770007222204867\n",
      "2022-03-26 23:42:07.527687 Epoch 200, Training Loss 0.17841252783680206\n",
      "2022-03-26 23:42:07.541690 Epoch 200, Training Loss 0.17906884326959205\n",
      "2022-03-26 23:42:07.555693 Epoch 200, Training Loss 0.17963812082929684\n",
      "2022-03-26 23:42:07.569696 Epoch 200, Training Loss 0.18042047607624317\n",
      "2022-03-26 23:42:07.584700 Epoch 200, Training Loss 0.18139369393248692\n",
      "2022-03-26 23:42:07.598703 Epoch 200, Training Loss 0.18215045058513846\n",
      "2022-03-26 23:42:07.612706 Epoch 200, Training Loss 0.18300033553177134\n",
      "2022-03-26 23:42:07.626709 Epoch 200, Training Loss 0.18381983293291856\n",
      "2022-03-26 23:42:07.640713 Epoch 200, Training Loss 0.1844279224153065\n",
      "2022-03-26 23:42:07.654716 Epoch 200, Training Loss 0.18550384562948477\n",
      "2022-03-26 23:42:07.668719 Epoch 200, Training Loss 0.1862967100442218\n",
      "2022-03-26 23:42:07.683723 Epoch 200, Training Loss 0.18690642211443323\n",
      "2022-03-26 23:42:07.697725 Epoch 200, Training Loss 0.18769396631918905\n",
      "2022-03-26 23:42:07.711729 Epoch 200, Training Loss 0.1885015145134743\n",
      "2022-03-26 23:42:07.725732 Epoch 200, Training Loss 0.1890619214614639\n",
      "2022-03-26 23:42:07.739735 Epoch 200, Training Loss 0.18974839013708217\n",
      "2022-03-26 23:42:07.753738 Epoch 200, Training Loss 0.19097342953809995\n",
      "2022-03-26 23:42:07.767742 Epoch 200, Training Loss 0.19163568782836884\n",
      "2022-03-26 23:42:07.782745 Epoch 200, Training Loss 0.19219953237134782\n",
      "2022-03-26 23:42:07.796748 Epoch 200, Training Loss 0.1929341192974154\n",
      "2022-03-26 23:42:07.810758 Epoch 200, Training Loss 0.19357981923443582\n",
      "2022-03-26 23:42:07.824754 Epoch 200, Training Loss 0.19434907693234857\n",
      "2022-03-26 23:42:07.838757 Epoch 200, Training Loss 0.1949954840457043\n",
      "2022-03-26 23:42:07.852760 Epoch 200, Training Loss 0.19588838593886637\n",
      "2022-03-26 23:42:07.866764 Epoch 200, Training Loss 0.1966428708313676\n",
      "2022-03-26 23:42:07.881768 Epoch 200, Training Loss 0.19729966416840664\n",
      "2022-03-26 23:42:07.895770 Epoch 200, Training Loss 0.1980077849553369\n",
      "2022-03-26 23:42:07.910781 Epoch 200, Training Loss 0.19856274341378372\n",
      "2022-03-26 23:42:07.924779 Epoch 200, Training Loss 0.19941786853858576\n",
      "2022-03-26 23:42:07.938780 Epoch 200, Training Loss 0.20008391949831678\n",
      "2022-03-26 23:42:07.952785 Epoch 200, Training Loss 0.20075858180480236\n",
      "2022-03-26 23:42:07.966786 Epoch 200, Training Loss 0.20133556154987697\n",
      "2022-03-26 23:42:07.981790 Epoch 200, Training Loss 0.20214160186860264\n",
      "2022-03-26 23:42:07.995793 Epoch 200, Training Loss 0.2028207057882148\n",
      "2022-03-26 23:42:08.009798 Epoch 200, Training Loss 0.2036001282122434\n",
      "2022-03-26 23:42:08.023799 Epoch 200, Training Loss 0.20421457603154586\n",
      "2022-03-26 23:42:08.037803 Epoch 200, Training Loss 0.204965000872112\n",
      "2022-03-26 23:42:08.052806 Epoch 200, Training Loss 0.20567769216149664\n",
      "2022-03-26 23:42:08.066810 Epoch 200, Training Loss 0.20664636344860887\n",
      "2022-03-26 23:42:08.081813 Epoch 200, Training Loss 0.20753908713760277\n",
      "2022-03-26 23:42:08.095817 Epoch 200, Training Loss 0.2079884510134797\n",
      "2022-03-26 23:42:08.109821 Epoch 200, Training Loss 0.20886165311421884\n",
      "2022-03-26 23:42:08.124822 Epoch 200, Training Loss 0.20978497769064305\n",
      "2022-03-26 23:42:08.138825 Epoch 200, Training Loss 0.21049465124716843\n",
      "2022-03-26 23:42:08.152830 Epoch 200, Training Loss 0.21132468998127277\n",
      "2022-03-26 23:42:08.166832 Epoch 200, Training Loss 0.21194569991372736\n",
      "2022-03-26 23:42:08.180835 Epoch 200, Training Loss 0.21298096948267553\n",
      "2022-03-26 23:42:08.194838 Epoch 200, Training Loss 0.21363758949367592\n",
      "2022-03-26 23:42:08.208842 Epoch 200, Training Loss 0.21445937862481607\n",
      "2022-03-26 23:42:08.222844 Epoch 200, Training Loss 0.21505734107226057\n",
      "2022-03-26 23:42:08.236848 Epoch 200, Training Loss 0.2156181919300343\n",
      "2022-03-26 23:42:08.251851 Epoch 200, Training Loss 0.21623358759276398\n",
      "2022-03-26 23:42:08.265854 Epoch 200, Training Loss 0.217055419483758\n",
      "2022-03-26 23:42:08.279857 Epoch 200, Training Loss 0.21774611841229832\n",
      "2022-03-26 23:42:08.293861 Epoch 200, Training Loss 0.21860547828704804\n",
      "2022-03-26 23:42:08.307865 Epoch 200, Training Loss 0.2193750298922629\n",
      "2022-03-26 23:42:08.322868 Epoch 200, Training Loss 0.2201193076036775\n",
      "2022-03-26 23:42:08.336871 Epoch 200, Training Loss 0.22086353470449863\n",
      "2022-03-26 23:42:08.351874 Epoch 200, Training Loss 0.22151610685888765\n",
      "2022-03-26 23:42:08.364877 Epoch 200, Training Loss 0.22262767258355076\n",
      "2022-03-26 23:42:08.379880 Epoch 200, Training Loss 0.22321763253577834\n",
      "2022-03-26 23:42:08.392883 Epoch 200, Training Loss 0.2237724549020343\n",
      "2022-03-26 23:42:08.407886 Epoch 200, Training Loss 0.22430135408783203\n",
      "2022-03-26 23:42:08.422890 Epoch 200, Training Loss 0.2250396336054863\n",
      "2022-03-26 23:42:08.436893 Epoch 200, Training Loss 0.225778208935962\n",
      "2022-03-26 23:42:08.450897 Epoch 200, Training Loss 0.2265938613801966\n",
      "2022-03-26 23:42:08.465900 Epoch 200, Training Loss 0.2271759052715643\n",
      "2022-03-26 23:42:08.480909 Epoch 200, Training Loss 0.2278515772746347\n",
      "2022-03-26 23:42:08.494906 Epoch 200, Training Loss 0.22864439115499902\n",
      "2022-03-26 23:42:08.508910 Epoch 200, Training Loss 0.2292934118786736\n",
      "2022-03-26 23:42:08.526920 Epoch 200, Training Loss 0.2299716625067279\n",
      "2022-03-26 23:42:08.545924 Epoch 200, Training Loss 0.23090283347822516\n",
      "2022-03-26 23:42:08.564934 Epoch 200, Training Loss 0.2317157100381144\n",
      "2022-03-26 23:42:08.583939 Epoch 200, Training Loss 0.23269205744309193\n",
      "2022-03-26 23:42:08.602943 Epoch 200, Training Loss 0.23330574496018003\n",
      "2022-03-26 23:42:08.621947 Epoch 200, Training Loss 0.2338924440733917\n",
      "2022-03-26 23:42:08.640945 Epoch 200, Training Loss 0.23493209953808114\n",
      "2022-03-26 23:42:08.659956 Epoch 200, Training Loss 0.23559735734444445\n",
      "2022-03-26 23:42:08.678960 Epoch 200, Training Loss 0.23659827245775697\n",
      "2022-03-26 23:42:08.697964 Epoch 200, Training Loss 0.23706893664796638\n",
      "2022-03-26 23:42:08.716969 Epoch 200, Training Loss 0.23780616233720803\n",
      "2022-03-26 23:42:08.735961 Epoch 200, Training Loss 0.23842530055424135\n",
      "2022-03-26 23:42:08.754977 Epoch 200, Training Loss 0.2391122296033308\n",
      "2022-03-26 23:42:08.773976 Epoch 200, Training Loss 0.23986145023189848\n",
      "2022-03-26 23:42:08.793980 Epoch 200, Training Loss 0.24103064320581344\n",
      "2022-03-26 23:42:08.812991 Epoch 200, Training Loss 0.2418374557934149\n",
      "2022-03-26 23:42:08.831989 Epoch 200, Training Loss 0.24254592696724037\n",
      "2022-03-26 23:42:08.849999 Epoch 200, Training Loss 0.24320339775451308\n",
      "2022-03-26 23:42:08.869003 Epoch 200, Training Loss 0.24366166642712206\n",
      "2022-03-26 23:42:08.890008 Epoch 200, Training Loss 0.24437073143699284\n",
      "2022-03-26 23:42:08.909012 Epoch 200, Training Loss 0.24531669731792585\n",
      "2022-03-26 23:42:08.928011 Epoch 200, Training Loss 0.24591797426381073\n",
      "2022-03-26 23:42:08.947021 Epoch 200, Training Loss 0.2466486692047485\n",
      "2022-03-26 23:42:08.966025 Epoch 200, Training Loss 0.24724457525383786\n",
      "2022-03-26 23:42:08.985030 Epoch 200, Training Loss 0.24808549480822384\n",
      "2022-03-26 23:42:09.004034 Epoch 200, Training Loss 0.24863017108434302\n",
      "2022-03-26 23:42:09.023038 Epoch 200, Training Loss 0.24979278056517892\n",
      "2022-03-26 23:42:09.042043 Epoch 200, Training Loss 0.2505546533848014\n",
      "2022-03-26 23:42:09.061047 Epoch 200, Training Loss 0.2512142760369479\n",
      "2022-03-26 23:42:09.080051 Epoch 200, Training Loss 0.2518809151161662\n",
      "2022-03-26 23:42:09.100055 Epoch 200, Training Loss 0.25256950829340064\n",
      "2022-03-26 23:42:09.119060 Epoch 200, Training Loss 0.2535817273285078\n",
      "2022-03-26 23:42:09.138064 Epoch 200, Training Loss 0.25450816247469327\n",
      "2022-03-26 23:42:09.157069 Epoch 200, Training Loss 0.25527389374230525\n",
      "2022-03-26 23:42:09.177061 Epoch 200, Training Loss 0.2559651100574552\n",
      "2022-03-26 23:42:09.196071 Epoch 200, Training Loss 0.25662114370204603\n",
      "2022-03-26 23:42:09.214082 Epoch 200, Training Loss 0.2575583103520181\n",
      "2022-03-26 23:42:09.233086 Epoch 200, Training Loss 0.2585467289933158\n",
      "2022-03-26 23:42:09.252090 Epoch 200, Training Loss 0.25944167634715204\n",
      "2022-03-26 23:42:09.271095 Epoch 200, Training Loss 0.26018571937480545\n",
      "2022-03-26 23:42:09.291099 Epoch 200, Training Loss 0.260717277598503\n",
      "2022-03-26 23:42:09.310098 Epoch 200, Training Loss 0.2615745774544109\n",
      "2022-03-26 23:42:09.329108 Epoch 200, Training Loss 0.26220204526811\n",
      "2022-03-26 23:42:09.348106 Epoch 200, Training Loss 0.26303230199362615\n",
      "2022-03-26 23:42:09.367116 Epoch 200, Training Loss 0.2640004488055968\n",
      "2022-03-26 23:42:09.387115 Epoch 200, Training Loss 0.2648456682786917\n",
      "2022-03-26 23:42:09.405125 Epoch 200, Training Loss 0.26575841234468134\n",
      "2022-03-26 23:42:09.424129 Epoch 200, Training Loss 0.2667392192746672\n",
      "2022-03-26 23:42:09.444128 Epoch 200, Training Loss 0.2672057181520535\n",
      "2022-03-26 23:42:09.463138 Epoch 200, Training Loss 0.267916193474894\n",
      "2022-03-26 23:42:09.482142 Epoch 200, Training Loss 0.2687250217208472\n",
      "2022-03-26 23:42:09.501134 Epoch 200, Training Loss 0.2698423725259883\n",
      "2022-03-26 23:42:09.521151 Epoch 200, Training Loss 0.2707992615297322\n",
      "2022-03-26 23:42:09.540155 Epoch 200, Training Loss 0.2715369171041357\n",
      "2022-03-26 23:42:09.559160 Epoch 200, Training Loss 0.27213275219168503\n",
      "2022-03-26 23:42:09.578158 Epoch 200, Training Loss 0.2730338944650977\n",
      "2022-03-26 23:42:09.597168 Epoch 200, Training Loss 0.2737680271915767\n",
      "2022-03-26 23:42:09.616173 Epoch 200, Training Loss 0.2745788628640382\n",
      "2022-03-26 23:42:09.635177 Epoch 200, Training Loss 0.2753764769000471\n",
      "2022-03-26 23:42:09.654181 Epoch 200, Training Loss 0.275883518407107\n",
      "2022-03-26 23:42:09.674186 Epoch 200, Training Loss 0.2766234099178973\n",
      "2022-03-26 23:42:09.693184 Epoch 200, Training Loss 0.27761606125118177\n",
      "2022-03-26 23:42:09.711194 Epoch 200, Training Loss 0.2785999106095575\n",
      "2022-03-26 23:42:09.731199 Epoch 200, Training Loss 0.2796389321460748\n",
      "2022-03-26 23:42:09.750203 Epoch 200, Training Loss 0.2806837755777037\n",
      "2022-03-26 23:42:09.769201 Epoch 200, Training Loss 0.2812645998985871\n",
      "2022-03-26 23:42:09.789206 Epoch 200, Training Loss 0.2817512896588391\n",
      "2022-03-26 23:42:09.807216 Epoch 200, Training Loss 0.2823253071978879\n",
      "2022-03-26 23:42:09.827214 Epoch 200, Training Loss 0.2832276343994433\n",
      "2022-03-26 23:42:09.847225 Epoch 200, Training Loss 0.28407627046870454\n",
      "2022-03-26 23:42:09.866229 Epoch 200, Training Loss 0.28496294199963057\n",
      "2022-03-26 23:42:09.885221 Epoch 200, Training Loss 0.2856361968133151\n",
      "2022-03-26 23:42:09.904238 Epoch 200, Training Loss 0.286876828088175\n",
      "2022-03-26 23:42:09.924242 Epoch 200, Training Loss 0.28775145627958393\n",
      "2022-03-26 23:42:09.944241 Epoch 200, Training Loss 0.28863947989080874\n",
      "2022-03-26 23:42:09.963251 Epoch 200, Training Loss 0.28954879310734744\n",
      "2022-03-26 23:42:09.982256 Epoch 200, Training Loss 0.29049300461473976\n",
      "2022-03-26 23:42:10.001260 Epoch 200, Training Loss 0.29107776886361947\n",
      "2022-03-26 23:42:10.020264 Epoch 200, Training Loss 0.29204845382734335\n",
      "2022-03-26 23:42:10.040269 Epoch 200, Training Loss 0.2930255456806144\n",
      "2022-03-26 23:42:10.059261 Epoch 200, Training Loss 0.29364094389674\n",
      "2022-03-26 23:42:10.078278 Epoch 200, Training Loss 0.2943592615749525\n",
      "2022-03-26 23:42:10.097282 Epoch 200, Training Loss 0.2950374530556867\n",
      "2022-03-26 23:42:10.116280 Epoch 200, Training Loss 0.29582869069045764\n",
      "2022-03-26 23:42:10.135284 Epoch 200, Training Loss 0.2966147197786804\n",
      "2022-03-26 23:42:10.155295 Epoch 200, Training Loss 0.2973497718039071\n",
      "2022-03-26 23:42:10.174299 Epoch 200, Training Loss 0.2982013887913941\n",
      "2022-03-26 23:42:10.193297 Epoch 200, Training Loss 0.29905214456036266\n",
      "2022-03-26 23:42:10.213308 Epoch 200, Training Loss 0.29970594867111167\n",
      "2022-03-26 23:42:10.231301 Epoch 200, Training Loss 0.30024727957937725\n",
      "2022-03-26 23:42:10.250316 Epoch 200, Training Loss 0.3010797813115522\n",
      "2022-03-26 23:42:10.269321 Epoch 200, Training Loss 0.3019021388209994\n",
      "2022-03-26 23:42:10.288325 Epoch 200, Training Loss 0.30239545662537254\n",
      "2022-03-26 23:42:10.308360 Epoch 200, Training Loss 0.303041138726732\n",
      "2022-03-26 23:42:10.327328 Epoch 200, Training Loss 0.3036926139879714\n",
      "2022-03-26 23:42:10.346338 Epoch 200, Training Loss 0.304556108001248\n",
      "2022-03-26 23:42:10.366343 Epoch 200, Training Loss 0.3052134866376057\n",
      "2022-03-26 23:42:10.385347 Epoch 200, Training Loss 0.30599164875114665\n",
      "2022-03-26 23:42:10.404351 Epoch 200, Training Loss 0.3067761523568112\n",
      "2022-03-26 23:42:10.425356 Epoch 200, Training Loss 0.30743788868722405\n",
      "2022-03-26 23:42:10.443354 Epoch 200, Training Loss 0.30823425994352305\n",
      "2022-03-26 23:42:10.462358 Epoch 200, Training Loss 0.3090445680157913\n",
      "2022-03-26 23:42:10.481369 Epoch 200, Training Loss 0.3101248078410278\n",
      "2022-03-26 23:42:10.500373 Epoch 200, Training Loss 0.31092423280639114\n",
      "2022-03-26 23:42:10.519365 Epoch 200, Training Loss 0.3116895421920225\n",
      "2022-03-26 23:42:10.539382 Epoch 200, Training Loss 0.3121242031569371\n",
      "2022-03-26 23:42:10.558371 Epoch 200, Training Loss 0.3130341449661938\n",
      "2022-03-26 23:42:10.578391 Epoch 200, Training Loss 0.31414354647821785\n",
      "2022-03-26 23:42:10.598389 Epoch 200, Training Loss 0.31517147888307984\n",
      "2022-03-26 23:42:10.617400 Epoch 200, Training Loss 0.3158119153945952\n",
      "2022-03-26 23:42:10.635404 Epoch 200, Training Loss 0.3166489890774193\n",
      "2022-03-26 23:42:10.654408 Epoch 200, Training Loss 0.31734737784356415\n",
      "2022-03-26 23:42:10.674406 Epoch 200, Training Loss 0.3178459292330095\n",
      "2022-03-26 23:42:10.693411 Epoch 200, Training Loss 0.3183956859666673\n",
      "2022-03-26 23:42:10.712421 Epoch 200, Training Loss 0.3192840428913341\n",
      "2022-03-26 23:42:10.731426 Epoch 200, Training Loss 0.31997819797462207\n",
      "2022-03-26 23:42:10.750430 Epoch 200, Training Loss 0.3208526552790571\n",
      "2022-03-26 23:42:10.769434 Epoch 200, Training Loss 0.3214368221857359\n",
      "2022-03-26 23:42:10.789439 Epoch 200, Training Loss 0.32199089694053623\n",
      "2022-03-26 23:42:10.808443 Epoch 200, Training Loss 0.3228730527717439\n",
      "2022-03-26 23:42:10.827447 Epoch 200, Training Loss 0.3238249298785349\n",
      "2022-03-26 23:42:10.846452 Epoch 200, Training Loss 0.324682447573413\n",
      "2022-03-26 23:42:10.865456 Epoch 200, Training Loss 0.32561335878451464\n",
      "2022-03-26 23:42:10.884460 Epoch 200, Training Loss 0.3264800827292835\n",
      "2022-03-26 23:42:10.904452 Epoch 200, Training Loss 0.3272913378446608\n",
      "2022-03-26 23:42:10.923469 Epoch 200, Training Loss 0.3280637321036185\n",
      "2022-03-26 23:42:10.942474 Epoch 200, Training Loss 0.3288677678160045\n",
      "2022-03-26 23:42:10.962478 Epoch 200, Training Loss 0.32961574208248606\n",
      "2022-03-26 23:42:10.981482 Epoch 200, Training Loss 0.3302875622687742\n",
      "2022-03-26 23:42:11.001480 Epoch 200, Training Loss 0.33102072798230153\n",
      "2022-03-26 23:42:11.020491 Epoch 200, Training Loss 0.33174569805717224\n",
      "2022-03-26 23:42:11.040490 Epoch 200, Training Loss 0.33247008306138653\n",
      "2022-03-26 23:42:11.059500 Epoch 200, Training Loss 0.33330475876245963\n",
      "2022-03-26 23:42:11.079504 Epoch 200, Training Loss 0.3341221287656013\n",
      "2022-03-26 23:42:11.098509 Epoch 200, Training Loss 0.33490109226435344\n",
      "2022-03-26 23:42:11.117507 Epoch 200, Training Loss 0.3355402177023461\n",
      "2022-03-26 23:42:11.136517 Epoch 200, Training Loss 0.3363127187466073\n",
      "2022-03-26 23:42:11.155522 Epoch 200, Training Loss 0.33725448265252517\n",
      "2022-03-26 23:42:11.174526 Epoch 200, Training Loss 0.338349409915907\n",
      "2022-03-26 23:42:11.193524 Epoch 200, Training Loss 0.33923764961302433\n",
      "2022-03-26 23:42:11.212535 Epoch 200, Training Loss 0.3400228285347409\n",
      "2022-03-26 23:42:11.231539 Epoch 200, Training Loss 0.3406720107702343\n",
      "2022-03-26 23:42:11.250543 Epoch 200, Training Loss 0.3414210204197013\n",
      "2022-03-26 23:42:11.269547 Epoch 200, Training Loss 0.342240215300599\n",
      "2022-03-26 23:42:11.288552 Epoch 200, Training Loss 0.34281345752194103\n",
      "2022-03-26 23:42:11.308556 Epoch 200, Training Loss 0.34371049667868164\n",
      "2022-03-26 23:42:11.327555 Epoch 200, Training Loss 0.344430332217375\n",
      "2022-03-26 23:42:11.346565 Epoch 200, Training Loss 0.34500666816368736\n",
      "2022-03-26 23:42:11.366570 Epoch 200, Training Loss 0.345800422303512\n",
      "2022-03-26 23:42:11.385574 Epoch 200, Training Loss 0.34638427499005253\n",
      "2022-03-26 23:42:11.404578 Epoch 200, Training Loss 0.3472754302841928\n",
      "2022-03-26 23:42:11.424583 Epoch 200, Training Loss 0.34802686459268145\n",
      "2022-03-26 23:42:11.444575 Epoch 200, Training Loss 0.34869888737378524\n",
      "2022-03-26 23:42:11.462592 Epoch 200, Training Loss 0.34933300240112997\n",
      "2022-03-26 23:42:11.481596 Epoch 200, Training Loss 0.3498851872618546\n",
      "2022-03-26 23:42:11.501601 Epoch 200, Training Loss 0.3504258378997178\n",
      "2022-03-26 23:42:11.520604 Epoch 200, Training Loss 0.3512663298555652\n",
      "2022-03-26 23:42:11.539609 Epoch 200, Training Loss 0.35176108304954246\n",
      "2022-03-26 23:42:11.558613 Epoch 200, Training Loss 0.3523577428458597\n",
      "2022-03-26 23:42:11.577611 Epoch 200, Training Loss 0.35322282385185855\n",
      "2022-03-26 23:42:11.596622 Epoch 200, Training Loss 0.35401006942362434\n",
      "2022-03-26 23:42:11.615626 Epoch 200, Training Loss 0.35489962659680935\n",
      "2022-03-26 23:42:11.635630 Epoch 200, Training Loss 0.35594589055499154\n",
      "2022-03-26 23:42:11.654629 Epoch 200, Training Loss 0.3569255629387658\n",
      "2022-03-26 23:42:11.673639 Epoch 200, Training Loss 0.35766218000513206\n",
      "2022-03-26 23:42:11.692644 Epoch 200, Training Loss 0.3585350044319392\n",
      "2022-03-26 23:42:11.712642 Epoch 200, Training Loss 0.35933332046127076\n",
      "2022-03-26 23:42:11.731652 Epoch 200, Training Loss 0.36022552241907096\n",
      "2022-03-26 23:42:11.751657 Epoch 200, Training Loss 0.36103076607827334\n",
      "2022-03-26 23:42:11.770661 Epoch 200, Training Loss 0.3618569647335945\n",
      "2022-03-26 23:42:11.789665 Epoch 200, Training Loss 0.3623553949320103\n",
      "2022-03-26 23:42:11.808669 Epoch 200, Training Loss 0.36325944373217384\n",
      "2022-03-26 23:42:11.827674 Epoch 200, Training Loss 0.36416476869674597\n",
      "2022-03-26 23:42:11.847679 Epoch 200, Training Loss 0.3648489159924905\n",
      "2022-03-26 23:42:11.866677 Epoch 200, Training Loss 0.36542184925292764\n",
      "2022-03-26 23:42:11.886687 Epoch 200, Training Loss 0.36647861750077104\n",
      "2022-03-26 23:42:11.905685 Epoch 200, Training Loss 0.36734990539300777\n",
      "2022-03-26 23:42:11.924691 Epoch 200, Training Loss 0.368531219680291\n",
      "2022-03-26 23:42:11.943694 Epoch 200, Training Loss 0.36930699638850856\n",
      "2022-03-26 23:42:11.962699 Epoch 200, Training Loss 0.3699811581531754\n",
      "2022-03-26 23:42:11.982709 Epoch 200, Training Loss 0.3709068368844059\n",
      "2022-03-26 23:42:12.001713 Epoch 200, Training Loss 0.3714733824629308\n",
      "2022-03-26 23:42:12.020718 Epoch 200, Training Loss 0.37220159100602046\n",
      "2022-03-26 23:42:12.040722 Epoch 200, Training Loss 0.373123735303769\n",
      "2022-03-26 23:42:12.059721 Epoch 200, Training Loss 0.37387038210926155\n",
      "2022-03-26 23:42:12.078731 Epoch 200, Training Loss 0.3746306247952039\n",
      "2022-03-26 23:42:12.097735 Epoch 200, Training Loss 0.3753054204880429\n",
      "2022-03-26 23:42:12.116740 Epoch 200, Training Loss 0.37592794267875157\n",
      "2022-03-26 23:42:12.136744 Epoch 200, Training Loss 0.37686334519892395\n",
      "2022-03-26 23:42:12.155748 Epoch 200, Training Loss 0.37770525516603914\n",
      "2022-03-26 23:42:12.174754 Epoch 200, Training Loss 0.37845020956547976\n",
      "2022-03-26 23:42:12.193751 Epoch 200, Training Loss 0.3794575493277796\n",
      "2022-03-26 23:42:12.212755 Epoch 200, Training Loss 0.3800624841466889\n",
      "2022-03-26 23:42:12.231766 Epoch 200, Training Loss 0.380918957921855\n",
      "2022-03-26 23:42:12.251770 Epoch 200, Training Loss 0.3817262010806052\n",
      "2022-03-26 23:42:12.270774 Epoch 200, Training Loss 0.38233428454155205\n",
      "2022-03-26 23:42:12.290779 Epoch 200, Training Loss 0.38286486420485066\n",
      "2022-03-26 23:42:12.310778 Epoch 200, Training Loss 0.383614452217546\n",
      "2022-03-26 23:42:12.330788 Epoch 200, Training Loss 0.3842834299787536\n",
      "2022-03-26 23:42:12.349792 Epoch 200, Training Loss 0.38514385595346046\n",
      "2022-03-26 23:42:12.368797 Epoch 200, Training Loss 0.38593346345455143\n",
      "2022-03-26 23:42:12.387795 Epoch 200, Training Loss 0.3867807869258744\n",
      "2022-03-26 23:42:12.408096 Epoch 200, Training Loss 0.38771548188860766\n",
      "2022-03-26 23:42:12.426804 Epoch 200, Training Loss 0.3886122635715758\n",
      "2022-03-26 23:42:12.447815 Epoch 200, Training Loss 0.38936025796034146\n",
      "2022-03-26 23:42:12.466813 Epoch 200, Training Loss 0.3901524751844918\n",
      "2022-03-26 23:42:12.486812 Epoch 200, Training Loss 0.3908573475182819\n",
      "2022-03-26 23:42:12.505828 Epoch 200, Training Loss 0.39161996897834034\n",
      "2022-03-26 23:42:12.524832 Epoch 200, Training Loss 0.3923902035979054\n",
      "2022-03-26 23:42:12.543830 Epoch 200, Training Loss 0.39328108045756055\n",
      "2022-03-26 23:42:12.563841 Epoch 200, Training Loss 0.39410601598222544\n",
      "2022-03-26 23:42:12.582845 Epoch 200, Training Loss 0.39466098831285296\n",
      "2022-03-26 23:42:12.601849 Epoch 200, Training Loss 0.39528390193534324\n",
      "2022-03-26 23:42:12.620848 Epoch 200, Training Loss 0.39590884619356725\n",
      "2022-03-26 23:42:12.639858 Epoch 200, Training Loss 0.3967997898988407\n",
      "2022-03-26 23:42:12.659857 Epoch 200, Training Loss 0.39780604389622387\n",
      "2022-03-26 23:42:12.679868 Epoch 200, Training Loss 0.3985508210823664\n",
      "2022-03-26 23:42:12.698871 Epoch 200, Training Loss 0.3993385682630417\n",
      "2022-03-26 23:42:12.717870 Epoch 200, Training Loss 0.40010674240644023\n",
      "2022-03-26 23:42:12.737874 Epoch 200, Training Loss 0.4008332593056857\n",
      "2022-03-26 23:42:12.756885 Epoch 200, Training Loss 0.40156082149661715\n",
      "2022-03-26 23:42:12.775889 Epoch 200, Training Loss 0.4023000217612137\n",
      "2022-03-26 23:42:12.795887 Epoch 200, Training Loss 0.402968147069292\n",
      "2022-03-26 23:42:12.814898 Epoch 200, Training Loss 0.40381525849442346\n",
      "2022-03-26 23:42:12.833902 Epoch 200, Training Loss 0.4044345951141299\n",
      "2022-03-26 23:42:12.853907 Epoch 200, Training Loss 0.4053744987758529\n",
      "2022-03-26 23:42:12.873905 Epoch 200, Training Loss 0.40618374044328087\n",
      "2022-03-26 23:42:12.892915 Epoch 200, Training Loss 0.4070251962870283\n",
      "2022-03-26 23:42:12.912920 Epoch 200, Training Loss 0.4078205228614075\n",
      "2022-03-26 23:42:12.930924 Epoch 200, Training Loss 0.4083440601825714\n",
      "2022-03-26 23:42:12.949928 Epoch 200, Training Loss 0.4091313144435053\n",
      "2022-03-26 23:42:12.968933 Epoch 200, Training Loss 0.4100994883138505\n",
      "2022-03-26 23:42:12.995939 Epoch 200, Training Loss 0.4109294143936518\n",
      "2022-03-26 23:42:13.021945 Epoch 200, Training Loss 0.41196244452005765\n",
      "2022-03-26 23:42:13.048951 Epoch 200, Training Loss 0.4129706434429149\n",
      "2022-03-26 23:42:13.074957 Epoch 200, Training Loss 0.4135985785280652\n",
      "2022-03-26 23:42:13.101963 Epoch 200, Training Loss 0.4141086821284745\n",
      "2022-03-26 23:42:13.127969 Epoch 200, Training Loss 0.4147137607378728\n",
      "2022-03-26 23:42:13.153975 Epoch 200, Training Loss 0.4154214928750797\n",
      "2022-03-26 23:42:13.180981 Epoch 200, Training Loss 0.4162439873532566\n",
      "2022-03-26 23:42:13.206986 Epoch 200, Training Loss 0.4172164752431538\n",
      "2022-03-26 23:42:13.232993 Epoch 200, Training Loss 0.4180661402928555\n",
      "2022-03-26 23:42:13.258999 Epoch 200, Training Loss 0.4189231844280687\n",
      "2022-03-26 23:42:13.285004 Epoch 200, Training Loss 0.41953600794458024\n",
      "2022-03-26 23:42:13.312010 Epoch 200, Training Loss 0.42043294793809466\n",
      "2022-03-26 23:42:13.338016 Epoch 200, Training Loss 0.4211690042482313\n",
      "2022-03-26 23:42:13.364022 Epoch 200, Training Loss 0.42186216647972535\n",
      "2022-03-26 23:42:13.385027 Epoch 200, Training Loss 0.4225943212009147\n",
      "2022-03-26 23:42:13.404031 Epoch 200, Training Loss 0.4231291237999411\n",
      "2022-03-26 23:42:13.421029 Epoch 200, Training Loss 0.4240950896307026\n",
      "2022-03-26 23:42:13.440039 Epoch 200, Training Loss 0.4247613059895118\n",
      "2022-03-26 23:42:13.458044 Epoch 200, Training Loss 0.4257219304971378\n",
      "2022-03-26 23:42:13.476035 Epoch 200, Training Loss 0.42663509415848483\n",
      "2022-03-26 23:42:13.494046 Epoch 200, Training Loss 0.4271786731984609\n",
      "2022-03-26 23:42:13.513056 Epoch 200, Training Loss 0.42786893096116496\n",
      "2022-03-26 23:42:13.531060 Epoch 200, Training Loss 0.4288016704037366\n",
      "2022-03-26 23:42:13.550064 Epoch 200, Training Loss 0.4295964552770795\n",
      "2022-03-26 23:42:13.568063 Epoch 200, Training Loss 0.4303068819710666\n",
      "2022-03-26 23:42:13.587067 Epoch 200, Training Loss 0.430992802390662\n",
      "2022-03-26 23:42:13.605077 Epoch 200, Training Loss 0.4322674660883901\n",
      "2022-03-26 23:42:13.623081 Epoch 200, Training Loss 0.4327972767221958\n",
      "2022-03-26 23:42:13.642079 Epoch 200, Training Loss 0.4336745984032941\n",
      "2022-03-26 23:42:13.660083 Epoch 200, Training Loss 0.43436241306154927\n",
      "2022-03-26 23:42:13.678087 Epoch 200, Training Loss 0.43538438622146614\n",
      "2022-03-26 23:42:13.696097 Epoch 200, Training Loss 0.43591606586485565\n",
      "2022-03-26 23:42:13.715102 Epoch 200, Training Loss 0.43678558619735797\n",
      "2022-03-26 23:42:13.733106 Epoch 200, Training Loss 0.43763913256128123\n",
      "2022-03-26 23:42:13.751097 Epoch 200, Training Loss 0.43867959947232393\n",
      "2022-03-26 23:42:13.769108 Epoch 200, Training Loss 0.43984049711081075\n",
      "2022-03-26 23:42:13.787118 Epoch 200, Training Loss 0.4403629201223783\n",
      "2022-03-26 23:42:13.806122 Epoch 200, Training Loss 0.44099281587259237\n",
      "2022-03-26 23:42:13.823120 Epoch 200, Training Loss 0.4415530124131371\n",
      "2022-03-26 23:42:13.841124 Epoch 200, Training Loss 0.4423750665639063\n",
      "2022-03-26 23:42:13.859128 Epoch 200, Training Loss 0.44345163597780113\n",
      "2022-03-26 23:42:13.878143 Epoch 200, Training Loss 0.44430150049726674\n",
      "2022-03-26 23:42:13.896143 Epoch 200, Training Loss 0.44502828211125817\n",
      "2022-03-26 23:42:13.915147 Epoch 200, Training Loss 0.44593126267728295\n",
      "2022-03-26 23:42:13.933145 Epoch 200, Training Loss 0.44684271274320303\n",
      "2022-03-26 23:42:13.952150 Epoch 200, Training Loss 0.44746313596625464\n",
      "2022-03-26 23:42:13.970160 Epoch 200, Training Loss 0.44848581302501356\n",
      "2022-03-26 23:42:13.988158 Epoch 200, Training Loss 0.4495254843436239\n",
      "2022-03-26 23:42:14.006168 Epoch 200, Training Loss 0.4503883908471793\n",
      "2022-03-26 23:42:14.025166 Epoch 200, Training Loss 0.4515055960706433\n",
      "2022-03-26 23:42:14.043170 Epoch 200, Training Loss 0.45232207749200903\n",
      "2022-03-26 23:42:14.061174 Epoch 200, Training Loss 0.45312541860448735\n",
      "2022-03-26 23:42:14.079184 Epoch 200, Training Loss 0.45371817128585124\n",
      "2022-03-26 23:42:14.097188 Epoch 200, Training Loss 0.45443792206704464\n",
      "2022-03-26 23:42:14.115193 Epoch 200, Training Loss 0.45506702283459244\n",
      "2022-03-26 23:42:14.134197 Epoch 200, Training Loss 0.45565944067809894\n",
      "2022-03-26 23:42:14.152201 Epoch 200, Training Loss 0.4564477471668092\n",
      "2022-03-26 23:42:14.170205 Epoch 200, Training Loss 0.4569926600322089\n",
      "2022-03-26 23:42:14.189190 Epoch 200, Training Loss 0.45803892017935244\n",
      "2022-03-26 23:42:14.207207 Epoch 200, Training Loss 0.45861863224860044\n",
      "2022-03-26 23:42:14.226212 Epoch 200, Training Loss 0.45952077408123504\n",
      "2022-03-26 23:42:14.244216 Epoch 200, Training Loss 0.4604838054122217\n",
      "2022-03-26 23:42:14.262220 Epoch 200, Training Loss 0.46127497094214115\n",
      "2022-03-26 23:42:14.280230 Epoch 200, Training Loss 0.46193700415246625\n",
      "2022-03-26 23:42:14.298234 Epoch 200, Training Loss 0.4626782052123638\n",
      "2022-03-26 23:42:14.316232 Epoch 200, Training Loss 0.46363778862044636\n",
      "2022-03-26 23:42:14.335243 Epoch 200, Training Loss 0.46448187827301757\n",
      "2022-03-26 23:42:14.353240 Epoch 200, Training Loss 0.4652437557421072\n",
      "2022-03-26 23:42:14.371245 Epoch 200, Training Loss 0.4658929315750556\n",
      "2022-03-26 23:42:14.389249 Epoch 200, Training Loss 0.46661338069097463\n",
      "2022-03-26 23:42:14.408246 Epoch 200, Training Loss 0.467322842048867\n",
      "2022-03-26 23:42:14.426257 Epoch 200, Training Loss 0.4679463185617686\n",
      "2022-03-26 23:42:14.444262 Epoch 200, Training Loss 0.46840394191119983\n",
      "2022-03-26 23:42:14.462265 Epoch 200, Training Loss 0.4689108661907103\n",
      "2022-03-26 23:42:14.480275 Epoch 200, Training Loss 0.4700157030692796\n",
      "2022-03-26 23:42:14.498273 Epoch 200, Training Loss 0.47078078638409715\n",
      "2022-03-26 23:42:14.517278 Epoch 200, Training Loss 0.4716318022945653\n",
      "2022-03-26 23:42:14.536288 Epoch 200, Training Loss 0.47246325911615816\n",
      "2022-03-26 23:42:14.555292 Epoch 200, Training Loss 0.47338724521267445\n",
      "2022-03-26 23:42:14.573296 Epoch 200, Training Loss 0.4741013785228705\n",
      "2022-03-26 23:42:14.592301 Epoch 200, Training Loss 0.4748572872194183\n",
      "2022-03-26 23:42:14.610299 Epoch 200, Training Loss 0.47548658048252923\n",
      "2022-03-26 23:42:14.628303 Epoch 200, Training Loss 0.47611405176427357\n",
      "2022-03-26 23:42:14.646313 Epoch 200, Training Loss 0.4768856499734742\n",
      "2022-03-26 23:42:14.665317 Epoch 200, Training Loss 0.4776412636194083\n",
      "2022-03-26 23:42:14.683315 Epoch 200, Training Loss 0.47857479324273744\n",
      "2022-03-26 23:42:14.701325 Epoch 200, Training Loss 0.4795627452604606\n",
      "2022-03-26 23:42:14.720330 Epoch 200, Training Loss 0.480454259706885\n",
      "2022-03-26 23:42:14.738334 Epoch 200, Training Loss 0.481170409285199\n",
      "2022-03-26 23:42:14.756332 Epoch 200, Training Loss 0.4818693727559751\n",
      "2022-03-26 23:42:14.775330 Epoch 200, Training Loss 0.4828320787004802\n",
      "2022-03-26 23:42:14.793340 Epoch 200, Training Loss 0.4835506077388973\n",
      "2022-03-26 23:42:14.811345 Epoch 200, Training Loss 0.4843762054696412\n",
      "2022-03-26 23:42:14.829354 Epoch 200, Training Loss 0.48513331532935655\n",
      "2022-03-26 23:42:14.847359 Epoch 200, Training Loss 0.48585552575490665\n",
      "2022-03-26 23:42:14.865362 Epoch 200, Training Loss 0.4866298096792777\n",
      "2022-03-26 23:42:14.883367 Epoch 200, Training Loss 0.487321166469313\n",
      "2022-03-26 23:42:14.902371 Epoch 200, Training Loss 0.4878499461409381\n",
      "2022-03-26 23:42:14.920369 Epoch 200, Training Loss 0.48850876923717196\n",
      "2022-03-26 23:42:14.938373 Epoch 200, Training Loss 0.48917573469374187\n",
      "2022-03-26 23:42:14.956383 Epoch 200, Training Loss 0.4899475986847792\n",
      "2022-03-26 23:42:14.975387 Epoch 200, Training Loss 0.4904992625765178\n",
      "2022-03-26 23:42:14.994386 Epoch 200, Training Loss 0.4911935152223958\n",
      "2022-03-26 23:42:15.012396 Epoch 200, Training Loss 0.49197072831108746\n",
      "2022-03-26 23:42:15.030400 Epoch 200, Training Loss 0.49284771225793894\n",
      "2022-03-26 23:42:15.048404 Epoch 200, Training Loss 0.493677892991344\n",
      "2022-03-26 23:42:15.066402 Epoch 200, Training Loss 0.4942926802598607\n",
      "2022-03-26 23:42:15.085412 Epoch 200, Training Loss 0.4953005763575854\n",
      "2022-03-26 23:42:15.103416 Epoch 200, Training Loss 0.4961723059492038\n",
      "2022-03-26 23:42:15.121421 Epoch 200, Training Loss 0.4966831043019624\n",
      "2022-03-26 23:42:15.140425 Epoch 200, Training Loss 0.4974678351979731\n",
      "2022-03-26 23:42:15.158423 Epoch 200, Training Loss 0.49833714150254377\n",
      "2022-03-26 23:42:15.176427 Epoch 200, Training Loss 0.4990068397787221\n",
      "2022-03-26 23:42:15.195431 Epoch 200, Training Loss 0.49993645005366383\n",
      "2022-03-26 23:42:15.221443 Epoch 200, Training Loss 0.5007706030326731\n",
      "2022-03-26 23:42:15.247449 Epoch 200, Training Loss 0.5016186794890162\n",
      "2022-03-26 23:42:15.273455 Epoch 200, Training Loss 0.5025576594311868\n",
      "2022-03-26 23:42:15.299461 Epoch 200, Training Loss 0.5033714900464963\n",
      "2022-03-26 23:42:15.325454 Epoch 200, Training Loss 0.5041273576600472\n",
      "2022-03-26 23:42:15.350473 Epoch 200, Training Loss 0.5052015867913165\n",
      "2022-03-26 23:42:15.376472 Epoch 200, Training Loss 0.5060741427685599\n",
      "2022-03-26 23:42:15.402484 Epoch 200, Training Loss 0.506735771703903\n",
      "2022-03-26 23:42:15.421489 Epoch 200, Training Loss 0.5075030524636168\n",
      "2022-03-26 23:42:15.439493 Epoch 200, Training Loss 0.5085487173265203\n",
      "2022-03-26 23:42:15.457481 Epoch 200, Training Loss 0.5096660440458971\n",
      "2022-03-26 23:42:15.475501 Epoch 200, Training Loss 0.5103885225399071\n",
      "2022-03-26 23:42:15.493499 Epoch 200, Training Loss 0.5110288190338618\n",
      "2022-03-26 23:42:15.512509 Epoch 200, Training Loss 0.5115798441192988\n",
      "2022-03-26 23:42:15.530513 Epoch 200, Training Loss 0.51246125816994\n",
      "2022-03-26 23:42:15.549518 Epoch 200, Training Loss 0.51322782794228\n",
      "2022-03-26 23:42:15.567522 Epoch 200, Training Loss 0.513988747773573\n",
      "2022-03-26 23:42:15.585526 Epoch 200, Training Loss 0.51523309336294\n",
      "2022-03-26 23:42:15.603530 Epoch 200, Training Loss 0.5158194573334111\n",
      "2022-03-26 23:42:15.622534 Epoch 200, Training Loss 0.5166458911298181\n",
      "2022-03-26 23:42:15.641539 Epoch 200, Training Loss 0.5175794308905102\n",
      "2022-03-26 23:42:15.660537 Epoch 200, Training Loss 0.518334698875237\n",
      "2022-03-26 23:42:15.677541 Epoch 200, Training Loss 0.5191896993028539\n",
      "2022-03-26 23:42:15.696545 Epoch 200, Training Loss 0.5199240181604614\n",
      "2022-03-26 23:42:15.714555 Epoch 200, Training Loss 0.5209001807300636\n",
      "2022-03-26 23:42:15.732559 Epoch 200, Training Loss 0.5218287856530046\n",
      "2022-03-26 23:42:15.750563 Epoch 200, Training Loss 0.5224341572169453\n",
      "2022-03-26 23:42:15.768567 Epoch 200, Training Loss 0.5237267691537243\n",
      "2022-03-26 23:42:15.786571 Epoch 200, Training Loss 0.5243228623629226\n",
      "2022-03-26 23:42:15.804569 Epoch 200, Training Loss 0.5249803797973086\n",
      "2022-03-26 23:42:15.822574 Epoch 200, Training Loss 0.5258239708898013\n",
      "2022-03-26 23:42:15.841584 Epoch 200, Training Loss 0.5264552249704175\n",
      "2022-03-26 23:42:15.859588 Epoch 200, Training Loss 0.5273640849782378\n",
      "2022-03-26 23:42:15.877586 Epoch 200, Training Loss 0.5281377320018266\n",
      "2022-03-26 23:42:15.895590 Epoch 200, Training Loss 0.5290601425387366\n",
      "2022-03-26 23:42:15.913600 Epoch 200, Training Loss 0.5298796505345713\n",
      "2022-03-26 23:42:15.931604 Epoch 200, Training Loss 0.5308331807937159\n",
      "2022-03-26 23:42:15.949608 Epoch 200, Training Loss 0.5318552798703503\n",
      "2022-03-26 23:42:15.967613 Epoch 200, Training Loss 0.5329084005730841\n",
      "2022-03-26 23:42:15.986611 Epoch 200, Training Loss 0.5336985610558859\n",
      "2022-03-26 23:42:16.004614 Epoch 200, Training Loss 0.5345080587488916\n",
      "2022-03-26 23:42:16.023625 Epoch 200, Training Loss 0.5351773647167494\n",
      "2022-03-26 23:42:16.041623 Epoch 200, Training Loss 0.5360380567019553\n",
      "2022-03-26 23:42:16.059627 Epoch 200, Training Loss 0.5368536384133122\n",
      "2022-03-26 23:42:16.077632 Epoch 200, Training Loss 0.5376366231676257\n",
      "2022-03-26 23:42:16.095641 Epoch 200, Training Loss 0.5382904403883478\n",
      "2022-03-26 23:42:16.113646 Epoch 200, Training Loss 0.5390686899271158\n",
      "2022-03-26 23:42:16.131650 Epoch 200, Training Loss 0.5398711875042952\n",
      "2022-03-26 23:42:16.149654 Epoch 200, Training Loss 0.5406538718725409\n",
      "2022-03-26 23:42:16.168652 Epoch 200, Training Loss 0.5413561303673498\n",
      "2022-03-26 23:42:16.185656 Epoch 200, Training Loss 0.5421064070347325\n",
      "2022-03-26 23:42:16.203666 Epoch 200, Training Loss 0.542672662021559\n",
      "2022-03-26 23:42:16.221671 Epoch 200, Training Loss 0.5431800934359851\n",
      "2022-03-26 23:42:16.241007 Epoch 200, Training Loss 0.5440094275852604\n",
      "2022-03-26 23:42:16.258672 Epoch 200, Training Loss 0.5447694584536735\n",
      "2022-03-26 23:42:16.276676 Epoch 200, Training Loss 0.5455154454921518\n",
      "2022-03-26 23:42:16.295681 Epoch 200, Training Loss 0.5462804195063803\n",
      "2022-03-26 23:42:16.313691 Epoch 200, Training Loss 0.546944531333416\n",
      "2022-03-26 23:42:16.331695 Epoch 200, Training Loss 0.5477702308951131\n",
      "2022-03-26 23:42:16.349693 Epoch 200, Training Loss 0.5484630903776955\n",
      "2022-03-26 23:42:16.367703 Epoch 200, Training Loss 0.5495820053093269\n",
      "2022-03-26 23:42:16.386708 Epoch 200, Training Loss 0.5508666390653156\n",
      "2022-03-26 23:42:16.404711 Epoch 200, Training Loss 0.5513974425128049\n",
      "2022-03-26 23:42:16.422710 Epoch 200, Training Loss 0.5522336884380301\n",
      "2022-03-26 23:42:16.439720 Epoch 200, Training Loss 0.5529122439491779\n",
      "2022-03-26 23:42:16.459718 Epoch 200, Training Loss 0.5536021586421811\n",
      "2022-03-26 23:42:16.477722 Epoch 200, Training Loss 0.5543299257907721\n",
      "2022-03-26 23:42:16.495732 Epoch 200, Training Loss 0.555169550887764\n",
      "2022-03-26 23:42:16.513736 Epoch 200, Training Loss 0.5559934323553539\n",
      "2022-03-26 23:42:16.531734 Epoch 200, Training Loss 0.5566971973537485\n",
      "2022-03-26 23:42:16.549738 Epoch 200, Training Loss 0.5573791111521709\n",
      "2022-03-26 23:42:16.567749 Epoch 200, Training Loss 0.558318483707545\n",
      "2022-03-26 23:42:16.586747 Epoch 200, Training Loss 0.5591821719313521\n",
      "2022-03-26 23:42:16.604757 Epoch 200, Training Loss 0.560308168001492\n",
      "2022-03-26 23:42:16.622761 Epoch 200, Training Loss 0.5610030163127138\n",
      "2022-03-26 23:42:16.640837 Epoch 200, Training Loss 0.5618998870977661\n",
      "2022-03-26 23:42:16.659763 Epoch 200, Training Loss 0.5627596600128867\n",
      "2022-03-26 23:42:16.678774 Epoch 200, Training Loss 0.5634913948338355\n",
      "2022-03-26 23:42:16.696778 Epoch 200, Training Loss 0.5642186690626851\n",
      "2022-03-26 23:42:16.714782 Epoch 200, Training Loss 0.565405857380089\n",
      "2022-03-26 23:42:16.732786 Epoch 200, Training Loss 0.5665267619787885\n",
      "2022-03-26 23:42:16.750790 Epoch 200, Training Loss 0.567138889980743\n",
      "2022-03-26 23:42:16.769788 Epoch 200, Training Loss 0.5676424542961218\n",
      "2022-03-26 23:42:16.787798 Epoch 200, Training Loss 0.5686690051232457\n",
      "2022-03-26 23:42:16.805802 Epoch 200, Training Loss 0.5692426616807118\n",
      "2022-03-26 23:42:16.823800 Epoch 200, Training Loss 0.5700575420085121\n",
      "2022-03-26 23:42:16.842811 Epoch 200, Training Loss 0.5709934292165825\n",
      "2022-03-26 23:42:16.860815 Epoch 200, Training Loss 0.5719949956364034\n",
      "2022-03-26 23:42:16.878819 Epoch 200, Training Loss 0.5725182336004798\n",
      "2022-03-26 23:42:16.897817 Epoch 200, Training Loss 0.5732340115263029\n",
      "2022-03-26 23:42:16.915815 Epoch 200, Training Loss 0.5738683503378382\n",
      "2022-03-26 23:42:16.933832 Epoch 200, Training Loss 0.5746822786681792\n",
      "2022-03-26 23:42:16.951836 Epoch 200, Training Loss 0.575527014901571\n",
      "2022-03-26 23:42:16.969834 Epoch 200, Training Loss 0.5762059825384404\n",
      "2022-03-26 23:42:16.987838 Epoch 200, Training Loss 0.5772598989098273\n",
      "2022-03-26 23:42:17.006848 Epoch 200, Training Loss 0.5779880672083486\n",
      "2022-03-26 23:42:17.024846 Epoch 200, Training Loss 0.5788533384613979\n",
      "2022-03-26 23:42:17.042850 Epoch 200, Training Loss 0.5793121521887572\n",
      "2022-03-26 23:42:17.060860 Epoch 200, Training Loss 0.5802400115200931\n",
      "2022-03-26 23:42:17.078947 Epoch 200, Training Loss 0.5811068398873215\n",
      "2022-03-26 23:42:17.096868 Epoch 200, Training Loss 0.5818655506119399\n",
      "2022-03-26 23:42:17.115873 Epoch 200, Training Loss 0.5824125085187994\n",
      "2022-03-26 23:42:17.133877 Epoch 200, Training Loss 0.5831659542172766\n",
      "2022-03-26 23:42:17.151875 Epoch 200, Training Loss 0.5838679933486997\n",
      "2022-03-26 23:42:17.169885 Epoch 200, Training Loss 0.5845494667434936\n",
      "2022-03-26 23:42:17.188883 Epoch 200, Training Loss 0.5851305423261565\n",
      "2022-03-26 23:42:17.206893 Epoch 200, Training Loss 0.5861613781327177\n",
      "2022-03-26 23:42:17.224897 Epoch 200, Training Loss 0.5871793612113694\n",
      "2022-03-26 23:42:17.242895 Epoch 200, Training Loss 0.5879011904568319\n",
      "2022-03-26 23:42:17.261906 Epoch 200, Training Loss 0.588810434236246\n",
      "2022-03-26 23:42:17.279910 Epoch 200, Training Loss 0.5895867952529121\n",
      "2022-03-26 23:42:17.297914 Epoch 200, Training Loss 0.5903840563867403\n",
      "2022-03-26 23:42:17.316918 Epoch 200, Training Loss 0.5912761351717707\n",
      "2022-03-26 23:42:17.335923 Epoch 200, Training Loss 0.5923241178321716\n",
      "2022-03-26 23:42:17.353927 Epoch 200, Training Loss 0.5930452947802556\n",
      "2022-03-26 23:42:17.371931 Epoch 200, Training Loss 0.594061705653015\n",
      "2022-03-26 23:42:17.390935 Epoch 200, Training Loss 0.5948741966882325\n",
      "2022-03-26 23:42:17.408934 Epoch 200, Training Loss 0.5956947210118594\n",
      "2022-03-26 23:42:17.427937 Epoch 200, Training Loss 0.5967170638425271\n",
      "2022-03-26 23:42:17.445935 Epoch 200, Training Loss 0.5977529088783142\n",
      "2022-03-26 23:42:17.463946 Epoch 200, Training Loss 0.598344803809205\n",
      "2022-03-26 23:42:17.481950 Epoch 200, Training Loss 0.5994743848471995\n",
      "2022-03-26 23:42:17.500960 Epoch 200, Training Loss 0.6001129001379013\n",
      "2022-03-26 23:42:17.518964 Epoch 200, Training Loss 0.6007161723911915\n",
      "2022-03-26 23:42:17.536962 Epoch 200, Training Loss 0.6014218892511505\n",
      "2022-03-26 23:42:17.555966 Epoch 200, Training Loss 0.6020723220408725\n",
      "2022-03-26 23:42:17.573976 Epoch 200, Training Loss 0.6030137184102212\n",
      "2022-03-26 23:42:17.592976 Epoch 200, Training Loss 0.6037846151596445\n",
      "2022-03-26 23:42:17.610979 Epoch 200, Training Loss 0.6047909994183294\n",
      "2022-03-26 23:42:17.628989 Epoch 200, Training Loss 0.6059392732961099\n",
      "2022-03-26 23:42:17.645987 Epoch 200, Training Loss 0.6070625758003396\n",
      "2022-03-26 23:42:17.653984 Epoch 200, Training Loss 0.6078532381207132\n",
      "2022-03-26 23:53:38.670493 Epoch 250, Training Loss 0.0010293665749337667\n",
      "2022-03-26 23:53:38.688496 Epoch 250, Training Loss 0.0017284516940641282\n",
      "2022-03-26 23:53:38.706501 Epoch 250, Training Loss 0.0023832603183853657\n",
      "2022-03-26 23:53:38.724505 Epoch 250, Training Loss 0.0033723901757193955\n",
      "2022-03-26 23:53:38.742509 Epoch 250, Training Loss 0.003949305781013215\n",
      "2022-03-26 23:53:38.761513 Epoch 250, Training Loss 0.004866122856469411\n",
      "2022-03-26 23:53:38.779517 Epoch 250, Training Loss 0.005753127319733505\n",
      "2022-03-26 23:53:38.798521 Epoch 250, Training Loss 0.006780635937095603\n",
      "2022-03-26 23:53:38.817526 Epoch 250, Training Loss 0.0075112234829636795\n",
      "2022-03-26 23:53:38.837530 Epoch 250, Training Loss 0.00831028571366654\n",
      "2022-03-26 23:53:38.855541 Epoch 250, Training Loss 0.008953882338445815\n",
      "2022-03-26 23:53:38.874349 Epoch 250, Training Loss 0.00956090652119473\n",
      "2022-03-26 23:53:38.893296 Epoch 250, Training Loss 0.010158744690668247\n",
      "2022-03-26 23:53:38.911326 Epoch 250, Training Loss 0.011030918496953862\n",
      "2022-03-26 23:53:38.929391 Epoch 250, Training Loss 0.011693150109952062\n",
      "2022-03-26 23:53:38.948395 Epoch 250, Training Loss 0.012303740319693486\n",
      "2022-03-26 23:53:38.967492 Epoch 250, Training Loss 0.012999429193604023\n",
      "2022-03-26 23:53:38.986585 Epoch 250, Training Loss 0.013650605593191083\n",
      "2022-03-26 23:53:39.004640 Epoch 250, Training Loss 0.014494086318003857\n",
      "2022-03-26 23:53:39.023522 Epoch 250, Training Loss 0.015385728281782106\n",
      "2022-03-26 23:53:39.041393 Epoch 250, Training Loss 0.016242206172869943\n",
      "2022-03-26 23:53:39.059428 Epoch 250, Training Loss 0.01692965047438736\n",
      "2022-03-26 23:53:39.079379 Epoch 250, Training Loss 0.017938538356815154\n",
      "2022-03-26 23:53:39.097310 Epoch 250, Training Loss 0.018630483799883166\n",
      "2022-03-26 23:53:39.115320 Epoch 250, Training Loss 0.019214568578678627\n",
      "2022-03-26 23:53:39.134403 Epoch 250, Training Loss 0.020010130522806015\n",
      "2022-03-26 23:53:39.152682 Epoch 250, Training Loss 0.020668581204341195\n",
      "2022-03-26 23:53:39.171558 Epoch 250, Training Loss 0.021132001982015723\n",
      "2022-03-26 23:53:39.189579 Epoch 250, Training Loss 0.02227063111179625\n",
      "2022-03-26 23:53:39.208533 Epoch 250, Training Loss 0.022901985003515278\n",
      "2022-03-26 23:53:39.227324 Epoch 250, Training Loss 0.023860289427020665\n",
      "2022-03-26 23:53:39.245334 Epoch 250, Training Loss 0.02455180041168047\n",
      "2022-03-26 23:53:39.264315 Epoch 250, Training Loss 0.0251717088396287\n",
      "2022-03-26 23:53:39.284382 Epoch 250, Training Loss 0.02586436854756397\n",
      "2022-03-26 23:53:39.303381 Epoch 250, Training Loss 0.02663413364716503\n",
      "2022-03-26 23:53:39.321466 Epoch 250, Training Loss 0.027202881167611808\n",
      "2022-03-26 23:53:39.339486 Epoch 250, Training Loss 0.02788969862948903\n",
      "2022-03-26 23:53:39.358374 Epoch 250, Training Loss 0.028379579829742842\n",
      "2022-03-26 23:53:39.376404 Epoch 250, Training Loss 0.029051940268872645\n",
      "2022-03-26 23:53:39.394714 Epoch 250, Training Loss 0.029632996110355154\n",
      "2022-03-26 23:53:39.413664 Epoch 250, Training Loss 0.030470863937416954\n",
      "2022-03-26 23:53:39.432438 Epoch 250, Training Loss 0.031323589098727914\n",
      "2022-03-26 23:53:39.450412 Epoch 250, Training Loss 0.031918406334069685\n",
      "2022-03-26 23:53:39.469430 Epoch 250, Training Loss 0.03272758146076251\n",
      "2022-03-26 23:53:39.488724 Epoch 250, Training Loss 0.03342960466204397\n",
      "2022-03-26 23:53:39.506605 Epoch 250, Training Loss 0.033846731838362905\n",
      "2022-03-26 23:53:39.524649 Epoch 250, Training Loss 0.03462887404824767\n",
      "2022-03-26 23:53:39.542737 Epoch 250, Training Loss 0.03544054921630703\n",
      "2022-03-26 23:53:39.561473 Epoch 250, Training Loss 0.035969365016578714\n",
      "2022-03-26 23:53:39.580525 Epoch 250, Training Loss 0.0365131307593392\n",
      "2022-03-26 23:53:39.600432 Epoch 250, Training Loss 0.03723387530697581\n",
      "2022-03-26 23:53:39.627380 Epoch 250, Training Loss 0.03797905432903553\n",
      "2022-03-26 23:53:39.653371 Epoch 250, Training Loss 0.03852403689833248\n",
      "2022-03-26 23:53:39.678383 Epoch 250, Training Loss 0.03927112654651827\n",
      "2022-03-26 23:53:39.705221 Epoch 250, Training Loss 0.03989902698932706\n",
      "2022-03-26 23:53:39.731379 Epoch 250, Training Loss 0.04076758206195539\n",
      "2022-03-26 23:53:39.757304 Epoch 250, Training Loss 0.04147871448408307\n",
      "2022-03-26 23:53:39.784175 Epoch 250, Training Loss 0.04214350841081966\n",
      "2022-03-26 23:53:39.802560 Epoch 250, Training Loss 0.042827357683340304\n",
      "2022-03-26 23:53:39.816588 Epoch 250, Training Loss 0.04361220100499175\n",
      "2022-03-26 23:53:39.831566 Epoch 250, Training Loss 0.04416208796184081\n",
      "2022-03-26 23:53:39.845817 Epoch 250, Training Loss 0.04483579888063319\n",
      "2022-03-26 23:53:39.860824 Epoch 250, Training Loss 0.045291709358734854\n",
      "2022-03-26 23:53:39.875323 Epoch 250, Training Loss 0.04611571742902936\n",
      "2022-03-26 23:53:39.889325 Epoch 250, Training Loss 0.04692866853283494\n",
      "2022-03-26 23:53:39.904353 Epoch 250, Training Loss 0.047413880562843264\n",
      "2022-03-26 23:53:39.918387 Epoch 250, Training Loss 0.048168141061387705\n",
      "2022-03-26 23:53:39.934076 Epoch 250, Training Loss 0.048809997337248626\n",
      "2022-03-26 23:53:39.948322 Epoch 250, Training Loss 0.04946607991557597\n",
      "2022-03-26 23:53:39.963389 Epoch 250, Training Loss 0.05035009393301766\n",
      "2022-03-26 23:53:39.977311 Epoch 250, Training Loss 0.051356587797174676\n",
      "2022-03-26 23:53:39.992361 Epoch 250, Training Loss 0.051936086067153366\n",
      "2022-03-26 23:53:40.006425 Epoch 250, Training Loss 0.052549305398141025\n",
      "2022-03-26 23:53:40.021398 Epoch 250, Training Loss 0.05310109322485716\n",
      "2022-03-26 23:53:40.035364 Epoch 250, Training Loss 0.05351562165390805\n",
      "2022-03-26 23:53:40.049379 Epoch 250, Training Loss 0.054192471572810125\n",
      "2022-03-26 23:53:40.064348 Epoch 250, Training Loss 0.054731858698913206\n",
      "2022-03-26 23:53:40.078361 Epoch 250, Training Loss 0.055333910238407454\n",
      "2022-03-26 23:53:40.092476 Epoch 250, Training Loss 0.05589556896015811\n",
      "2022-03-26 23:53:40.107012 Epoch 250, Training Loss 0.056691009637035064\n",
      "2022-03-26 23:53:40.121117 Epoch 250, Training Loss 0.057396870478034934\n",
      "2022-03-26 23:53:40.135227 Epoch 250, Training Loss 0.058318679251939134\n",
      "2022-03-26 23:53:40.149750 Epoch 250, Training Loss 0.059247522136134566\n",
      "2022-03-26 23:53:40.164675 Epoch 250, Training Loss 0.05991627672291778\n",
      "2022-03-26 23:53:40.178505 Epoch 250, Training Loss 0.06054197518569429\n",
      "2022-03-26 23:53:40.193684 Epoch 250, Training Loss 0.061263790635196755\n",
      "2022-03-26 23:53:40.207383 Epoch 250, Training Loss 0.06225044438448708\n",
      "2022-03-26 23:53:40.221407 Epoch 250, Training Loss 0.06296741188792011\n",
      "2022-03-26 23:53:40.235393 Epoch 250, Training Loss 0.06398800301277424\n",
      "2022-03-26 23:53:40.250403 Epoch 250, Training Loss 0.06444623494697044\n",
      "2022-03-26 23:53:40.264365 Epoch 250, Training Loss 0.06527131567220859\n",
      "2022-03-26 23:53:40.279368 Epoch 250, Training Loss 0.0660984077874352\n",
      "2022-03-26 23:53:40.293670 Epoch 250, Training Loss 0.06672327372881458\n",
      "2022-03-26 23:53:40.308737 Epoch 250, Training Loss 0.06746755334574853\n",
      "2022-03-26 23:53:40.322500 Epoch 250, Training Loss 0.06807290337732076\n",
      "2022-03-26 23:53:40.337482 Epoch 250, Training Loss 0.0686925873732018\n",
      "2022-03-26 23:53:40.351527 Epoch 250, Training Loss 0.06951264537813719\n",
      "2022-03-26 23:53:40.365856 Epoch 250, Training Loss 0.07016532843375145\n",
      "2022-03-26 23:53:40.379771 Epoch 250, Training Loss 0.07088405564617928\n",
      "2022-03-26 23:53:40.394659 Epoch 250, Training Loss 0.0715177700190288\n",
      "2022-03-26 23:53:40.409285 Epoch 250, Training Loss 0.07236281769050051\n",
      "2022-03-26 23:53:40.423291 Epoch 250, Training Loss 0.0729033428689708\n",
      "2022-03-26 23:53:40.437201 Epoch 250, Training Loss 0.07387145057968471\n",
      "2022-03-26 23:53:40.452239 Epoch 250, Training Loss 0.07446785648460583\n",
      "2022-03-26 23:53:40.466196 Epoch 250, Training Loss 0.07528992648929586\n",
      "2022-03-26 23:53:40.481612 Epoch 250, Training Loss 0.07580740563095073\n",
      "2022-03-26 23:53:40.495642 Epoch 250, Training Loss 0.07667511610118934\n",
      "2022-03-26 23:53:40.510593 Epoch 250, Training Loss 0.07737359076814579\n",
      "2022-03-26 23:53:40.525152 Epoch 250, Training Loss 0.07811360255531642\n",
      "2022-03-26 23:53:40.539207 Epoch 250, Training Loss 0.0789211578381336\n",
      "2022-03-26 23:53:40.553322 Epoch 250, Training Loss 0.07965297566350464\n",
      "2022-03-26 23:53:40.568304 Epoch 250, Training Loss 0.08060167726043545\n",
      "2022-03-26 23:53:40.582326 Epoch 250, Training Loss 0.08123021673820817\n",
      "2022-03-26 23:53:40.597517 Epoch 250, Training Loss 0.08190693662447088\n",
      "2022-03-26 23:53:40.611379 Epoch 250, Training Loss 0.08300192250162744\n",
      "2022-03-26 23:53:40.626373 Epoch 250, Training Loss 0.08376606883447799\n",
      "2022-03-26 23:53:40.640503 Epoch 250, Training Loss 0.08452056165393966\n",
      "2022-03-26 23:53:40.655536 Epoch 250, Training Loss 0.084938582976151\n",
      "2022-03-26 23:53:40.669412 Epoch 250, Training Loss 0.08562092467799516\n",
      "2022-03-26 23:53:40.683416 Epoch 250, Training Loss 0.08621536484917107\n",
      "2022-03-26 23:53:40.698525 Epoch 250, Training Loss 0.08712005283674011\n",
      "2022-03-26 23:53:40.713520 Epoch 250, Training Loss 0.0878170859402098\n",
      "2022-03-26 23:53:40.727966 Epoch 250, Training Loss 0.08843003842226989\n",
      "2022-03-26 23:53:40.742565 Epoch 250, Training Loss 0.08908248679412295\n",
      "2022-03-26 23:53:40.756519 Epoch 250, Training Loss 0.08958018778839989\n",
      "2022-03-26 23:53:40.771553 Epoch 250, Training Loss 0.09036382270591034\n",
      "2022-03-26 23:53:40.785580 Epoch 250, Training Loss 0.09089508667931227\n",
      "2022-03-26 23:53:40.800530 Epoch 250, Training Loss 0.09179191889665315\n",
      "2022-03-26 23:53:40.814550 Epoch 250, Training Loss 0.09239228934888034\n",
      "2022-03-26 23:53:40.828650 Epoch 250, Training Loss 0.09303774576052985\n",
      "2022-03-26 23:53:40.843481 Epoch 250, Training Loss 0.09389401877017887\n",
      "2022-03-26 23:53:40.857521 Epoch 250, Training Loss 0.09470735959079869\n",
      "2022-03-26 23:53:40.871549 Epoch 250, Training Loss 0.09545521746815928\n",
      "2022-03-26 23:53:40.886496 Epoch 250, Training Loss 0.09607869699178144\n",
      "2022-03-26 23:53:40.900525 Epoch 250, Training Loss 0.09677878410919853\n",
      "2022-03-26 23:53:40.915555 Epoch 250, Training Loss 0.09753268766586128\n",
      "2022-03-26 23:53:40.929388 Epoch 250, Training Loss 0.09833792736158346\n",
      "2022-03-26 23:53:40.944729 Epoch 250, Training Loss 0.09910781578639584\n",
      "2022-03-26 23:53:40.958609 Epoch 250, Training Loss 0.0998859268320186\n",
      "2022-03-26 23:53:40.972638 Epoch 250, Training Loss 0.10044440101174747\n",
      "2022-03-26 23:53:40.987317 Epoch 250, Training Loss 0.10124686070720253\n",
      "2022-03-26 23:53:41.001337 Epoch 250, Training Loss 0.10209596820194702\n",
      "2022-03-26 23:53:41.016350 Epoch 250, Training Loss 0.10276581030672469\n",
      "2022-03-26 23:53:41.031365 Epoch 250, Training Loss 0.1035136454702948\n",
      "2022-03-26 23:53:41.045440 Epoch 250, Training Loss 0.10432823074748145\n",
      "2022-03-26 23:53:41.060356 Epoch 250, Training Loss 0.10517296370337992\n",
      "2022-03-26 23:53:41.074376 Epoch 250, Training Loss 0.1060678974136977\n",
      "2022-03-26 23:53:41.089367 Epoch 250, Training Loss 0.10702197364224192\n",
      "2022-03-26 23:53:41.103396 Epoch 250, Training Loss 0.10768682946024648\n",
      "2022-03-26 23:53:41.117520 Epoch 250, Training Loss 0.1085331130515584\n",
      "2022-03-26 23:53:41.132541 Epoch 250, Training Loss 0.1094381958627335\n",
      "2022-03-26 23:53:41.147363 Epoch 250, Training Loss 0.11022072397839383\n",
      "2022-03-26 23:53:41.161392 Epoch 250, Training Loss 0.11100142957914211\n",
      "2022-03-26 23:53:41.176366 Epoch 250, Training Loss 0.1116319644024305\n",
      "2022-03-26 23:53:41.190390 Epoch 250, Training Loss 0.11254931624283267\n",
      "2022-03-26 23:53:41.204419 Epoch 250, Training Loss 0.11320269938625033\n",
      "2022-03-26 23:53:41.219417 Epoch 250, Training Loss 0.1139086254722322\n",
      "2022-03-26 23:53:41.234438 Epoch 250, Training Loss 0.11474952597142485\n",
      "2022-03-26 23:53:41.249144 Epoch 250, Training Loss 0.115413038322078\n",
      "2022-03-26 23:53:41.263147 Epoch 250, Training Loss 0.11594037875494993\n",
      "2022-03-26 23:53:41.278165 Epoch 250, Training Loss 0.11655199661126832\n",
      "2022-03-26 23:53:41.292221 Epoch 250, Training Loss 0.11725513217851634\n",
      "2022-03-26 23:53:41.307234 Epoch 250, Training Loss 0.11817849356957409\n",
      "2022-03-26 23:53:41.322230 Epoch 250, Training Loss 0.11871670803907887\n",
      "2022-03-26 23:53:41.336466 Epoch 250, Training Loss 0.11943538883305571\n",
      "2022-03-26 23:53:41.351407 Epoch 250, Training Loss 0.12036698107676738\n",
      "2022-03-26 23:53:41.366438 Epoch 250, Training Loss 0.12115963954297478\n",
      "2022-03-26 23:53:41.381354 Epoch 250, Training Loss 0.12187195704568682\n",
      "2022-03-26 23:53:41.395472 Epoch 250, Training Loss 0.12286478200989306\n",
      "2022-03-26 23:53:41.410204 Epoch 250, Training Loss 0.12357410655149718\n",
      "2022-03-26 23:53:41.425232 Epoch 250, Training Loss 0.1242534762529461\n",
      "2022-03-26 23:53:41.439272 Epoch 250, Training Loss 0.12498025230282103\n",
      "2022-03-26 23:53:41.454381 Epoch 250, Training Loss 0.12569859597231725\n",
      "2022-03-26 23:53:41.468420 Epoch 250, Training Loss 0.1266136967083987\n",
      "2022-03-26 23:53:41.483473 Epoch 250, Training Loss 0.12742799516681516\n",
      "2022-03-26 23:53:41.497477 Epoch 250, Training Loss 0.12828900575485375\n",
      "2022-03-26 23:53:41.512364 Epoch 250, Training Loss 0.12891176270554439\n",
      "2022-03-26 23:53:41.527622 Epoch 250, Training Loss 0.12951364309129204\n",
      "2022-03-26 23:53:41.541637 Epoch 250, Training Loss 0.13024365181660713\n",
      "2022-03-26 23:53:41.556505 Epoch 250, Training Loss 0.1310270110435803\n",
      "2022-03-26 23:53:41.570513 Epoch 250, Training Loss 0.13199117371950614\n",
      "2022-03-26 23:53:41.584542 Epoch 250, Training Loss 0.13264963751101433\n",
      "2022-03-26 23:53:41.599508 Epoch 250, Training Loss 0.13344126143266477\n",
      "2022-03-26 23:53:41.618505 Epoch 250, Training Loss 0.13406595995511544\n",
      "2022-03-26 23:53:41.637475 Epoch 250, Training Loss 0.13490097811612325\n",
      "2022-03-26 23:53:41.657417 Epoch 250, Training Loss 0.1356024437624475\n",
      "2022-03-26 23:53:41.676204 Epoch 250, Training Loss 0.13632152273374445\n",
      "2022-03-26 23:53:41.695222 Epoch 250, Training Loss 0.136947646965761\n",
      "2022-03-26 23:53:41.714410 Epoch 250, Training Loss 0.13768670454506984\n",
      "2022-03-26 23:53:41.734396 Epoch 250, Training Loss 0.1383002469759158\n",
      "2022-03-26 23:53:41.753585 Epoch 250, Training Loss 0.13920629337010787\n",
      "2022-03-26 23:53:41.772533 Epoch 250, Training Loss 0.1398405826762509\n",
      "2022-03-26 23:53:41.791486 Epoch 250, Training Loss 0.14050392825585192\n",
      "2022-03-26 23:53:41.810379 Epoch 250, Training Loss 0.14121335752479866\n",
      "2022-03-26 23:53:41.829324 Epoch 250, Training Loss 0.14208401186996714\n",
      "2022-03-26 23:53:41.849377 Epoch 250, Training Loss 0.14267943608943764\n",
      "2022-03-26 23:53:41.869413 Epoch 250, Training Loss 0.143733634172803\n",
      "2022-03-26 23:53:41.888364 Epoch 250, Training Loss 0.14448902258635177\n",
      "2022-03-26 23:53:41.907423 Epoch 250, Training Loss 0.14506122561366966\n",
      "2022-03-26 23:53:41.926571 Epoch 250, Training Loss 0.14586323210040628\n",
      "2022-03-26 23:53:41.946632 Epoch 250, Training Loss 0.1464642178067161\n",
      "2022-03-26 23:53:41.966507 Epoch 250, Training Loss 0.14710927253488995\n",
      "2022-03-26 23:53:41.986478 Epoch 250, Training Loss 0.14786471407431775\n",
      "2022-03-26 23:53:42.005516 Epoch 250, Training Loss 0.14848779404864593\n",
      "2022-03-26 23:53:42.024412 Epoch 250, Training Loss 0.1491699567841142\n",
      "2022-03-26 23:53:42.043515 Epoch 250, Training Loss 0.14995618625675017\n",
      "2022-03-26 23:53:42.063526 Epoch 250, Training Loss 0.15082455939039244\n",
      "2022-03-26 23:53:42.082529 Epoch 250, Training Loss 0.15143818654062802\n",
      "2022-03-26 23:53:42.102506 Epoch 250, Training Loss 0.15202803254279945\n",
      "2022-03-26 23:53:42.121529 Epoch 250, Training Loss 0.1525872824213389\n",
      "2022-03-26 23:53:42.139585 Epoch 250, Training Loss 0.15341597288618308\n",
      "2022-03-26 23:53:42.159590 Epoch 250, Training Loss 0.1540811121692438\n",
      "2022-03-26 23:53:42.179531 Epoch 250, Training Loss 0.15511931440866816\n",
      "2022-03-26 23:53:42.198645 Epoch 250, Training Loss 0.15587854015705227\n",
      "2022-03-26 23:53:42.217488 Epoch 250, Training Loss 0.1567184294352446\n",
      "2022-03-26 23:53:42.237478 Epoch 250, Training Loss 0.1575430194511438\n",
      "2022-03-26 23:53:42.256527 Epoch 250, Training Loss 0.15844179125850463\n",
      "2022-03-26 23:53:42.276543 Epoch 250, Training Loss 0.1593784255825955\n",
      "2022-03-26 23:53:42.295605 Epoch 250, Training Loss 0.16005715281914568\n",
      "2022-03-26 23:53:42.315553 Epoch 250, Training Loss 0.16100368532530793\n",
      "2022-03-26 23:53:42.334480 Epoch 250, Training Loss 0.16172294478739618\n",
      "2022-03-26 23:53:42.354587 Epoch 250, Training Loss 0.1624530954357913\n",
      "2022-03-26 23:53:42.373683 Epoch 250, Training Loss 0.16324228250309633\n",
      "2022-03-26 23:53:42.393656 Epoch 250, Training Loss 0.1636531582802458\n",
      "2022-03-26 23:53:42.412577 Epoch 250, Training Loss 0.16438673116514446\n",
      "2022-03-26 23:53:42.432610 Epoch 250, Training Loss 0.16523671451279576\n",
      "2022-03-26 23:53:42.451227 Epoch 250, Training Loss 0.16595720090067295\n",
      "2022-03-26 23:53:42.472257 Epoch 250, Training Loss 0.16713583366492826\n",
      "2022-03-26 23:53:42.490438 Epoch 250, Training Loss 0.1678626314377236\n",
      "2022-03-26 23:53:42.510733 Epoch 250, Training Loss 0.16862887605224425\n",
      "2022-03-26 23:53:42.529770 Epoch 250, Training Loss 0.16948581203018004\n",
      "2022-03-26 23:53:42.548720 Epoch 250, Training Loss 0.17018236425679054\n",
      "2022-03-26 23:53:42.567593 Epoch 250, Training Loss 0.17110778566669016\n",
      "2022-03-26 23:53:42.588563 Epoch 250, Training Loss 0.17186404475013314\n",
      "2022-03-26 23:53:42.608533 Epoch 250, Training Loss 0.17258514311459974\n",
      "2022-03-26 23:53:42.627476 Epoch 250, Training Loss 0.1733449651380939\n",
      "2022-03-26 23:53:42.647395 Epoch 250, Training Loss 0.1740969663766949\n",
      "2022-03-26 23:53:42.667301 Epoch 250, Training Loss 0.17476601349880627\n",
      "2022-03-26 23:53:42.687306 Epoch 250, Training Loss 0.17539110471068137\n",
      "2022-03-26 23:53:42.707434 Epoch 250, Training Loss 0.17618305508590415\n",
      "2022-03-26 23:53:42.726498 Epoch 250, Training Loss 0.17695036915409595\n",
      "2022-03-26 23:53:42.745558 Epoch 250, Training Loss 0.17766501954601854\n",
      "2022-03-26 23:53:42.765560 Epoch 250, Training Loss 0.17846057455405553\n",
      "2022-03-26 23:53:42.784554 Epoch 250, Training Loss 0.17956770994626653\n",
      "2022-03-26 23:53:42.804683 Epoch 250, Training Loss 0.18028233305115224\n",
      "2022-03-26 23:53:42.823742 Epoch 250, Training Loss 0.18107282082595483\n",
      "2022-03-26 23:53:42.843434 Epoch 250, Training Loss 0.18183878182297777\n",
      "2022-03-26 23:53:42.862272 Epoch 250, Training Loss 0.18255327939224975\n",
      "2022-03-26 23:53:42.881294 Epoch 250, Training Loss 0.18343583530629687\n",
      "2022-03-26 23:53:42.901877 Epoch 250, Training Loss 0.1839491363300387\n",
      "2022-03-26 23:53:42.921220 Epoch 250, Training Loss 0.18490149019776708\n",
      "2022-03-26 23:53:42.940172 Epoch 250, Training Loss 0.18589604179115246\n",
      "2022-03-26 23:53:42.959223 Epoch 250, Training Loss 0.18645590688566419\n",
      "2022-03-26 23:53:42.979135 Epoch 250, Training Loss 0.18716478340156242\n",
      "2022-03-26 23:53:42.998198 Epoch 250, Training Loss 0.18808023681116226\n",
      "2022-03-26 23:53:43.019200 Epoch 250, Training Loss 0.1885511957852127\n",
      "2022-03-26 23:53:43.037743 Epoch 250, Training Loss 0.18922717892148\n",
      "2022-03-26 23:53:43.057792 Epoch 250, Training Loss 0.19016286665978638\n",
      "2022-03-26 23:53:43.076705 Epoch 250, Training Loss 0.1908527167175737\n",
      "2022-03-26 23:53:43.097272 Epoch 250, Training Loss 0.19147448836232694\n",
      "2022-03-26 23:53:43.117338 Epoch 250, Training Loss 0.19212227065087584\n",
      "2022-03-26 23:53:43.137303 Epoch 250, Training Loss 0.1928009890458163\n",
      "2022-03-26 23:53:43.156442 Epoch 250, Training Loss 0.1936960758836678\n",
      "2022-03-26 23:53:43.175442 Epoch 250, Training Loss 0.19416884407210533\n",
      "2022-03-26 23:53:43.194368 Epoch 250, Training Loss 0.19516516353010827\n",
      "2022-03-26 23:53:43.215426 Epoch 250, Training Loss 0.1958786507930292\n",
      "2022-03-26 23:53:43.234482 Epoch 250, Training Loss 0.19644097983837128\n",
      "2022-03-26 23:53:43.254498 Epoch 250, Training Loss 0.19695786453421463\n",
      "2022-03-26 23:53:43.273502 Epoch 250, Training Loss 0.19795235225459193\n",
      "2022-03-26 23:53:43.293510 Epoch 250, Training Loss 0.19866187657084305\n",
      "2022-03-26 23:53:43.312406 Epoch 250, Training Loss 0.1996024422099828\n",
      "2022-03-26 23:53:43.332387 Epoch 250, Training Loss 0.20024134805592733\n",
      "2022-03-26 23:53:43.351636 Epoch 250, Training Loss 0.20108560326001834\n",
      "2022-03-26 23:53:43.371401 Epoch 250, Training Loss 0.20192580664401774\n",
      "2022-03-26 23:53:43.390325 Epoch 250, Training Loss 0.20317721858506313\n",
      "2022-03-26 23:53:43.409398 Epoch 250, Training Loss 0.20403786765797363\n",
      "2022-03-26 23:53:43.429419 Epoch 250, Training Loss 0.2047559979855252\n",
      "2022-03-26 23:53:43.449499 Epoch 250, Training Loss 0.20541295130996753\n",
      "2022-03-26 23:53:43.469359 Epoch 250, Training Loss 0.2060516970160672\n",
      "2022-03-26 23:53:43.488340 Epoch 250, Training Loss 0.20679263983998458\n",
      "2022-03-26 23:53:43.507404 Epoch 250, Training Loss 0.2076537553840281\n",
      "2022-03-26 23:53:43.527370 Epoch 250, Training Loss 0.20855270127963532\n",
      "2022-03-26 23:53:43.547333 Epoch 250, Training Loss 0.20938187910010442\n",
      "2022-03-26 23:53:43.567373 Epoch 250, Training Loss 0.21005351788095197\n",
      "2022-03-26 23:53:43.586376 Epoch 250, Training Loss 0.21059922142254422\n",
      "2022-03-26 23:53:43.605428 Epoch 250, Training Loss 0.2111810635956352\n",
      "2022-03-26 23:53:43.624412 Epoch 250, Training Loss 0.21183422905252414\n",
      "2022-03-26 23:53:43.645544 Epoch 250, Training Loss 0.21253306066136227\n",
      "2022-03-26 23:53:43.664514 Epoch 250, Training Loss 0.21334686734334893\n",
      "2022-03-26 23:53:43.684563 Epoch 250, Training Loss 0.21402387999360215\n",
      "2022-03-26 23:53:43.704600 Epoch 250, Training Loss 0.21487265600420324\n",
      "2022-03-26 23:53:43.723604 Epoch 250, Training Loss 0.21582586804161902\n",
      "2022-03-26 23:53:43.742512 Epoch 250, Training Loss 0.21646085697824083\n",
      "2022-03-26 23:53:43.761357 Epoch 250, Training Loss 0.21738823893887307\n",
      "2022-03-26 23:53:43.781418 Epoch 250, Training Loss 0.21828014233990398\n",
      "2022-03-26 23:53:43.801545 Epoch 250, Training Loss 0.21905930096383594\n",
      "2022-03-26 23:53:43.820541 Epoch 250, Training Loss 0.2200497335866284\n",
      "2022-03-26 23:53:43.840549 Epoch 250, Training Loss 0.22088314669059061\n",
      "2022-03-26 23:53:43.860530 Epoch 250, Training Loss 0.22171967265093723\n",
      "2022-03-26 23:53:43.880592 Epoch 250, Training Loss 0.22241603356340658\n",
      "2022-03-26 23:53:43.899521 Epoch 250, Training Loss 0.22317986723864475\n",
      "2022-03-26 23:53:43.918388 Epoch 250, Training Loss 0.2238976839177139\n",
      "2022-03-26 23:53:43.937306 Epoch 250, Training Loss 0.22449179854996673\n",
      "2022-03-26 23:53:43.957428 Epoch 250, Training Loss 0.22539107996941832\n",
      "2022-03-26 23:53:43.976511 Epoch 250, Training Loss 0.22629098312171828\n",
      "2022-03-26 23:53:43.996207 Epoch 250, Training Loss 0.22711239442648484\n",
      "2022-03-26 23:53:44.016316 Epoch 250, Training Loss 0.22774235542168092\n",
      "2022-03-26 23:53:44.035482 Epoch 250, Training Loss 0.22858421996121517\n",
      "2022-03-26 23:53:44.055483 Epoch 250, Training Loss 0.22941726728168596\n",
      "2022-03-26 23:53:44.074574 Epoch 250, Training Loss 0.23043009455856459\n",
      "2022-03-26 23:53:44.094213 Epoch 250, Training Loss 0.2313657181189798\n",
      "2022-03-26 23:53:44.114117 Epoch 250, Training Loss 0.23199625370447594\n",
      "2022-03-26 23:53:44.134207 Epoch 250, Training Loss 0.23262813005148603\n",
      "2022-03-26 23:53:44.154209 Epoch 250, Training Loss 0.23334004289811225\n",
      "2022-03-26 23:53:44.173205 Epoch 250, Training Loss 0.2340367437552308\n",
      "2022-03-26 23:53:44.192223 Epoch 250, Training Loss 0.23480194772753266\n",
      "2022-03-26 23:53:44.211544 Epoch 250, Training Loss 0.23554012038366265\n",
      "2022-03-26 23:53:44.231590 Epoch 250, Training Loss 0.23624002678162606\n",
      "2022-03-26 23:53:44.250195 Epoch 250, Training Loss 0.2369021818689678\n",
      "2022-03-26 23:53:44.270240 Epoch 250, Training Loss 0.2375822457129998\n",
      "2022-03-26 23:53:44.290151 Epoch 250, Training Loss 0.2381647135061986\n",
      "2022-03-26 23:53:44.310182 Epoch 250, Training Loss 0.2388822838397282\n",
      "2022-03-26 23:53:44.330202 Epoch 250, Training Loss 0.23968865488038954\n",
      "2022-03-26 23:53:44.349211 Epoch 250, Training Loss 0.24049414099787203\n",
      "2022-03-26 23:53:44.368322 Epoch 250, Training Loss 0.24121477251010173\n",
      "2022-03-26 23:53:44.388325 Epoch 250, Training Loss 0.24217757990445626\n",
      "2022-03-26 23:53:44.407327 Epoch 250, Training Loss 0.24272816721588145\n",
      "2022-03-26 23:53:44.427321 Epoch 250, Training Loss 0.24348347331099499\n",
      "2022-03-26 23:53:44.447272 Epoch 250, Training Loss 0.2442184395878516\n",
      "2022-03-26 23:53:44.466552 Epoch 250, Training Loss 0.24528480143955603\n",
      "2022-03-26 23:53:44.487492 Epoch 250, Training Loss 0.24619964599761818\n",
      "2022-03-26 23:53:44.506517 Epoch 250, Training Loss 0.2470064223041315\n",
      "2022-03-26 23:53:44.525517 Epoch 250, Training Loss 0.24754396069537649\n",
      "2022-03-26 23:53:44.545610 Epoch 250, Training Loss 0.24844321155029794\n",
      "2022-03-26 23:53:44.564533 Epoch 250, Training Loss 0.24919911681691095\n",
      "2022-03-26 23:53:44.584530 Epoch 250, Training Loss 0.24989727074685303\n",
      "2022-03-26 23:53:44.603565 Epoch 250, Training Loss 0.2506537503369934\n",
      "2022-03-26 23:53:44.623601 Epoch 250, Training Loss 0.2515523166531492\n",
      "2022-03-26 23:53:44.642515 Epoch 250, Training Loss 0.25225455251038836\n",
      "2022-03-26 23:53:44.662533 Epoch 250, Training Loss 0.25316497085192013\n",
      "2022-03-26 23:53:44.682495 Epoch 250, Training Loss 0.2537881690827782\n",
      "2022-03-26 23:53:44.702543 Epoch 250, Training Loss 0.25465635509442186\n",
      "2022-03-26 23:53:44.721389 Epoch 250, Training Loss 0.25513398372913565\n",
      "2022-03-26 23:53:44.741403 Epoch 250, Training Loss 0.25585440174697915\n",
      "2022-03-26 23:53:44.760411 Epoch 250, Training Loss 0.25663420108273205\n",
      "2022-03-26 23:53:44.780502 Epoch 250, Training Loss 0.25718866505891164\n",
      "2022-03-26 23:53:44.800032 Epoch 250, Training Loss 0.2578574019624754\n",
      "2022-03-26 23:53:44.819077 Epoch 250, Training Loss 0.2585659276341538\n",
      "2022-03-26 23:53:44.838605 Epoch 250, Training Loss 0.2590919636247103\n",
      "2022-03-26 23:53:44.857491 Epoch 250, Training Loss 0.25983327162235287\n",
      "2022-03-26 23:53:44.877531 Epoch 250, Training Loss 0.26056735686329013\n",
      "2022-03-26 23:53:44.898507 Epoch 250, Training Loss 0.2610928977999236\n",
      "2022-03-26 23:53:44.918493 Epoch 250, Training Loss 0.261712571086786\n",
      "2022-03-26 23:53:44.938409 Epoch 250, Training Loss 0.2622196095069046\n",
      "2022-03-26 23:53:44.957421 Epoch 250, Training Loss 0.26319368629504347\n",
      "2022-03-26 23:53:44.977377 Epoch 250, Training Loss 0.26417920077243423\n",
      "2022-03-26 23:53:44.997410 Epoch 250, Training Loss 0.26524330862342854\n",
      "2022-03-26 23:53:45.016363 Epoch 250, Training Loss 0.2659754471858139\n",
      "2022-03-26 23:53:45.036322 Epoch 250, Training Loss 0.2666000032135288\n",
      "2022-03-26 23:53:45.057305 Epoch 250, Training Loss 0.26735930823151716\n",
      "2022-03-26 23:53:45.076244 Epoch 250, Training Loss 0.2681775134618935\n",
      "2022-03-26 23:53:45.097275 Epoch 250, Training Loss 0.26897827320544004\n",
      "2022-03-26 23:53:45.116380 Epoch 250, Training Loss 0.269584103770878\n",
      "2022-03-26 23:53:45.137379 Epoch 250, Training Loss 0.27037444596400345\n",
      "2022-03-26 23:53:45.157322 Epoch 250, Training Loss 0.2708717531636548\n",
      "2022-03-26 23:53:45.177256 Epoch 250, Training Loss 0.2715879268277332\n",
      "2022-03-26 23:53:45.196293 Epoch 250, Training Loss 0.2722424330080257\n",
      "2022-03-26 23:53:45.216298 Epoch 250, Training Loss 0.27287775396233627\n",
      "2022-03-26 23:53:45.235420 Epoch 250, Training Loss 0.2734770572094051\n",
      "2022-03-26 23:53:45.255398 Epoch 250, Training Loss 0.2742407515530696\n",
      "2022-03-26 23:53:45.274324 Epoch 250, Training Loss 0.27517424436176524\n",
      "2022-03-26 23:53:45.294274 Epoch 250, Training Loss 0.275989855037016\n",
      "2022-03-26 23:53:45.315367 Epoch 250, Training Loss 0.27674002087939426\n",
      "2022-03-26 23:53:45.335788 Epoch 250, Training Loss 0.27739151778733334\n",
      "2022-03-26 23:53:45.355671 Epoch 250, Training Loss 0.27810487692313424\n",
      "2022-03-26 23:53:45.375515 Epoch 250, Training Loss 0.27869928553891\n",
      "2022-03-26 23:53:45.395572 Epoch 250, Training Loss 0.2792478987323049\n",
      "2022-03-26 23:53:45.415643 Epoch 250, Training Loss 0.2801418696218135\n",
      "2022-03-26 23:53:45.434652 Epoch 250, Training Loss 0.2811601572024548\n",
      "2022-03-26 23:53:45.454628 Epoch 250, Training Loss 0.28174773902844286\n",
      "2022-03-26 23:53:45.473654 Epoch 250, Training Loss 0.2822222339603907\n",
      "2022-03-26 23:53:45.493550 Epoch 250, Training Loss 0.28298498530064703\n",
      "2022-03-26 23:53:45.512526 Epoch 250, Training Loss 0.2837185858536864\n",
      "2022-03-26 23:53:45.532511 Epoch 250, Training Loss 0.2843582723527918\n",
      "2022-03-26 23:53:45.553523 Epoch 250, Training Loss 0.2850762483332773\n",
      "2022-03-26 23:53:45.573535 Epoch 250, Training Loss 0.2858368322596221\n",
      "2022-03-26 23:53:45.592500 Epoch 250, Training Loss 0.28654079482226114\n",
      "2022-03-26 23:53:45.611504 Epoch 250, Training Loss 0.28715665108712435\n",
      "2022-03-26 23:53:45.631545 Epoch 250, Training Loss 0.287846008987378\n",
      "2022-03-26 23:53:45.651588 Epoch 250, Training Loss 0.2888819026520185\n",
      "2022-03-26 23:53:45.670538 Epoch 250, Training Loss 0.28979066448748264\n",
      "2022-03-26 23:53:45.690497 Epoch 250, Training Loss 0.2904955237875204\n",
      "2022-03-26 23:53:45.709501 Epoch 250, Training Loss 0.2910227554914592\n",
      "2022-03-26 23:53:45.729556 Epoch 250, Training Loss 0.29179105600889993\n",
      "2022-03-26 23:53:45.749477 Epoch 250, Training Loss 0.2926027480217502\n",
      "2022-03-26 23:53:45.769405 Epoch 250, Training Loss 0.29318640889871456\n",
      "2022-03-26 23:53:45.789327 Epoch 250, Training Loss 0.29409266406160484\n",
      "2022-03-26 23:53:45.809317 Epoch 250, Training Loss 0.2949675263270088\n",
      "2022-03-26 23:53:45.828410 Epoch 250, Training Loss 0.29568922294832556\n",
      "2022-03-26 23:53:45.848273 Epoch 250, Training Loss 0.29639441792464927\n",
      "2022-03-26 23:53:45.867298 Epoch 250, Training Loss 0.296966002001177\n",
      "2022-03-26 23:53:45.887373 Epoch 250, Training Loss 0.2978100324302073\n",
      "2022-03-26 23:53:45.907204 Epoch 250, Training Loss 0.2984040373045465\n",
      "2022-03-26 23:53:45.926675 Epoch 250, Training Loss 0.29893585746093176\n",
      "2022-03-26 23:53:45.946602 Epoch 250, Training Loss 0.2999373691542374\n",
      "2022-03-26 23:53:45.966595 Epoch 250, Training Loss 0.3005386690425751\n",
      "2022-03-26 23:53:45.987001 Epoch 250, Training Loss 0.3010974683420128\n",
      "2022-03-26 23:53:46.006504 Epoch 250, Training Loss 0.3016354296442188\n",
      "2022-03-26 23:53:46.026509 Epoch 250, Training Loss 0.30239273409556855\n",
      "2022-03-26 23:53:46.046502 Epoch 250, Training Loss 0.30317592144469774\n",
      "2022-03-26 23:53:46.065539 Epoch 250, Training Loss 0.3037448595932987\n",
      "2022-03-26 23:53:46.085550 Epoch 250, Training Loss 0.3045018028344035\n",
      "2022-03-26 23:53:46.104571 Epoch 250, Training Loss 0.305346495088409\n",
      "2022-03-26 23:53:46.124580 Epoch 250, Training Loss 0.3059779783267804\n",
      "2022-03-26 23:53:46.144260 Epoch 250, Training Loss 0.3067243598839816\n",
      "2022-03-26 23:53:46.163301 Epoch 250, Training Loss 0.3073915645975591\n",
      "2022-03-26 23:53:46.183284 Epoch 250, Training Loss 0.30801500494370376\n",
      "2022-03-26 23:53:46.203214 Epoch 250, Training Loss 0.30874151810813133\n",
      "2022-03-26 23:53:46.222248 Epoch 250, Training Loss 0.3093284415771894\n",
      "2022-03-26 23:53:46.241270 Epoch 250, Training Loss 0.3101648766823742\n",
      "2022-03-26 23:53:46.261274 Epoch 250, Training Loss 0.31088503692156216\n",
      "2022-03-26 23:53:46.288310 Epoch 250, Training Loss 0.31165698086819077\n",
      "2022-03-26 23:53:46.314508 Epoch 250, Training Loss 0.3122026507202012\n",
      "2022-03-26 23:53:46.341389 Epoch 250, Training Loss 0.3131003876019012\n",
      "2022-03-26 23:53:46.368510 Epoch 250, Training Loss 0.3138228020704616\n",
      "2022-03-26 23:53:46.395406 Epoch 250, Training Loss 0.3144436551024542\n",
      "2022-03-26 23:53:46.421418 Epoch 250, Training Loss 0.3151848294088603\n",
      "2022-03-26 23:53:46.448550 Epoch 250, Training Loss 0.3158123416592703\n",
      "2022-03-26 23:53:46.474551 Epoch 250, Training Loss 0.31661650969091887\n",
      "2022-03-26 23:53:46.500498 Epoch 250, Training Loss 0.31720093590066867\n",
      "2022-03-26 23:53:46.526531 Epoch 250, Training Loss 0.3179527641181141\n",
      "2022-03-26 23:53:46.553473 Epoch 250, Training Loss 0.3184447793856911\n",
      "2022-03-26 23:53:46.580544 Epoch 250, Training Loss 0.31904250665393935\n",
      "2022-03-26 23:53:46.606532 Epoch 250, Training Loss 0.3195175324254634\n",
      "2022-03-26 23:53:46.632585 Epoch 250, Training Loss 0.3200284477771091\n",
      "2022-03-26 23:53:46.658714 Epoch 250, Training Loss 0.32059248435832655\n",
      "2022-03-26 23:53:46.677990 Epoch 250, Training Loss 0.3213800076023697\n",
      "2022-03-26 23:53:46.695997 Epoch 250, Training Loss 0.32202959091157257\n",
      "2022-03-26 23:53:46.715026 Epoch 250, Training Loss 0.32303342017371334\n",
      "2022-03-26 23:53:46.733075 Epoch 250, Training Loss 0.3238738808790436\n",
      "2022-03-26 23:53:46.752384 Epoch 250, Training Loss 0.3245075538259028\n",
      "2022-03-26 23:53:46.770394 Epoch 250, Training Loss 0.3254205117673825\n",
      "2022-03-26 23:53:46.788423 Epoch 250, Training Loss 0.326372527595981\n",
      "2022-03-26 23:53:46.806363 Epoch 250, Training Loss 0.3272050682769712\n",
      "2022-03-26 23:53:46.824399 Epoch 250, Training Loss 0.3282646896208034\n",
      "2022-03-26 23:53:46.843373 Epoch 250, Training Loss 0.3288107440447259\n",
      "2022-03-26 23:53:46.862394 Epoch 250, Training Loss 0.3294616934588498\n",
      "2022-03-26 23:53:46.881502 Epoch 250, Training Loss 0.33031492846091387\n",
      "2022-03-26 23:53:46.899343 Epoch 250, Training Loss 0.33116738113296\n",
      "2022-03-26 23:53:46.918352 Epoch 250, Training Loss 0.3317202344117567\n",
      "2022-03-26 23:53:46.936256 Epoch 250, Training Loss 0.3324977775363971\n",
      "2022-03-26 23:53:46.955617 Epoch 250, Training Loss 0.3332134450183195\n",
      "2022-03-26 23:53:46.973524 Epoch 250, Training Loss 0.33393194676969973\n",
      "2022-03-26 23:53:46.991508 Epoch 250, Training Loss 0.334684607775315\n",
      "2022-03-26 23:53:47.010434 Epoch 250, Training Loss 0.3354371441599658\n",
      "2022-03-26 23:53:47.028480 Epoch 250, Training Loss 0.3362793828672765\n",
      "2022-03-26 23:53:47.049508 Epoch 250, Training Loss 0.3371723679173023\n",
      "2022-03-26 23:53:47.068354 Epoch 250, Training Loss 0.33802517981785335\n",
      "2022-03-26 23:53:47.086323 Epoch 250, Training Loss 0.33884071190948684\n",
      "2022-03-26 23:53:47.103998 Epoch 250, Training Loss 0.33932510814855776\n",
      "2022-03-26 23:53:47.122065 Epoch 250, Training Loss 0.340033877040724\n",
      "2022-03-26 23:53:47.140249 Epoch 250, Training Loss 0.34087157123686407\n",
      "2022-03-26 23:53:47.159274 Epoch 250, Training Loss 0.3415451360404339\n",
      "2022-03-26 23:53:47.178363 Epoch 250, Training Loss 0.3421260075038656\n",
      "2022-03-26 23:53:47.197379 Epoch 250, Training Loss 0.3427668111708463\n",
      "2022-03-26 23:53:47.216413 Epoch 250, Training Loss 0.3434762576656878\n",
      "2022-03-26 23:53:47.234536 Epoch 250, Training Loss 0.344326142840983\n",
      "2022-03-26 23:53:47.253521 Epoch 250, Training Loss 0.34511039278391376\n",
      "2022-03-26 23:53:47.271524 Epoch 250, Training Loss 0.34602493672724577\n",
      "2022-03-26 23:53:47.290784 Epoch 250, Training Loss 0.3466547575143292\n",
      "2022-03-26 23:53:47.308843 Epoch 250, Training Loss 0.3471873849630356\n",
      "2022-03-26 23:53:47.327791 Epoch 250, Training Loss 0.3477994951293292\n",
      "2022-03-26 23:53:47.346711 Epoch 250, Training Loss 0.3483713276474677\n",
      "2022-03-26 23:53:47.364660 Epoch 250, Training Loss 0.3490729139512762\n",
      "2022-03-26 23:53:47.382697 Epoch 250, Training Loss 0.3500163811628166\n",
      "2022-03-26 23:53:47.401989 Epoch 250, Training Loss 0.3506776053658532\n",
      "2022-03-26 23:53:47.419806 Epoch 250, Training Loss 0.3513676998636607\n",
      "2022-03-26 23:53:47.437864 Epoch 250, Training Loss 0.35240593045721275\n",
      "2022-03-26 23:53:47.456770 Epoch 250, Training Loss 0.35312435606404036\n",
      "2022-03-26 23:53:47.475818 Epoch 250, Training Loss 0.35372886172188517\n",
      "2022-03-26 23:53:47.494289 Epoch 250, Training Loss 0.3545922035984981\n",
      "2022-03-26 23:53:47.513490 Epoch 250, Training Loss 0.3555559257945746\n",
      "2022-03-26 23:53:47.531388 Epoch 250, Training Loss 0.3567247426951938\n",
      "2022-03-26 23:53:47.550331 Epoch 250, Training Loss 0.3574831406859791\n",
      "2022-03-26 23:53:47.568423 Epoch 250, Training Loss 0.3580887225811439\n",
      "2022-03-26 23:53:47.586799 Epoch 250, Training Loss 0.35884531619756116\n",
      "2022-03-26 23:53:47.605858 Epoch 250, Training Loss 0.3594607979516544\n",
      "2022-03-26 23:53:47.623821 Epoch 250, Training Loss 0.3602371429619582\n",
      "2022-03-26 23:53:47.641669 Epoch 250, Training Loss 0.3610405664690925\n",
      "2022-03-26 23:53:47.661339 Epoch 250, Training Loss 0.36152495584829386\n",
      "2022-03-26 23:53:47.679422 Epoch 250, Training Loss 0.36206403717665414\n",
      "2022-03-26 23:53:47.698492 Epoch 250, Training Loss 0.3627551430479035\n",
      "2022-03-26 23:53:47.717489 Epoch 250, Training Loss 0.3633968542756327\n",
      "2022-03-26 23:53:47.736121 Epoch 250, Training Loss 0.36437632337860437\n",
      "2022-03-26 23:53:47.754128 Epoch 250, Training Loss 0.36510847809979374\n",
      "2022-03-26 23:53:47.773151 Epoch 250, Training Loss 0.36575477484546964\n",
      "2022-03-26 23:53:47.790523 Epoch 250, Training Loss 0.36647591406427077\n",
      "2022-03-26 23:53:47.809545 Epoch 250, Training Loss 0.3671005139951511\n",
      "2022-03-26 23:53:47.827496 Epoch 250, Training Loss 0.3676472313111395\n",
      "2022-03-26 23:53:47.845504 Epoch 250, Training Loss 0.36846594028460705\n",
      "2022-03-26 23:53:47.864572 Epoch 250, Training Loss 0.3691890189409866\n",
      "2022-03-26 23:53:47.883499 Epoch 250, Training Loss 0.36991381774777954\n",
      "2022-03-26 23:53:47.902517 Epoch 250, Training Loss 0.37049217381135885\n",
      "2022-03-26 23:53:47.921472 Epoch 250, Training Loss 0.3711585666974792\n",
      "2022-03-26 23:53:47.939400 Epoch 250, Training Loss 0.37194024167402323\n",
      "2022-03-26 23:53:47.957384 Epoch 250, Training Loss 0.3725253201049307\n",
      "2022-03-26 23:53:47.975375 Epoch 250, Training Loss 0.3732409006951715\n",
      "2022-03-26 23:53:47.993384 Epoch 250, Training Loss 0.3743441751241074\n",
      "2022-03-26 23:53:48.012391 Epoch 250, Training Loss 0.3748549856721898\n",
      "2022-03-26 23:53:48.030395 Epoch 250, Training Loss 0.3753544253004176\n",
      "2022-03-26 23:53:48.049396 Epoch 250, Training Loss 0.37596940796088685\n",
      "2022-03-26 23:53:48.068421 Epoch 250, Training Loss 0.3766003497649946\n",
      "2022-03-26 23:53:48.086368 Epoch 250, Training Loss 0.37717584869288423\n",
      "2022-03-26 23:53:48.105404 Epoch 250, Training Loss 0.37790795760538876\n",
      "2022-03-26 23:53:48.122415 Epoch 250, Training Loss 0.3787063676911547\n",
      "2022-03-26 23:53:48.141121 Epoch 250, Training Loss 0.3793315857343966\n",
      "2022-03-26 23:53:48.159132 Epoch 250, Training Loss 0.38022469098458206\n",
      "2022-03-26 23:53:48.177138 Epoch 250, Training Loss 0.380724535192675\n",
      "2022-03-26 23:53:48.196074 Epoch 250, Training Loss 0.38163120171907916\n",
      "2022-03-26 23:53:48.215121 Epoch 250, Training Loss 0.3822094376587197\n",
      "2022-03-26 23:53:48.234202 Epoch 250, Training Loss 0.3829400660589223\n",
      "2022-03-26 23:53:48.253254 Epoch 250, Training Loss 0.3835207673594775\n",
      "2022-03-26 23:53:48.271286 Epoch 250, Training Loss 0.38437486434226753\n",
      "2022-03-26 23:53:48.290318 Epoch 250, Training Loss 0.3849313055996395\n",
      "2022-03-26 23:53:48.309334 Epoch 250, Training Loss 0.3857504014697526\n",
      "2022-03-26 23:53:48.329323 Epoch 250, Training Loss 0.38672294111355493\n",
      "2022-03-26 23:53:48.348341 Epoch 250, Training Loss 0.3877039424065129\n",
      "2022-03-26 23:53:48.366362 Epoch 250, Training Loss 0.388290973994738\n",
      "2022-03-26 23:53:48.394361 Epoch 250, Training Loss 0.38888408213167847\n",
      "2022-03-26 23:53:48.418364 Epoch 250, Training Loss 0.3896674870148949\n",
      "2022-03-26 23:53:48.443402 Epoch 250, Training Loss 0.3908677273013098\n",
      "2022-03-26 23:53:48.464415 Epoch 250, Training Loss 0.3915668734732796\n",
      "2022-03-26 23:53:48.484472 Epoch 250, Training Loss 0.39211771444743854\n",
      "2022-03-26 23:53:48.510382 Epoch 250, Training Loss 0.3928918626226123\n",
      "2022-03-26 23:53:48.538415 Epoch 250, Training Loss 0.39356283713941986\n",
      "2022-03-26 23:53:48.565356 Epoch 250, Training Loss 0.3941967601285261\n",
      "2022-03-26 23:53:48.591397 Epoch 250, Training Loss 0.39498878542876914\n",
      "2022-03-26 23:53:48.617385 Epoch 250, Training Loss 0.3958211836531339\n",
      "2022-03-26 23:53:48.643726 Epoch 250, Training Loss 0.3963924612840423\n",
      "2022-03-26 23:53:48.670937 Epoch 250, Training Loss 0.3970561020666986\n",
      "2022-03-26 23:53:48.696652 Epoch 250, Training Loss 0.3978181742036434\n",
      "2022-03-26 23:53:48.715654 Epoch 250, Training Loss 0.39884893478030137\n",
      "2022-03-26 23:53:48.733686 Epoch 250, Training Loss 0.39945968760706274\n",
      "2022-03-26 23:53:48.752734 Epoch 250, Training Loss 0.4001764411587849\n",
      "2022-03-26 23:53:48.769784 Epoch 250, Training Loss 0.40090856523922336\n",
      "2022-03-26 23:53:48.788040 Epoch 250, Training Loss 0.40152597423557124\n",
      "2022-03-26 23:53:48.806082 Epoch 250, Training Loss 0.4023635765856794\n",
      "2022-03-26 23:53:48.824086 Epoch 250, Training Loss 0.40313183845918804\n",
      "2022-03-26 23:53:48.842096 Epoch 250, Training Loss 0.4040312281883586\n",
      "2022-03-26 23:53:48.861117 Epoch 250, Training Loss 0.4048311103259206\n",
      "2022-03-26 23:53:48.879142 Epoch 250, Training Loss 0.4057361084558165\n",
      "2022-03-26 23:53:48.898224 Epoch 250, Training Loss 0.40653306013330476\n",
      "2022-03-26 23:53:48.918282 Epoch 250, Training Loss 0.407096077909555\n",
      "2022-03-26 23:53:48.936325 Epoch 250, Training Loss 0.4077910368171189\n",
      "2022-03-26 23:53:48.955337 Epoch 250, Training Loss 0.40838588248280916\n",
      "2022-03-26 23:53:48.973352 Epoch 250, Training Loss 0.4090389003762809\n",
      "2022-03-26 23:53:48.991275 Epoch 250, Training Loss 0.40940973185517293\n",
      "2022-03-26 23:53:49.010386 Epoch 250, Training Loss 0.410358634827387\n",
      "2022-03-26 23:53:49.029435 Epoch 250, Training Loss 0.4111406223091018\n",
      "2022-03-26 23:53:49.047401 Epoch 250, Training Loss 0.41179861863860695\n",
      "2022-03-26 23:53:49.065443 Epoch 250, Training Loss 0.4123429515020317\n",
      "2022-03-26 23:53:49.085423 Epoch 250, Training Loss 0.41323733078244396\n",
      "2022-03-26 23:53:49.103338 Epoch 250, Training Loss 0.41385368282532753\n",
      "2022-03-26 23:53:49.122331 Epoch 250, Training Loss 0.41493556779973645\n",
      "2022-03-26 23:53:49.140371 Epoch 250, Training Loss 0.4157386409961964\n",
      "2022-03-26 23:53:49.158355 Epoch 250, Training Loss 0.41622095610327126\n",
      "2022-03-26 23:53:49.177328 Epoch 250, Training Loss 0.41715943702803854\n",
      "2022-03-26 23:53:49.195354 Epoch 250, Training Loss 0.41772191763838845\n",
      "2022-03-26 23:53:49.214449 Epoch 250, Training Loss 0.41857582528877746\n",
      "2022-03-26 23:53:49.233347 Epoch 250, Training Loss 0.41922554320386607\n",
      "2022-03-26 23:53:49.252522 Epoch 250, Training Loss 0.4198760008415603\n",
      "2022-03-26 23:53:49.270586 Epoch 250, Training Loss 0.42083504399680116\n",
      "2022-03-26 23:53:49.288487 Epoch 250, Training Loss 0.42162899974057133\n",
      "2022-03-26 23:53:49.306534 Epoch 250, Training Loss 0.4224559287433429\n",
      "2022-03-26 23:53:49.324562 Epoch 250, Training Loss 0.423317720441867\n",
      "2022-03-26 23:53:49.343623 Epoch 250, Training Loss 0.4238233427943476\n",
      "2022-03-26 23:53:49.362541 Epoch 250, Training Loss 0.42420666411404717\n",
      "2022-03-26 23:53:49.381535 Epoch 250, Training Loss 0.42473248333272423\n",
      "2022-03-26 23:53:49.399857 Epoch 250, Training Loss 0.4255230226327696\n",
      "2022-03-26 23:53:49.418784 Epoch 250, Training Loss 0.42615459333447847\n",
      "2022-03-26 23:53:49.436515 Epoch 250, Training Loss 0.42665222382454004\n",
      "2022-03-26 23:53:49.454389 Epoch 250, Training Loss 0.4274505230090807\n",
      "2022-03-26 23:53:49.472407 Epoch 250, Training Loss 0.428293782922313\n",
      "2022-03-26 23:53:49.490720 Epoch 250, Training Loss 0.4290257922142668\n",
      "2022-03-26 23:53:49.508675 Epoch 250, Training Loss 0.429695324710263\n",
      "2022-03-26 23:53:49.527529 Epoch 250, Training Loss 0.4303869172511503\n",
      "2022-03-26 23:53:49.545949 Epoch 250, Training Loss 0.43118212690286317\n",
      "2022-03-26 23:53:49.565004 Epoch 250, Training Loss 0.4315927695968877\n",
      "2022-03-26 23:53:49.584058 Epoch 250, Training Loss 0.43258714778801366\n",
      "2022-03-26 23:53:49.602987 Epoch 250, Training Loss 0.43349960069064897\n",
      "2022-03-26 23:53:49.620058 Epoch 250, Training Loss 0.4341941916805399\n",
      "2022-03-26 23:53:49.639118 Epoch 250, Training Loss 0.4349648164742438\n",
      "2022-03-26 23:53:49.656214 Epoch 250, Training Loss 0.43572688114155284\n",
      "2022-03-26 23:53:49.674661 Epoch 250, Training Loss 0.4363368286577332\n",
      "2022-03-26 23:53:49.693537 Epoch 250, Training Loss 0.4373123562320724\n",
      "2022-03-26 23:53:49.711593 Epoch 250, Training Loss 0.437917680081809\n",
      "2022-03-26 23:53:49.730142 Epoch 250, Training Loss 0.43861591815948486\n",
      "2022-03-26 23:53:49.749140 Epoch 250, Training Loss 0.4392881504714946\n",
      "2022-03-26 23:53:49.767199 Epoch 250, Training Loss 0.43985252009938136\n",
      "2022-03-26 23:53:49.786354 Epoch 250, Training Loss 0.44047848819314367\n",
      "2022-03-26 23:53:49.804406 Epoch 250, Training Loss 0.44113901825359714\n",
      "2022-03-26 23:53:49.822393 Epoch 250, Training Loss 0.4417098955348934\n",
      "2022-03-26 23:53:49.839401 Epoch 250, Training Loss 0.4421873299209663\n",
      "2022-03-26 23:53:49.858390 Epoch 250, Training Loss 0.44287606608837154\n",
      "2022-03-26 23:53:49.876435 Epoch 250, Training Loss 0.44356328241355586\n",
      "2022-03-26 23:53:49.895636 Epoch 250, Training Loss 0.4444160759449005\n",
      "2022-03-26 23:53:49.913620 Epoch 250, Training Loss 0.44517055580683074\n",
      "2022-03-26 23:53:49.932563 Epoch 250, Training Loss 0.44603201777428925\n",
      "2022-03-26 23:53:49.950223 Epoch 250, Training Loss 0.44666216722534746\n",
      "2022-03-26 23:53:49.969300 Epoch 250, Training Loss 0.44733371454126697\n",
      "2022-03-26 23:53:49.987322 Epoch 250, Training Loss 0.4482014862930073\n",
      "2022-03-26 23:53:50.006351 Epoch 250, Training Loss 0.44895416917398456\n",
      "2022-03-26 23:53:50.024381 Epoch 250, Training Loss 0.449724877200773\n",
      "2022-03-26 23:53:50.042398 Epoch 250, Training Loss 0.4505217986186142\n",
      "2022-03-26 23:53:50.060422 Epoch 250, Training Loss 0.4511983191113338\n",
      "2022-03-26 23:53:50.079350 Epoch 250, Training Loss 0.45233418607650816\n",
      "2022-03-26 23:53:50.097287 Epoch 250, Training Loss 0.4532392710980857\n",
      "2022-03-26 23:53:50.116309 Epoch 250, Training Loss 0.4538943139488435\n",
      "2022-03-26 23:53:50.135285 Epoch 250, Training Loss 0.45466816006109234\n",
      "2022-03-26 23:53:50.153340 Epoch 250, Training Loss 0.4551970327982817\n",
      "2022-03-26 23:53:50.172367 Epoch 250, Training Loss 0.45612482840905105\n",
      "2022-03-26 23:53:50.189783 Epoch 250, Training Loss 0.45689810705764217\n",
      "2022-03-26 23:53:50.207742 Epoch 250, Training Loss 0.4576347399779293\n",
      "2022-03-26 23:53:50.226700 Epoch 250, Training Loss 0.4586141693698781\n",
      "2022-03-26 23:53:50.244770 Epoch 250, Training Loss 0.4594089561487403\n",
      "2022-03-26 23:53:50.263717 Epoch 250, Training Loss 0.45998590654881716\n",
      "2022-03-26 23:53:50.281598 Epoch 250, Training Loss 0.46077152968520096\n",
      "2022-03-26 23:53:50.301080 Epoch 250, Training Loss 0.46179382594497614\n",
      "2022-03-26 23:53:50.319550 Epoch 250, Training Loss 0.4627983360872854\n",
      "2022-03-26 23:53:50.338484 Epoch 250, Training Loss 0.4634400589767929\n",
      "2022-03-26 23:53:50.356553 Epoch 250, Training Loss 0.4644191637444679\n",
      "2022-03-26 23:53:50.374534 Epoch 250, Training Loss 0.465081695057547\n",
      "2022-03-26 23:53:50.392512 Epoch 250, Training Loss 0.4657068253325684\n",
      "2022-03-26 23:53:50.411560 Epoch 250, Training Loss 0.46662497093610444\n",
      "2022-03-26 23:53:50.429671 Epoch 250, Training Loss 0.4672437180643496\n",
      "2022-03-26 23:53:50.448512 Epoch 250, Training Loss 0.4679418649819806\n",
      "2022-03-26 23:53:50.466375 Epoch 250, Training Loss 0.4685022463197903\n",
      "2022-03-26 23:53:50.485400 Epoch 250, Training Loss 0.46933596948985856\n",
      "2022-03-26 23:53:50.503421 Epoch 250, Training Loss 0.4702450005370942\n",
      "2022-03-26 23:53:50.522424 Epoch 250, Training Loss 0.4710229038048888\n",
      "2022-03-26 23:53:50.540945 Epoch 250, Training Loss 0.47180698507124813\n",
      "2022-03-26 23:53:50.559664 Epoch 250, Training Loss 0.4723978564714837\n",
      "2022-03-26 23:53:50.578376 Epoch 250, Training Loss 0.4732341505682377\n",
      "2022-03-26 23:53:50.596390 Epoch 250, Training Loss 0.4737974891958334\n",
      "2022-03-26 23:53:50.615414 Epoch 250, Training Loss 0.4745043723860665\n",
      "2022-03-26 23:53:50.634000 Epoch 250, Training Loss 0.47531998740590137\n",
      "2022-03-26 23:53:50.652532 Epoch 250, Training Loss 0.4762449639913676\n",
      "2022-03-26 23:53:50.670573 Epoch 250, Training Loss 0.47726470121489767\n",
      "2022-03-26 23:53:50.689245 Epoch 250, Training Loss 0.4778982352875078\n",
      "2022-03-26 23:53:50.707356 Epoch 250, Training Loss 0.4783850713535343\n",
      "2022-03-26 23:53:50.726915 Epoch 250, Training Loss 0.47910793842104693\n",
      "2022-03-26 23:53:50.745683 Epoch 250, Training Loss 0.4798114629429015\n",
      "2022-03-26 23:53:50.764695 Epoch 250, Training Loss 0.4806022658143812\n",
      "2022-03-26 23:53:50.784688 Epoch 250, Training Loss 0.48128316412344\n",
      "2022-03-26 23:53:50.802942 Epoch 250, Training Loss 0.4821712510741275\n",
      "2022-03-26 23:53:50.821791 Epoch 250, Training Loss 0.4829485861922774\n",
      "2022-03-26 23:53:50.839449 Epoch 250, Training Loss 0.48365344663562676\n",
      "2022-03-26 23:53:50.858469 Epoch 250, Training Loss 0.4844840340068578\n",
      "2022-03-26 23:53:50.876532 Epoch 250, Training Loss 0.48526964952116425\n",
      "2022-03-26 23:53:50.895534 Epoch 250, Training Loss 0.48601314604587265\n",
      "2022-03-26 23:53:50.914557 Epoch 250, Training Loss 0.486533850507663\n",
      "2022-03-26 23:53:50.933562 Epoch 250, Training Loss 0.4871527038495559\n",
      "2022-03-26 23:53:50.952758 Epoch 250, Training Loss 0.48792708061082896\n",
      "2022-03-26 23:53:50.971593 Epoch 250, Training Loss 0.4885695906322631\n",
      "2022-03-26 23:53:50.989622 Epoch 250, Training Loss 0.48964360825088626\n",
      "2022-03-26 23:53:51.007682 Epoch 250, Training Loss 0.49044420168070535\n",
      "2022-03-26 23:53:51.026660 Epoch 250, Training Loss 0.4912029307745302\n",
      "2022-03-26 23:53:51.045134 Epoch 250, Training Loss 0.49193751892013016\n",
      "2022-03-26 23:53:51.063564 Epoch 250, Training Loss 0.49276159589400376\n",
      "2022-03-26 23:53:51.083498 Epoch 250, Training Loss 0.49364518330377694\n",
      "2022-03-26 23:53:51.103148 Epoch 250, Training Loss 0.4943586779601129\n",
      "2022-03-26 23:53:51.127391 Epoch 250, Training Loss 0.4951452757315258\n",
      "2022-03-26 23:53:51.153408 Epoch 250, Training Loss 0.49576282748938216\n",
      "2022-03-26 23:53:51.180421 Epoch 250, Training Loss 0.4967671832846254\n",
      "2022-03-26 23:53:51.206413 Epoch 250, Training Loss 0.4976401968151712\n",
      "2022-03-26 23:53:51.232534 Epoch 250, Training Loss 0.4982853218188981\n",
      "2022-03-26 23:53:51.258656 Epoch 250, Training Loss 0.49884932044217045\n",
      "2022-03-26 23:53:51.285713 Epoch 250, Training Loss 0.4993921784336305\n",
      "2022-03-26 23:53:51.311501 Epoch 250, Training Loss 0.5002148453231967\n",
      "2022-03-26 23:53:51.326571 Epoch 250, Training Loss 0.500796403078472\n",
      "2022-03-26 23:53:51.340511 Epoch 250, Training Loss 0.5017123365265024\n",
      "2022-03-26 23:53:51.354542 Epoch 250, Training Loss 0.5026721495877752\n",
      "2022-03-26 23:53:51.368581 Epoch 250, Training Loss 0.5033431404158283\n",
      "2022-03-26 23:53:51.382529 Epoch 250, Training Loss 0.5039984572040456\n",
      "2022-03-26 23:53:51.397609 Epoch 250, Training Loss 0.5049480063759763\n",
      "2022-03-26 23:53:51.412173 Epoch 250, Training Loss 0.5059395680570846\n",
      "2022-03-26 23:53:51.426203 Epoch 250, Training Loss 0.5066738866098092\n",
      "2022-03-26 23:53:51.447484 Epoch 250, Training Loss 0.5076449744003203\n",
      "2022-03-26 23:53:51.463508 Epoch 250, Training Loss 0.5081507878001693\n",
      "2022-03-26 23:53:51.478373 Epoch 250, Training Loss 0.5092054527357716\n",
      "2022-03-26 23:53:51.492376 Epoch 250, Training Loss 0.5102764260204856\n",
      "2022-03-26 23:53:51.507409 Epoch 250, Training Loss 0.5110979233022845\n",
      "2022-03-26 23:53:51.521414 Epoch 250, Training Loss 0.5116835595168117\n",
      "2022-03-26 23:53:51.536459 Epoch 250, Training Loss 0.512436487523796\n",
      "2022-03-26 23:53:51.550472 Epoch 250, Training Loss 0.5130826576667673\n",
      "2022-03-26 23:53:51.564708 Epoch 250, Training Loss 0.5135963718071008\n",
      "2022-03-26 23:53:51.579525 Epoch 250, Training Loss 0.5141832713809464\n",
      "2022-03-26 23:53:51.594238 Epoch 250, Training Loss 0.5148919048287984\n",
      "2022-03-26 23:53:51.608323 Epoch 250, Training Loss 0.5155205038731056\n",
      "2022-03-26 23:53:51.622326 Epoch 250, Training Loss 0.5163169048555062\n",
      "2022-03-26 23:53:51.636429 Epoch 250, Training Loss 0.5170889195350125\n",
      "2022-03-26 23:53:51.651459 Epoch 250, Training Loss 0.5178732668880917\n",
      "2022-03-26 23:53:51.665521 Epoch 250, Training Loss 0.518333474998279\n",
      "2022-03-26 23:53:51.680508 Epoch 250, Training Loss 0.5189516321015175\n",
      "2022-03-26 23:53:51.694812 Epoch 250, Training Loss 0.5195260095550581\n",
      "2022-03-26 23:53:51.708733 Epoch 250, Training Loss 0.5201538833587066\n",
      "2022-03-26 23:53:51.722755 Epoch 250, Training Loss 0.5207395762052682\n",
      "2022-03-26 23:53:51.736961 Epoch 250, Training Loss 0.5217039503176194\n",
      "2022-03-26 23:53:51.751993 Epoch 250, Training Loss 0.5227319408789315\n",
      "2022-03-26 23:53:51.766068 Epoch 250, Training Loss 0.523289449760676\n",
      "2022-03-26 23:53:51.782095 Epoch 250, Training Loss 0.5243414845460516\n",
      "2022-03-26 23:53:51.797405 Epoch 250, Training Loss 0.5249823030760831\n",
      "2022-03-26 23:53:51.811409 Epoch 250, Training Loss 0.5257854132396181\n",
      "2022-03-26 23:53:51.825795 Epoch 250, Training Loss 0.5265005018247668\n",
      "2022-03-26 23:53:51.839740 Epoch 250, Training Loss 0.5270432010483559\n",
      "2022-03-26 23:53:51.853738 Epoch 250, Training Loss 0.5278048554192418\n",
      "2022-03-26 23:53:51.867705 Epoch 250, Training Loss 0.5288336023192881\n",
      "2022-03-26 23:53:51.881732 Epoch 250, Training Loss 0.5294002947371329\n",
      "2022-03-26 23:53:51.896338 Epoch 250, Training Loss 0.5300741010843335\n",
      "2022-03-26 23:53:51.911337 Epoch 250, Training Loss 0.5309892772027599\n",
      "2022-03-26 23:53:51.925422 Epoch 250, Training Loss 0.5318097639495455\n",
      "2022-03-26 23:53:51.939315 Epoch 250, Training Loss 0.5325965403824511\n",
      "2022-03-26 23:53:51.953247 Epoch 250, Training Loss 0.5332593630875468\n",
      "2022-03-26 23:53:51.968293 Epoch 250, Training Loss 0.534058795827429\n",
      "2022-03-26 23:53:51.982242 Epoch 250, Training Loss 0.5348908275823154\n",
      "2022-03-26 23:53:51.997258 Epoch 250, Training Loss 0.5356823701001799\n",
      "2022-03-26 23:53:52.012272 Epoch 250, Training Loss 0.5364987052538815\n",
      "2022-03-26 23:53:52.026375 Epoch 250, Training Loss 0.5372595482546351\n",
      "2022-03-26 23:53:52.040380 Epoch 250, Training Loss 0.5380183218613915\n",
      "2022-03-26 23:53:52.054401 Epoch 250, Training Loss 0.5387035210800293\n",
      "2022-03-26 23:53:52.069513 Epoch 250, Training Loss 0.5393616427927066\n",
      "2022-03-26 23:53:52.083494 Epoch 250, Training Loss 0.540043362228157\n",
      "2022-03-26 23:53:52.098400 Epoch 250, Training Loss 0.5406991857320756\n",
      "2022-03-26 23:53:52.113419 Epoch 250, Training Loss 0.541475662375655\n",
      "2022-03-26 23:53:52.127392 Epoch 250, Training Loss 0.5419901124275553\n",
      "2022-03-26 23:53:52.142404 Epoch 250, Training Loss 0.542700646897716\n",
      "2022-03-26 23:53:52.156426 Epoch 250, Training Loss 0.5434156555272734\n",
      "2022-03-26 23:53:52.170462 Epoch 250, Training Loss 0.5443233306831716\n",
      "2022-03-26 23:53:52.184498 Epoch 250, Training Loss 0.5448957834478534\n",
      "2022-03-26 23:53:52.199378 Epoch 250, Training Loss 0.5454502285212812\n",
      "2022-03-26 23:53:52.214400 Epoch 250, Training Loss 0.546368397753257\n",
      "2022-03-26 23:53:52.228439 Epoch 250, Training Loss 0.5470464819151423\n",
      "2022-03-26 23:53:52.243752 Epoch 250, Training Loss 0.5475570067496556\n",
      "2022-03-26 23:53:52.258019 Epoch 250, Training Loss 0.5484123128987944\n",
      "2022-03-26 23:53:52.272055 Epoch 250, Training Loss 0.549036844459641\n",
      "2022-03-26 23:53:52.286109 Epoch 250, Training Loss 0.549790048385825\n",
      "2022-03-26 23:53:52.300078 Epoch 250, Training Loss 0.5505004407042433\n",
      "2022-03-26 23:53:52.314177 Epoch 250, Training Loss 0.5510836876642978\n",
      "2022-03-26 23:53:52.328197 Epoch 250, Training Loss 0.5523128974087098\n",
      "2022-03-26 23:53:52.343097 Epoch 250, Training Loss 0.5532270975963539\n",
      "2022-03-26 23:53:52.357126 Epoch 250, Training Loss 0.5541652779826118\n",
      "2022-03-26 23:53:52.371156 Epoch 250, Training Loss 0.5552212033049225\n",
      "2022-03-26 23:53:52.386171 Epoch 250, Training Loss 0.5561635174486034\n",
      "2022-03-26 23:53:52.400209 Epoch 250, Training Loss 0.5567029983643681\n",
      "2022-03-26 23:53:52.415255 Epoch 250, Training Loss 0.5575390553382962\n",
      "2022-03-26 23:53:52.429361 Epoch 250, Training Loss 0.5583614272534695\n",
      "2022-03-26 23:53:52.444374 Epoch 250, Training Loss 0.5592308202210594\n",
      "2022-03-26 23:53:52.458407 Epoch 250, Training Loss 0.5602519525896252\n",
      "2022-03-26 23:53:52.473410 Epoch 250, Training Loss 0.5610631979487436\n",
      "2022-03-26 23:53:52.486806 Epoch 250, Training Loss 0.562002435593349\n",
      "2022-03-26 23:53:52.500828 Epoch 250, Training Loss 0.5628928857691148\n",
      "2022-03-26 23:53:52.514822 Epoch 250, Training Loss 0.5633865280072098\n",
      "2022-03-26 23:53:52.529803 Epoch 250, Training Loss 0.5644935357296254\n",
      "2022-03-26 23:53:52.544674 Epoch 250, Training Loss 0.5650738196452255\n",
      "2022-03-26 23:53:52.558728 Epoch 250, Training Loss 0.5659102016245313\n",
      "2022-03-26 23:53:52.572859 Epoch 250, Training Loss 0.566808247352805\n",
      "2022-03-26 23:53:52.587383 Epoch 250, Training Loss 0.5677195129644536\n",
      "2022-03-26 23:53:52.602411 Epoch 250, Training Loss 0.5688424238463496\n",
      "2022-03-26 23:53:52.616522 Epoch 250, Training Loss 0.5696202157555944\n",
      "2022-03-26 23:53:52.631526 Epoch 250, Training Loss 0.5702704749143946\n",
      "2022-03-26 23:53:52.646207 Epoch 250, Training Loss 0.5713169398667562\n",
      "2022-03-26 23:53:52.660158 Epoch 250, Training Loss 0.5720382098804044\n",
      "2022-03-26 23:53:52.674544 Epoch 250, Training Loss 0.5726167100012455\n",
      "2022-03-26 23:53:52.689380 Epoch 250, Training Loss 0.5734172290395898\n",
      "2022-03-26 23:53:52.703408 Epoch 250, Training Loss 0.5742728175866939\n",
      "2022-03-26 23:53:52.717835 Epoch 250, Training Loss 0.575333657776913\n",
      "2022-03-26 23:53:52.731763 Epoch 250, Training Loss 0.5760175211502768\n",
      "2022-03-26 23:53:52.747413 Epoch 250, Training Loss 0.576672034495322\n",
      "2022-03-26 23:53:52.761531 Epoch 250, Training Loss 0.5776158227487598\n",
      "2022-03-26 23:53:52.775660 Epoch 250, Training Loss 0.578339233803932\n",
      "2022-03-26 23:53:52.789662 Epoch 250, Training Loss 0.5792404162456922\n",
      "2022-03-26 23:53:52.796640 Epoch 250, Training Loss 0.5800650257741093\n",
      "2022-03-27 00:05:14.642020 Epoch 300, Training Loss 0.0007357473873421359\n",
      "2022-03-27 00:05:14.662048 Epoch 300, Training Loss 0.0012060025768816623\n",
      "2022-03-27 00:05:14.680035 Epoch 300, Training Loss 0.0017011169429935152\n",
      "2022-03-27 00:05:14.699241 Epoch 300, Training Loss 0.0023461612289214073\n",
      "2022-03-27 00:05:14.718070 Epoch 300, Training Loss 0.003212218165702527\n",
      "2022-03-27 00:05:14.736957 Epoch 300, Training Loss 0.003930103748350802\n",
      "2022-03-27 00:05:14.756968 Epoch 300, Training Loss 0.004384715576915789\n",
      "2022-03-27 00:05:14.775974 Epoch 300, Training Loss 0.005291134919351934\n",
      "2022-03-27 00:05:14.794978 Epoch 300, Training Loss 0.006062298898806657\n",
      "2022-03-27 00:05:14.813983 Epoch 300, Training Loss 0.006684930656877015\n",
      "2022-03-27 00:05:14.834007 Epoch 300, Training Loss 0.007234921083425927\n",
      "2022-03-27 00:05:14.853904 Epoch 300, Training Loss 0.007961778537086819\n",
      "2022-03-27 00:05:14.873791 Epoch 300, Training Loss 0.008728036795125897\n",
      "2022-03-27 00:05:14.892812 Epoch 300, Training Loss 0.009400263695460756\n",
      "2022-03-27 00:05:14.912280 Epoch 300, Training Loss 0.010095426264931174\n",
      "2022-03-27 00:05:14.931319 Epoch 300, Training Loss 0.010677334102218413\n",
      "2022-03-27 00:05:14.950299 Epoch 300, Training Loss 0.011384325144845811\n",
      "2022-03-27 00:05:14.969842 Epoch 300, Training Loss 0.011991831576427841\n",
      "2022-03-27 00:05:14.988874 Epoch 300, Training Loss 0.012561832623713461\n",
      "2022-03-27 00:05:15.007833 Epoch 300, Training Loss 0.013217900720093866\n",
      "2022-03-27 00:05:15.026225 Epoch 300, Training Loss 0.013991918862628206\n",
      "2022-03-27 00:05:15.046312 Epoch 300, Training Loss 0.014680006162589774\n",
      "2022-03-27 00:05:15.065351 Epoch 300, Training Loss 0.015339925389765474\n",
      "2022-03-27 00:05:15.084279 Epoch 300, Training Loss 0.01585096425717444\n",
      "2022-03-27 00:05:15.103590 Epoch 300, Training Loss 0.016379552004892196\n",
      "2022-03-27 00:05:15.122594 Epoch 300, Training Loss 0.01696044824007527\n",
      "2022-03-27 00:05:15.141474 Epoch 300, Training Loss 0.017538921569314456\n",
      "2022-03-27 00:05:15.161477 Epoch 300, Training Loss 0.018182370173351843\n",
      "2022-03-27 00:05:15.180272 Epoch 300, Training Loss 0.018732702869283573\n",
      "2022-03-27 00:05:15.198313 Epoch 300, Training Loss 0.019505277847694923\n",
      "2022-03-27 00:05:15.217318 Epoch 300, Training Loss 0.02022382708461693\n",
      "2022-03-27 00:05:15.236308 Epoch 300, Training Loss 0.02103806296577844\n",
      "2022-03-27 00:05:15.255424 Epoch 300, Training Loss 0.021928363672607695\n",
      "2022-03-27 00:05:15.274387 Epoch 300, Training Loss 0.022673162886553713\n",
      "2022-03-27 00:05:15.293412 Epoch 300, Training Loss 0.023223633275312537\n",
      "2022-03-27 00:05:15.316447 Epoch 300, Training Loss 0.02399014786381246\n",
      "2022-03-27 00:05:15.342313 Epoch 300, Training Loss 0.024726863712301036\n",
      "2022-03-27 00:05:15.368419 Epoch 300, Training Loss 0.02555023526291713\n",
      "2022-03-27 00:05:15.395387 Epoch 300, Training Loss 0.026260154021670448\n",
      "2022-03-27 00:05:15.421476 Epoch 300, Training Loss 0.026828642848812406\n",
      "2022-03-27 00:05:15.448301 Epoch 300, Training Loss 0.02754670366301866\n",
      "2022-03-27 00:05:15.474292 Epoch 300, Training Loss 0.02800665776747877\n",
      "2022-03-27 00:05:15.500448 Epoch 300, Training Loss 0.029036344164777596\n",
      "2022-03-27 00:05:15.526434 Epoch 300, Training Loss 0.029531044194765408\n",
      "2022-03-27 00:05:15.552411 Epoch 300, Training Loss 0.030157304938187076\n",
      "2022-03-27 00:05:15.577474 Epoch 300, Training Loss 0.03071021656398578\n",
      "2022-03-27 00:05:15.603911 Epoch 300, Training Loss 0.0314770714782388\n",
      "2022-03-27 00:05:15.630409 Epoch 300, Training Loss 0.0322195016743277\n",
      "2022-03-27 00:05:15.655322 Epoch 300, Training Loss 0.033283280175360266\n",
      "2022-03-27 00:05:15.681414 Epoch 300, Training Loss 0.03399779512296857\n",
      "2022-03-27 00:05:15.707422 Epoch 300, Training Loss 0.034801994024030385\n",
      "2022-03-27 00:05:15.726475 Epoch 300, Training Loss 0.035560530126857025\n",
      "2022-03-27 00:05:15.744393 Epoch 300, Training Loss 0.03601627516777009\n",
      "2022-03-27 00:05:15.762441 Epoch 300, Training Loss 0.03678049261460219\n",
      "2022-03-27 00:05:15.780449 Epoch 300, Training Loss 0.03776842069900249\n",
      "2022-03-27 00:05:15.798857 Epoch 300, Training Loss 0.038456665623523394\n",
      "2022-03-27 00:05:15.816889 Epoch 300, Training Loss 0.03939003724118938\n",
      "2022-03-27 00:05:15.834278 Epoch 300, Training Loss 0.03985834171247604\n",
      "2022-03-27 00:05:15.852096 Epoch 300, Training Loss 0.040299896007913456\n",
      "2022-03-27 00:05:15.871112 Epoch 300, Training Loss 0.04104388964450573\n",
      "2022-03-27 00:05:15.889082 Epoch 300, Training Loss 0.04171489129590866\n",
      "2022-03-27 00:05:15.906167 Epoch 300, Training Loss 0.04230249343473283\n",
      "2022-03-27 00:05:15.925460 Epoch 300, Training Loss 0.04308347769862855\n",
      "2022-03-27 00:05:15.942921 Epoch 300, Training Loss 0.0436867605847166\n",
      "2022-03-27 00:05:15.960933 Epoch 300, Training Loss 0.04407731216886769\n",
      "2022-03-27 00:05:15.979421 Epoch 300, Training Loss 0.04482605458830324\n",
      "2022-03-27 00:05:15.997302 Epoch 300, Training Loss 0.04553937027826334\n",
      "2022-03-27 00:05:16.015285 Epoch 300, Training Loss 0.046205247835734924\n",
      "2022-03-27 00:05:16.033217 Epoch 300, Training Loss 0.046731907655211055\n",
      "2022-03-27 00:05:16.051279 Epoch 300, Training Loss 0.047384456943368056\n",
      "2022-03-27 00:05:16.068314 Epoch 300, Training Loss 0.048238892727495764\n",
      "2022-03-27 00:05:16.087333 Epoch 300, Training Loss 0.049238632059158266\n",
      "2022-03-27 00:05:16.104829 Epoch 300, Training Loss 0.050095598220520315\n",
      "2022-03-27 00:05:16.123695 Epoch 300, Training Loss 0.05076000090602718\n",
      "2022-03-27 00:05:16.141740 Epoch 300, Training Loss 0.05148492662040779\n",
      "2022-03-27 00:05:16.159746 Epoch 300, Training Loss 0.052242192122942344\n",
      "2022-03-27 00:05:16.178954 Epoch 300, Training Loss 0.053017468670445024\n",
      "2022-03-27 00:05:16.197978 Epoch 300, Training Loss 0.053765585774655844\n",
      "2022-03-27 00:05:16.216291 Epoch 300, Training Loss 0.054451814049954916\n",
      "2022-03-27 00:05:16.234216 Epoch 300, Training Loss 0.055126313320206254\n",
      "2022-03-27 00:05:16.251267 Epoch 300, Training Loss 0.05599303741741668\n",
      "2022-03-27 00:05:16.270290 Epoch 300, Training Loss 0.05689437252938595\n",
      "2022-03-27 00:05:16.288079 Epoch 300, Training Loss 0.05756960778742495\n",
      "2022-03-27 00:05:16.306598 Epoch 300, Training Loss 0.058181445921778374\n",
      "2022-03-27 00:05:16.325127 Epoch 300, Training Loss 0.0590725323885603\n",
      "2022-03-27 00:05:16.343137 Epoch 300, Training Loss 0.05969815379213494\n",
      "2022-03-27 00:05:16.361514 Epoch 300, Training Loss 0.06041092846704566\n",
      "2022-03-27 00:05:16.379534 Epoch 300, Training Loss 0.06093379115814443\n",
      "2022-03-27 00:05:16.398284 Epoch 300, Training Loss 0.06191190017763611\n",
      "2022-03-27 00:05:16.416381 Epoch 300, Training Loss 0.06261307237398289\n",
      "2022-03-27 00:05:16.433300 Epoch 300, Training Loss 0.06355592372167446\n",
      "2022-03-27 00:05:16.452297 Epoch 300, Training Loss 0.06400707535579077\n",
      "2022-03-27 00:05:16.470414 Epoch 300, Training Loss 0.06481162418642313\n",
      "2022-03-27 00:05:16.488083 Epoch 300, Training Loss 0.06566481802926953\n",
      "2022-03-27 00:05:16.506110 Epoch 300, Training Loss 0.06618162265519047\n",
      "2022-03-27 00:05:16.524150 Epoch 300, Training Loss 0.06674568107365952\n",
      "2022-03-27 00:05:16.542163 Epoch 300, Training Loss 0.06733632049597132\n",
      "2022-03-27 00:05:16.560197 Epoch 300, Training Loss 0.06789061968283885\n",
      "2022-03-27 00:05:16.578232 Epoch 300, Training Loss 0.06874372488092584\n",
      "2022-03-27 00:05:16.596159 Epoch 300, Training Loss 0.06911546292969638\n",
      "2022-03-27 00:05:16.614102 Epoch 300, Training Loss 0.06980963074185355\n",
      "2022-03-27 00:05:16.631994 Epoch 300, Training Loss 0.07042009008052709\n",
      "2022-03-27 00:05:16.650023 Epoch 300, Training Loss 0.07087647754822851\n",
      "2022-03-27 00:05:16.668932 Epoch 300, Training Loss 0.0718512471831973\n",
      "2022-03-27 00:05:16.685962 Epoch 300, Training Loss 0.07241443775194076\n",
      "2022-03-27 00:05:16.703979 Epoch 300, Training Loss 0.07305097351293735\n",
      "2022-03-27 00:05:16.721921 Epoch 300, Training Loss 0.07375611292431726\n",
      "2022-03-27 00:05:16.740908 Epoch 300, Training Loss 0.07460024114459982\n",
      "2022-03-27 00:05:16.758946 Epoch 300, Training Loss 0.07532597098813947\n",
      "2022-03-27 00:05:16.776962 Epoch 300, Training Loss 0.07609298619467889\n",
      "2022-03-27 00:05:16.794982 Epoch 300, Training Loss 0.07686860116241533\n",
      "2022-03-27 00:05:16.813024 Epoch 300, Training Loss 0.07754870929071665\n",
      "2022-03-27 00:05:16.830029 Epoch 300, Training Loss 0.07843063310589023\n",
      "2022-03-27 00:05:16.849072 Epoch 300, Training Loss 0.0793523966808758\n",
      "2022-03-27 00:05:16.866002 Epoch 300, Training Loss 0.07976717225578435\n",
      "2022-03-27 00:05:16.885048 Epoch 300, Training Loss 0.08026083427317002\n",
      "2022-03-27 00:05:16.903002 Epoch 300, Training Loss 0.08069251176646298\n",
      "2022-03-27 00:05:16.921029 Epoch 300, Training Loss 0.08133107256096647\n",
      "2022-03-27 00:05:16.939053 Epoch 300, Training Loss 0.08205957379182587\n",
      "2022-03-27 00:05:16.958102 Epoch 300, Training Loss 0.08279962033566916\n",
      "2022-03-27 00:05:16.976121 Epoch 300, Training Loss 0.0835245596935682\n",
      "2022-03-27 00:05:16.994173 Epoch 300, Training Loss 0.08397713890465934\n",
      "2022-03-27 00:05:17.013160 Epoch 300, Training Loss 0.08443566619435235\n",
      "2022-03-27 00:05:17.031221 Epoch 300, Training Loss 0.08502073452600738\n",
      "2022-03-27 00:05:17.049167 Epoch 300, Training Loss 0.08590067088451532\n",
      "2022-03-27 00:05:17.067303 Epoch 300, Training Loss 0.08667555085533415\n",
      "2022-03-27 00:05:17.086748 Epoch 300, Training Loss 0.08731751688910872\n",
      "2022-03-27 00:05:17.104782 Epoch 300, Training Loss 0.08791595457307518\n",
      "2022-03-27 00:05:17.122791 Epoch 300, Training Loss 0.08868985015260594\n",
      "2022-03-27 00:05:17.139822 Epoch 300, Training Loss 0.08956771338229899\n",
      "2022-03-27 00:05:17.158843 Epoch 300, Training Loss 0.09027260804877561\n",
      "2022-03-27 00:05:17.176856 Epoch 300, Training Loss 0.09094555089083474\n",
      "2022-03-27 00:05:17.195854 Epoch 300, Training Loss 0.09163843628847995\n",
      "2022-03-27 00:05:17.214268 Epoch 300, Training Loss 0.09234316967180013\n",
      "2022-03-27 00:05:17.233299 Epoch 300, Training Loss 0.09303490611750756\n",
      "2022-03-27 00:05:17.251749 Epoch 300, Training Loss 0.09386895726556363\n",
      "2022-03-27 00:05:17.269135 Epoch 300, Training Loss 0.09440854268000863\n",
      "2022-03-27 00:05:17.287205 Epoch 300, Training Loss 0.09515670071477475\n",
      "2022-03-27 00:05:17.305655 Epoch 300, Training Loss 0.0958652352280629\n",
      "2022-03-27 00:05:17.323571 Epoch 300, Training Loss 0.09662205933609887\n",
      "2022-03-27 00:05:17.341593 Epoch 300, Training Loss 0.09723232126296938\n",
      "2022-03-27 00:05:17.360123 Epoch 300, Training Loss 0.0980156557181912\n",
      "2022-03-27 00:05:17.377170 Epoch 300, Training Loss 0.09862650408769202\n",
      "2022-03-27 00:05:17.396211 Epoch 300, Training Loss 0.09918156834057225\n",
      "2022-03-27 00:05:17.415247 Epoch 300, Training Loss 0.09985250947268112\n",
      "2022-03-27 00:05:17.433311 Epoch 300, Training Loss 0.10018017773737993\n",
      "2022-03-27 00:05:17.451198 Epoch 300, Training Loss 0.10077705628731672\n",
      "2022-03-27 00:05:17.470223 Epoch 300, Training Loss 0.10143336280227622\n",
      "2022-03-27 00:05:17.489266 Epoch 300, Training Loss 0.10215267295117879\n",
      "2022-03-27 00:05:17.506569 Epoch 300, Training Loss 0.10278519088654872\n",
      "2022-03-27 00:05:17.524598 Epoch 300, Training Loss 0.10342601993504692\n",
      "2022-03-27 00:05:17.542471 Epoch 300, Training Loss 0.10413623488772555\n",
      "2022-03-27 00:05:17.561540 Epoch 300, Training Loss 0.10487278144987648\n",
      "2022-03-27 00:05:17.579533 Epoch 300, Training Loss 0.10565437990076401\n",
      "2022-03-27 00:05:17.597404 Epoch 300, Training Loss 0.10626158804234946\n",
      "2022-03-27 00:05:17.615974 Epoch 300, Training Loss 0.1067615229912731\n",
      "2022-03-27 00:05:17.634662 Epoch 300, Training Loss 0.10767016371192835\n",
      "2022-03-27 00:05:17.653128 Epoch 300, Training Loss 0.10853863288374509\n",
      "2022-03-27 00:05:17.670527 Epoch 300, Training Loss 0.10915226113918187\n",
      "2022-03-27 00:05:17.689291 Epoch 300, Training Loss 0.10983329085285401\n",
      "2022-03-27 00:05:17.707292 Epoch 300, Training Loss 0.11049039532308993\n",
      "2022-03-27 00:05:17.725308 Epoch 300, Training Loss 0.11128916684776316\n",
      "2022-03-27 00:05:17.743217 Epoch 300, Training Loss 0.11229676019657603\n",
      "2022-03-27 00:05:17.761322 Epoch 300, Training Loss 0.11313615449706611\n",
      "2022-03-27 00:05:17.779364 Epoch 300, Training Loss 0.11396524123371105\n",
      "2022-03-27 00:05:17.797432 Epoch 300, Training Loss 0.11458010685718273\n",
      "2022-03-27 00:05:17.816411 Epoch 300, Training Loss 0.115314020906263\n",
      "2022-03-27 00:05:17.835282 Epoch 300, Training Loss 0.11618259213769527\n",
      "2022-03-27 00:05:17.853639 Epoch 300, Training Loss 0.11694917715418979\n",
      "2022-03-27 00:05:17.871490 Epoch 300, Training Loss 0.11761196616970365\n",
      "2022-03-27 00:05:17.889479 Epoch 300, Training Loss 0.11817312930398585\n",
      "2022-03-27 00:05:17.908406 Epoch 300, Training Loss 0.1189564167309905\n",
      "2022-03-27 00:05:17.926307 Epoch 300, Training Loss 0.1194919463999741\n",
      "2022-03-27 00:05:17.945341 Epoch 300, Training Loss 0.12008500141103554\n",
      "2022-03-27 00:05:17.963369 Epoch 300, Training Loss 0.1209192446354405\n",
      "2022-03-27 00:05:17.981446 Epoch 300, Training Loss 0.12169047142081249\n",
      "2022-03-27 00:05:17.999435 Epoch 300, Training Loss 0.12235730349102898\n",
      "2022-03-27 00:05:18.017300 Epoch 300, Training Loss 0.12309062827731032\n",
      "2022-03-27 00:05:18.034952 Epoch 300, Training Loss 0.12356724797764702\n",
      "2022-03-27 00:05:18.053967 Epoch 300, Training Loss 0.12425431590098554\n",
      "2022-03-27 00:05:18.073009 Epoch 300, Training Loss 0.12493209845727057\n",
      "2022-03-27 00:05:18.092071 Epoch 300, Training Loss 0.12561814349783046\n",
      "2022-03-27 00:05:18.110308 Epoch 300, Training Loss 0.1261872904151297\n",
      "2022-03-27 00:05:18.128268 Epoch 300, Training Loss 0.12687700274197952\n",
      "2022-03-27 00:05:18.146216 Epoch 300, Training Loss 0.12756126109139085\n",
      "2022-03-27 00:05:18.164950 Epoch 300, Training Loss 0.1284579085495771\n",
      "2022-03-27 00:05:18.182874 Epoch 300, Training Loss 0.12917803753824794\n",
      "2022-03-27 00:05:18.201198 Epoch 300, Training Loss 0.13003868379098985\n",
      "2022-03-27 00:05:18.219313 Epoch 300, Training Loss 0.13056369682254693\n",
      "2022-03-27 00:05:18.237285 Epoch 300, Training Loss 0.13113627809545267\n",
      "2022-03-27 00:05:18.255542 Epoch 300, Training Loss 0.13183439833581295\n",
      "2022-03-27 00:05:18.274558 Epoch 300, Training Loss 0.13263204911023455\n",
      "2022-03-27 00:05:18.293507 Epoch 300, Training Loss 0.13331656767736616\n",
      "2022-03-27 00:05:18.312449 Epoch 300, Training Loss 0.1341971563332526\n",
      "2022-03-27 00:05:18.330500 Epoch 300, Training Loss 0.13479257262576266\n",
      "2022-03-27 00:05:18.348320 Epoch 300, Training Loss 0.13577049581900888\n",
      "2022-03-27 00:05:18.366367 Epoch 300, Training Loss 0.1365322671125612\n",
      "2022-03-27 00:05:18.384281 Epoch 300, Training Loss 0.13752884023329792\n",
      "2022-03-27 00:05:18.402325 Epoch 300, Training Loss 0.1381271685404546\n",
      "2022-03-27 00:05:18.420424 Epoch 300, Training Loss 0.13883059367042064\n",
      "2022-03-27 00:05:18.438334 Epoch 300, Training Loss 0.13943150963472284\n",
      "2022-03-27 00:05:18.456357 Epoch 300, Training Loss 0.13989410555118795\n",
      "2022-03-27 00:05:18.475615 Epoch 300, Training Loss 0.1406191899572187\n",
      "2022-03-27 00:05:18.494650 Epoch 300, Training Loss 0.14144222651753585\n",
      "2022-03-27 00:05:18.512649 Epoch 300, Training Loss 0.14203195544459934\n",
      "2022-03-27 00:05:18.531031 Epoch 300, Training Loss 0.1427064958740683\n",
      "2022-03-27 00:05:18.556613 Epoch 300, Training Loss 0.14366222251101832\n",
      "2022-03-27 00:05:18.582905 Epoch 300, Training Loss 0.1443612343057647\n",
      "2022-03-27 00:05:18.608891 Epoch 300, Training Loss 0.14490766083950277\n",
      "2022-03-27 00:05:18.634953 Epoch 300, Training Loss 0.14585791147121077\n",
      "2022-03-27 00:05:18.661335 Epoch 300, Training Loss 0.14656742305859274\n",
      "2022-03-27 00:05:18.688098 Epoch 300, Training Loss 0.14704710683401892\n",
      "2022-03-27 00:05:18.714236 Epoch 300, Training Loss 0.14779612875503043\n",
      "2022-03-27 00:05:18.739835 Epoch 300, Training Loss 0.14847646348769097\n",
      "2022-03-27 00:05:18.758852 Epoch 300, Training Loss 0.14927976260252315\n",
      "2022-03-27 00:05:18.773536 Epoch 300, Training Loss 0.14987176908251573\n",
      "2022-03-27 00:05:18.787540 Epoch 300, Training Loss 0.15038783848285675\n",
      "2022-03-27 00:05:18.801735 Epoch 300, Training Loss 0.15104598923564871\n",
      "2022-03-27 00:05:18.815069 Epoch 300, Training Loss 0.15176874212444286\n",
      "2022-03-27 00:05:18.830072 Epoch 300, Training Loss 0.15267965163263825\n",
      "2022-03-27 00:05:18.844110 Epoch 300, Training Loss 0.15361194000067308\n",
      "2022-03-27 00:05:18.859150 Epoch 300, Training Loss 0.15437030704582438\n",
      "2022-03-27 00:05:18.873102 Epoch 300, Training Loss 0.1548985385376474\n",
      "2022-03-27 00:05:18.887099 Epoch 300, Training Loss 0.15538866905605092\n",
      "2022-03-27 00:05:18.901996 Epoch 300, Training Loss 0.1562547807955681\n",
      "2022-03-27 00:05:18.916000 Epoch 300, Training Loss 0.15710062344970604\n",
      "2022-03-27 00:05:18.930541 Epoch 300, Training Loss 0.1580386643519487\n",
      "2022-03-27 00:05:18.944037 Epoch 300, Training Loss 0.15886137560200508\n",
      "2022-03-27 00:05:18.958039 Epoch 300, Training Loss 0.1595826310574856\n",
      "2022-03-27 00:05:18.972393 Epoch 300, Training Loss 0.1603876445299524\n",
      "2022-03-27 00:05:18.986417 Epoch 300, Training Loss 0.1611998892196304\n",
      "2022-03-27 00:05:19.000430 Epoch 300, Training Loss 0.16171798525411454\n",
      "2022-03-27 00:05:19.014312 Epoch 300, Training Loss 0.16249604866175396\n",
      "2022-03-27 00:05:19.028318 Epoch 300, Training Loss 0.16331180209851326\n",
      "2022-03-27 00:05:19.042312 Epoch 300, Training Loss 0.16408308162866042\n",
      "2022-03-27 00:05:19.057307 Epoch 300, Training Loss 0.1648434797287597\n",
      "2022-03-27 00:05:19.071350 Epoch 300, Training Loss 0.165403114750867\n",
      "2022-03-27 00:05:19.086308 Epoch 300, Training Loss 0.16603720740741476\n",
      "2022-03-27 00:05:19.100405 Epoch 300, Training Loss 0.16708609450351247\n",
      "2022-03-27 00:05:19.113393 Epoch 300, Training Loss 0.16768308246836944\n",
      "2022-03-27 00:05:19.127447 Epoch 300, Training Loss 0.16826700314383983\n",
      "2022-03-27 00:05:19.141912 Epoch 300, Training Loss 0.16935697742892653\n",
      "2022-03-27 00:05:19.156185 Epoch 300, Training Loss 0.17002050761524065\n",
      "2022-03-27 00:05:19.170133 Epoch 300, Training Loss 0.1704441121853221\n",
      "2022-03-27 00:05:19.185160 Epoch 300, Training Loss 0.17143826057081638\n",
      "2022-03-27 00:05:19.199190 Epoch 300, Training Loss 0.17207931873895932\n",
      "2022-03-27 00:05:19.213207 Epoch 300, Training Loss 0.172694344121172\n",
      "2022-03-27 00:05:19.228259 Epoch 300, Training Loss 0.17350417611848973\n",
      "2022-03-27 00:05:19.242267 Epoch 300, Training Loss 0.17412531814154456\n",
      "2022-03-27 00:05:19.256223 Epoch 300, Training Loss 0.1749503250850741\n",
      "2022-03-27 00:05:19.270301 Epoch 300, Training Loss 0.17574943426777334\n",
      "2022-03-27 00:05:19.284727 Epoch 300, Training Loss 0.17661991757352638\n",
      "2022-03-27 00:05:19.298235 Epoch 300, Training Loss 0.17745232524926705\n",
      "2022-03-27 00:05:19.312957 Epoch 300, Training Loss 0.17813890734139612\n",
      "2022-03-27 00:05:19.326429 Epoch 300, Training Loss 0.1787090123157062\n",
      "2022-03-27 00:05:19.341460 Epoch 300, Training Loss 0.1795630769808884\n",
      "2022-03-27 00:05:19.355448 Epoch 300, Training Loss 0.18036510503810385\n",
      "2022-03-27 00:05:19.370478 Epoch 300, Training Loss 0.1811924935759181\n",
      "2022-03-27 00:05:19.384028 Epoch 300, Training Loss 0.181801873361668\n",
      "2022-03-27 00:05:19.397576 Epoch 300, Training Loss 0.1822337187693247\n",
      "2022-03-27 00:05:19.411371 Epoch 300, Training Loss 0.18299335001222314\n",
      "2022-03-27 00:05:19.425398 Epoch 300, Training Loss 0.1836297862288897\n",
      "2022-03-27 00:05:19.439420 Epoch 300, Training Loss 0.18434713498863112\n",
      "2022-03-27 00:05:19.453623 Epoch 300, Training Loss 0.18469521083185436\n",
      "2022-03-27 00:05:19.468626 Epoch 300, Training Loss 0.1855157583265963\n",
      "2022-03-27 00:05:19.482888 Epoch 300, Training Loss 0.1863041684755584\n",
      "2022-03-27 00:05:19.496891 Epoch 300, Training Loss 0.18684208884720913\n",
      "2022-03-27 00:05:19.511120 Epoch 300, Training Loss 0.18762022542679097\n",
      "2022-03-27 00:05:19.525052 Epoch 300, Training Loss 0.18833120693178737\n",
      "2022-03-27 00:05:19.539082 Epoch 300, Training Loss 0.18897405140997503\n",
      "2022-03-27 00:05:19.553095 Epoch 300, Training Loss 0.1896448695217557\n",
      "2022-03-27 00:05:19.568097 Epoch 300, Training Loss 0.1902340950487215\n",
      "2022-03-27 00:05:19.581445 Epoch 300, Training Loss 0.19100648763082217\n",
      "2022-03-27 00:05:19.596294 Epoch 300, Training Loss 0.19175448461109415\n",
      "2022-03-27 00:05:19.611329 Epoch 300, Training Loss 0.19235685608728462\n",
      "2022-03-27 00:05:19.625300 Epoch 300, Training Loss 0.1931496705774151\n",
      "2022-03-27 00:05:19.639320 Epoch 300, Training Loss 0.19391846454814268\n",
      "2022-03-27 00:05:19.653777 Epoch 300, Training Loss 0.1946879942017748\n",
      "2022-03-27 00:05:19.668083 Epoch 300, Training Loss 0.1950971779539762\n",
      "2022-03-27 00:05:19.683088 Epoch 300, Training Loss 0.195681808473509\n",
      "2022-03-27 00:05:19.697640 Epoch 300, Training Loss 0.1964486436465817\n",
      "2022-03-27 00:05:19.712065 Epoch 300, Training Loss 0.19711885908070734\n",
      "2022-03-27 00:05:19.726069 Epoch 300, Training Loss 0.19784950806051874\n",
      "2022-03-27 00:05:19.740098 Epoch 300, Training Loss 0.19869589089127757\n",
      "2022-03-27 00:05:19.754068 Epoch 300, Training Loss 0.19944532074586815\n",
      "2022-03-27 00:05:19.768289 Epoch 300, Training Loss 0.20008704432136262\n",
      "2022-03-27 00:05:19.782323 Epoch 300, Training Loss 0.2008279198423371\n",
      "2022-03-27 00:05:19.796314 Epoch 300, Training Loss 0.20152271251239434\n",
      "2022-03-27 00:05:19.810429 Epoch 300, Training Loss 0.2022039368177009\n",
      "2022-03-27 00:05:19.825414 Epoch 300, Training Loss 0.2029352083687892\n",
      "2022-03-27 00:05:19.839312 Epoch 300, Training Loss 0.20357087441265126\n",
      "2022-03-27 00:05:19.853337 Epoch 300, Training Loss 0.20404350913851463\n",
      "2022-03-27 00:05:19.868228 Epoch 300, Training Loss 0.20456899653005478\n",
      "2022-03-27 00:05:19.882885 Epoch 300, Training Loss 0.20546400211656185\n",
      "2022-03-27 00:05:19.896914 Epoch 300, Training Loss 0.2060431161195116\n",
      "2022-03-27 00:05:19.910950 Epoch 300, Training Loss 0.2067262476972302\n",
      "2022-03-27 00:05:19.925456 Epoch 300, Training Loss 0.2073620486137507\n",
      "2022-03-27 00:05:19.939508 Epoch 300, Training Loss 0.2081903366329115\n",
      "2022-03-27 00:05:19.953394 Epoch 300, Training Loss 0.20936549533053736\n",
      "2022-03-27 00:05:19.967427 Epoch 300, Training Loss 0.21026164834456676\n",
      "2022-03-27 00:05:19.981466 Epoch 300, Training Loss 0.21106440042290847\n",
      "2022-03-27 00:05:19.995307 Epoch 300, Training Loss 0.21148835781894987\n",
      "2022-03-27 00:05:20.009401 Epoch 300, Training Loss 0.21213107256938124\n",
      "2022-03-27 00:05:20.025399 Epoch 300, Training Loss 0.21282316717650274\n",
      "2022-03-27 00:05:20.039426 Epoch 300, Training Loss 0.2136499320759493\n",
      "2022-03-27 00:05:20.053463 Epoch 300, Training Loss 0.21444286310764224\n",
      "2022-03-27 00:05:20.067493 Epoch 300, Training Loss 0.21511567919455526\n",
      "2022-03-27 00:05:20.083105 Epoch 300, Training Loss 0.21578607565301763\n",
      "2022-03-27 00:05:20.097315 Epoch 300, Training Loss 0.21654156894635057\n",
      "2022-03-27 00:05:20.111356 Epoch 300, Training Loss 0.21719223550518454\n",
      "2022-03-27 00:05:20.124310 Epoch 300, Training Loss 0.21824043394659487\n",
      "2022-03-27 00:05:20.139330 Epoch 300, Training Loss 0.21874367657219967\n",
      "2022-03-27 00:05:20.153218 Epoch 300, Training Loss 0.21971094715015968\n",
      "2022-03-27 00:05:20.167873 Epoch 300, Training Loss 0.2205444966893062\n",
      "2022-03-27 00:05:20.180877 Epoch 300, Training Loss 0.2214073448077492\n",
      "2022-03-27 00:05:20.196066 Epoch 300, Training Loss 0.2225526607097567\n",
      "2022-03-27 00:05:20.210069 Epoch 300, Training Loss 0.22336891003886758\n",
      "2022-03-27 00:05:20.224055 Epoch 300, Training Loss 0.22420724974873732\n",
      "2022-03-27 00:05:20.238058 Epoch 300, Training Loss 0.22496204668908473\n",
      "2022-03-27 00:05:20.252264 Epoch 300, Training Loss 0.22579751805881101\n",
      "2022-03-27 00:05:20.266828 Epoch 300, Training Loss 0.22682448596600682\n",
      "2022-03-27 00:05:20.280845 Epoch 300, Training Loss 0.2273892950447624\n",
      "2022-03-27 00:05:20.295206 Epoch 300, Training Loss 0.2282043291860834\n",
      "2022-03-27 00:05:20.309235 Epoch 300, Training Loss 0.22874378315780475\n",
      "2022-03-27 00:05:20.323384 Epoch 300, Training Loss 0.2293032860512014\n",
      "2022-03-27 00:05:20.337405 Epoch 300, Training Loss 0.23021350492296927\n",
      "2022-03-27 00:05:20.351011 Epoch 300, Training Loss 0.2308490656678329\n",
      "2022-03-27 00:05:20.364903 Epoch 300, Training Loss 0.23139783759098834\n",
      "2022-03-27 00:05:20.379258 Epoch 300, Training Loss 0.23213169980994272\n",
      "2022-03-27 00:05:20.393234 Epoch 300, Training Loss 0.23269145053518397\n",
      "2022-03-27 00:05:20.407292 Epoch 300, Training Loss 0.2333911015935566\n",
      "2022-03-27 00:05:20.421334 Epoch 300, Training Loss 0.23380510093610915\n",
      "2022-03-27 00:05:20.435414 Epoch 300, Training Loss 0.23453210709649888\n",
      "2022-03-27 00:05:20.449427 Epoch 300, Training Loss 0.23524699186729958\n",
      "2022-03-27 00:05:20.463440 Epoch 300, Training Loss 0.235978386133833\n",
      "2022-03-27 00:05:20.477466 Epoch 300, Training Loss 0.2366734649366735\n",
      "2022-03-27 00:05:20.491510 Epoch 300, Training Loss 0.23746099061978138\n",
      "2022-03-27 00:05:20.506483 Epoch 300, Training Loss 0.23809493262597056\n",
      "2022-03-27 00:05:20.520576 Epoch 300, Training Loss 0.2387700727604844\n",
      "2022-03-27 00:05:20.535559 Epoch 300, Training Loss 0.23942402809324775\n",
      "2022-03-27 00:05:20.549587 Epoch 300, Training Loss 0.24014719394619202\n",
      "2022-03-27 00:05:20.565388 Epoch 300, Training Loss 0.24113291681117718\n",
      "2022-03-27 00:05:20.580433 Epoch 300, Training Loss 0.24175542116622487\n",
      "2022-03-27 00:05:20.596439 Epoch 300, Training Loss 0.2424783695613027\n",
      "2022-03-27 00:05:20.612416 Epoch 300, Training Loss 0.24329681244805035\n",
      "2022-03-27 00:05:20.628452 Epoch 300, Training Loss 0.24401249380215354\n",
      "2022-03-27 00:05:20.643414 Epoch 300, Training Loss 0.24472696477037562\n",
      "2022-03-27 00:05:20.659443 Epoch 300, Training Loss 0.24565503767231847\n",
      "2022-03-27 00:05:20.675439 Epoch 300, Training Loss 0.2464287232636186\n",
      "2022-03-27 00:05:20.691489 Epoch 300, Training Loss 0.24726816573563745\n",
      "2022-03-27 00:05:20.707484 Epoch 300, Training Loss 0.2480493408563497\n",
      "2022-03-27 00:05:20.723962 Epoch 300, Training Loss 0.2488973515341654\n",
      "2022-03-27 00:05:20.739991 Epoch 300, Training Loss 0.24960523428362044\n",
      "2022-03-27 00:05:20.756123 Epoch 300, Training Loss 0.2504349663815535\n",
      "2022-03-27 00:05:20.771133 Epoch 300, Training Loss 0.25140272279072295\n",
      "2022-03-27 00:05:20.787132 Epoch 300, Training Loss 0.2523664144984901\n",
      "2022-03-27 00:05:20.803154 Epoch 300, Training Loss 0.2529841015101089\n",
      "2022-03-27 00:05:20.819189 Epoch 300, Training Loss 0.2537768052514557\n",
      "2022-03-27 00:05:20.834242 Epoch 300, Training Loss 0.25438825023906003\n",
      "2022-03-27 00:05:20.850265 Epoch 300, Training Loss 0.25494591537339\n",
      "2022-03-27 00:05:20.865292 Epoch 300, Training Loss 0.2558473895120499\n",
      "2022-03-27 00:05:20.881338 Epoch 300, Training Loss 0.25647618166168634\n",
      "2022-03-27 00:05:20.897293 Epoch 300, Training Loss 0.2571835199661572\n",
      "2022-03-27 00:05:20.914053 Epoch 300, Training Loss 0.2579534535898882\n",
      "2022-03-27 00:05:20.930329 Epoch 300, Training Loss 0.258559182629256\n",
      "2022-03-27 00:05:20.946311 Epoch 300, Training Loss 0.2592411031351065\n",
      "2022-03-27 00:05:20.961393 Epoch 300, Training Loss 0.26002901327579525\n",
      "2022-03-27 00:05:20.977433 Epoch 300, Training Loss 0.2605475975424432\n",
      "2022-03-27 00:05:20.994424 Epoch 300, Training Loss 0.26116550540375283\n",
      "2022-03-27 00:05:21.010054 Epoch 300, Training Loss 0.261935636058183\n",
      "2022-03-27 00:05:21.025018 Epoch 300, Training Loss 0.26251187352725613\n",
      "2022-03-27 00:05:21.041039 Epoch 300, Training Loss 0.26354111792029017\n",
      "2022-03-27 00:05:21.057144 Epoch 300, Training Loss 0.2643443974463836\n",
      "2022-03-27 00:05:21.073166 Epoch 300, Training Loss 0.26478522063216287\n",
      "2022-03-27 00:05:21.089604 Epoch 300, Training Loss 0.26557435884195213\n",
      "2022-03-27 00:05:21.105612 Epoch 300, Training Loss 0.26627985214638283\n",
      "2022-03-27 00:05:21.122443 Epoch 300, Training Loss 0.267048219128338\n",
      "2022-03-27 00:05:21.137474 Epoch 300, Training Loss 0.2681117195759893\n",
      "2022-03-27 00:05:21.153393 Epoch 300, Training Loss 0.2688059044615997\n",
      "2022-03-27 00:05:21.169402 Epoch 300, Training Loss 0.26930351585835755\n",
      "2022-03-27 00:05:21.185408 Epoch 300, Training Loss 0.26971346376192235\n",
      "2022-03-27 00:05:21.201472 Epoch 300, Training Loss 0.2703039870618859\n",
      "2022-03-27 00:05:21.217442 Epoch 300, Training Loss 0.2710683401816946\n",
      "2022-03-27 00:05:21.233482 Epoch 300, Training Loss 0.27175794038778683\n",
      "2022-03-27 00:05:21.248488 Epoch 300, Training Loss 0.2725274691267697\n",
      "2022-03-27 00:05:21.264840 Epoch 300, Training Loss 0.2731440684679524\n",
      "2022-03-27 00:05:21.281137 Epoch 300, Training Loss 0.2737410629877959\n",
      "2022-03-27 00:05:21.297240 Epoch 300, Training Loss 0.2743633570878402\n",
      "2022-03-27 00:05:21.313229 Epoch 300, Training Loss 0.2750648788326537\n",
      "2022-03-27 00:05:21.330265 Epoch 300, Training Loss 0.2757652076461431\n",
      "2022-03-27 00:05:21.346102 Epoch 300, Training Loss 0.2765066404171917\n",
      "2022-03-27 00:05:21.363963 Epoch 300, Training Loss 0.2772293072527327\n",
      "2022-03-27 00:05:21.382996 Epoch 300, Training Loss 0.27792344983581385\n",
      "2022-03-27 00:05:21.403046 Epoch 300, Training Loss 0.27903075550523254\n",
      "2022-03-27 00:05:21.422320 Epoch 300, Training Loss 0.27970623040138304\n",
      "2022-03-27 00:05:21.442438 Epoch 300, Training Loss 0.2804544331777431\n",
      "2022-03-27 00:05:21.461481 Epoch 300, Training Loss 0.28108373311017176\n",
      "2022-03-27 00:05:21.480476 Epoch 300, Training Loss 0.28166607388145176\n",
      "2022-03-27 00:05:21.499880 Epoch 300, Training Loss 0.2824335608183575\n",
      "2022-03-27 00:05:21.519249 Epoch 300, Training Loss 0.28310833059613355\n",
      "2022-03-27 00:05:21.538867 Epoch 300, Training Loss 0.2840577046889478\n",
      "2022-03-27 00:05:21.558939 Epoch 300, Training Loss 0.2849270520765153\n",
      "2022-03-27 00:05:21.577702 Epoch 300, Training Loss 0.28540234683114857\n",
      "2022-03-27 00:05:21.597522 Epoch 300, Training Loss 0.28598516379171013\n",
      "2022-03-27 00:05:21.616562 Epoch 300, Training Loss 0.28671648160880786\n",
      "2022-03-27 00:05:21.636576 Epoch 300, Training Loss 0.2873340454476569\n",
      "2022-03-27 00:05:21.654929 Epoch 300, Training Loss 0.2877441141230371\n",
      "2022-03-27 00:05:21.674633 Epoch 300, Training Loss 0.2883137385634815\n",
      "2022-03-27 00:05:21.693616 Epoch 300, Training Loss 0.288869119704227\n",
      "2022-03-27 00:05:21.713897 Epoch 300, Training Loss 0.28991245686093253\n",
      "2022-03-27 00:05:21.732924 Epoch 300, Training Loss 0.2907841113750892\n",
      "2022-03-27 00:05:21.752966 Epoch 300, Training Loss 0.2916594601576895\n",
      "2022-03-27 00:05:21.773006 Epoch 300, Training Loss 0.2923577486935174\n",
      "2022-03-27 00:05:21.792323 Epoch 300, Training Loss 0.29313148573383957\n",
      "2022-03-27 00:05:21.810434 Epoch 300, Training Loss 0.29375231475628855\n",
      "2022-03-27 00:05:21.829487 Epoch 300, Training Loss 0.2941673332849122\n",
      "2022-03-27 00:05:21.848549 Epoch 300, Training Loss 0.2947904712632489\n",
      "2022-03-27 00:05:21.867994 Epoch 300, Training Loss 0.2954126449725817\n",
      "2022-03-27 00:05:21.886998 Epoch 300, Training Loss 0.2960397589313405\n",
      "2022-03-27 00:05:21.906765 Epoch 300, Training Loss 0.296668156371702\n",
      "2022-03-27 00:05:21.924924 Epoch 300, Training Loss 0.29718883209826086\n",
      "2022-03-27 00:05:21.945040 Epoch 300, Training Loss 0.2978140109259149\n",
      "2022-03-27 00:05:21.964011 Epoch 300, Training Loss 0.2986128646165819\n",
      "2022-03-27 00:05:21.984014 Epoch 300, Training Loss 0.2993322662304125\n",
      "2022-03-27 00:05:22.003313 Epoch 300, Training Loss 0.2999494232408836\n",
      "2022-03-27 00:05:22.022300 Epoch 300, Training Loss 0.3005270781114583\n",
      "2022-03-27 00:05:22.042331 Epoch 300, Training Loss 0.30107631376179894\n",
      "2022-03-27 00:05:22.061338 Epoch 300, Training Loss 0.30172225691930715\n",
      "2022-03-27 00:05:22.080346 Epoch 300, Training Loss 0.3023555110330167\n",
      "2022-03-27 00:05:22.099421 Epoch 300, Training Loss 0.3033530968991692\n",
      "2022-03-27 00:05:22.118410 Epoch 300, Training Loss 0.3039570757952493\n",
      "2022-03-27 00:05:22.138707 Epoch 300, Training Loss 0.30463987535527903\n",
      "2022-03-27 00:05:22.157634 Epoch 300, Training Loss 0.3054121337888186\n",
      "2022-03-27 00:05:22.176544 Epoch 300, Training Loss 0.30613975116359\n",
      "2022-03-27 00:05:22.198345 Epoch 300, Training Loss 0.30683653830262403\n",
      "2022-03-27 00:05:22.217127 Epoch 300, Training Loss 0.3073088331600589\n",
      "2022-03-27 00:05:22.236182 Epoch 300, Training Loss 0.30807329397982036\n",
      "2022-03-27 00:05:22.256189 Epoch 300, Training Loss 0.30880046111848347\n",
      "2022-03-27 00:05:22.275255 Epoch 300, Training Loss 0.30954647148051834\n",
      "2022-03-27 00:05:22.294215 Epoch 300, Training Loss 0.3104717450221176\n",
      "2022-03-27 00:05:22.314050 Epoch 300, Training Loss 0.31138396827156284\n",
      "2022-03-27 00:05:22.333076 Epoch 300, Training Loss 0.3122581159672164\n",
      "2022-03-27 00:05:22.352354 Epoch 300, Training Loss 0.3127757923682327\n",
      "2022-03-27 00:05:22.371267 Epoch 300, Training Loss 0.3133674722803218\n",
      "2022-03-27 00:05:22.391299 Epoch 300, Training Loss 0.31426596618674296\n",
      "2022-03-27 00:05:22.410274 Epoch 300, Training Loss 0.3149424285992332\n",
      "2022-03-27 00:05:22.429045 Epoch 300, Training Loss 0.31568953585441767\n",
      "2022-03-27 00:05:22.449004 Epoch 300, Training Loss 0.3164286638617211\n",
      "2022-03-27 00:05:22.468025 Epoch 300, Training Loss 0.3171710610542151\n",
      "2022-03-27 00:05:22.487169 Epoch 300, Training Loss 0.3176415863320651\n",
      "2022-03-27 00:05:22.506705 Epoch 300, Training Loss 0.3184616790174523\n",
      "2022-03-27 00:05:22.526309 Epoch 300, Training Loss 0.31935608680443384\n",
      "2022-03-27 00:05:22.546010 Epoch 300, Training Loss 0.3201438376437063\n",
      "2022-03-27 00:05:22.565142 Epoch 300, Training Loss 0.32064252638298535\n",
      "2022-03-27 00:05:22.584441 Epoch 300, Training Loss 0.321342467004076\n",
      "2022-03-27 00:05:22.604289 Epoch 300, Training Loss 0.32178805981908004\n",
      "2022-03-27 00:05:22.624092 Epoch 300, Training Loss 0.3224096402259129\n",
      "2022-03-27 00:05:22.643226 Epoch 300, Training Loss 0.3233511634266285\n",
      "2022-03-27 00:05:22.663009 Epoch 300, Training Loss 0.32404868990716423\n",
      "2022-03-27 00:05:22.682048 Epoch 300, Training Loss 0.32483146147197467\n",
      "2022-03-27 00:05:22.699955 Epoch 300, Training Loss 0.325233184277554\n",
      "2022-03-27 00:05:22.720045 Epoch 300, Training Loss 0.3260670954080494\n",
      "2022-03-27 00:05:22.741094 Epoch 300, Training Loss 0.32693318824481477\n",
      "2022-03-27 00:05:22.760003 Epoch 300, Training Loss 0.3275749310279441\n",
      "2022-03-27 00:05:22.779024 Epoch 300, Training Loss 0.32819239398859956\n",
      "2022-03-27 00:05:22.798963 Epoch 300, Training Loss 0.3290649384565061\n",
      "2022-03-27 00:05:22.817889 Epoch 300, Training Loss 0.3297651379233431\n",
      "2022-03-27 00:05:22.837726 Epoch 300, Training Loss 0.3306800126648315\n",
      "2022-03-27 00:05:22.856856 Epoch 300, Training Loss 0.3310754697798463\n",
      "2022-03-27 00:05:22.876957 Epoch 300, Training Loss 0.33199442850659266\n",
      "2022-03-27 00:05:22.896015 Epoch 300, Training Loss 0.33256896289870563\n",
      "2022-03-27 00:05:22.915127 Epoch 300, Training Loss 0.3331833483313051\n",
      "2022-03-27 00:05:22.934190 Epoch 300, Training Loss 0.3338920240816863\n",
      "2022-03-27 00:05:22.954237 Epoch 300, Training Loss 0.33477234840393066\n",
      "2022-03-27 00:05:22.973037 Epoch 300, Training Loss 0.33517897605438673\n",
      "2022-03-27 00:05:22.991998 Epoch 300, Training Loss 0.335676745380587\n",
      "2022-03-27 00:05:23.011116 Epoch 300, Training Loss 0.33650690324776006\n",
      "2022-03-27 00:05:23.031279 Epoch 300, Training Loss 0.33703429886447195\n",
      "2022-03-27 00:05:23.050313 Epoch 300, Training Loss 0.3379338190836065\n",
      "2022-03-27 00:05:23.070116 Epoch 300, Training Loss 0.3386717412020544\n",
      "2022-03-27 00:05:23.090660 Epoch 300, Training Loss 0.3390373124948243\n",
      "2022-03-27 00:05:23.109666 Epoch 300, Training Loss 0.3397813953859422\n",
      "2022-03-27 00:05:23.128697 Epoch 300, Training Loss 0.3406532414429023\n",
      "2022-03-27 00:05:23.147732 Epoch 300, Training Loss 0.3413841700767312\n",
      "2022-03-27 00:05:23.166616 Epoch 300, Training Loss 0.34207355640733333\n",
      "2022-03-27 00:05:23.185652 Epoch 300, Training Loss 0.3426370243815815\n",
      "2022-03-27 00:05:23.204542 Epoch 300, Training Loss 0.34348904854043977\n",
      "2022-03-27 00:05:23.224479 Epoch 300, Training Loss 0.34438164524562526\n",
      "2022-03-27 00:05:23.243424 Epoch 300, Training Loss 0.3450390721297325\n",
      "2022-03-27 00:05:23.262985 Epoch 300, Training Loss 0.34583034757000713\n",
      "2022-03-27 00:05:23.282251 Epoch 300, Training Loss 0.3465259513053138\n",
      "2022-03-27 00:05:23.302267 Epoch 300, Training Loss 0.34744172057379846\n",
      "2022-03-27 00:05:23.321333 Epoch 300, Training Loss 0.347913840161565\n",
      "2022-03-27 00:05:23.340439 Epoch 300, Training Loss 0.3485177815570246\n",
      "2022-03-27 00:05:23.359481 Epoch 300, Training Loss 0.3492076000594117\n",
      "2022-03-27 00:05:23.378405 Epoch 300, Training Loss 0.35015478714957565\n",
      "2022-03-27 00:05:23.397398 Epoch 300, Training Loss 0.3510497907543426\n",
      "2022-03-27 00:05:23.416481 Epoch 300, Training Loss 0.3521273574408363\n",
      "2022-03-27 00:05:23.436128 Epoch 300, Training Loss 0.3528759124333901\n",
      "2022-03-27 00:05:23.455845 Epoch 300, Training Loss 0.3536250978479605\n",
      "2022-03-27 00:05:23.475867 Epoch 300, Training Loss 0.35437363652926884\n",
      "2022-03-27 00:05:23.495319 Epoch 300, Training Loss 0.3549551180637706\n",
      "2022-03-27 00:05:23.514739 Epoch 300, Training Loss 0.3553794116696433\n",
      "2022-03-27 00:05:23.533658 Epoch 300, Training Loss 0.355938672409643\n",
      "2022-03-27 00:05:23.553671 Epoch 300, Training Loss 0.35673432658090615\n",
      "2022-03-27 00:05:23.572515 Epoch 300, Training Loss 0.35744071563186547\n",
      "2022-03-27 00:05:23.592502 Epoch 300, Training Loss 0.3582305486702248\n",
      "2022-03-27 00:05:23.611554 Epoch 300, Training Loss 0.3589983399566787\n",
      "2022-03-27 00:05:23.630601 Epoch 300, Training Loss 0.35962593986097807\n",
      "2022-03-27 00:05:23.650631 Epoch 300, Training Loss 0.36030163473028054\n",
      "2022-03-27 00:05:23.669694 Epoch 300, Training Loss 0.3610340056898039\n",
      "2022-03-27 00:05:23.688638 Epoch 300, Training Loss 0.36190637286819155\n",
      "2022-03-27 00:05:23.707933 Epoch 300, Training Loss 0.3625445647541519\n",
      "2022-03-27 00:05:23.726948 Epoch 300, Training Loss 0.36340256614605787\n",
      "2022-03-27 00:05:23.745965 Epoch 300, Training Loss 0.3639908748514512\n",
      "2022-03-27 00:05:23.765536 Epoch 300, Training Loss 0.3648147481634184\n",
      "2022-03-27 00:05:23.784010 Epoch 300, Training Loss 0.3654449225767799\n",
      "2022-03-27 00:05:23.804136 Epoch 300, Training Loss 0.36599235042281775\n",
      "2022-03-27 00:05:23.823105 Epoch 300, Training Loss 0.3668326612781076\n",
      "2022-03-27 00:05:23.843094 Epoch 300, Training Loss 0.36762269172826995\n",
      "2022-03-27 00:05:23.863168 Epoch 300, Training Loss 0.36814832980827905\n",
      "2022-03-27 00:05:23.882212 Epoch 300, Training Loss 0.36884937090489567\n",
      "2022-03-27 00:05:23.901254 Epoch 300, Training Loss 0.3694654227903737\n",
      "2022-03-27 00:05:23.920190 Epoch 300, Training Loss 0.37009448857258653\n",
      "2022-03-27 00:05:23.940265 Epoch 300, Training Loss 0.3708394656858176\n",
      "2022-03-27 00:05:23.959993 Epoch 300, Training Loss 0.3715768543350727\n",
      "2022-03-27 00:05:23.979014 Epoch 300, Training Loss 0.37233066802744363\n",
      "2022-03-27 00:05:23.998744 Epoch 300, Training Loss 0.37310141645124195\n",
      "2022-03-27 00:05:24.017688 Epoch 300, Training Loss 0.37366389454630633\n",
      "2022-03-27 00:05:24.037712 Epoch 300, Training Loss 0.3743025942150589\n",
      "2022-03-27 00:05:24.057603 Epoch 300, Training Loss 0.37479006077932275\n",
      "2022-03-27 00:05:24.078153 Epoch 300, Training Loss 0.37533347956512286\n",
      "2022-03-27 00:05:24.097439 Epoch 300, Training Loss 0.37600871139322706\n",
      "2022-03-27 00:05:24.116443 Epoch 300, Training Loss 0.3768028070021163\n",
      "2022-03-27 00:05:24.135132 Epoch 300, Training Loss 0.37720175178916865\n",
      "2022-03-27 00:05:24.155227 Epoch 300, Training Loss 0.37801793457754435\n",
      "2022-03-27 00:05:24.174652 Epoch 300, Training Loss 0.3788239736767376\n",
      "2022-03-27 00:05:24.193676 Epoch 300, Training Loss 0.37949324976605225\n",
      "2022-03-27 00:05:24.212580 Epoch 300, Training Loss 0.38019591642310246\n",
      "2022-03-27 00:05:24.231620 Epoch 300, Training Loss 0.3808812927025968\n",
      "2022-03-27 00:05:24.250453 Epoch 300, Training Loss 0.3814573048249535\n",
      "2022-03-27 00:05:24.270740 Epoch 300, Training Loss 0.38218491198614124\n",
      "2022-03-27 00:05:24.289710 Epoch 300, Training Loss 0.38282842697847225\n",
      "2022-03-27 00:05:24.310084 Epoch 300, Training Loss 0.38382862988487837\n",
      "2022-03-27 00:05:24.329776 Epoch 300, Training Loss 0.3846014670627501\n",
      "2022-03-27 00:05:24.349822 Epoch 300, Training Loss 0.3852608759918481\n",
      "2022-03-27 00:05:24.369866 Epoch 300, Training Loss 0.38579618286751116\n",
      "2022-03-27 00:05:24.389810 Epoch 300, Training Loss 0.3864962632775002\n",
      "2022-03-27 00:05:24.409814 Epoch 300, Training Loss 0.3874099867804276\n",
      "2022-03-27 00:05:24.429207 Epoch 300, Training Loss 0.38845559485885495\n",
      "2022-03-27 00:05:24.448174 Epoch 300, Training Loss 0.38910552901227763\n",
      "2022-03-27 00:05:24.467226 Epoch 300, Training Loss 0.3897335769422829\n",
      "2022-03-27 00:05:24.487216 Epoch 300, Training Loss 0.39036140665221397\n",
      "2022-03-27 00:05:24.507441 Epoch 300, Training Loss 0.3908549231641433\n",
      "2022-03-27 00:05:24.526615 Epoch 300, Training Loss 0.39153184145307907\n",
      "2022-03-27 00:05:24.545638 Epoch 300, Training Loss 0.3922932277555051\n",
      "2022-03-27 00:05:24.565517 Epoch 300, Training Loss 0.39306130685159923\n",
      "2022-03-27 00:05:24.585456 Epoch 300, Training Loss 0.39401808724074106\n",
      "2022-03-27 00:05:24.604401 Epoch 300, Training Loss 0.39472817795355913\n",
      "2022-03-27 00:05:24.624412 Epoch 300, Training Loss 0.39542236123853325\n",
      "2022-03-27 00:05:24.643530 Epoch 300, Training Loss 0.39623733646119647\n",
      "2022-03-27 00:05:24.663413 Epoch 300, Training Loss 0.397050417750083\n",
      "2022-03-27 00:05:24.683432 Epoch 300, Training Loss 0.39766053912584737\n",
      "2022-03-27 00:05:24.702435 Epoch 300, Training Loss 0.39847554544658614\n",
      "2022-03-27 00:05:24.722283 Epoch 300, Training Loss 0.3990776498832971\n",
      "2022-03-27 00:05:24.741333 Epoch 300, Training Loss 0.39958566274789287\n",
      "2022-03-27 00:05:24.760673 Epoch 300, Training Loss 0.4005169213732795\n",
      "2022-03-27 00:05:24.779927 Epoch 300, Training Loss 0.40121280079912347\n",
      "2022-03-27 00:05:24.798754 Epoch 300, Training Loss 0.401873465694125\n",
      "2022-03-27 00:05:24.818778 Epoch 300, Training Loss 0.4025708734989166\n",
      "2022-03-27 00:05:24.837793 Epoch 300, Training Loss 0.4034130029818591\n",
      "2022-03-27 00:05:24.857085 Epoch 300, Training Loss 0.4042262439532658\n",
      "2022-03-27 00:05:24.877254 Epoch 300, Training Loss 0.4050213008585488\n",
      "2022-03-27 00:05:24.897047 Epoch 300, Training Loss 0.40558219527649453\n",
      "2022-03-27 00:05:24.916018 Epoch 300, Training Loss 0.4061449579037059\n",
      "2022-03-27 00:05:24.936195 Epoch 300, Training Loss 0.40704777627192495\n",
      "2022-03-27 00:05:24.955756 Epoch 300, Training Loss 0.4077139016993515\n",
      "2022-03-27 00:05:24.974645 Epoch 300, Training Loss 0.40897168188601196\n",
      "2022-03-27 00:05:24.994576 Epoch 300, Training Loss 0.40954521790032494\n",
      "2022-03-27 00:05:25.014488 Epoch 300, Training Loss 0.41019956958110987\n",
      "2022-03-27 00:05:25.033324 Epoch 300, Training Loss 0.4109120341899145\n",
      "2022-03-27 00:05:25.052900 Epoch 300, Training Loss 0.4116547634000973\n",
      "2022-03-27 00:05:25.072894 Epoch 300, Training Loss 0.41243165529445003\n",
      "2022-03-27 00:05:25.092341 Epoch 300, Training Loss 0.4132418315810011\n",
      "2022-03-27 00:05:25.113284 Epoch 300, Training Loss 0.4139687634261368\n",
      "2022-03-27 00:05:25.132323 Epoch 300, Training Loss 0.41458224865329235\n",
      "2022-03-27 00:05:25.152304 Epoch 300, Training Loss 0.4152293169056363\n",
      "2022-03-27 00:05:25.171321 Epoch 300, Training Loss 0.4159166170736713\n",
      "2022-03-27 00:05:25.191351 Epoch 300, Training Loss 0.4166031048807037\n",
      "2022-03-27 00:05:25.210635 Epoch 300, Training Loss 0.4173287326646278\n",
      "2022-03-27 00:05:25.230593 Epoch 300, Training Loss 0.4180051376066549\n",
      "2022-03-27 00:05:25.249258 Epoch 300, Training Loss 0.41885899578976205\n",
      "2022-03-27 00:05:25.268181 Epoch 300, Training Loss 0.41988126118012403\n",
      "2022-03-27 00:05:25.288237 Epoch 300, Training Loss 0.42065915537764653\n",
      "2022-03-27 00:05:25.308183 Epoch 300, Training Loss 0.42151510612586574\n",
      "2022-03-27 00:05:25.328214 Epoch 300, Training Loss 0.4221981239059697\n",
      "2022-03-27 00:05:25.347206 Epoch 300, Training Loss 0.4227284483440087\n",
      "2022-03-27 00:05:25.366243 Epoch 300, Training Loss 0.4234960108919217\n",
      "2022-03-27 00:05:25.385345 Epoch 300, Training Loss 0.4241686235455906\n",
      "2022-03-27 00:05:25.405306 Epoch 300, Training Loss 0.42487907935591307\n",
      "2022-03-27 00:05:25.424311 Epoch 300, Training Loss 0.4255657402603218\n",
      "2022-03-27 00:05:25.444095 Epoch 300, Training Loss 0.4261153294988301\n",
      "2022-03-27 00:05:25.462999 Epoch 300, Training Loss 0.42665125726891295\n",
      "2022-03-27 00:05:25.482714 Epoch 300, Training Loss 0.42739646182493174\n",
      "2022-03-27 00:05:25.502275 Epoch 300, Training Loss 0.42792750933133733\n",
      "2022-03-27 00:05:25.522203 Epoch 300, Training Loss 0.42863878009416867\n",
      "2022-03-27 00:05:25.541270 Epoch 300, Training Loss 0.4294773939701602\n",
      "2022-03-27 00:05:25.561325 Epoch 300, Training Loss 0.4301321707341982\n",
      "2022-03-27 00:05:25.579859 Epoch 300, Training Loss 0.430782483552423\n",
      "2022-03-27 00:05:25.599733 Epoch 300, Training Loss 0.4316146686635054\n",
      "2022-03-27 00:05:25.619686 Epoch 300, Training Loss 0.43246802638101456\n",
      "2022-03-27 00:05:25.638709 Epoch 300, Training Loss 0.43301126764863346\n",
      "2022-03-27 00:05:25.658651 Epoch 300, Training Loss 0.4336340044389295\n",
      "2022-03-27 00:05:25.678197 Epoch 300, Training Loss 0.434472247729521\n",
      "2022-03-27 00:05:25.697256 Epoch 300, Training Loss 0.43520090376477105\n",
      "2022-03-27 00:05:25.716256 Epoch 300, Training Loss 0.43600747740024803\n",
      "2022-03-27 00:05:25.736201 Epoch 300, Training Loss 0.4366800373853625\n",
      "2022-03-27 00:05:25.757253 Epoch 300, Training Loss 0.4375568142403727\n",
      "2022-03-27 00:05:25.776463 Epoch 300, Training Loss 0.4381346863782619\n",
      "2022-03-27 00:05:25.795291 Epoch 300, Training Loss 0.4390856560005251\n",
      "2022-03-27 00:05:25.815235 Epoch 300, Training Loss 0.43966228055679585\n",
      "2022-03-27 00:05:25.834280 Epoch 300, Training Loss 0.4403340411384392\n",
      "2022-03-27 00:05:25.853718 Epoch 300, Training Loss 0.44103282991120274\n",
      "2022-03-27 00:05:25.873717 Epoch 300, Training Loss 0.44157531182936693\n",
      "2022-03-27 00:05:25.893035 Epoch 300, Training Loss 0.4423101781045689\n",
      "2022-03-27 00:05:25.913902 Epoch 300, Training Loss 0.44285829834011203\n",
      "2022-03-27 00:05:25.933435 Epoch 300, Training Loss 0.4433839914134091\n",
      "2022-03-27 00:05:25.953440 Epoch 300, Training Loss 0.4441498129264168\n",
      "2022-03-27 00:05:25.972483 Epoch 300, Training Loss 0.44492003458845036\n",
      "2022-03-27 00:05:25.992483 Epoch 300, Training Loss 0.4458286172288763\n",
      "2022-03-27 00:05:26.011485 Epoch 300, Training Loss 0.44641514751307493\n",
      "2022-03-27 00:05:26.030429 Epoch 300, Training Loss 0.4472806015435387\n",
      "2022-03-27 00:05:26.049433 Epoch 300, Training Loss 0.44771424088331746\n",
      "2022-03-27 00:05:26.068746 Epoch 300, Training Loss 0.44818152799783156\n",
      "2022-03-27 00:05:26.087698 Epoch 300, Training Loss 0.4489361556518413\n",
      "2022-03-27 00:05:26.107740 Epoch 300, Training Loss 0.4495448463636896\n",
      "2022-03-27 00:05:26.126763 Epoch 300, Training Loss 0.45021486392868754\n",
      "2022-03-27 00:05:26.147621 Epoch 300, Training Loss 0.45075995598912544\n",
      "2022-03-27 00:05:26.166479 Epoch 300, Training Loss 0.4517498004924306\n",
      "2022-03-27 00:05:26.187053 Epoch 300, Training Loss 0.4523850703026023\n",
      "2022-03-27 00:05:26.206199 Epoch 300, Training Loss 0.4530205624868803\n",
      "2022-03-27 00:05:26.232262 Epoch 300, Training Loss 0.4536477770570599\n",
      "2022-03-27 00:05:26.259332 Epoch 300, Training Loss 0.4544189124537246\n",
      "2022-03-27 00:05:26.286344 Epoch 300, Training Loss 0.4550858571020234\n",
      "2022-03-27 00:05:26.312438 Epoch 300, Training Loss 0.45576374430943023\n",
      "2022-03-27 00:05:26.337392 Epoch 300, Training Loss 0.45663301410425045\n",
      "2022-03-27 00:05:26.365285 Epoch 300, Training Loss 0.45730585130431767\n",
      "2022-03-27 00:05:26.392252 Epoch 300, Training Loss 0.45809073433699204\n",
      "2022-03-27 00:05:26.418303 Epoch 300, Training Loss 0.45874749337468307\n",
      "2022-03-27 00:05:26.444314 Epoch 300, Training Loss 0.4595081581712684\n",
      "2022-03-27 00:05:26.470321 Epoch 300, Training Loss 0.4601155158199008\n",
      "2022-03-27 00:05:26.495622 Epoch 300, Training Loss 0.4607898336847115\n",
      "2022-03-27 00:05:26.521569 Epoch 300, Training Loss 0.461500232329454\n",
      "2022-03-27 00:05:26.548478 Epoch 300, Training Loss 0.46205319685246937\n",
      "2022-03-27 00:05:26.573416 Epoch 300, Training Loss 0.4630436147646526\n",
      "2022-03-27 00:05:26.600289 Epoch 300, Training Loss 0.4637332251843284\n",
      "2022-03-27 00:05:26.622432 Epoch 300, Training Loss 0.46440611337609305\n",
      "2022-03-27 00:05:26.641475 Epoch 300, Training Loss 0.46503320488783406\n",
      "2022-03-27 00:05:26.659448 Epoch 300, Training Loss 0.46581350865266513\n",
      "2022-03-27 00:05:26.677401 Epoch 300, Training Loss 0.46667770808919923\n",
      "2022-03-27 00:05:26.695725 Epoch 300, Training Loss 0.46744672088976713\n",
      "2022-03-27 00:05:26.713657 Epoch 300, Training Loss 0.46806394329766177\n",
      "2022-03-27 00:05:26.731672 Epoch 300, Training Loss 0.46881810211769454\n",
      "2022-03-27 00:05:26.750574 Epoch 300, Training Loss 0.46920594317681347\n",
      "2022-03-27 00:05:26.769921 Epoch 300, Training Loss 0.46968214587329904\n",
      "2022-03-27 00:05:26.787963 Epoch 300, Training Loss 0.47051010461871884\n",
      "2022-03-27 00:05:26.806954 Epoch 300, Training Loss 0.47110970017245357\n",
      "2022-03-27 00:05:26.824962 Epoch 300, Training Loss 0.47183432496722094\n",
      "2022-03-27 00:05:26.843464 Epoch 300, Training Loss 0.4726371183572218\n",
      "2022-03-27 00:05:26.862487 Epoch 300, Training Loss 0.4734999833966765\n",
      "2022-03-27 00:05:26.881489 Epoch 300, Training Loss 0.474181752223188\n",
      "2022-03-27 00:05:26.898449 Epoch 300, Training Loss 0.47511420263658705\n",
      "2022-03-27 00:05:26.916455 Epoch 300, Training Loss 0.4757285299889572\n",
      "2022-03-27 00:05:26.935132 Epoch 300, Training Loss 0.47646980902270586\n",
      "2022-03-27 00:05:26.952836 Epoch 300, Training Loss 0.4773736951677391\n",
      "2022-03-27 00:05:26.971870 Epoch 300, Training Loss 0.47797739822084034\n",
      "2022-03-27 00:05:26.989950 Epoch 300, Training Loss 0.47859905179961565\n",
      "2022-03-27 00:05:27.009213 Epoch 300, Training Loss 0.4793826274935852\n",
      "2022-03-27 00:05:27.027283 Epoch 300, Training Loss 0.48038652718372055\n",
      "2022-03-27 00:05:27.045468 Epoch 300, Training Loss 0.4814151469475168\n",
      "2022-03-27 00:05:27.063771 Epoch 300, Training Loss 0.48193467295993014\n",
      "2022-03-27 00:05:27.082165 Epoch 300, Training Loss 0.48271286723863743\n",
      "2022-03-27 00:05:27.100273 Epoch 300, Training Loss 0.4837004009567563\n",
      "2022-03-27 00:05:27.117920 Epoch 300, Training Loss 0.48433508962164146\n",
      "2022-03-27 00:05:27.136251 Epoch 300, Training Loss 0.4854123466993537\n",
      "2022-03-27 00:05:27.154991 Epoch 300, Training Loss 0.48639541517590623\n",
      "2022-03-27 00:05:27.174052 Epoch 300, Training Loss 0.4873814251645447\n",
      "2022-03-27 00:05:27.192066 Epoch 300, Training Loss 0.48805076520308815\n",
      "2022-03-27 00:05:27.212101 Epoch 300, Training Loss 0.48879707739938555\n",
      "2022-03-27 00:05:27.230324 Epoch 300, Training Loss 0.4893331006359871\n",
      "2022-03-27 00:05:27.248406 Epoch 300, Training Loss 0.49007951748340634\n",
      "2022-03-27 00:05:27.266302 Epoch 300, Training Loss 0.49090992847976783\n",
      "2022-03-27 00:05:27.285089 Epoch 300, Training Loss 0.4918620137454909\n",
      "2022-03-27 00:05:27.303441 Epoch 300, Training Loss 0.4925451422743785\n",
      "2022-03-27 00:05:27.321475 Epoch 300, Training Loss 0.49326583674496705\n",
      "2022-03-27 00:05:27.339423 Epoch 300, Training Loss 0.4942461211815515\n",
      "2022-03-27 00:05:27.357434 Epoch 300, Training Loss 0.4949461868047104\n",
      "2022-03-27 00:05:27.377435 Epoch 300, Training Loss 0.4957874671882376\n",
      "2022-03-27 00:05:27.395192 Epoch 300, Training Loss 0.4967190852707914\n",
      "2022-03-27 00:05:27.414122 Epoch 300, Training Loss 0.4975557682459312\n",
      "2022-03-27 00:05:27.432424 Epoch 300, Training Loss 0.49814686689840254\n",
      "2022-03-27 00:05:27.451473 Epoch 300, Training Loss 0.4987954058305687\n",
      "2022-03-27 00:05:27.469424 Epoch 300, Training Loss 0.4993399109148308\n",
      "2022-03-27 00:05:27.487154 Epoch 300, Training Loss 0.5001166987678279\n",
      "2022-03-27 00:05:27.505158 Epoch 300, Training Loss 0.500886474614558\n",
      "2022-03-27 00:05:27.523123 Epoch 300, Training Loss 0.5015478033924956\n",
      "2022-03-27 00:05:27.541128 Epoch 300, Training Loss 0.5020178416196037\n",
      "2022-03-27 00:05:27.559420 Epoch 300, Training Loss 0.5027070229163255\n",
      "2022-03-27 00:05:27.577415 Epoch 300, Training Loss 0.5032803801548146\n",
      "2022-03-27 00:05:27.596323 Epoch 300, Training Loss 0.5039131465317953\n",
      "2022-03-27 00:05:27.614223 Epoch 300, Training Loss 0.5046143202525576\n",
      "2022-03-27 00:05:27.633291 Epoch 300, Training Loss 0.5055551424508204\n",
      "2022-03-27 00:05:27.653145 Epoch 300, Training Loss 0.5062045928309945\n",
      "2022-03-27 00:05:27.671150 Epoch 300, Training Loss 0.507182162817177\n",
      "2022-03-27 00:05:27.689372 Epoch 300, Training Loss 0.5079705138950397\n",
      "2022-03-27 00:05:27.708330 Epoch 300, Training Loss 0.5086568789104061\n",
      "2022-03-27 00:05:27.727313 Epoch 300, Training Loss 0.5094899446763042\n",
      "2022-03-27 00:05:27.745512 Epoch 300, Training Loss 0.5100950406640387\n",
      "2022-03-27 00:05:27.763530 Epoch 300, Training Loss 0.5107845905644205\n",
      "2022-03-27 00:05:27.781512 Epoch 300, Training Loss 0.5115263753992212\n",
      "2022-03-27 00:05:27.799477 Epoch 300, Training Loss 0.5121759951419538\n",
      "2022-03-27 00:05:27.817616 Epoch 300, Training Loss 0.5130305276502429\n",
      "2022-03-27 00:05:27.836572 Epoch 300, Training Loss 0.5134913076830032\n",
      "2022-03-27 00:05:27.855596 Epoch 300, Training Loss 0.5141125922770147\n",
      "2022-03-27 00:05:27.873427 Epoch 300, Training Loss 0.5146429637813812\n",
      "2022-03-27 00:05:27.892466 Epoch 300, Training Loss 0.5152697333746858\n",
      "2022-03-27 00:05:27.910455 Epoch 300, Training Loss 0.5158604711217953\n",
      "2022-03-27 00:05:27.929444 Epoch 300, Training Loss 0.5167472912070087\n",
      "2022-03-27 00:05:27.947243 Epoch 300, Training Loss 0.5174006315905725\n",
      "2022-03-27 00:05:27.966190 Epoch 300, Training Loss 0.5181944080630837\n",
      "2022-03-27 00:05:27.983461 Epoch 300, Training Loss 0.5189301867009429\n",
      "2022-03-27 00:05:28.002417 Epoch 300, Training Loss 0.5197291108196044\n",
      "2022-03-27 00:05:28.020450 Epoch 300, Training Loss 0.520402564721949\n",
      "2022-03-27 00:05:28.039235 Epoch 300, Training Loss 0.5214210279152521\n",
      "2022-03-27 00:05:28.058336 Epoch 300, Training Loss 0.5223311768163501\n",
      "2022-03-27 00:05:28.076323 Epoch 300, Training Loss 0.5230009879754938\n",
      "2022-03-27 00:05:28.094087 Epoch 300, Training Loss 0.5236382588858495\n",
      "2022-03-27 00:05:28.112207 Epoch 300, Training Loss 0.524509820258221\n",
      "2022-03-27 00:05:28.130230 Epoch 300, Training Loss 0.5250849981442132\n",
      "2022-03-27 00:05:28.148201 Epoch 300, Training Loss 0.5256111212932241\n",
      "2022-03-27 00:05:28.166408 Epoch 300, Training Loss 0.5264016371172713\n",
      "2022-03-27 00:05:28.185123 Epoch 300, Training Loss 0.5273244304349051\n",
      "2022-03-27 00:05:28.203305 Epoch 300, Training Loss 0.5281880764324037\n",
      "2022-03-27 00:05:28.222320 Epoch 300, Training Loss 0.5289267885029468\n",
      "2022-03-27 00:05:28.240340 Epoch 300, Training Loss 0.5295996222730792\n",
      "2022-03-27 00:05:28.259524 Epoch 300, Training Loss 0.5306037507017555\n",
      "2022-03-27 00:05:28.278453 Epoch 300, Training Loss 0.5313989717865844\n",
      "2022-03-27 00:05:28.296404 Epoch 300, Training Loss 0.5319829150996245\n",
      "2022-03-27 00:05:28.315312 Epoch 300, Training Loss 0.5327894494814032\n",
      "2022-03-27 00:05:28.333322 Epoch 300, Training Loss 0.5334565419980022\n",
      "2022-03-27 00:05:28.351364 Epoch 300, Training Loss 0.5341304392003647\n",
      "2022-03-27 00:05:28.370315 Epoch 300, Training Loss 0.534915831464026\n",
      "2022-03-27 00:05:28.387331 Epoch 300, Training Loss 0.535432277547429\n",
      "2022-03-27 00:05:28.406349 Epoch 300, Training Loss 0.5364068521334387\n",
      "2022-03-27 00:05:28.425204 Epoch 300, Training Loss 0.5368914713563822\n",
      "2022-03-27 00:05:28.450315 Epoch 300, Training Loss 0.5374713588096297\n",
      "2022-03-27 00:05:28.477319 Epoch 300, Training Loss 0.5382497324358166\n",
      "2022-03-27 00:05:28.502525 Epoch 300, Training Loss 0.538840453604908\n",
      "2022-03-27 00:05:28.528409 Epoch 300, Training Loss 0.5398800980175853\n",
      "2022-03-27 00:05:28.555447 Epoch 300, Training Loss 0.5405387558671825\n",
      "2022-03-27 00:05:28.580438 Epoch 300, Training Loss 0.5411640694912743\n",
      "2022-03-27 00:05:28.607104 Epoch 300, Training Loss 0.5418878765895848\n",
      "2022-03-27 00:05:28.633149 Epoch 300, Training Loss 0.5426847942726082\n",
      "2022-03-27 00:05:28.651465 Epoch 300, Training Loss 0.5432653029251586\n",
      "2022-03-27 00:05:28.669471 Epoch 300, Training Loss 0.5439919487899526\n",
      "2022-03-27 00:05:28.688442 Epoch 300, Training Loss 0.5447941310417926\n",
      "2022-03-27 00:05:28.706439 Epoch 300, Training Loss 0.5456278157203703\n",
      "2022-03-27 00:05:28.725142 Epoch 300, Training Loss 0.5462035487603654\n",
      "2022-03-27 00:05:28.743315 Epoch 300, Training Loss 0.5469771501277109\n",
      "2022-03-27 00:05:28.761356 Epoch 300, Training Loss 0.5479671757315736\n",
      "2022-03-27 00:05:28.779326 Epoch 300, Training Loss 0.5487892633432623\n",
      "2022-03-27 00:05:28.798344 Epoch 300, Training Loss 0.5496199242675396\n",
      "2022-03-27 00:05:28.815297 Epoch 300, Training Loss 0.5501224549911211\n",
      "2022-03-27 00:05:28.833330 Epoch 300, Training Loss 0.5509814008346299\n",
      "2022-03-27 00:05:28.851331 Epoch 300, Training Loss 0.5517682543267375\n",
      "2022-03-27 00:05:28.870264 Epoch 300, Training Loss 0.5523773424536981\n",
      "2022-03-27 00:05:28.889219 Epoch 300, Training Loss 0.5529745082416193\n",
      "2022-03-27 00:05:28.907273 Epoch 300, Training Loss 0.5538214142517666\n",
      "2022-03-27 00:05:28.926184 Epoch 300, Training Loss 0.5547905611565046\n",
      "2022-03-27 00:05:28.943675 Epoch 300, Training Loss 0.5555940060054555\n",
      "2022-03-27 00:05:28.962519 Epoch 300, Training Loss 0.5563829234036644\n",
      "2022-03-27 00:05:28.969524 Epoch 300, Training Loss 0.5572240282507503\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vStqCoxllaGj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG4ueDUbl9J7"
   },
   "source": [
    "Problem 2 Part 2 Subsection C: Weight Decay with lambda of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xGcrHy8rlZkC"
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "  for epoch in range(1, n_epochs +1):\n",
    "    loss_train = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "      outputs = model(imgs.to('cuda:0'))\n",
    "      loss = loss_fn(outputs.to('cuda:0'), labels.to('cuda:0'))\n",
    "      \n",
    "      ambda = 0.001\n",
    "      norm = sum(p.pow(2.0).sum()\n",
    "                    for p in model.parameters())\n",
    "    \n",
    "      loss = loss + ambda*norm\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_train += loss.item()\n",
    "      if epoch == 1 or epoch % 50 == 0:\n",
    "        print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BiosZ4yBlZde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet10(\n",
       "  (conv1): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (9): ResBlock(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv1_dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet10()\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "w4VYf38NlZUn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 00:13:00.326515 Epoch 1, Training Loss 0.002978465441242813\n",
      "2022-03-27 00:13:00.358614 Epoch 1, Training Loss 0.005968467353859826\n",
      "2022-03-27 00:13:00.388956 Epoch 1, Training Loss 0.008944835504302587\n",
      "2022-03-27 00:13:00.420205 Epoch 1, Training Loss 0.011918019455717042\n",
      "2022-03-27 00:13:00.452213 Epoch 1, Training Loss 0.014888463422770391\n",
      "2022-03-27 00:13:00.483219 Epoch 1, Training Loss 0.017841788509007916\n",
      "2022-03-27 00:13:00.512240 Epoch 1, Training Loss 0.020803628370280155\n",
      "2022-03-27 00:13:00.539246 Epoch 1, Training Loss 0.02381051928186051\n",
      "2022-03-27 00:13:00.567246 Epoch 1, Training Loss 0.026784718189093157\n",
      "2022-03-27 00:13:00.594259 Epoch 1, Training Loss 0.029749081262847043\n",
      "2022-03-27 00:13:00.622259 Epoch 1, Training Loss 0.032688901552458856\n",
      "2022-03-27 00:13:00.649265 Epoch 1, Training Loss 0.035659949797803486\n",
      "2022-03-27 00:13:00.676395 Epoch 1, Training Loss 0.038625204654605796\n",
      "2022-03-27 00:13:00.702764 Epoch 1, Training Loss 0.04159971301817833\n",
      "2022-03-27 00:13:00.729185 Epoch 1, Training Loss 0.044557991235152535\n",
      "2022-03-27 00:13:00.757198 Epoch 1, Training Loss 0.04750136036397246\n",
      "2022-03-27 00:13:00.784204 Epoch 1, Training Loss 0.05046249533553258\n",
      "2022-03-27 00:13:00.810509 Epoch 1, Training Loss 0.05341552499005252\n",
      "2022-03-27 00:13:00.836935 Epoch 1, Training Loss 0.0563830771409642\n",
      "2022-03-27 00:13:00.863176 Epoch 1, Training Loss 0.059325090454667426\n",
      "2022-03-27 00:13:00.890599 Epoch 1, Training Loss 0.06227332673719167\n",
      "2022-03-27 00:13:00.911610 Epoch 1, Training Loss 0.06523528306380562\n",
      "2022-03-27 00:13:00.930615 Epoch 1, Training Loss 0.0681895081649351\n",
      "2022-03-27 00:13:00.948617 Epoch 1, Training Loss 0.0711476135131953\n",
      "2022-03-27 00:13:00.968610 Epoch 1, Training Loss 0.07412123680114746\n",
      "2022-03-27 00:13:00.987614 Epoch 1, Training Loss 0.07707599483792434\n",
      "2022-03-27 00:13:01.006626 Epoch 1, Training Loss 0.08004366680789177\n",
      "2022-03-27 00:13:01.025622 Epoch 1, Training Loss 0.08301321716259813\n",
      "2022-03-27 00:13:01.043640 Epoch 1, Training Loss 0.08596639377076912\n",
      "2022-03-27 00:13:01.063645 Epoch 1, Training Loss 0.0889323050408717\n",
      "2022-03-27 00:13:01.083636 Epoch 1, Training Loss 0.09189870412392384\n",
      "2022-03-27 00:13:01.102807 Epoch 1, Training Loss 0.09487078378877371\n",
      "2022-03-27 00:13:01.121804 Epoch 1, Training Loss 0.09783350201823827\n",
      "2022-03-27 00:13:01.140969 Epoch 1, Training Loss 0.10078612465382841\n",
      "2022-03-27 00:13:01.159988 Epoch 1, Training Loss 0.10377388384640979\n",
      "2022-03-27 00:13:01.178986 Epoch 1, Training Loss 0.10674351895861614\n",
      "2022-03-27 00:13:01.198997 Epoch 1, Training Loss 0.1097028231071999\n",
      "2022-03-27 00:13:01.218001 Epoch 1, Training Loss 0.11266295684268103\n",
      "2022-03-27 00:13:01.237005 Epoch 1, Training Loss 0.11561014707131154\n",
      "2022-03-27 00:13:01.255009 Epoch 1, Training Loss 0.11858269564635918\n",
      "2022-03-27 00:13:01.274014 Epoch 1, Training Loss 0.12154031745003313\n",
      "2022-03-27 00:13:01.293018 Epoch 1, Training Loss 0.12448688693668532\n",
      "2022-03-27 00:13:01.312022 Epoch 1, Training Loss 0.12741833513655015\n",
      "2022-03-27 00:13:01.332020 Epoch 1, Training Loss 0.13037693409053872\n",
      "2022-03-27 00:13:01.351279 Epoch 1, Training Loss 0.1333319207896357\n",
      "2022-03-27 00:13:01.370283 Epoch 1, Training Loss 0.13628771725822897\n",
      "2022-03-27 00:13:01.389287 Epoch 1, Training Loss 0.1392441933112376\n",
      "2022-03-27 00:13:01.407292 Epoch 1, Training Loss 0.14218889935242246\n",
      "2022-03-27 00:13:01.427296 Epoch 1, Training Loss 0.14513145627268134\n",
      "2022-03-27 00:13:01.446300 Epoch 1, Training Loss 0.14806301392557675\n",
      "2022-03-27 00:13:01.465035 Epoch 1, Training Loss 0.15101543289926045\n",
      "2022-03-27 00:13:01.483224 Epoch 1, Training Loss 0.15395693400936664\n",
      "2022-03-27 00:13:01.502735 Epoch 1, Training Loss 0.15689571068414945\n",
      "2022-03-27 00:13:01.521069 Epoch 1, Training Loss 0.15983960146794235\n",
      "2022-03-27 00:13:01.539600 Epoch 1, Training Loss 0.16278392457596177\n",
      "2022-03-27 00:13:01.557605 Epoch 1, Training Loss 0.16571825788454023\n",
      "2022-03-27 00:13:01.576733 Epoch 1, Training Loss 0.16868488502014628\n",
      "2022-03-27 00:13:01.596738 Epoch 1, Training Loss 0.17164120527789417\n",
      "2022-03-27 00:13:01.616742 Epoch 1, Training Loss 0.17457812429998842\n",
      "2022-03-27 00:13:01.634746 Epoch 1, Training Loss 0.1775174180564978\n",
      "2022-03-27 00:13:01.653751 Epoch 1, Training Loss 0.1804477692869923\n",
      "2022-03-27 00:13:01.672741 Epoch 1, Training Loss 0.1833854435045091\n",
      "2022-03-27 00:13:01.691759 Epoch 1, Training Loss 0.18630524669461848\n",
      "2022-03-27 00:13:01.709763 Epoch 1, Training Loss 0.1892310850455633\n",
      "2022-03-27 00:13:01.728768 Epoch 1, Training Loss 0.19218312870815893\n",
      "2022-03-27 00:13:01.747772 Epoch 1, Training Loss 0.19511965717501042\n",
      "2022-03-27 00:13:01.766776 Epoch 1, Training Loss 0.19804646657860797\n",
      "2022-03-27 00:13:01.786769 Epoch 1, Training Loss 0.2010024618309782\n",
      "2022-03-27 00:13:01.805771 Epoch 1, Training Loss 0.2039332432515176\n",
      "2022-03-27 00:13:01.824789 Epoch 1, Training Loss 0.20688789396944557\n",
      "2022-03-27 00:13:01.843794 Epoch 1, Training Loss 0.20982428036077552\n",
      "2022-03-27 00:13:01.863798 Epoch 1, Training Loss 0.21272304631255168\n",
      "2022-03-27 00:13:01.882802 Epoch 1, Training Loss 0.21562335009465133\n",
      "2022-03-27 00:13:01.900807 Epoch 1, Training Loss 0.21853119820889916\n",
      "2022-03-27 00:13:01.920806 Epoch 1, Training Loss 0.2214644936954274\n",
      "2022-03-27 00:13:01.938816 Epoch 1, Training Loss 0.2244042633744457\n",
      "2022-03-27 00:13:01.957819 Epoch 1, Training Loss 0.22734181258989417\n",
      "2022-03-27 00:13:01.976824 Epoch 1, Training Loss 0.2302490515477212\n",
      "2022-03-27 00:13:01.995828 Epoch 1, Training Loss 0.23319705032631563\n",
      "2022-03-27 00:13:02.014832 Epoch 1, Training Loss 0.23612312892513812\n",
      "2022-03-27 00:13:02.034837 Epoch 1, Training Loss 0.23905877109683688\n",
      "2022-03-27 00:13:02.053835 Epoch 1, Training Loss 0.24194187550898402\n",
      "2022-03-27 00:13:02.071839 Epoch 1, Training Loss 0.24485956585925558\n",
      "2022-03-27 00:13:02.089844 Epoch 1, Training Loss 0.24774994996502575\n",
      "2022-03-27 00:13:02.108854 Epoch 1, Training Loss 0.25065770173621604\n",
      "2022-03-27 00:13:02.126858 Epoch 1, Training Loss 0.2535965140823208\n",
      "2022-03-27 00:13:02.147863 Epoch 1, Training Loss 0.25647494920989133\n",
      "2022-03-27 00:13:02.166860 Epoch 1, Training Loss 0.25937717009688277\n",
      "2022-03-27 00:13:02.185872 Epoch 1, Training Loss 0.2622894448087648\n",
      "2022-03-27 00:13:02.204876 Epoch 1, Training Loss 0.26520380675030486\n",
      "2022-03-27 00:13:02.223880 Epoch 1, Training Loss 0.26812568527963154\n",
      "2022-03-27 00:13:02.241884 Epoch 1, Training Loss 0.27101099948444024\n",
      "2022-03-27 00:13:02.260888 Epoch 1, Training Loss 0.27392092903556725\n",
      "2022-03-27 00:13:02.279892 Epoch 1, Training Loss 0.27682088464117416\n",
      "2022-03-27 00:13:02.298897 Epoch 1, Training Loss 0.2797428878676861\n",
      "2022-03-27 00:13:02.317901 Epoch 1, Training Loss 0.28263467230150463\n",
      "2022-03-27 00:13:02.336905 Epoch 1, Training Loss 0.28550146607791677\n",
      "2022-03-27 00:13:02.355910 Epoch 1, Training Loss 0.28838325644393104\n",
      "2022-03-27 00:13:02.374041 Epoch 1, Training Loss 0.2913100551766203\n",
      "2022-03-27 00:13:02.392433 Epoch 1, Training Loss 0.2942621137784875\n",
      "2022-03-27 00:13:02.411712 Epoch 1, Training Loss 0.2971604638697241\n",
      "2022-03-27 00:13:02.430895 Epoch 1, Training Loss 0.30006628756023124\n",
      "2022-03-27 00:13:02.450900 Epoch 1, Training Loss 0.30295194811223414\n",
      "2022-03-27 00:13:02.470904 Epoch 1, Training Loss 0.30584970002284134\n",
      "2022-03-27 00:13:02.489354 Epoch 1, Training Loss 0.30875104772465306\n",
      "2022-03-27 00:13:02.508238 Epoch 1, Training Loss 0.3116404437043173\n",
      "2022-03-27 00:13:02.527582 Epoch 1, Training Loss 0.31450872256627777\n",
      "2022-03-27 00:13:02.545941 Epoch 1, Training Loss 0.317412390733314\n",
      "2022-03-27 00:13:02.565282 Epoch 1, Training Loss 0.3202535871349637\n",
      "2022-03-27 00:13:02.583437 Epoch 1, Training Loss 0.3231033513613064\n",
      "2022-03-27 00:13:02.602617 Epoch 1, Training Loss 0.3260051600463555\n",
      "2022-03-27 00:13:02.621622 Epoch 1, Training Loss 0.32890291500579366\n",
      "2022-03-27 00:13:02.640804 Epoch 1, Training Loss 0.3317936918009882\n",
      "2022-03-27 00:13:02.659795 Epoch 1, Training Loss 0.3346939504603901\n",
      "2022-03-27 00:13:02.679415 Epoch 1, Training Loss 0.33757577040006437\n",
      "2022-03-27 00:13:02.698419 Epoch 1, Training Loss 0.34046388526096977\n",
      "2022-03-27 00:13:02.721425 Epoch 1, Training Loss 0.34335854321794435\n",
      "2022-03-27 00:13:02.747431 Epoch 1, Training Loss 0.34623502892301516\n",
      "2022-03-27 00:13:02.773436 Epoch 1, Training Loss 0.3491040154186356\n",
      "2022-03-27 00:13:02.799442 Epoch 1, Training Loss 0.3520076561461934\n",
      "2022-03-27 00:13:02.826435 Epoch 1, Training Loss 0.3549313554373544\n",
      "2022-03-27 00:13:02.852454 Epoch 1, Training Loss 0.357800659011392\n",
      "2022-03-27 00:13:02.878460 Epoch 1, Training Loss 0.36066564391641054\n",
      "2022-03-27 00:13:02.904466 Epoch 1, Training Loss 0.3635243494492358\n",
      "2022-03-27 00:13:02.926471 Epoch 1, Training Loss 0.36637950065495717\n",
      "2022-03-27 00:13:02.944475 Epoch 1, Training Loss 0.3692752689961582\n",
      "2022-03-27 00:13:02.963479 Epoch 1, Training Loss 0.3721495571038912\n",
      "2022-03-27 00:13:02.981483 Epoch 1, Training Loss 0.37496215791043724\n",
      "2022-03-27 00:13:03.000488 Epoch 1, Training Loss 0.37787138104743667\n",
      "2022-03-27 00:13:03.018492 Epoch 1, Training Loss 0.3807284944807477\n",
      "2022-03-27 00:13:03.036496 Epoch 1, Training Loss 0.3835941208597949\n",
      "2022-03-27 00:13:03.056501 Epoch 1, Training Loss 0.38645483923080326\n",
      "2022-03-27 00:13:03.075505 Epoch 1, Training Loss 0.38929229075341576\n",
      "2022-03-27 00:13:03.093509 Epoch 1, Training Loss 0.3921064766471648\n",
      "2022-03-27 00:13:03.113515 Epoch 1, Training Loss 0.39493917504235\n",
      "2022-03-27 00:13:03.131517 Epoch 1, Training Loss 0.39782464595706873\n",
      "2022-03-27 00:13:03.150522 Epoch 1, Training Loss 0.40072401130900664\n",
      "2022-03-27 00:13:03.168526 Epoch 1, Training Loss 0.4036088843479791\n",
      "2022-03-27 00:13:03.188637 Epoch 1, Training Loss 0.40643561587614174\n",
      "2022-03-27 00:13:03.207656 Epoch 1, Training Loss 0.40927065821254954\n",
      "2022-03-27 00:13:03.225660 Epoch 1, Training Loss 0.4121431374488889\n",
      "2022-03-27 00:13:03.243664 Epoch 1, Training Loss 0.4149668323414405\n",
      "2022-03-27 00:13:03.263669 Epoch 1, Training Loss 0.41775643581624533\n",
      "2022-03-27 00:13:03.282673 Epoch 1, Training Loss 0.42061441938590516\n",
      "2022-03-27 00:13:03.301671 Epoch 1, Training Loss 0.42346470404768843\n",
      "2022-03-27 00:13:03.319973 Epoch 1, Training Loss 0.4263473813186216\n",
      "2022-03-27 00:13:03.338241 Epoch 1, Training Loss 0.42919225278108014\n",
      "2022-03-27 00:13:03.356736 Epoch 1, Training Loss 0.43205828983765426\n",
      "2022-03-27 00:13:03.375049 Epoch 1, Training Loss 0.43490645739123646\n",
      "2022-03-27 00:13:03.394053 Epoch 1, Training Loss 0.4377160029643027\n",
      "2022-03-27 00:13:03.413057 Epoch 1, Training Loss 0.44052887908028215\n",
      "2022-03-27 00:13:03.431061 Epoch 1, Training Loss 0.443330142504114\n",
      "2022-03-27 00:13:03.450066 Epoch 1, Training Loss 0.44618382508797416\n",
      "2022-03-27 00:13:03.470071 Epoch 1, Training Loss 0.4490169573318013\n",
      "2022-03-27 00:13:03.488075 Epoch 1, Training Loss 0.45186457243721806\n",
      "2022-03-27 00:13:03.506079 Epoch 1, Training Loss 0.4547208578080472\n",
      "2022-03-27 00:13:03.525243 Epoch 1, Training Loss 0.45752924756930613\n",
      "2022-03-27 00:13:03.543389 Epoch 1, Training Loss 0.4603297719565194\n",
      "2022-03-27 00:13:03.562055 Epoch 1, Training Loss 0.46319537821328244\n",
      "2022-03-27 00:13:03.580424 Epoch 1, Training Loss 0.4659988358807381\n",
      "2022-03-27 00:13:03.599260 Epoch 1, Training Loss 0.46883906305903367\n",
      "2022-03-27 00:13:03.619597 Epoch 1, Training Loss 0.47172418152889634\n",
      "2022-03-27 00:13:03.638068 Epoch 1, Training Loss 0.47458041781354743\n",
      "2022-03-27 00:13:03.656072 Epoch 1, Training Loss 0.4774717748012689\n",
      "2022-03-27 00:13:03.676070 Epoch 1, Training Loss 0.4803153160587906\n",
      "2022-03-27 00:13:03.695345 Epoch 1, Training Loss 0.48313772617398626\n",
      "2022-03-27 00:13:03.714349 Epoch 1, Training Loss 0.4859836491782342\n",
      "2022-03-27 00:13:03.733347 Epoch 1, Training Loss 0.48875501546103634\n",
      "2022-03-27 00:13:03.752352 Epoch 1, Training Loss 0.4915585152023589\n",
      "2022-03-27 00:13:03.770435 Epoch 1, Training Loss 0.4943324757354034\n",
      "2022-03-27 00:13:03.789452 Epoch 1, Training Loss 0.49707951448152743\n",
      "2022-03-27 00:13:03.807608 Epoch 1, Training Loss 0.4998952747915712\n",
      "2022-03-27 00:13:03.826966 Epoch 1, Training Loss 0.5027170141639612\n",
      "2022-03-27 00:13:03.844619 Epoch 1, Training Loss 0.505517912337847\n",
      "2022-03-27 00:13:03.863997 Epoch 1, Training Loss 0.5083152022203217\n",
      "2022-03-27 00:13:03.883511 Epoch 1, Training Loss 0.5111770111581554\n",
      "2022-03-27 00:13:03.901496 Epoch 1, Training Loss 0.5140309785028248\n",
      "2022-03-27 00:13:03.920513 Epoch 1, Training Loss 0.5168579521081637\n",
      "2022-03-27 00:13:03.938831 Epoch 1, Training Loss 0.5195963510771846\n",
      "2022-03-27 00:13:03.957347 Epoch 1, Training Loss 0.522369678368044\n",
      "2022-03-27 00:13:03.975870 Epoch 1, Training Loss 0.52514767494348\n",
      "2022-03-27 00:13:03.993198 Epoch 1, Training Loss 0.527911435002866\n",
      "2022-03-27 00:13:04.012229 Epoch 1, Training Loss 0.530731028303161\n",
      "2022-03-27 00:13:04.030801 Epoch 1, Training Loss 0.5335092587239297\n",
      "2022-03-27 00:13:04.049954 Epoch 1, Training Loss 0.5363466556724685\n",
      "2022-03-27 00:13:04.067941 Epoch 1, Training Loss 0.5391727432875377\n",
      "2022-03-27 00:13:04.086951 Epoch 1, Training Loss 0.5420044706300702\n",
      "2022-03-27 00:13:04.105961 Epoch 1, Training Loss 0.5448069395616536\n",
      "2022-03-27 00:13:04.124971 Epoch 1, Training Loss 0.5475922518069177\n",
      "2022-03-27 00:13:04.142963 Epoch 1, Training Loss 0.550365885505286\n",
      "2022-03-27 00:13:04.162204 Epoch 1, Training Loss 0.55321477959528\n",
      "2022-03-27 00:13:04.180573 Epoch 1, Training Loss 0.5560139070081589\n",
      "2022-03-27 00:13:04.199920 Epoch 1, Training Loss 0.5587990424212288\n",
      "2022-03-27 00:13:04.218433 Epoch 1, Training Loss 0.5615637659111901\n",
      "2022-03-27 00:13:04.238013 Epoch 1, Training Loss 0.5643100403153988\n",
      "2022-03-27 00:13:04.256488 Epoch 1, Training Loss 0.5671705111213352\n",
      "2022-03-27 00:13:04.275011 Epoch 1, Training Loss 0.5700157380774807\n",
      "2022-03-27 00:13:04.294022 Epoch 1, Training Loss 0.5728146148764569\n",
      "2022-03-27 00:13:04.313022 Epoch 1, Training Loss 0.5757086368472984\n",
      "2022-03-27 00:13:04.332030 Epoch 1, Training Loss 0.5784738454062616\n",
      "2022-03-27 00:13:04.350028 Epoch 1, Training Loss 0.5813356717224316\n",
      "2022-03-27 00:13:04.370033 Epoch 1, Training Loss 0.5840890858789234\n",
      "2022-03-27 00:13:04.388037 Epoch 1, Training Loss 0.5868344264262167\n",
      "2022-03-27 00:13:04.406047 Epoch 1, Training Loss 0.5896200112369664\n",
      "2022-03-27 00:13:04.425489 Epoch 1, Training Loss 0.5924454028039332\n",
      "2022-03-27 00:13:04.443612 Epoch 1, Training Loss 0.5951129112707074\n",
      "2022-03-27 00:13:04.461955 Epoch 1, Training Loss 0.5978652738854099\n",
      "2022-03-27 00:13:04.479644 Epoch 1, Training Loss 0.6006672809191067\n",
      "2022-03-27 00:13:04.499655 Epoch 1, Training Loss 0.6033814054011079\n",
      "2022-03-27 00:13:04.518648 Epoch 1, Training Loss 0.6061691496988086\n",
      "2022-03-27 00:13:04.537784 Epoch 1, Training Loss 0.6088934292268875\n",
      "2022-03-27 00:13:04.556285 Epoch 1, Training Loss 0.6116580002753021\n",
      "2022-03-27 00:13:04.575095 Epoch 1, Training Loss 0.6144600935909145\n",
      "2022-03-27 00:13:04.593245 Epoch 1, Training Loss 0.6172111881968311\n",
      "2022-03-27 00:13:04.612250 Epoch 1, Training Loss 0.6199891701378786\n",
      "2022-03-27 00:13:04.630260 Epoch 1, Training Loss 0.6227440163302604\n",
      "2022-03-27 00:13:04.649258 Epoch 1, Training Loss 0.625519676281668\n",
      "2022-03-27 00:13:04.668264 Epoch 1, Training Loss 0.6282963923481114\n",
      "2022-03-27 00:13:04.686255 Epoch 1, Training Loss 0.6310549087231726\n",
      "2022-03-27 00:13:04.705273 Epoch 1, Training Loss 0.6337934140963932\n",
      "2022-03-27 00:13:04.725275 Epoch 1, Training Loss 0.6365576567857162\n",
      "2022-03-27 00:13:04.743757 Epoch 1, Training Loss 0.639244514048252\n",
      "2022-03-27 00:13:04.761754 Epoch 1, Training Loss 0.6420462652850334\n",
      "2022-03-27 00:13:04.780758 Epoch 1, Training Loss 0.6447389781322626\n",
      "2022-03-27 00:13:04.799753 Epoch 1, Training Loss 0.6474653326946757\n",
      "2022-03-27 00:13:04.818204 Epoch 1, Training Loss 0.6501692567030182\n",
      "2022-03-27 00:13:04.836766 Epoch 1, Training Loss 0.6528877397937238\n",
      "2022-03-27 00:13:04.855769 Epoch 1, Training Loss 0.6555973501766429\n",
      "2022-03-27 00:13:04.873779 Epoch 1, Training Loss 0.658306380062152\n",
      "2022-03-27 00:13:04.892777 Epoch 1, Training Loss 0.6610551453612344\n",
      "2022-03-27 00:13:04.912796 Epoch 1, Training Loss 0.6637768544199522\n",
      "2022-03-27 00:13:04.930786 Epoch 1, Training Loss 0.6665212461710586\n",
      "2022-03-27 00:13:04.950797 Epoch 1, Training Loss 0.6692935670428264\n",
      "2022-03-27 00:13:04.969795 Epoch 1, Training Loss 0.6720132888735407\n",
      "2022-03-27 00:13:04.988812 Epoch 1, Training Loss 0.6747507234973371\n",
      "2022-03-27 00:13:05.007810 Epoch 1, Training Loss 0.677524708116146\n",
      "2022-03-27 00:13:05.025814 Epoch 1, Training Loss 0.680302887316555\n",
      "2022-03-27 00:13:05.044824 Epoch 1, Training Loss 0.6830047570226138\n",
      "2022-03-27 00:13:05.063828 Epoch 1, Training Loss 0.6856149819196032\n",
      "2022-03-27 00:13:05.082827 Epoch 1, Training Loss 0.6883529074051801\n",
      "2022-03-27 00:13:05.101825 Epoch 1, Training Loss 0.6910582658884775\n",
      "2022-03-27 00:13:05.120836 Epoch 1, Training Loss 0.6937066962956773\n",
      "2022-03-27 00:13:05.145841 Epoch 1, Training Loss 0.696512796384904\n",
      "2022-03-27 00:13:05.172847 Epoch 1, Training Loss 0.6992851669526161\n",
      "2022-03-27 00:13:05.198853 Epoch 1, Training Loss 0.7019706510216989\n",
      "2022-03-27 00:13:05.224865 Epoch 1, Training Loss 0.7046045763108432\n",
      "2022-03-27 00:13:05.250865 Epoch 1, Training Loss 0.7073652582705173\n",
      "2022-03-27 00:13:05.276877 Epoch 1, Training Loss 0.710086761533147\n",
      "2022-03-27 00:13:05.303378 Epoch 1, Training Loss 0.7127636164960349\n",
      "2022-03-27 00:13:05.330552 Epoch 1, Training Loss 0.7155199303956288\n",
      "2022-03-27 00:13:05.345389 Epoch 1, Training Loss 0.7182553372419703\n",
      "2022-03-27 00:13:05.360392 Epoch 1, Training Loss 0.7209739992990518\n",
      "2022-03-27 00:13:05.375396 Epoch 1, Training Loss 0.7237350144959471\n",
      "2022-03-27 00:13:05.389399 Epoch 1, Training Loss 0.7264932482443807\n",
      "2022-03-27 00:13:05.404402 Epoch 1, Training Loss 0.729115738893104\n",
      "2022-03-27 00:13:05.418398 Epoch 1, Training Loss 0.7317419780794617\n",
      "2022-03-27 00:13:05.433408 Epoch 1, Training Loss 0.7344339028038942\n",
      "2022-03-27 00:13:05.447413 Epoch 1, Training Loss 0.7372257352789955\n",
      "2022-03-27 00:13:05.462415 Epoch 1, Training Loss 0.7399371886802146\n",
      "2022-03-27 00:13:05.477419 Epoch 1, Training Loss 0.7426340366568407\n",
      "2022-03-27 00:13:05.492422 Epoch 1, Training Loss 0.7453274800039619\n",
      "2022-03-27 00:13:05.507425 Epoch 1, Training Loss 0.7480427575538225\n",
      "2022-03-27 00:13:05.522429 Epoch 1, Training Loss 0.7507627592672168\n",
      "2022-03-27 00:13:05.536793 Epoch 1, Training Loss 0.7534320119701688\n",
      "2022-03-27 00:13:05.551789 Epoch 1, Training Loss 0.756135289016587\n",
      "2022-03-27 00:13:05.565792 Epoch 1, Training Loss 0.758837456288545\n",
      "2022-03-27 00:13:05.580795 Epoch 1, Training Loss 0.7615159987793554\n",
      "2022-03-27 00:13:05.594800 Epoch 1, Training Loss 0.7642389657857168\n",
      "2022-03-27 00:13:05.609798 Epoch 1, Training Loss 0.7668453738512591\n",
      "2022-03-27 00:13:05.623800 Epoch 1, Training Loss 0.7695817858971599\n",
      "2022-03-27 00:13:05.637809 Epoch 1, Training Loss 0.772381690152161\n",
      "2022-03-27 00:13:05.652812 Epoch 1, Training Loss 0.7750244732098202\n",
      "2022-03-27 00:13:05.668181 Epoch 1, Training Loss 0.7777358366705268\n",
      "2022-03-27 00:13:05.682410 Epoch 1, Training Loss 0.7804771402607793\n",
      "2022-03-27 00:13:05.696803 Epoch 1, Training Loss 0.7830919752950254\n",
      "2022-03-27 00:13:05.711152 Epoch 1, Training Loss 0.7857472713646072\n",
      "2022-03-27 00:13:05.726156 Epoch 1, Training Loss 0.7883650780943654\n",
      "2022-03-27 00:13:05.741154 Epoch 1, Training Loss 0.7910178113166634\n",
      "2022-03-27 00:13:05.756162 Epoch 1, Training Loss 0.7938192134623028\n",
      "2022-03-27 00:13:05.771165 Epoch 1, Training Loss 0.7964676597234234\n",
      "2022-03-27 00:13:05.786330 Epoch 1, Training Loss 0.7990849231515089\n",
      "2022-03-27 00:13:05.801558 Epoch 1, Training Loss 0.8017520572218444\n",
      "2022-03-27 00:13:05.815921 Epoch 1, Training Loss 0.8044925095784999\n",
      "2022-03-27 00:13:05.831267 Epoch 1, Training Loss 0.8071549356441059\n",
      "2022-03-27 00:13:05.845631 Epoch 1, Training Loss 0.8098309555322009\n",
      "2022-03-27 00:13:05.860011 Epoch 1, Training Loss 0.8124381395252159\n",
      "2022-03-27 00:13:05.875592 Epoch 1, Training Loss 0.8150861129126585\n",
      "2022-03-27 00:13:05.889810 Epoch 1, Training Loss 0.8177883981743737\n",
      "2022-03-27 00:13:05.903814 Epoch 1, Training Loss 0.8203870308063829\n",
      "2022-03-27 00:13:05.919817 Epoch 1, Training Loss 0.8230142453137566\n",
      "2022-03-27 00:13:05.934813 Epoch 1, Training Loss 0.8256309069026156\n",
      "2022-03-27 00:13:05.948825 Epoch 1, Training Loss 0.8282725076236384\n",
      "2022-03-27 00:13:05.963827 Epoch 1, Training Loss 0.8308987029068305\n",
      "2022-03-27 00:13:05.978830 Epoch 1, Training Loss 0.8335137550178391\n",
      "2022-03-27 00:13:05.992834 Epoch 1, Training Loss 0.8362341942384725\n",
      "2022-03-27 00:13:06.007837 Epoch 1, Training Loss 0.8388943586812909\n",
      "2022-03-27 00:13:06.022833 Epoch 1, Training Loss 0.8415664544190897\n",
      "2022-03-27 00:13:06.036845 Epoch 1, Training Loss 0.8442152842231418\n",
      "2022-03-27 00:13:06.051848 Epoch 1, Training Loss 0.8468801682562475\n",
      "2022-03-27 00:13:06.066852 Epoch 1, Training Loss 0.8495414522297852\n",
      "2022-03-27 00:13:06.081856 Epoch 1, Training Loss 0.8522075533561999\n",
      "2022-03-27 00:13:06.096857 Epoch 1, Training Loss 0.854825017397361\n",
      "2022-03-27 00:13:06.110861 Epoch 1, Training Loss 0.8574743554415301\n",
      "2022-03-27 00:13:06.125864 Epoch 1, Training Loss 0.8601722476427512\n",
      "2022-03-27 00:13:06.140868 Epoch 1, Training Loss 0.8628416924220522\n",
      "2022-03-27 00:13:06.155871 Epoch 1, Training Loss 0.8654656401070793\n",
      "2022-03-27 00:13:06.170875 Epoch 1, Training Loss 0.8681045718815016\n",
      "2022-03-27 00:13:06.185006 Epoch 1, Training Loss 0.8707198282641828\n",
      "2022-03-27 00:13:06.200366 Epoch 1, Training Loss 0.8732798660502714\n",
      "2022-03-27 00:13:06.214101 Epoch 1, Training Loss 0.8759205094383805\n",
      "2022-03-27 00:13:06.229316 Epoch 1, Training Loss 0.8784557044353631\n",
      "2022-03-27 00:13:06.243826 Epoch 1, Training Loss 0.8811064425026974\n",
      "2022-03-27 00:13:06.259255 Epoch 1, Training Loss 0.8836843796703212\n",
      "2022-03-27 00:13:06.273771 Epoch 1, Training Loss 0.886280679336899\n",
      "2022-03-27 00:13:06.288175 Epoch 1, Training Loss 0.8889469298560296\n",
      "2022-03-27 00:13:06.302535 Epoch 1, Training Loss 0.8916732517959517\n",
      "2022-03-27 00:13:06.317167 Epoch 1, Training Loss 0.8942559457495999\n",
      "2022-03-27 00:13:06.332171 Epoch 1, Training Loss 0.8969595490209282\n",
      "2022-03-27 00:13:06.346676 Epoch 1, Training Loss 0.8997094768392461\n",
      "2022-03-27 00:13:06.360680 Epoch 1, Training Loss 0.9023997832442183\n",
      "2022-03-27 00:13:06.375860 Epoch 1, Training Loss 0.905067224331829\n",
      "2022-03-27 00:13:06.390072 Epoch 1, Training Loss 0.9076558449079314\n",
      "2022-03-27 00:13:06.405439 Epoch 1, Training Loss 0.9102490301937094\n",
      "2022-03-27 00:13:06.419616 Epoch 1, Training Loss 0.9128704220437638\n",
      "2022-03-27 00:13:06.434766 Epoch 1, Training Loss 0.9155160173430772\n",
      "2022-03-27 00:13:06.449773 Epoch 1, Training Loss 0.9180628477459978\n",
      "2022-03-27 00:13:06.464773 Epoch 1, Training Loss 0.9207063027659951\n",
      "2022-03-27 00:13:06.479783 Epoch 1, Training Loss 0.9233405250112724\n",
      "2022-03-27 00:13:06.494006 Epoch 1, Training Loss 0.9259762707573679\n",
      "2022-03-27 00:13:06.509008 Epoch 1, Training Loss 0.9286881604462939\n",
      "2022-03-27 00:13:06.524013 Epoch 1, Training Loss 0.9313503892525382\n",
      "2022-03-27 00:13:06.538025 Epoch 1, Training Loss 0.9339703033342386\n",
      "2022-03-27 00:13:06.553026 Epoch 1, Training Loss 0.9366692257354327\n",
      "2022-03-27 00:13:06.567031 Epoch 1, Training Loss 0.9392331410247041\n",
      "2022-03-27 00:13:06.583033 Epoch 1, Training Loss 0.9418908950617856\n",
      "2022-03-27 00:13:06.597037 Epoch 1, Training Loss 0.9444468202798263\n",
      "2022-03-27 00:13:06.612041 Epoch 1, Training Loss 0.9469495638252219\n",
      "2022-03-27 00:13:06.627044 Epoch 1, Training Loss 0.9495508879651804\n",
      "2022-03-27 00:13:06.642047 Epoch 1, Training Loss 0.9520527725024601\n",
      "2022-03-27 00:13:06.657050 Epoch 1, Training Loss 0.9547096484762323\n",
      "2022-03-27 00:13:06.673054 Epoch 1, Training Loss 0.957301710572694\n",
      "2022-03-27 00:13:06.688058 Epoch 1, Training Loss 0.9600021025103986\n",
      "2022-03-27 00:13:06.703054 Epoch 1, Training Loss 0.9626680640003565\n",
      "2022-03-27 00:13:06.718063 Epoch 1, Training Loss 0.9651913194705153\n",
      "2022-03-27 00:13:06.734068 Epoch 1, Training Loss 0.9678278798642366\n",
      "2022-03-27 00:13:06.749072 Epoch 1, Training Loss 0.9703968083462142\n",
      "2022-03-27 00:13:06.764074 Epoch 1, Training Loss 0.9729433753301421\n",
      "2022-03-27 00:13:06.779079 Epoch 1, Training Loss 0.975524835726794\n",
      "2022-03-27 00:13:06.794082 Epoch 1, Training Loss 0.9781200701318433\n",
      "2022-03-27 00:13:06.809084 Epoch 1, Training Loss 0.9807363917760532\n",
      "2022-03-27 00:13:06.824088 Epoch 1, Training Loss 0.9833644928834627\n",
      "2022-03-27 00:13:06.839316 Epoch 1, Training Loss 0.9859510494010223\n",
      "2022-03-27 00:13:06.856315 Epoch 1, Training Loss 0.9886184072555484\n",
      "2022-03-27 00:13:06.871324 Epoch 1, Training Loss 0.9913754800091619\n",
      "2022-03-27 00:13:06.885320 Epoch 1, Training Loss 0.9939321099644731\n",
      "2022-03-27 00:13:06.900331 Epoch 1, Training Loss 0.9965236284543791\n",
      "2022-03-27 00:13:06.915333 Epoch 1, Training Loss 0.9990547785673605\n",
      "2022-03-27 00:13:06.930336 Epoch 1, Training Loss 1.0016238003435647\n",
      "2022-03-27 00:13:06.944342 Epoch 1, Training Loss 1.0042299373680368\n",
      "2022-03-27 00:13:06.959343 Epoch 1, Training Loss 1.0068612193207607\n",
      "2022-03-27 00:13:06.973355 Epoch 1, Training Loss 1.009542334719997\n",
      "2022-03-27 00:13:06.988350 Epoch 1, Training Loss 1.0121282022017652\n",
      "2022-03-27 00:13:07.003347 Epoch 1, Training Loss 1.014816810407907\n",
      "2022-03-27 00:13:07.017581 Epoch 1, Training Loss 1.0173764067232762\n",
      "2022-03-27 00:13:07.032594 Epoch 1, Training Loss 1.020024557247796\n",
      "2022-03-27 00:13:07.047591 Epoch 1, Training Loss 1.0225986870353485\n",
      "2022-03-27 00:13:07.062594 Epoch 1, Training Loss 1.0251886692193464\n",
      "2022-03-27 00:13:07.078597 Epoch 1, Training Loss 1.02780619362736\n",
      "2022-03-27 00:13:07.093601 Epoch 1, Training Loss 1.030441458572817\n",
      "2022-03-27 00:13:07.108604 Epoch 1, Training Loss 1.0330851895120137\n",
      "2022-03-27 00:13:07.122608 Epoch 1, Training Loss 1.0356752091966321\n",
      "2022-03-27 00:13:07.137836 Epoch 1, Training Loss 1.0382338969603828\n",
      "2022-03-27 00:13:07.156840 Epoch 1, Training Loss 1.0406494864722347\n",
      "2022-03-27 00:13:07.175850 Epoch 1, Training Loss 1.0432165054713978\n",
      "2022-03-27 00:13:07.195849 Epoch 1, Training Loss 1.045825933403981\n",
      "2022-03-27 00:13:07.214859 Epoch 1, Training Loss 1.0485323437339509\n",
      "2022-03-27 00:13:07.234864 Epoch 1, Training Loss 1.0512751086288705\n",
      "2022-03-27 00:13:07.253849 Epoch 1, Training Loss 1.0539384743441707\n",
      "2022-03-27 00:13:07.273866 Epoch 1, Training Loss 1.0565194191835117\n",
      "2022-03-27 00:13:07.292325 Epoch 1, Training Loss 1.059189847515672\n",
      "2022-03-27 00:13:07.312329 Epoch 1, Training Loss 1.0617557527768948\n",
      "2022-03-27 00:13:07.333328 Epoch 1, Training Loss 1.06439877791173\n",
      "2022-03-27 00:13:07.353327 Epoch 1, Training Loss 1.0669484748254956\n",
      "2022-03-27 00:13:07.373337 Epoch 1, Training Loss 1.0694935809620811\n",
      "2022-03-27 00:13:07.393342 Epoch 1, Training Loss 1.0720092360016025\n",
      "2022-03-27 00:13:07.412352 Epoch 1, Training Loss 1.0745535384663536\n",
      "2022-03-27 00:13:07.432357 Epoch 1, Training Loss 1.0771968852528526\n",
      "2022-03-27 00:13:07.451361 Epoch 1, Training Loss 1.0798606259743575\n",
      "2022-03-27 00:13:07.471353 Epoch 1, Training Loss 1.0822993080939174\n",
      "2022-03-27 00:13:07.490364 Epoch 1, Training Loss 1.084944131124355\n",
      "2022-03-27 00:13:07.509361 Epoch 1, Training Loss 1.0875639400213881\n",
      "2022-03-27 00:13:07.529379 Epoch 1, Training Loss 1.0901051628620118\n",
      "2022-03-27 00:13:07.548383 Epoch 1, Training Loss 1.0926209375681475\n",
      "2022-03-27 00:13:07.568376 Epoch 1, Training Loss 1.0951700286792063\n",
      "2022-03-27 00:13:07.588381 Epoch 1, Training Loss 1.0977446664019923\n",
      "2022-03-27 00:13:07.607376 Epoch 1, Training Loss 1.1002740180096053\n",
      "2022-03-27 00:13:07.626401 Epoch 1, Training Loss 1.1027271413742123\n",
      "2022-03-27 00:13:07.645399 Epoch 1, Training Loss 1.1052996558911354\n",
      "2022-03-27 00:13:07.665403 Epoch 1, Training Loss 1.1078685426041293\n",
      "2022-03-27 00:13:07.685548 Epoch 1, Training Loss 1.1103810655796313\n",
      "2022-03-27 00:13:07.704542 Epoch 1, Training Loss 1.1129803363319553\n",
      "2022-03-27 00:13:07.723550 Epoch 1, Training Loss 1.1156459328768504\n",
      "2022-03-27 00:13:07.743561 Epoch 1, Training Loss 1.118241426432529\n",
      "2022-03-27 00:13:07.762571 Epoch 1, Training Loss 1.1209640010543491\n",
      "2022-03-27 00:13:07.782576 Epoch 1, Training Loss 1.1236094472658298\n",
      "2022-03-27 00:13:07.802568 Epoch 1, Training Loss 1.1260981896649236\n",
      "2022-03-27 00:13:07.821585 Epoch 1, Training Loss 1.1287079691277135\n",
      "2022-03-27 00:13:07.841246 Epoch 1, Training Loss 1.131202679308479\n",
      "2022-03-27 00:13:07.859745 Epoch 1, Training Loss 1.133792974302531\n",
      "2022-03-27 00:13:07.879365 Epoch 1, Training Loss 1.1363797451529052\n",
      "2022-03-27 00:13:07.898727 Epoch 1, Training Loss 1.138854440673233\n",
      "2022-03-27 00:13:07.918423 Epoch 1, Training Loss 1.1415302770217057\n",
      "2022-03-27 00:13:07.936940 Epoch 1, Training Loss 1.144241505724085\n",
      "2022-03-27 00:13:07.956297 Epoch 1, Training Loss 1.1467861161207604\n",
      "2022-03-27 00:13:07.975302 Epoch 1, Training Loss 1.149418408456056\n",
      "2022-03-27 00:13:07.994312 Epoch 1, Training Loss 1.1519382746933062\n",
      "2022-03-27 00:13:08.013310 Epoch 1, Training Loss 1.1545350691851448\n",
      "2022-03-27 00:13:08.033315 Epoch 1, Training Loss 1.1571670801133451\n",
      "2022-03-27 00:13:08.053313 Epoch 1, Training Loss 1.1598020499319677\n",
      "2022-03-27 00:13:08.072474 Epoch 1, Training Loss 1.1623661716270934\n",
      "2022-03-27 00:13:08.092334 Epoch 1, Training Loss 1.1648095422388647\n",
      "2022-03-27 00:13:08.111333 Epoch 1, Training Loss 1.1673826680463903\n",
      "2022-03-27 00:13:08.130706 Epoch 1, Training Loss 1.1697964049361247\n",
      "2022-03-27 00:13:08.149409 Epoch 1, Training Loss 1.1722538335548947\n",
      "2022-03-27 00:13:08.168915 Epoch 1, Training Loss 1.174847130427885\n",
      "2022-03-27 00:13:08.188932 Epoch 1, Training Loss 1.177311502454226\n",
      "2022-03-27 00:13:08.207937 Epoch 1, Training Loss 1.179890050302686\n",
      "2022-03-27 00:13:08.228105 Epoch 1, Training Loss 1.1823855193374713\n",
      "2022-03-27 00:13:08.246422 Epoch 1, Training Loss 1.1848260446277725\n",
      "2022-03-27 00:13:08.265820 Epoch 1, Training Loss 1.1874044869866822\n",
      "2022-03-27 00:13:08.285330 Epoch 1, Training Loss 1.1899123798550852\n",
      "2022-03-27 00:13:08.303787 Epoch 1, Training Loss 1.1924540318186632\n",
      "2022-03-27 00:13:08.322809 Epoch 1, Training Loss 1.1951731542492157\n",
      "2022-03-27 00:13:08.342813 Epoch 1, Training Loss 1.1976750942752183\n",
      "2022-03-27 00:13:08.362812 Epoch 1, Training Loss 1.2002972193691126\n",
      "2022-03-27 00:13:08.381803 Epoch 1, Training Loss 1.202790371444829\n",
      "2022-03-27 00:13:08.401809 Epoch 1, Training Loss 1.205187969957776\n",
      "2022-03-27 00:13:08.420811 Epoch 1, Training Loss 1.207696620613108\n",
      "2022-03-27 00:13:08.440829 Epoch 1, Training Loss 1.2102577437830093\n",
      "2022-03-27 00:13:08.459834 Epoch 1, Training Loss 1.2127242794122233\n",
      "2022-03-27 00:13:08.479838 Epoch 1, Training Loss 1.215166194969431\n",
      "2022-03-27 00:13:08.499414 Epoch 1, Training Loss 1.2178694711012\n",
      "2022-03-27 00:13:08.519412 Epoch 1, Training Loss 1.2203745413619234\n",
      "2022-03-27 00:13:08.539815 Epoch 1, Training Loss 1.2229121685637843\n",
      "2022-03-27 00:13:08.559408 Epoch 1, Training Loss 1.2253860575158884\n",
      "2022-03-27 00:13:08.578853 Epoch 1, Training Loss 1.2279266122052126\n",
      "2022-03-27 00:13:08.598430 Epoch 1, Training Loss 1.2304418477255974\n",
      "2022-03-27 00:13:08.618441 Epoch 1, Training Loss 1.2330557790863546\n",
      "2022-03-27 00:13:08.637451 Epoch 1, Training Loss 1.2356421374299031\n",
      "2022-03-27 00:13:08.657609 Epoch 1, Training Loss 1.2380918651590567\n",
      "2022-03-27 00:13:08.677116 Epoch 1, Training Loss 1.2406305817082106\n",
      "2022-03-27 00:13:08.695358 Epoch 1, Training Loss 1.243223618515922\n",
      "2022-03-27 00:13:08.716042 Epoch 1, Training Loss 1.2457036414109837\n",
      "2022-03-27 00:13:08.735313 Epoch 1, Training Loss 1.2481220367619448\n",
      "2022-03-27 00:13:08.754540 Epoch 1, Training Loss 1.2505893175254392\n",
      "2022-03-27 00:13:08.773541 Epoch 1, Training Loss 1.2529750269697146\n",
      "2022-03-27 00:13:08.793561 Epoch 1, Training Loss 1.2553943563300325\n",
      "2022-03-27 00:13:08.812565 Epoch 1, Training Loss 1.2578485530355703\n",
      "2022-03-27 00:13:08.833563 Epoch 1, Training Loss 1.2603804171847566\n",
      "2022-03-27 00:13:08.853570 Epoch 1, Training Loss 1.2630957218692125\n",
      "2022-03-27 00:13:08.872568 Epoch 1, Training Loss 1.2657663380093587\n",
      "2022-03-27 00:13:08.892584 Epoch 1, Training Loss 1.2682066718330773\n",
      "2022-03-27 00:13:08.912587 Epoch 1, Training Loss 1.2707414605733378\n",
      "2022-03-27 00:13:08.931586 Epoch 1, Training Loss 1.2732495752441915\n",
      "2022-03-27 00:13:08.951579 Epoch 1, Training Loss 1.275796829739495\n",
      "2022-03-27 00:13:08.970589 Epoch 1, Training Loss 1.2783202195106564\n",
      "2022-03-27 00:13:08.989599 Epoch 1, Training Loss 1.2808142464484096\n",
      "2022-03-27 00:13:09.008603 Epoch 1, Training Loss 1.2833063696961269\n",
      "2022-03-27 00:13:09.028608 Epoch 1, Training Loss 1.2858392884359335\n",
      "2022-03-27 00:13:09.049613 Epoch 1, Training Loss 1.288275259535026\n",
      "2022-03-27 00:13:09.070619 Epoch 1, Training Loss 1.2908599989493486\n",
      "2022-03-27 00:13:09.090616 Epoch 1, Training Loss 1.2933611391145554\n",
      "2022-03-27 00:13:09.110627 Epoch 1, Training Loss 1.2960078213220971\n",
      "2022-03-27 00:13:09.129637 Epoch 1, Training Loss 1.2985590560661862\n",
      "2022-03-27 00:13:09.149075 Epoch 1, Training Loss 1.3010226014020192\n",
      "2022-03-27 00:13:09.169076 Epoch 1, Training Loss 1.3037331249098034\n",
      "2022-03-27 00:13:09.189085 Epoch 1, Training Loss 1.306176201461831\n",
      "2022-03-27 00:13:09.209083 Epoch 1, Training Loss 1.3087624192542737\n",
      "2022-03-27 00:13:09.229086 Epoch 1, Training Loss 1.3111883225038534\n",
      "2022-03-27 00:13:09.249090 Epoch 1, Training Loss 1.3136879946569653\n",
      "2022-03-27 00:13:09.268095 Epoch 1, Training Loss 1.3161054078270407\n",
      "2022-03-27 00:13:09.289099 Epoch 1, Training Loss 1.3186111448670896\n",
      "2022-03-27 00:13:09.308105 Epoch 1, Training Loss 1.3211626226030042\n",
      "2022-03-27 00:13:09.327122 Epoch 1, Training Loss 1.3237762472513692\n",
      "2022-03-27 00:13:09.347126 Epoch 1, Training Loss 1.3262350002822973\n",
      "2022-03-27 00:13:09.367151 Epoch 1, Training Loss 1.3287715056668157\n",
      "2022-03-27 00:13:09.387129 Epoch 1, Training Loss 1.3314050867429474\n",
      "2022-03-27 00:13:09.406139 Epoch 1, Training Loss 1.3339069289014773\n",
      "2022-03-27 00:13:09.427151 Epoch 1, Training Loss 1.3365032456415085\n",
      "2022-03-27 00:13:09.447149 Epoch 1, Training Loss 1.3389872545781343\n",
      "2022-03-27 00:13:09.468154 Epoch 1, Training Loss 1.3413722940418116\n",
      "2022-03-27 00:13:09.487158 Epoch 1, Training Loss 1.3439012290266774\n",
      "2022-03-27 00:13:09.507169 Epoch 1, Training Loss 1.346376852916025\n",
      "2022-03-27 00:13:09.526173 Epoch 1, Training Loss 1.3489423447557727\n",
      "2022-03-27 00:13:09.546160 Epoch 1, Training Loss 1.3514975445044926\n",
      "2022-03-27 00:13:09.566176 Epoch 1, Training Loss 1.353911335510976\n",
      "2022-03-27 00:13:09.585177 Epoch 1, Training Loss 1.356451419155921\n",
      "2022-03-27 00:13:09.605191 Epoch 1, Training Loss 1.3589687103505634\n",
      "2022-03-27 00:13:09.625182 Epoch 1, Training Loss 1.361583774961779\n",
      "2022-03-27 00:13:09.644194 Epoch 1, Training Loss 1.3641648877917043\n",
      "2022-03-27 00:13:09.664198 Epoch 1, Training Loss 1.3666116069344914\n",
      "2022-03-27 00:13:09.684207 Epoch 1, Training Loss 1.3692157719751148\n",
      "2022-03-27 00:13:09.704202 Epoch 1, Training Loss 1.3717130794549537\n",
      "2022-03-27 00:13:09.724218 Epoch 1, Training Loss 1.3743390485148905\n",
      "2022-03-27 00:13:09.745210 Epoch 1, Training Loss 1.3768104087666173\n",
      "2022-03-27 00:13:09.765221 Epoch 1, Training Loss 1.3793307524507918\n",
      "2022-03-27 00:13:09.785226 Epoch 1, Training Loss 1.3817844206414869\n",
      "2022-03-27 00:13:09.805236 Epoch 1, Training Loss 1.3841771318784455\n",
      "2022-03-27 00:13:09.825241 Epoch 1, Training Loss 1.386745537790801\n",
      "2022-03-27 00:13:09.845239 Epoch 1, Training Loss 1.389127724006048\n",
      "2022-03-27 00:13:09.865231 Epoch 1, Training Loss 1.3917000330317661\n",
      "2022-03-27 00:13:09.885236 Epoch 1, Training Loss 1.3941207883303122\n",
      "2022-03-27 00:13:09.905259 Epoch 1, Training Loss 1.3966769836747739\n",
      "2022-03-27 00:13:09.925264 Epoch 1, Training Loss 1.3991606686731128\n",
      "2022-03-27 00:13:09.944268 Epoch 1, Training Loss 1.4015852600107412\n",
      "2022-03-27 00:13:09.964273 Epoch 1, Training Loss 1.4041218181400348\n",
      "2022-03-27 00:13:09.984278 Epoch 1, Training Loss 1.4067573032110854\n",
      "2022-03-27 00:13:10.004267 Epoch 1, Training Loss 1.4092276510985002\n",
      "2022-03-27 00:13:10.024274 Epoch 1, Training Loss 1.4117621706269892\n",
      "2022-03-27 00:13:10.045272 Epoch 1, Training Loss 1.4143097761951748\n",
      "2022-03-27 00:13:10.066276 Epoch 1, Training Loss 1.4167410461494074\n",
      "2022-03-27 00:13:10.086280 Epoch 1, Training Loss 1.419218099025814\n",
      "2022-03-27 00:13:10.109284 Epoch 1, Training Loss 1.4217859497460563\n",
      "2022-03-27 00:13:10.131290 Epoch 1, Training Loss 1.4243769612153778\n",
      "2022-03-27 00:13:10.153297 Epoch 1, Training Loss 1.4269012772213772\n",
      "2022-03-27 00:13:10.173455 Epoch 1, Training Loss 1.4295648783064254\n",
      "2022-03-27 00:13:10.194483 Epoch 1, Training Loss 1.4320381541386284\n",
      "2022-03-27 00:13:10.215494 Epoch 1, Training Loss 1.4345904724372318\n",
      "2022-03-27 00:13:10.236888 Epoch 1, Training Loss 1.4370335729225823\n",
      "2022-03-27 00:13:10.256530 Epoch 1, Training Loss 1.4393994306664333\n",
      "2022-03-27 00:13:10.278154 Epoch 1, Training Loss 1.4418761112805827\n",
      "2022-03-27 00:13:10.297773 Epoch 1, Training Loss 1.4442200450336231\n",
      "2022-03-27 00:13:10.318793 Epoch 1, Training Loss 1.446769784173697\n",
      "2022-03-27 00:13:10.338944 Epoch 1, Training Loss 1.4494270479587643\n",
      "2022-03-27 00:13:10.360025 Epoch 1, Training Loss 1.4520936774475801\n",
      "2022-03-27 00:13:10.380069 Epoch 1, Training Loss 1.4544909226010219\n",
      "2022-03-27 00:13:10.399912 Epoch 1, Training Loss 1.457013525164036\n",
      "2022-03-27 00:13:10.421021 Epoch 1, Training Loss 1.459625624177401\n",
      "2022-03-27 00:13:10.441031 Epoch 1, Training Loss 1.4619856819777233\n",
      "2022-03-27 00:13:10.461853 Epoch 1, Training Loss 1.4644686081220426\n",
      "2022-03-27 00:13:10.481916 Epoch 1, Training Loss 1.4668410097241706\n",
      "2022-03-27 00:13:10.502847 Epoch 1, Training Loss 1.4692879938103658\n",
      "2022-03-27 00:13:10.522958 Epoch 1, Training Loss 1.4718039140981787\n",
      "2022-03-27 00:13:10.543075 Epoch 1, Training Loss 1.4741745493601046\n",
      "2022-03-27 00:13:10.563139 Epoch 1, Training Loss 1.4766980264802723\n",
      "2022-03-27 00:13:10.583961 Epoch 1, Training Loss 1.4792492101564432\n",
      "2022-03-27 00:13:10.605060 Epoch 1, Training Loss 1.481814982183754\n",
      "2022-03-27 00:13:10.624090 Epoch 1, Training Loss 1.4844807128772102\n",
      "2022-03-27 00:13:10.645009 Epoch 1, Training Loss 1.4869154973713028\n",
      "2022-03-27 00:13:10.667045 Epoch 1, Training Loss 1.4893605431632313\n",
      "2022-03-27 00:13:10.687083 Epoch 1, Training Loss 1.4918458291026941\n",
      "2022-03-27 00:13:10.706341 Epoch 1, Training Loss 1.4943405920282349\n",
      "2022-03-27 00:13:10.727372 Epoch 1, Training Loss 1.4968874753283723\n",
      "2022-03-27 00:13:10.748390 Epoch 1, Training Loss 1.4993247880655176\n",
      "2022-03-27 00:13:10.770398 Epoch 1, Training Loss 1.5017242143526102\n",
      "2022-03-27 00:13:10.796616 Epoch 1, Training Loss 1.5042057659315027\n",
      "2022-03-27 00:13:10.824008 Epoch 1, Training Loss 1.5067309385065533\n",
      "2022-03-27 00:13:10.850878 Epoch 1, Training Loss 1.509253645492027\n",
      "2022-03-27 00:13:10.877916 Epoch 1, Training Loss 1.511589778048913\n",
      "2022-03-27 00:13:10.904900 Epoch 1, Training Loss 1.5140690893468345\n",
      "2022-03-27 00:13:10.931942 Epoch 1, Training Loss 1.5165106103853192\n",
      "2022-03-27 00:13:10.958948 Epoch 1, Training Loss 1.5188584525871764\n",
      "2022-03-27 00:13:10.987018 Epoch 1, Training Loss 1.5214300600768964\n",
      "2022-03-27 00:13:11.012824 Epoch 1, Training Loss 1.5239192428796187\n",
      "2022-03-27 00:13:11.039781 Epoch 1, Training Loss 1.5264188287508151\n",
      "2022-03-27 00:13:11.067489 Epoch 1, Training Loss 1.5292011936912147\n",
      "2022-03-27 00:13:11.093524 Epoch 1, Training Loss 1.5316054093868225\n",
      "2022-03-27 00:13:11.121514 Epoch 1, Training Loss 1.5341190351244738\n",
      "2022-03-27 00:13:11.148530 Epoch 1, Training Loss 1.536478140774895\n",
      "2022-03-27 00:13:11.175421 Epoch 1, Training Loss 1.5390688873008085\n",
      "2022-03-27 00:13:11.194452 Epoch 1, Training Loss 1.5415229230280727\n",
      "2022-03-27 00:13:11.214480 Epoch 1, Training Loss 1.5439419124437415\n",
      "2022-03-27 00:13:11.234574 Epoch 1, Training Loss 1.5464189762959395\n",
      "2022-03-27 00:13:11.253129 Epoch 1, Training Loss 1.5488531243465746\n",
      "2022-03-27 00:13:11.272173 Epoch 1, Training Loss 1.5511942579008429\n",
      "2022-03-27 00:13:11.291091 Epoch 1, Training Loss 1.5537539969015\n",
      "2022-03-27 00:13:11.310160 Epoch 1, Training Loss 1.556221324921874\n",
      "2022-03-27 00:13:11.329172 Epoch 1, Training Loss 1.5586407975772458\n",
      "2022-03-27 00:13:11.349208 Epoch 1, Training Loss 1.5612291035139958\n",
      "2022-03-27 00:13:11.368120 Epoch 1, Training Loss 1.5637486566363088\n",
      "2022-03-27 00:13:11.388149 Epoch 1, Training Loss 1.5662364275254252\n",
      "2022-03-27 00:13:11.406864 Epoch 1, Training Loss 1.5686069873287856\n",
      "2022-03-27 00:13:11.427931 Epoch 1, Training Loss 1.5709455696213277\n",
      "2022-03-27 00:13:11.446906 Epoch 1, Training Loss 1.5736100914533182\n",
      "2022-03-27 00:13:11.466924 Epoch 1, Training Loss 1.5759979408720266\n",
      "2022-03-27 00:13:11.485839 Epoch 1, Training Loss 1.578479805565856\n",
      "2022-03-27 00:13:11.505847 Epoch 1, Training Loss 1.5810224735523428\n",
      "2022-03-27 00:13:11.524939 Epoch 1, Training Loss 1.5833813188326022\n",
      "2022-03-27 00:13:11.544908 Epoch 1, Training Loss 1.5859050672987234\n",
      "2022-03-27 00:13:11.564062 Epoch 1, Training Loss 1.5884198842146207\n",
      "2022-03-27 00:13:11.584101 Epoch 1, Training Loss 1.5908532084711373\n",
      "2022-03-27 00:13:11.603269 Epoch 1, Training Loss 1.5934066153548259\n",
      "2022-03-27 00:13:11.622216 Epoch 1, Training Loss 1.5958831072463404\n",
      "2022-03-27 00:13:11.641263 Epoch 1, Training Loss 1.5985047036729505\n",
      "2022-03-27 00:13:11.662241 Epoch 1, Training Loss 1.6009573354135693\n",
      "2022-03-27 00:13:11.681293 Epoch 1, Training Loss 1.6033220519800016\n",
      "2022-03-27 00:13:11.701252 Epoch 1, Training Loss 1.605728532347228\n",
      "2022-03-27 00:13:11.720070 Epoch 1, Training Loss 1.6082735647021047\n",
      "2022-03-27 00:13:11.739024 Epoch 1, Training Loss 1.610584355986027\n",
      "2022-03-27 00:13:11.758765 Epoch 1, Training Loss 1.6130384347018074\n",
      "2022-03-27 00:13:11.777783 Epoch 1, Training Loss 1.615533052350554\n",
      "2022-03-27 00:13:11.796514 Epoch 1, Training Loss 1.6179705635665933\n",
      "2022-03-27 00:13:11.817054 Epoch 1, Training Loss 1.6205421609951711\n",
      "2022-03-27 00:13:11.836046 Epoch 1, Training Loss 1.6231235774886577\n",
      "2022-03-27 00:13:11.856083 Epoch 1, Training Loss 1.6255898359791396\n",
      "2022-03-27 00:13:11.875051 Epoch 1, Training Loss 1.6279858958995557\n",
      "2022-03-27 00:13:11.895904 Epoch 1, Training Loss 1.630477805576666\n",
      "2022-03-27 00:13:11.914893 Epoch 1, Training Loss 1.6330213633644612\n",
      "2022-03-27 00:13:11.934684 Epoch 1, Training Loss 1.635573473580353\n",
      "2022-03-27 00:13:11.953947 Epoch 1, Training Loss 1.637966402351399\n",
      "2022-03-27 00:13:11.973464 Epoch 1, Training Loss 1.640501946591965\n",
      "2022-03-27 00:13:11.992520 Epoch 1, Training Loss 1.6429549356555695\n",
      "2022-03-27 00:13:12.012483 Epoch 1, Training Loss 1.645454875343596\n",
      "2022-03-27 00:13:12.031519 Epoch 1, Training Loss 1.6477823289459015\n",
      "2022-03-27 00:13:12.051868 Epoch 1, Training Loss 1.6503331982876028\n",
      "2022-03-27 00:13:12.072519 Epoch 1, Training Loss 1.6528111790757045\n",
      "2022-03-27 00:13:12.091370 Epoch 1, Training Loss 1.6552635278848127\n",
      "2022-03-27 00:13:12.111505 Epoch 1, Training Loss 1.6576756584979688\n",
      "2022-03-27 00:13:12.131497 Epoch 1, Training Loss 1.6603220202733793\n",
      "2022-03-27 00:13:12.150559 Epoch 1, Training Loss 1.6628032892256441\n",
      "2022-03-27 00:13:12.169534 Epoch 1, Training Loss 1.6651865222569926\n",
      "2022-03-27 00:13:12.189509 Epoch 1, Training Loss 1.6676510455053482\n",
      "2022-03-27 00:13:12.208509 Epoch 1, Training Loss 1.6700187791948733\n",
      "2022-03-27 00:13:12.229571 Epoch 1, Training Loss 1.672358657850329\n",
      "2022-03-27 00:13:12.248568 Epoch 1, Training Loss 1.6749132882298716\n",
      "2022-03-27 00:13:12.268584 Epoch 1, Training Loss 1.6771165219414266\n",
      "2022-03-27 00:13:12.287361 Epoch 1, Training Loss 1.6796608408698646\n",
      "2022-03-27 00:13:12.306640 Epoch 1, Training Loss 1.6822089397388955\n",
      "2022-03-27 00:13:12.326733 Epoch 1, Training Loss 1.684528639706809\n",
      "2022-03-27 00:13:12.345706 Epoch 1, Training Loss 1.6869001266596568\n",
      "2022-03-27 00:13:12.365477 Epoch 1, Training Loss 1.689378989474548\n",
      "2022-03-27 00:13:12.384436 Epoch 1, Training Loss 1.6919523073584222\n",
      "2022-03-27 00:13:12.403680 Epoch 1, Training Loss 1.694364509771547\n",
      "2022-03-27 00:13:12.422788 Epoch 1, Training Loss 1.696939208775835\n",
      "2022-03-27 00:13:12.443811 Epoch 1, Training Loss 1.6995284824115235\n",
      "2022-03-27 00:13:12.462950 Epoch 1, Training Loss 1.7021357633573624\n",
      "2022-03-27 00:13:12.483440 Epoch 1, Training Loss 1.7046170306327704\n",
      "2022-03-27 00:13:12.502483 Epoch 1, Training Loss 1.7070920412497752\n",
      "2022-03-27 00:13:12.522305 Epoch 1, Training Loss 1.7096612694318338\n",
      "2022-03-27 00:13:12.541316 Epoch 1, Training Loss 1.7120671915581158\n",
      "2022-03-27 00:13:12.561332 Epoch 1, Training Loss 1.7145985938094157\n",
      "2022-03-27 00:13:12.580262 Epoch 1, Training Loss 1.7169569742953992\n",
      "2022-03-27 00:13:12.600284 Epoch 1, Training Loss 1.719328943115976\n",
      "2022-03-27 00:13:12.619149 Epoch 1, Training Loss 1.7216953763266658\n",
      "2022-03-27 00:13:12.637633 Epoch 1, Training Loss 1.724196749574998\n",
      "2022-03-27 00:13:12.657650 Epoch 1, Training Loss 1.72665052981023\n",
      "2022-03-27 00:13:12.677705 Epoch 1, Training Loss 1.7289720557229904\n",
      "2022-03-27 00:13:12.697763 Epoch 1, Training Loss 1.7316158232481584\n",
      "2022-03-27 00:13:12.717686 Epoch 1, Training Loss 1.7342962959538335\n",
      "2022-03-27 00:13:12.736634 Epoch 1, Training Loss 1.7367440599309818\n",
      "2022-03-27 00:13:12.755687 Epoch 1, Training Loss 1.739165496338359\n",
      "2022-03-27 00:13:12.775791 Epoch 1, Training Loss 1.7416319126058417\n",
      "2022-03-27 00:13:12.795791 Epoch 1, Training Loss 1.7438109950031466\n",
      "2022-03-27 00:13:12.814642 Epoch 1, Training Loss 1.7461740126085403\n",
      "2022-03-27 00:13:12.834574 Epoch 1, Training Loss 1.7484851510018644\n",
      "2022-03-27 00:13:12.854621 Epoch 1, Training Loss 1.750872826484768\n",
      "2022-03-27 00:13:12.873878 Epoch 1, Training Loss 1.7533433184294445\n",
      "2022-03-27 00:13:12.893889 Epoch 1, Training Loss 1.7557736337947114\n",
      "2022-03-27 00:13:12.913970 Epoch 1, Training Loss 1.7581124096880179\n",
      "2022-03-27 00:13:12.933060 Epoch 1, Training Loss 1.7606395947963684\n",
      "2022-03-27 00:13:12.952813 Epoch 1, Training Loss 1.7629913832525463\n",
      "2022-03-27 00:13:12.971710 Epoch 1, Training Loss 1.7653961071882711\n",
      "2022-03-27 00:13:12.990103 Epoch 1, Training Loss 1.76805748293162\n",
      "2022-03-27 00:13:13.015113 Epoch 1, Training Loss 1.7703262852585835\n",
      "2022-03-27 00:13:13.042185 Epoch 1, Training Loss 1.7729254977782365\n",
      "2022-03-27 00:13:13.069598 Epoch 1, Training Loss 1.7752974167504274\n",
      "2022-03-27 00:13:13.096158 Epoch 1, Training Loss 1.7777574169056496\n",
      "2022-03-27 00:13:13.122029 Epoch 1, Training Loss 1.7801837745834799\n",
      "2022-03-27 00:13:13.148912 Epoch 1, Training Loss 1.7825293611077702\n",
      "2022-03-27 00:13:13.176001 Epoch 1, Training Loss 1.7849064419031753\n",
      "2022-03-27 00:13:13.202072 Epoch 1, Training Loss 1.7874017490450378\n",
      "2022-03-27 00:13:13.221101 Epoch 1, Training Loss 1.7897353260718343\n",
      "2022-03-27 00:13:13.239123 Epoch 1, Training Loss 1.7920846562556294\n",
      "2022-03-27 00:13:13.257047 Epoch 1, Training Loss 1.7943894855506586\n",
      "2022-03-27 00:13:13.276342 Epoch 1, Training Loss 1.7968127925682555\n",
      "2022-03-27 00:13:13.296418 Epoch 1, Training Loss 1.7991689332305927\n",
      "2022-03-27 00:13:13.316189 Epoch 1, Training Loss 1.801626758349826\n",
      "2022-03-27 00:13:13.335073 Epoch 1, Training Loss 1.804089241930286\n",
      "2022-03-27 00:13:13.354453 Epoch 1, Training Loss 1.806752117698455\n",
      "2022-03-27 00:13:13.373080 Epoch 1, Training Loss 1.8092296042710618\n",
      "2022-03-27 00:13:13.392293 Epoch 1, Training Loss 1.8116091712356528\n",
      "2022-03-27 00:13:13.411309 Epoch 1, Training Loss 1.8141884526328358\n",
      "2022-03-27 00:13:13.430337 Epoch 1, Training Loss 1.8166630254377185\n",
      "2022-03-27 00:13:13.449517 Epoch 1, Training Loss 1.8188936635661308\n",
      "2022-03-27 00:13:13.467512 Epoch 1, Training Loss 1.8212093591994947\n",
      "2022-03-27 00:13:13.486632 Epoch 1, Training Loss 1.8236668396483906\n",
      "2022-03-27 00:13:13.505677 Epoch 1, Training Loss 1.8259886593160117\n",
      "2022-03-27 00:13:13.523781 Epoch 1, Training Loss 1.8285775625187417\n",
      "2022-03-27 00:13:13.542830 Epoch 1, Training Loss 1.8310915548783129\n",
      "2022-03-27 00:13:13.560884 Epoch 1, Training Loss 1.833434064065099\n",
      "2022-03-27 00:13:13.578915 Epoch 1, Training Loss 1.8358358053295203\n",
      "2022-03-27 00:13:13.598160 Epoch 1, Training Loss 1.8382605998717305\n",
      "2022-03-27 00:13:13.617176 Epoch 1, Training Loss 1.8407391913406683\n",
      "2022-03-27 00:13:13.635184 Epoch 1, Training Loss 1.8431823988399847\n",
      "2022-03-27 00:13:13.654496 Epoch 1, Training Loss 1.8456434115119602\n",
      "2022-03-27 00:13:13.672573 Epoch 1, Training Loss 1.847985464441197\n",
      "2022-03-27 00:13:13.691614 Epoch 1, Training Loss 1.850298677259089\n",
      "2022-03-27 00:13:13.710269 Epoch 1, Training Loss 1.8525115849112002\n",
      "2022-03-27 00:13:13.729305 Epoch 1, Training Loss 1.8549709036527082\n",
      "2022-03-27 00:13:13.748334 Epoch 1, Training Loss 1.8573289222424598\n",
      "2022-03-27 00:13:13.767528 Epoch 1, Training Loss 1.8599392892149709\n",
      "2022-03-27 00:13:13.786577 Epoch 1, Training Loss 1.862323793608819\n",
      "2022-03-27 00:13:13.805564 Epoch 1, Training Loss 1.8645997515419865\n",
      "2022-03-27 00:13:13.823605 Epoch 1, Training Loss 1.8669677297477527\n",
      "2022-03-27 00:13:13.842731 Epoch 1, Training Loss 1.8693053612623678\n",
      "2022-03-27 00:13:13.860714 Epoch 1, Training Loss 1.8718526617950186\n",
      "2022-03-27 00:13:13.879626 Epoch 1, Training Loss 1.8743308752089205\n",
      "2022-03-27 00:13:13.899618 Epoch 1, Training Loss 1.87653925107873\n",
      "2022-03-27 00:13:13.918563 Epoch 1, Training Loss 1.8789155780506865\n",
      "2022-03-27 00:13:13.937039 Epoch 1, Training Loss 1.8812714392876686\n",
      "2022-03-27 00:13:13.954903 Epoch 1, Training Loss 1.8835856507501334\n",
      "2022-03-27 00:13:13.973602 Epoch 1, Training Loss 1.8859440698038281\n",
      "2022-03-27 00:13:13.992649 Epoch 1, Training Loss 1.8884169412086078\n",
      "2022-03-27 00:13:14.010726 Epoch 1, Training Loss 1.8909028000234034\n",
      "2022-03-27 00:13:14.029668 Epoch 1, Training Loss 1.8932332806575023\n",
      "2022-03-27 00:13:14.047620 Epoch 1, Training Loss 1.8957771861644657\n",
      "2022-03-27 00:13:14.067594 Epoch 1, Training Loss 1.8982934252075527\n",
      "2022-03-27 00:13:14.085630 Epoch 1, Training Loss 1.9008597783420398\n",
      "2022-03-27 00:13:14.104718 Epoch 1, Training Loss 1.9033662708823944\n",
      "2022-03-27 00:13:14.123792 Epoch 1, Training Loss 1.9057053792507141\n",
      "2022-03-27 00:13:14.142845 Epoch 1, Training Loss 1.9081562983105556\n",
      "2022-03-27 00:13:14.160827 Epoch 1, Training Loss 1.9105024330146478\n",
      "2022-03-27 00:13:14.179866 Epoch 1, Training Loss 1.912981333787484\n",
      "2022-03-27 00:13:14.197968 Epoch 1, Training Loss 1.915503476586793\n",
      "2022-03-27 00:13:14.216745 Epoch 1, Training Loss 1.917897755989943\n",
      "2022-03-27 00:13:14.235741 Epoch 1, Training Loss 1.9203674578300827\n",
      "2022-03-27 00:13:14.253723 Epoch 1, Training Loss 1.922712859290335\n",
      "2022-03-27 00:13:14.271766 Epoch 1, Training Loss 1.9251637129527528\n",
      "2022-03-27 00:13:14.290743 Epoch 1, Training Loss 1.9276130984506339\n",
      "2022-03-27 00:13:14.309748 Epoch 1, Training Loss 1.9300195269877343\n",
      "2022-03-27 00:13:14.328825 Epoch 1, Training Loss 1.9325494318057204\n",
      "2022-03-27 00:13:14.347375 Epoch 1, Training Loss 1.9351126361076179\n",
      "2022-03-27 00:13:14.365373 Epoch 1, Training Loss 1.9375173550127718\n",
      "2022-03-27 00:13:14.385513 Epoch 1, Training Loss 1.9398312436040406\n",
      "2022-03-27 00:13:14.403522 Epoch 1, Training Loss 1.9422929681780394\n",
      "2022-03-27 00:13:14.422723 Epoch 1, Training Loss 1.944734164973354\n",
      "2022-03-27 00:13:14.441908 Epoch 1, Training Loss 1.9472726354818515\n",
      "2022-03-27 00:13:14.461109 Epoch 1, Training Loss 1.9497346432922442\n",
      "2022-03-27 00:13:14.480050 Epoch 1, Training Loss 1.9523134079125837\n",
      "2022-03-27 00:13:14.498919 Epoch 1, Training Loss 1.9549282618495814\n",
      "2022-03-27 00:13:14.518223 Epoch 1, Training Loss 1.9573890624753654\n",
      "2022-03-27 00:13:14.538131 Epoch 1, Training Loss 1.9598658943115292\n",
      "2022-03-27 00:13:14.557195 Epoch 1, Training Loss 1.9625763254397361\n",
      "2022-03-27 00:13:14.576250 Epoch 1, Training Loss 1.964935870121812\n",
      "2022-03-27 00:13:14.595191 Epoch 1, Training Loss 1.9673523861733848\n",
      "2022-03-27 00:13:14.614190 Epoch 1, Training Loss 1.9700095078829305\n",
      "2022-03-27 00:13:14.633075 Epoch 1, Training Loss 1.9723397598547094\n",
      "2022-03-27 00:13:14.653280 Epoch 1, Training Loss 1.9747523099870024\n",
      "2022-03-27 00:13:14.672039 Epoch 1, Training Loss 1.9771659833086117\n",
      "2022-03-27 00:13:14.690113 Epoch 1, Training Loss 1.9797033655369067\n",
      "2022-03-27 00:13:14.710025 Epoch 1, Training Loss 1.9821204808362\n",
      "2022-03-27 00:13:14.729581 Epoch 1, Training Loss 1.9843381684454506\n",
      "2022-03-27 00:13:14.749953 Epoch 1, Training Loss 1.986722981533431\n",
      "2022-03-27 00:13:14.769286 Epoch 1, Training Loss 1.9890765813000673\n",
      "2022-03-27 00:13:14.787305 Epoch 1, Training Loss 1.9913361002417171\n",
      "2022-03-27 00:13:14.807335 Epoch 1, Training Loss 1.9938094858318338\n",
      "2022-03-27 00:13:14.827366 Epoch 1, Training Loss 1.9961069081445484\n",
      "2022-03-27 00:13:14.846240 Epoch 1, Training Loss 1.9985040703697887\n",
      "2022-03-27 00:13:14.864792 Epoch 1, Training Loss 2.000949728671852\n",
      "2022-03-27 00:13:14.884686 Epoch 1, Training Loss 2.00335624760679\n",
      "2022-03-27 00:13:14.902908 Epoch 1, Training Loss 2.0059305478239913\n",
      "2022-03-27 00:13:14.922465 Epoch 1, Training Loss 2.008382570560631\n",
      "2022-03-27 00:13:14.942280 Epoch 1, Training Loss 2.010737669284996\n",
      "2022-03-27 00:13:14.961130 Epoch 1, Training Loss 2.01310744096556\n",
      "2022-03-27 00:13:14.981647 Epoch 1, Training Loss 2.015270776303528\n",
      "2022-03-27 00:13:15.001569 Epoch 1, Training Loss 2.0175961301759684\n",
      "2022-03-27 00:13:15.020720 Epoch 1, Training Loss 2.0199008952931066\n",
      "2022-03-27 00:13:15.038805 Epoch 1, Training Loss 2.0223283714345652\n",
      "2022-03-27 00:13:15.060230 Epoch 1, Training Loss 2.02474535776831\n",
      "2022-03-27 00:13:15.079253 Epoch 1, Training Loss 2.0272386988715443\n",
      "2022-03-27 00:13:15.099183 Epoch 1, Training Loss 2.029603894561758\n",
      "2022-03-27 00:13:15.118050 Epoch 1, Training Loss 2.0319479211517004\n",
      "2022-03-27 00:13:15.137072 Epoch 1, Training Loss 2.03432784574416\n",
      "2022-03-27 00:13:15.156904 Epoch 1, Training Loss 2.0367042768336927\n",
      "2022-03-27 00:13:15.176346 Epoch 1, Training Loss 2.039029718207581\n",
      "2022-03-27 00:13:15.195362 Epoch 1, Training Loss 2.041477476239509\n",
      "2022-03-27 00:13:15.215248 Epoch 1, Training Loss 2.0439178596067307\n",
      "2022-03-27 00:13:15.235177 Epoch 1, Training Loss 2.046094483274328\n",
      "2022-03-27 00:13:15.255187 Epoch 1, Training Loss 2.0483204021173367\n",
      "2022-03-27 00:13:15.274226 Epoch 1, Training Loss 2.050667282413034\n",
      "2022-03-27 00:13:15.293092 Epoch 1, Training Loss 2.053030038886058\n",
      "2022-03-27 00:13:15.303096 Epoch 1, Training Loss 2.0551747385498205\n",
      "2022-03-27 00:24:50.027449 Epoch 50, Training Loss 0.0011686770355000215\n",
      "2022-03-27 00:24:50.045211 Epoch 50, Training Loss 0.002378291867273238\n",
      "2022-03-27 00:24:50.065215 Epoch 50, Training Loss 0.0034236096207747985\n",
      "2022-03-27 00:24:50.084219 Epoch 50, Training Loss 0.004749628512755684\n",
      "2022-03-27 00:24:50.104224 Epoch 50, Training Loss 0.0057696276308630435\n",
      "2022-03-27 00:24:50.123228 Epoch 50, Training Loss 0.00721084217891059\n",
      "2022-03-27 00:24:50.145233 Epoch 50, Training Loss 0.008407294064226663\n",
      "2022-03-27 00:24:50.163237 Epoch 50, Training Loss 0.009601531964738655\n",
      "2022-03-27 00:24:50.182242 Epoch 50, Training Loss 0.010595400619994649\n",
      "2022-03-27 00:24:50.202249 Epoch 50, Training Loss 0.011954089564740505\n",
      "2022-03-27 00:24:50.221034 Epoch 50, Training Loss 0.013328054372002096\n",
      "2022-03-27 00:24:50.241453 Epoch 50, Training Loss 0.014542988346665717\n",
      "2022-03-27 00:24:50.260375 Epoch 50, Training Loss 0.01580835943636687\n",
      "2022-03-27 00:24:50.286413 Epoch 50, Training Loss 0.016886982009234025\n",
      "2022-03-27 00:24:50.312469 Epoch 50, Training Loss 0.018228947506536305\n",
      "2022-03-27 00:24:50.338762 Epoch 50, Training Loss 0.019442450359959125\n",
      "2022-03-27 00:24:50.365691 Epoch 50, Training Loss 0.02080141949226789\n",
      "2022-03-27 00:24:50.391554 Epoch 50, Training Loss 0.021956561547715952\n",
      "2022-03-27 00:24:50.418620 Epoch 50, Training Loss 0.023337342931181573\n",
      "2022-03-27 00:24:50.444586 Epoch 50, Training Loss 0.024303588492181297\n",
      "2022-03-27 00:24:50.472570 Epoch 50, Training Loss 0.02556958634530187\n",
      "2022-03-27 00:24:50.490553 Epoch 50, Training Loss 0.026820373199784846\n",
      "2022-03-27 00:24:50.509735 Epoch 50, Training Loss 0.028123297654759245\n",
      "2022-03-27 00:24:50.528747 Epoch 50, Training Loss 0.029160020448972503\n",
      "2022-03-27 00:24:50.547811 Epoch 50, Training Loss 0.030708566346131932\n",
      "2022-03-27 00:24:50.565847 Epoch 50, Training Loss 0.03184687092785945\n",
      "2022-03-27 00:24:50.584885 Epoch 50, Training Loss 0.033343637462162304\n",
      "2022-03-27 00:24:50.602547 Epoch 50, Training Loss 0.03463791497528096\n",
      "2022-03-27 00:24:50.621569 Epoch 50, Training Loss 0.035838751917909785\n",
      "2022-03-27 00:24:50.640616 Epoch 50, Training Loss 0.03695239008540083\n",
      "2022-03-27 00:24:50.659781 Epoch 50, Training Loss 0.03798558362914473\n",
      "2022-03-27 00:24:50.678840 Epoch 50, Training Loss 0.039148121081349796\n",
      "2022-03-27 00:24:50.697054 Epoch 50, Training Loss 0.040464552314690004\n",
      "2022-03-27 00:24:50.716107 Epoch 50, Training Loss 0.04210452929787014\n",
      "2022-03-27 00:24:50.735006 Epoch 50, Training Loss 0.04367138510165007\n",
      "2022-03-27 00:24:50.753848 Epoch 50, Training Loss 0.044947110630972\n",
      "2022-03-27 00:24:50.772800 Epoch 50, Training Loss 0.04614086574910547\n",
      "2022-03-27 00:24:50.790850 Epoch 50, Training Loss 0.047318019068149655\n",
      "2022-03-27 00:24:50.809663 Epoch 50, Training Loss 0.04865914613694486\n",
      "2022-03-27 00:24:50.827700 Epoch 50, Training Loss 0.04994446313594613\n",
      "2022-03-27 00:24:50.846751 Epoch 50, Training Loss 0.05106051087074572\n",
      "2022-03-27 00:24:50.864846 Epoch 50, Training Loss 0.052029577210126327\n",
      "2022-03-27 00:24:50.884888 Epoch 50, Training Loss 0.05315216826965742\n",
      "2022-03-27 00:24:50.902550 Epoch 50, Training Loss 0.05433974127330438\n",
      "2022-03-27 00:24:50.921581 Epoch 50, Training Loss 0.055493458457615066\n",
      "2022-03-27 00:24:50.940611 Epoch 50, Training Loss 0.05683936837993924\n",
      "2022-03-27 00:24:50.959241 Epoch 50, Training Loss 0.05796758003551942\n",
      "2022-03-27 00:24:50.977462 Epoch 50, Training Loss 0.05978294970739223\n",
      "2022-03-27 00:24:50.996499 Epoch 50, Training Loss 0.06125473366369067\n",
      "2022-03-27 00:24:51.015509 Epoch 50, Training Loss 0.06255610702592698\n",
      "2022-03-27 00:24:51.034232 Epoch 50, Training Loss 0.06370506186009672\n",
      "2022-03-27 00:24:51.052662 Epoch 50, Training Loss 0.0648727249306486\n",
      "2022-03-27 00:24:51.070710 Epoch 50, Training Loss 0.06611033367074054\n",
      "2022-03-27 00:24:51.090637 Epoch 50, Training Loss 0.06725046419731491\n",
      "2022-03-27 00:24:51.109632 Epoch 50, Training Loss 0.0685457616968228\n",
      "2022-03-27 00:24:51.127732 Epoch 50, Training Loss 0.07022255544772234\n",
      "2022-03-27 00:24:51.146647 Epoch 50, Training Loss 0.07150874166842312\n",
      "2022-03-27 00:24:51.164690 Epoch 50, Training Loss 0.07259053190041076\n",
      "2022-03-27 00:24:51.183711 Epoch 50, Training Loss 0.07371659138623406\n",
      "2022-03-27 00:24:51.202744 Epoch 50, Training Loss 0.07521687627143567\n",
      "2022-03-27 00:24:51.221740 Epoch 50, Training Loss 0.07637684950438302\n",
      "2022-03-27 00:24:51.240678 Epoch 50, Training Loss 0.0776651958218011\n",
      "2022-03-27 00:24:51.259644 Epoch 50, Training Loss 0.0788317617705411\n",
      "2022-03-27 00:24:51.277573 Epoch 50, Training Loss 0.07987377855478955\n",
      "2022-03-27 00:24:51.297446 Epoch 50, Training Loss 0.08075149833698712\n",
      "2022-03-27 00:24:51.315601 Epoch 50, Training Loss 0.08212882448035433\n",
      "2022-03-27 00:24:51.334602 Epoch 50, Training Loss 0.08321603957344503\n",
      "2022-03-27 00:24:51.353706 Epoch 50, Training Loss 0.08421234820809816\n",
      "2022-03-27 00:24:51.371678 Epoch 50, Training Loss 0.0854002753334582\n",
      "2022-03-27 00:24:51.390669 Epoch 50, Training Loss 0.08635225770113718\n",
      "2022-03-27 00:24:51.410390 Epoch 50, Training Loss 0.08770241296809653\n",
      "2022-03-27 00:24:51.428472 Epoch 50, Training Loss 0.08890706087317309\n",
      "2022-03-27 00:24:51.448465 Epoch 50, Training Loss 0.0902759766639651\n",
      "2022-03-27 00:24:51.467502 Epoch 50, Training Loss 0.09162818623320831\n",
      "2022-03-27 00:24:51.486519 Epoch 50, Training Loss 0.09277077670902242\n",
      "2022-03-27 00:24:51.505457 Epoch 50, Training Loss 0.09396492794651509\n",
      "2022-03-27 00:24:51.524741 Epoch 50, Training Loss 0.09511482906158623\n",
      "2022-03-27 00:24:51.544933 Epoch 50, Training Loss 0.09630783241423195\n",
      "2022-03-27 00:24:51.563616 Epoch 50, Training Loss 0.09747824301500149\n",
      "2022-03-27 00:24:51.581652 Epoch 50, Training Loss 0.09894619702987964\n",
      "2022-03-27 00:24:51.600092 Epoch 50, Training Loss 0.10033454576416699\n",
      "2022-03-27 00:24:51.618962 Epoch 50, Training Loss 0.10144417388055026\n",
      "2022-03-27 00:24:51.637958 Epoch 50, Training Loss 0.1024737486144161\n",
      "2022-03-27 00:24:51.655882 Epoch 50, Training Loss 0.1038118549015211\n",
      "2022-03-27 00:24:51.674965 Epoch 50, Training Loss 0.10511094042102394\n",
      "2022-03-27 00:24:51.695000 Epoch 50, Training Loss 0.10665398180637213\n",
      "2022-03-27 00:24:51.713984 Epoch 50, Training Loss 0.10784742274247777\n",
      "2022-03-27 00:24:51.732939 Epoch 50, Training Loss 0.1090992048878194\n",
      "2022-03-27 00:24:51.750986 Epoch 50, Training Loss 0.1103202183075878\n",
      "2022-03-27 00:24:51.769739 Epoch 50, Training Loss 0.1116303203203489\n",
      "2022-03-27 00:24:51.788784 Epoch 50, Training Loss 0.11298853806827379\n",
      "2022-03-27 00:24:51.808410 Epoch 50, Training Loss 0.114145729624097\n",
      "2022-03-27 00:24:51.827504 Epoch 50, Training Loss 0.11539702327050211\n",
      "2022-03-27 00:24:51.844855 Epoch 50, Training Loss 0.11649789948902471\n",
      "2022-03-27 00:24:51.863978 Epoch 50, Training Loss 0.11765507343784927\n",
      "2022-03-27 00:24:51.882017 Epoch 50, Training Loss 0.11865758613857162\n",
      "2022-03-27 00:24:51.901114 Epoch 50, Training Loss 0.11987746089620663\n",
      "2022-03-27 00:24:51.920422 Epoch 50, Training Loss 0.12090108880911336\n",
      "2022-03-27 00:24:51.939427 Epoch 50, Training Loss 0.12204995111126424\n",
      "2022-03-27 00:24:51.958450 Epoch 50, Training Loss 0.1231909703720561\n",
      "2022-03-27 00:24:51.977468 Epoch 50, Training Loss 0.1244702214170295\n",
      "2022-03-27 00:24:51.995813 Epoch 50, Training Loss 0.12561697255619955\n",
      "2022-03-27 00:24:52.013890 Epoch 50, Training Loss 0.12696276662294823\n",
      "2022-03-27 00:24:52.032367 Epoch 50, Training Loss 0.12836016062885294\n",
      "2022-03-27 00:24:52.051471 Epoch 50, Training Loss 0.12968918932673265\n",
      "2022-03-27 00:24:52.070476 Epoch 50, Training Loss 0.13067235673784905\n",
      "2022-03-27 00:24:52.088847 Epoch 50, Training Loss 0.13181438150308322\n",
      "2022-03-27 00:24:52.107819 Epoch 50, Training Loss 0.13311311381552227\n",
      "2022-03-27 00:24:52.128176 Epoch 50, Training Loss 0.13461015093357057\n",
      "2022-03-27 00:24:52.147148 Epoch 50, Training Loss 0.13591022747556877\n",
      "2022-03-27 00:24:52.165176 Epoch 50, Training Loss 0.1370282777587471\n",
      "2022-03-27 00:24:52.184208 Epoch 50, Training Loss 0.13816629498815902\n",
      "2022-03-27 00:24:52.203076 Epoch 50, Training Loss 0.13973755909658758\n",
      "2022-03-27 00:24:52.221052 Epoch 50, Training Loss 0.1412749638033035\n",
      "2022-03-27 00:24:52.240495 Epoch 50, Training Loss 0.1423796035749528\n",
      "2022-03-27 00:24:52.259492 Epoch 50, Training Loss 0.14361276216519153\n",
      "2022-03-27 00:24:52.278534 Epoch 50, Training Loss 0.14495956890113518\n",
      "2022-03-27 00:24:52.296936 Epoch 50, Training Loss 0.1464415435748332\n",
      "2022-03-27 00:24:52.314983 Epoch 50, Training Loss 0.1478308301295161\n",
      "2022-03-27 00:24:52.333952 Epoch 50, Training Loss 0.14934936287762868\n",
      "2022-03-27 00:24:52.353472 Epoch 50, Training Loss 0.1508244447543493\n",
      "2022-03-27 00:24:52.372544 Epoch 50, Training Loss 0.15197028665591383\n",
      "2022-03-27 00:24:52.390561 Epoch 50, Training Loss 0.1533110903961884\n",
      "2022-03-27 00:24:52.409589 Epoch 50, Training Loss 0.15454364821429142\n",
      "2022-03-27 00:24:52.428253 Epoch 50, Training Loss 0.1559272953463942\n",
      "2022-03-27 00:24:52.447638 Epoch 50, Training Loss 0.15719175887534687\n",
      "2022-03-27 00:24:52.468620 Epoch 50, Training Loss 0.15853717244799484\n",
      "2022-03-27 00:24:52.488620 Epoch 50, Training Loss 0.15996563023008653\n",
      "2022-03-27 00:24:52.507526 Epoch 50, Training Loss 0.16130102549672432\n",
      "2022-03-27 00:24:52.526667 Epoch 50, Training Loss 0.16245225933201782\n",
      "2022-03-27 00:24:52.546744 Epoch 50, Training Loss 0.16349274468848773\n",
      "2022-03-27 00:24:52.565912 Epoch 50, Training Loss 0.16468761537386023\n",
      "2022-03-27 00:24:52.584951 Epoch 50, Training Loss 0.1658278566492183\n",
      "2022-03-27 00:24:52.603850 Epoch 50, Training Loss 0.1669176566936171\n",
      "2022-03-27 00:24:52.624073 Epoch 50, Training Loss 0.16821064881961365\n",
      "2022-03-27 00:24:52.643257 Epoch 50, Training Loss 0.16949228603211816\n",
      "2022-03-27 00:24:52.662182 Epoch 50, Training Loss 0.17070507332492058\n",
      "2022-03-27 00:24:52.680308 Epoch 50, Training Loss 0.17197687866742653\n",
      "2022-03-27 00:24:52.699416 Epoch 50, Training Loss 0.17335504141000227\n",
      "2022-03-27 00:24:52.718433 Epoch 50, Training Loss 0.17460773187830014\n",
      "2022-03-27 00:24:52.737546 Epoch 50, Training Loss 0.17576966467111008\n",
      "2022-03-27 00:24:52.756869 Epoch 50, Training Loss 0.17697527051886633\n",
      "2022-03-27 00:24:52.774900 Epoch 50, Training Loss 0.17832914581689077\n",
      "2022-03-27 00:24:52.794320 Epoch 50, Training Loss 0.17939080164560575\n",
      "2022-03-27 00:24:52.814230 Epoch 50, Training Loss 0.1807526968934042\n",
      "2022-03-27 00:24:52.832273 Epoch 50, Training Loss 0.18188991990235762\n",
      "2022-03-27 00:24:52.851144 Epoch 50, Training Loss 0.18320846046938005\n",
      "2022-03-27 00:24:52.870838 Epoch 50, Training Loss 0.18454785793638595\n",
      "2022-03-27 00:24:52.889869 Epoch 50, Training Loss 0.18572352952359583\n",
      "2022-03-27 00:24:52.908815 Epoch 50, Training Loss 0.18708537065464517\n",
      "2022-03-27 00:24:52.928137 Epoch 50, Training Loss 0.18848228507944384\n",
      "2022-03-27 00:24:52.947238 Epoch 50, Training Loss 0.1899423877448987\n",
      "2022-03-27 00:24:52.965248 Epoch 50, Training Loss 0.19104277866575725\n",
      "2022-03-27 00:24:52.985191 Epoch 50, Training Loss 0.19229215238710193\n",
      "2022-03-27 00:24:53.004128 Epoch 50, Training Loss 0.19351636060058613\n",
      "2022-03-27 00:24:53.022971 Epoch 50, Training Loss 0.19492663972822907\n",
      "2022-03-27 00:24:53.041971 Epoch 50, Training Loss 0.19618642703651468\n",
      "2022-03-27 00:24:53.060428 Epoch 50, Training Loss 0.19757004063147718\n",
      "2022-03-27 00:24:53.079459 Epoch 50, Training Loss 0.19859689314042211\n",
      "2022-03-27 00:24:53.097474 Epoch 50, Training Loss 0.19995301451219621\n",
      "2022-03-27 00:24:53.116559 Epoch 50, Training Loss 0.20139123404117495\n",
      "2022-03-27 00:24:53.135807 Epoch 50, Training Loss 0.2026836103033227\n",
      "2022-03-27 00:24:53.154752 Epoch 50, Training Loss 0.20376602844204134\n",
      "2022-03-27 00:24:53.174738 Epoch 50, Training Loss 0.2048773817394091\n",
      "2022-03-27 00:24:53.195738 Epoch 50, Training Loss 0.20602159000113798\n",
      "2022-03-27 00:24:53.213723 Epoch 50, Training Loss 0.20716986433624307\n",
      "2022-03-27 00:24:53.232713 Epoch 50, Training Loss 0.2083348690548821\n",
      "2022-03-27 00:24:53.250719 Epoch 50, Training Loss 0.2097880696244252\n",
      "2022-03-27 00:24:53.269505 Epoch 50, Training Loss 0.21106377015333347\n",
      "2022-03-27 00:24:53.289528 Epoch 50, Training Loss 0.2121723047302812\n",
      "2022-03-27 00:24:53.308853 Epoch 50, Training Loss 0.21343269479244262\n",
      "2022-03-27 00:24:53.328174 Epoch 50, Training Loss 0.2147971899308207\n",
      "2022-03-27 00:24:53.347739 Epoch 50, Training Loss 0.21592403753944064\n",
      "2022-03-27 00:24:53.366613 Epoch 50, Training Loss 0.21728864624677108\n",
      "2022-03-27 00:24:53.385830 Epoch 50, Training Loss 0.21861398799340134\n",
      "2022-03-27 00:24:53.404833 Epoch 50, Training Loss 0.21980539284398792\n",
      "2022-03-27 00:24:53.423862 Epoch 50, Training Loss 0.2211858879422288\n",
      "2022-03-27 00:24:53.443255 Epoch 50, Training Loss 0.2226182592037084\n",
      "2022-03-27 00:24:53.461266 Epoch 50, Training Loss 0.22399834575860397\n",
      "2022-03-27 00:24:53.481256 Epoch 50, Training Loss 0.2251724649573226\n",
      "2022-03-27 00:24:53.502252 Epoch 50, Training Loss 0.22619416585663701\n",
      "2022-03-27 00:24:53.529029 Epoch 50, Training Loss 0.2271064411648704\n",
      "2022-03-27 00:24:53.556508 Epoch 50, Training Loss 0.22830507113500628\n",
      "2022-03-27 00:24:53.583301 Epoch 50, Training Loss 0.22953725951102077\n",
      "2022-03-27 00:24:53.610015 Epoch 50, Training Loss 0.23059577748293766\n",
      "2022-03-27 00:24:53.636452 Epoch 50, Training Loss 0.23184576081802777\n",
      "2022-03-27 00:24:53.663405 Epoch 50, Training Loss 0.23309419039265275\n",
      "2022-03-27 00:24:53.690527 Epoch 50, Training Loss 0.23415536534450854\n",
      "2022-03-27 00:24:53.709408 Epoch 50, Training Loss 0.23530673797783033\n",
      "2022-03-27 00:24:53.728451 Epoch 50, Training Loss 0.23636409404027797\n",
      "2022-03-27 00:24:53.747492 Epoch 50, Training Loss 0.23788818099614603\n",
      "2022-03-27 00:24:53.766097 Epoch 50, Training Loss 0.23923178966088063\n",
      "2022-03-27 00:24:53.785797 Epoch 50, Training Loss 0.2407031705617295\n",
      "2022-03-27 00:24:53.804855 Epoch 50, Training Loss 0.24191625991745677\n",
      "2022-03-27 00:24:53.822837 Epoch 50, Training Loss 0.24319244894530156\n",
      "2022-03-27 00:24:53.842435 Epoch 50, Training Loss 0.24425737319699944\n",
      "2022-03-27 00:24:53.860725 Epoch 50, Training Loss 0.2456229852745905\n",
      "2022-03-27 00:24:53.880319 Epoch 50, Training Loss 0.24717897092899704\n",
      "2022-03-27 00:24:53.898423 Epoch 50, Training Loss 0.24848909961902882\n",
      "2022-03-27 00:24:53.917526 Epoch 50, Training Loss 0.24991607505952002\n",
      "2022-03-27 00:24:53.936541 Epoch 50, Training Loss 0.25114919188077495\n",
      "2022-03-27 00:24:53.955551 Epoch 50, Training Loss 0.2524032644603563\n",
      "2022-03-27 00:24:53.974870 Epoch 50, Training Loss 0.253471131596114\n",
      "2022-03-27 00:24:53.993453 Epoch 50, Training Loss 0.25466640495583226\n",
      "2022-03-27 00:24:54.012415 Epoch 50, Training Loss 0.25563038454946047\n",
      "2022-03-27 00:24:54.030376 Epoch 50, Training Loss 0.25682833531628485\n",
      "2022-03-27 00:24:54.049315 Epoch 50, Training Loss 0.25794470340699494\n",
      "2022-03-27 00:24:54.068233 Epoch 50, Training Loss 0.25909614486767507\n",
      "2022-03-27 00:24:54.087634 Epoch 50, Training Loss 0.26049594729757675\n",
      "2022-03-27 00:24:54.105607 Epoch 50, Training Loss 0.2614264742797598\n",
      "2022-03-27 00:24:54.124709 Epoch 50, Training Loss 0.26301581627877474\n",
      "2022-03-27 00:24:54.144688 Epoch 50, Training Loss 0.26431306945088573\n",
      "2022-03-27 00:24:54.164691 Epoch 50, Training Loss 0.2653574507559657\n",
      "2022-03-27 00:24:54.182814 Epoch 50, Training Loss 0.2666392189157588\n",
      "2022-03-27 00:24:54.201774 Epoch 50, Training Loss 0.26788081232544103\n",
      "2022-03-27 00:24:54.221219 Epoch 50, Training Loss 0.2692093164719584\n",
      "2022-03-27 00:24:54.240250 Epoch 50, Training Loss 0.27036168447235964\n",
      "2022-03-27 00:24:54.259137 Epoch 50, Training Loss 0.2718426223910983\n",
      "2022-03-27 00:24:54.278047 Epoch 50, Training Loss 0.27305309432546804\n",
      "2022-03-27 00:24:54.296040 Epoch 50, Training Loss 0.2743092259330213\n",
      "2022-03-27 00:24:54.314021 Epoch 50, Training Loss 0.27554662712394734\n",
      "2022-03-27 00:24:54.333196 Epoch 50, Training Loss 0.27676241416150654\n",
      "2022-03-27 00:24:54.351244 Epoch 50, Training Loss 0.2779223443297169\n",
      "2022-03-27 00:24:54.369939 Epoch 50, Training Loss 0.2791171228641744\n",
      "2022-03-27 00:24:54.389124 Epoch 50, Training Loss 0.2803130133072739\n",
      "2022-03-27 00:24:54.408088 Epoch 50, Training Loss 0.28129765246530325\n",
      "2022-03-27 00:24:54.426949 Epoch 50, Training Loss 0.2824041916586249\n",
      "2022-03-27 00:24:54.444984 Epoch 50, Training Loss 0.28360252352931614\n",
      "2022-03-27 00:24:54.463857 Epoch 50, Training Loss 0.2846750020980835\n",
      "2022-03-27 00:24:54.482131 Epoch 50, Training Loss 0.2862221911130354\n",
      "2022-03-27 00:24:54.500683 Epoch 50, Training Loss 0.2875310678006438\n",
      "2022-03-27 00:24:54.519653 Epoch 50, Training Loss 0.2888963248418725\n",
      "2022-03-27 00:24:54.538815 Epoch 50, Training Loss 0.29006037809659757\n",
      "2022-03-27 00:24:54.557823 Epoch 50, Training Loss 0.2910610962554317\n",
      "2022-03-27 00:24:54.576947 Epoch 50, Training Loss 0.29246377236093096\n",
      "2022-03-27 00:24:54.594999 Epoch 50, Training Loss 0.2935422204644479\n",
      "2022-03-27 00:24:54.614035 Epoch 50, Training Loss 0.2950855324335415\n",
      "2022-03-27 00:24:54.632382 Epoch 50, Training Loss 0.296357517199748\n",
      "2022-03-27 00:24:54.651134 Epoch 50, Training Loss 0.2977443642323584\n",
      "2022-03-27 00:24:54.670171 Epoch 50, Training Loss 0.2988503092847517\n",
      "2022-03-27 00:24:54.688944 Epoch 50, Training Loss 0.2998865928948688\n",
      "2022-03-27 00:24:54.707942 Epoch 50, Training Loss 0.3010414045332643\n",
      "2022-03-27 00:24:54.726484 Epoch 50, Training Loss 0.3022114748845015\n",
      "2022-03-27 00:24:54.745895 Epoch 50, Training Loss 0.3034377538639566\n",
      "2022-03-27 00:24:54.763468 Epoch 50, Training Loss 0.3044786794716135\n",
      "2022-03-27 00:24:54.781486 Epoch 50, Training Loss 0.30582640863135646\n",
      "2022-03-27 00:24:54.800480 Epoch 50, Training Loss 0.30715044746008674\n",
      "2022-03-27 00:24:54.818873 Epoch 50, Training Loss 0.3085289501473117\n",
      "2022-03-27 00:24:54.838062 Epoch 50, Training Loss 0.3097024453265588\n",
      "2022-03-27 00:24:54.857133 Epoch 50, Training Loss 0.3109489863790819\n",
      "2022-03-27 00:24:54.875982 Epoch 50, Training Loss 0.31223729184216553\n",
      "2022-03-27 00:24:54.895018 Epoch 50, Training Loss 0.31367459519744834\n",
      "2022-03-27 00:24:54.913981 Epoch 50, Training Loss 0.3149555273677992\n",
      "2022-03-27 00:24:54.932218 Epoch 50, Training Loss 0.3162171063215836\n",
      "2022-03-27 00:24:54.951249 Epoch 50, Training Loss 0.3174533281484833\n",
      "2022-03-27 00:24:54.969220 Epoch 50, Training Loss 0.3189275225105188\n",
      "2022-03-27 00:24:54.988218 Epoch 50, Training Loss 0.319912087734398\n",
      "2022-03-27 00:24:55.007041 Epoch 50, Training Loss 0.32112483013316495\n",
      "2022-03-27 00:24:55.025950 Epoch 50, Training Loss 0.32227759653954857\n",
      "2022-03-27 00:24:55.044971 Epoch 50, Training Loss 0.32359895270193934\n",
      "2022-03-27 00:24:55.063836 Epoch 50, Training Loss 0.3249231105875176\n",
      "2022-03-27 00:24:55.082082 Epoch 50, Training Loss 0.32617659802022186\n",
      "2022-03-27 00:24:55.101610 Epoch 50, Training Loss 0.32730205155089687\n",
      "2022-03-27 00:24:55.120614 Epoch 50, Training Loss 0.3286592398610566\n",
      "2022-03-27 00:24:55.139619 Epoch 50, Training Loss 0.3299633112862287\n",
      "2022-03-27 00:24:55.157590 Epoch 50, Training Loss 0.33129827788723704\n",
      "2022-03-27 00:24:55.177816 Epoch 50, Training Loss 0.3328946543014263\n",
      "2022-03-27 00:24:55.196423 Epoch 50, Training Loss 0.3339182825954369\n",
      "2022-03-27 00:24:55.215435 Epoch 50, Training Loss 0.33534879605178636\n",
      "2022-03-27 00:24:55.234509 Epoch 50, Training Loss 0.33671933976585605\n",
      "2022-03-27 00:24:55.253501 Epoch 50, Training Loss 0.33787561087962004\n",
      "2022-03-27 00:24:55.273534 Epoch 50, Training Loss 0.33887341298410656\n",
      "2022-03-27 00:24:55.291542 Epoch 50, Training Loss 0.34006240331303433\n",
      "2022-03-27 00:24:55.310552 Epoch 50, Training Loss 0.34109837091182504\n",
      "2022-03-27 00:24:55.328821 Epoch 50, Training Loss 0.3420854323660321\n",
      "2022-03-27 00:24:55.346838 Epoch 50, Training Loss 0.34345274950232346\n",
      "2022-03-27 00:24:55.364957 Epoch 50, Training Loss 0.34470851547882686\n",
      "2022-03-27 00:24:55.384036 Epoch 50, Training Loss 0.3459771103261377\n",
      "2022-03-27 00:24:55.402967 Epoch 50, Training Loss 0.34737344913165585\n",
      "2022-03-27 00:24:55.421820 Epoch 50, Training Loss 0.34854664514436745\n",
      "2022-03-27 00:24:55.440199 Epoch 50, Training Loss 0.34992396275100807\n",
      "2022-03-27 00:24:55.459254 Epoch 50, Training Loss 0.3514491285357024\n",
      "2022-03-27 00:24:55.477964 Epoch 50, Training Loss 0.35282318968602155\n",
      "2022-03-27 00:24:55.496982 Epoch 50, Training Loss 0.353942197759438\n",
      "2022-03-27 00:24:55.514960 Epoch 50, Training Loss 0.3549652698704654\n",
      "2022-03-27 00:24:55.534081 Epoch 50, Training Loss 0.35624210243029975\n",
      "2022-03-27 00:24:55.552971 Epoch 50, Training Loss 0.35737306862840873\n",
      "2022-03-27 00:24:55.571034 Epoch 50, Training Loss 0.3586402431016078\n",
      "2022-03-27 00:24:55.590105 Epoch 50, Training Loss 0.35967522028766935\n",
      "2022-03-27 00:24:55.609055 Epoch 50, Training Loss 0.3611422015730377\n",
      "2022-03-27 00:24:55.627932 Epoch 50, Training Loss 0.36237606230904074\n",
      "2022-03-27 00:24:55.646819 Epoch 50, Training Loss 0.36369007307550183\n",
      "2022-03-27 00:24:55.665873 Epoch 50, Training Loss 0.36484463394755295\n",
      "2022-03-27 00:24:55.686038 Epoch 50, Training Loss 0.36629968088911014\n",
      "2022-03-27 00:24:55.704953 Epoch 50, Training Loss 0.3673959136618983\n",
      "2022-03-27 00:24:55.724989 Epoch 50, Training Loss 0.3686488924733818\n",
      "2022-03-27 00:24:55.745228 Epoch 50, Training Loss 0.3696567293475656\n",
      "2022-03-27 00:24:55.764244 Epoch 50, Training Loss 0.37092884513728147\n",
      "2022-03-27 00:24:55.783033 Epoch 50, Training Loss 0.3723757141996223\n",
      "2022-03-27 00:24:55.801580 Epoch 50, Training Loss 0.3734944147222182\n",
      "2022-03-27 00:24:55.820671 Epoch 50, Training Loss 0.37474172850094184\n",
      "2022-03-27 00:24:55.839729 Epoch 50, Training Loss 0.3762790455537684\n",
      "2022-03-27 00:24:55.859731 Epoch 50, Training Loss 0.37746440457261127\n",
      "2022-03-27 00:24:55.877820 Epoch 50, Training Loss 0.3787860639412385\n",
      "2022-03-27 00:24:55.896781 Epoch 50, Training Loss 0.38008154269374544\n",
      "2022-03-27 00:24:55.915837 Epoch 50, Training Loss 0.3812169016474653\n",
      "2022-03-27 00:24:55.935001 Epoch 50, Training Loss 0.3822349615566566\n",
      "2022-03-27 00:24:55.953818 Epoch 50, Training Loss 0.38352699878880436\n",
      "2022-03-27 00:24:55.972609 Epoch 50, Training Loss 0.3846898502705957\n",
      "2022-03-27 00:24:55.991613 Epoch 50, Training Loss 0.38579149060237133\n",
      "2022-03-27 00:24:56.010856 Epoch 50, Training Loss 0.38683502616175\n",
      "2022-03-27 00:24:56.029879 Epoch 50, Training Loss 0.3882490771505839\n",
      "2022-03-27 00:24:56.047969 Epoch 50, Training Loss 0.38949638422187943\n",
      "2022-03-27 00:24:56.066388 Epoch 50, Training Loss 0.39092932043173123\n",
      "2022-03-27 00:24:56.085394 Epoch 50, Training Loss 0.39213488870264623\n",
      "2022-03-27 00:24:56.106611 Epoch 50, Training Loss 0.39326078156985894\n",
      "2022-03-27 00:24:56.126003 Epoch 50, Training Loss 0.3943299839411245\n",
      "2022-03-27 00:24:56.144973 Epoch 50, Training Loss 0.39561626955371376\n",
      "2022-03-27 00:24:56.163568 Epoch 50, Training Loss 0.39705891629009293\n",
      "2022-03-27 00:24:56.182660 Epoch 50, Training Loss 0.398271640472095\n",
      "2022-03-27 00:24:56.201701 Epoch 50, Training Loss 0.399683780453699\n",
      "2022-03-27 00:24:56.220711 Epoch 50, Training Loss 0.4012444710640041\n",
      "2022-03-27 00:24:56.239726 Epoch 50, Training Loss 0.4023898807937837\n",
      "2022-03-27 00:24:56.258804 Epoch 50, Training Loss 0.40358163747946013\n",
      "2022-03-27 00:24:56.278798 Epoch 50, Training Loss 0.4047276934089563\n",
      "2022-03-27 00:24:56.296990 Epoch 50, Training Loss 0.4057526765272136\n",
      "2022-03-27 00:24:56.317691 Epoch 50, Training Loss 0.40698893615961684\n",
      "2022-03-27 00:24:56.336792 Epoch 50, Training Loss 0.4082005725949622\n",
      "2022-03-27 00:24:56.355815 Epoch 50, Training Loss 0.4093959410019848\n",
      "2022-03-27 00:24:56.374842 Epoch 50, Training Loss 0.410695327745984\n",
      "2022-03-27 00:24:56.394039 Epoch 50, Training Loss 0.41198010235796195\n",
      "2022-03-27 00:24:56.412945 Epoch 50, Training Loss 0.41316517128054137\n",
      "2022-03-27 00:24:56.431946 Epoch 50, Training Loss 0.41473473215956824\n",
      "2022-03-27 00:24:56.450993 Epoch 50, Training Loss 0.4159676085805039\n",
      "2022-03-27 00:24:56.470869 Epoch 50, Training Loss 0.41729909585564945\n",
      "2022-03-27 00:24:56.489000 Epoch 50, Training Loss 0.41873101101202126\n",
      "2022-03-27 00:24:56.508922 Epoch 50, Training Loss 0.4200728826053307\n",
      "2022-03-27 00:24:56.527875 Epoch 50, Training Loss 0.421444116574724\n",
      "2022-03-27 00:24:56.546224 Epoch 50, Training Loss 0.4225533138912962\n",
      "2022-03-27 00:24:56.565231 Epoch 50, Training Loss 0.42418911885422517\n",
      "2022-03-27 00:24:56.585214 Epoch 50, Training Loss 0.4253719051933045\n",
      "2022-03-27 00:24:56.604163 Epoch 50, Training Loss 0.4263583612259087\n",
      "2022-03-27 00:24:56.623026 Epoch 50, Training Loss 0.4276034572087895\n",
      "2022-03-27 00:24:56.641847 Epoch 50, Training Loss 0.4290224056871956\n",
      "2022-03-27 00:24:56.660846 Epoch 50, Training Loss 0.4302087597682348\n",
      "2022-03-27 00:24:56.679849 Epoch 50, Training Loss 0.4317839181484164\n",
      "2022-03-27 00:24:56.698819 Epoch 50, Training Loss 0.4327876505339542\n",
      "2022-03-27 00:24:56.717768 Epoch 50, Training Loss 0.43427529740516485\n",
      "2022-03-27 00:24:56.744841 Epoch 50, Training Loss 0.4357465818105146\n",
      "2022-03-27 00:24:56.771868 Epoch 50, Training Loss 0.4369377975573625\n",
      "2022-03-27 00:24:56.797802 Epoch 50, Training Loss 0.43830758988704827\n",
      "2022-03-27 00:24:56.823841 Epoch 50, Training Loss 0.4396769718440902\n",
      "2022-03-27 00:24:56.850691 Epoch 50, Training Loss 0.44101075687066976\n",
      "2022-03-27 00:24:56.876081 Epoch 50, Training Loss 0.4422582592958075\n",
      "2022-03-27 00:24:56.901982 Epoch 50, Training Loss 0.4432571012803051\n",
      "2022-03-27 00:24:56.925832 Epoch 50, Training Loss 0.4442232884561924\n",
      "2022-03-27 00:24:56.945892 Epoch 50, Training Loss 0.4451632808388956\n",
      "2022-03-27 00:24:56.963869 Epoch 50, Training Loss 0.44652547502456724\n",
      "2022-03-27 00:24:56.982851 Epoch 50, Training Loss 0.44790488855003396\n",
      "2022-03-27 00:24:57.001723 Epoch 50, Training Loss 0.4489079903611137\n",
      "2022-03-27 00:24:57.019974 Epoch 50, Training Loss 0.4501994140160358\n",
      "2022-03-27 00:24:57.038836 Epoch 50, Training Loss 0.45145654503036947\n",
      "2022-03-27 00:24:57.058838 Epoch 50, Training Loss 0.4529050866813611\n",
      "2022-03-27 00:24:57.077742 Epoch 50, Training Loss 0.4542041917133819\n",
      "2022-03-27 00:24:57.096425 Epoch 50, Training Loss 0.455577931364479\n",
      "2022-03-27 00:24:57.115431 Epoch 50, Training Loss 0.45694994156622826\n",
      "2022-03-27 00:24:57.134467 Epoch 50, Training Loss 0.45796193757935255\n",
      "2022-03-27 00:24:57.153520 Epoch 50, Training Loss 0.459314532825709\n",
      "2022-03-27 00:24:57.172534 Epoch 50, Training Loss 0.4603540630596678\n",
      "2022-03-27 00:24:57.190518 Epoch 50, Training Loss 0.4613464415988044\n",
      "2022-03-27 00:24:57.209449 Epoch 50, Training Loss 0.4626302163466773\n",
      "2022-03-27 00:24:57.228771 Epoch 50, Training Loss 0.463938531287186\n",
      "2022-03-27 00:24:57.246711 Epoch 50, Training Loss 0.4650720282436332\n",
      "2022-03-27 00:24:57.264789 Epoch 50, Training Loss 0.46610658187085713\n",
      "2022-03-27 00:24:57.283841 Epoch 50, Training Loss 0.46736146155220776\n",
      "2022-03-27 00:24:57.302416 Epoch 50, Training Loss 0.468713579961406\n",
      "2022-03-27 00:24:57.321521 Epoch 50, Training Loss 0.46977690883609646\n",
      "2022-03-27 00:24:57.340525 Epoch 50, Training Loss 0.47093590842488475\n",
      "2022-03-27 00:24:57.360043 Epoch 50, Training Loss 0.4721986537089433\n",
      "2022-03-27 00:24:57.378936 Epoch 50, Training Loss 0.47357566231656867\n",
      "2022-03-27 00:24:57.397841 Epoch 50, Training Loss 0.4747371159093764\n",
      "2022-03-27 00:24:57.415888 Epoch 50, Training Loss 0.47584647031696253\n",
      "2022-03-27 00:24:57.434679 Epoch 50, Training Loss 0.47708694617766556\n",
      "2022-03-27 00:24:57.453783 Epoch 50, Training Loss 0.4786027009834719\n",
      "2022-03-27 00:24:57.472774 Epoch 50, Training Loss 0.4797690850694466\n",
      "2022-03-27 00:24:57.490624 Epoch 50, Training Loss 0.4812147496911266\n",
      "2022-03-27 00:24:57.509274 Epoch 50, Training Loss 0.48243006743738415\n",
      "2022-03-27 00:24:57.528400 Epoch 50, Training Loss 0.4837511578941589\n",
      "2022-03-27 00:24:57.546452 Epoch 50, Training Loss 0.48497131985166797\n",
      "2022-03-27 00:24:57.565464 Epoch 50, Training Loss 0.48637528027719856\n",
      "2022-03-27 00:24:57.584499 Epoch 50, Training Loss 0.4874398914901802\n",
      "2022-03-27 00:24:57.603521 Epoch 50, Training Loss 0.4885888249063126\n",
      "2022-03-27 00:24:57.622543 Epoch 50, Training Loss 0.48983881517749306\n",
      "2022-03-27 00:24:57.640557 Epoch 50, Training Loss 0.4910491141669281\n",
      "2022-03-27 00:24:57.659552 Epoch 50, Training Loss 0.4923927764910871\n",
      "2022-03-27 00:24:57.678476 Epoch 50, Training Loss 0.4937462761731404\n",
      "2022-03-27 00:24:57.697434 Epoch 50, Training Loss 0.4949555856644955\n",
      "2022-03-27 00:24:57.715503 Epoch 50, Training Loss 0.4961139095561279\n",
      "2022-03-27 00:24:57.733996 Epoch 50, Training Loss 0.49716868310633217\n",
      "2022-03-27 00:24:57.751974 Epoch 50, Training Loss 0.49842892956855656\n",
      "2022-03-27 00:24:57.771827 Epoch 50, Training Loss 0.4996137187608977\n",
      "2022-03-27 00:24:57.790000 Epoch 50, Training Loss 0.5008156369714176\n",
      "2022-03-27 00:24:57.808977 Epoch 50, Training Loss 0.5021708465903006\n",
      "2022-03-27 00:24:57.827597 Epoch 50, Training Loss 0.5034000137273003\n",
      "2022-03-27 00:24:57.845719 Epoch 50, Training Loss 0.5045146894881792\n",
      "2022-03-27 00:24:57.863634 Epoch 50, Training Loss 0.5057367065830913\n",
      "2022-03-27 00:24:57.882418 Epoch 50, Training Loss 0.5071420719099167\n",
      "2022-03-27 00:24:57.901457 Epoch 50, Training Loss 0.5082030406845804\n",
      "2022-03-27 00:24:57.920472 Epoch 50, Training Loss 0.509520274979989\n",
      "2022-03-27 00:24:57.939506 Epoch 50, Training Loss 0.5105361715149697\n",
      "2022-03-27 00:24:57.958492 Epoch 50, Training Loss 0.5116819114331395\n",
      "2022-03-27 00:24:57.976626 Epoch 50, Training Loss 0.5129060285627994\n",
      "2022-03-27 00:24:57.996056 Epoch 50, Training Loss 0.5142557725424657\n",
      "2022-03-27 00:24:58.014078 Epoch 50, Training Loss 0.5154108772497348\n",
      "2022-03-27 00:24:58.032775 Epoch 50, Training Loss 0.5167417428682527\n",
      "2022-03-27 00:24:58.051782 Epoch 50, Training Loss 0.5178577421266405\n",
      "2022-03-27 00:24:58.070251 Epoch 50, Training Loss 0.5195476836560632\n",
      "2022-03-27 00:24:58.088715 Epoch 50, Training Loss 0.5206944831192036\n",
      "2022-03-27 00:24:58.107766 Epoch 50, Training Loss 0.5218743130831462\n",
      "2022-03-27 00:24:58.126583 Epoch 50, Training Loss 0.5231548088133487\n",
      "2022-03-27 00:24:58.145650 Epoch 50, Training Loss 0.5245446095533688\n",
      "2022-03-27 00:24:58.163054 Epoch 50, Training Loss 0.5257039843008037\n",
      "2022-03-27 00:24:58.182722 Epoch 50, Training Loss 0.5270319343222987\n",
      "2022-03-27 00:24:58.201779 Epoch 50, Training Loss 0.5281613232077235\n",
      "2022-03-27 00:24:58.220869 Epoch 50, Training Loss 0.5296686434227488\n",
      "2022-03-27 00:24:58.239573 Epoch 50, Training Loss 0.5307442932330129\n",
      "2022-03-27 00:24:58.258532 Epoch 50, Training Loss 0.5319453717192726\n",
      "2022-03-27 00:24:58.277480 Epoch 50, Training Loss 0.5328972899090604\n",
      "2022-03-27 00:24:58.296599 Epoch 50, Training Loss 0.5343732352147017\n",
      "2022-03-27 00:24:58.314614 Epoch 50, Training Loss 0.5358591283983587\n",
      "2022-03-27 00:24:58.332613 Epoch 50, Training Loss 0.5369832433398117\n",
      "2022-03-27 00:24:58.351585 Epoch 50, Training Loss 0.5381904442596923\n",
      "2022-03-27 00:24:58.369821 Epoch 50, Training Loss 0.5395433393585712\n",
      "2022-03-27 00:24:58.389578 Epoch 50, Training Loss 0.5405479809054938\n",
      "2022-03-27 00:24:58.407538 Epoch 50, Training Loss 0.5420428778966675\n",
      "2022-03-27 00:24:58.427516 Epoch 50, Training Loss 0.5432062251945896\n",
      "2022-03-27 00:24:58.446544 Epoch 50, Training Loss 0.5442958934532712\n",
      "2022-03-27 00:24:58.464578 Epoch 50, Training Loss 0.5456295156722788\n",
      "2022-03-27 00:24:58.482831 Epoch 50, Training Loss 0.5472003233707164\n",
      "2022-03-27 00:24:58.501832 Epoch 50, Training Loss 0.5487684180669468\n",
      "2022-03-27 00:24:58.520888 Epoch 50, Training Loss 0.5501002137313413\n",
      "2022-03-27 00:24:58.539859 Epoch 50, Training Loss 0.5513111637986224\n",
      "2022-03-27 00:24:58.558976 Epoch 50, Training Loss 0.5527054219294691\n",
      "2022-03-27 00:24:58.577416 Epoch 50, Training Loss 0.5539516711326511\n",
      "2022-03-27 00:24:58.596443 Epoch 50, Training Loss 0.5550133842031669\n",
      "2022-03-27 00:24:58.615532 Epoch 50, Training Loss 0.5561056632519988\n",
      "2022-03-27 00:24:58.634207 Epoch 50, Training Loss 0.5574301597102523\n",
      "2022-03-27 00:24:58.652209 Epoch 50, Training Loss 0.5589613037950852\n",
      "2022-03-27 00:24:58.671208 Epoch 50, Training Loss 0.5603620097460345\n",
      "2022-03-27 00:24:58.689219 Epoch 50, Training Loss 0.5619677177170659\n",
      "2022-03-27 00:24:58.708252 Epoch 50, Training Loss 0.5632826792614539\n",
      "2022-03-27 00:24:58.726828 Epoch 50, Training Loss 0.5644762481722381\n",
      "2022-03-27 00:24:58.746295 Epoch 50, Training Loss 0.5656321941281829\n",
      "2022-03-27 00:24:58.763589 Epoch 50, Training Loss 0.5669426891352515\n",
      "2022-03-27 00:24:58.782607 Epoch 50, Training Loss 0.5682370358568323\n",
      "2022-03-27 00:24:58.802551 Epoch 50, Training Loss 0.5698096958725044\n",
      "2022-03-27 00:24:58.821484 Epoch 50, Training Loss 0.5712257904927139\n",
      "2022-03-27 00:24:58.840676 Epoch 50, Training Loss 0.572585088136556\n",
      "2022-03-27 00:24:58.859639 Epoch 50, Training Loss 0.5735121421954211\n",
      "2022-03-27 00:24:58.878787 Epoch 50, Training Loss 0.5743707101363356\n",
      "2022-03-27 00:24:58.897635 Epoch 50, Training Loss 0.5756262238220791\n",
      "2022-03-27 00:24:58.915682 Epoch 50, Training Loss 0.5768240296169925\n",
      "2022-03-27 00:24:58.935641 Epoch 50, Training Loss 0.5783091589160587\n",
      "2022-03-27 00:24:58.953666 Epoch 50, Training Loss 0.5797276680579271\n",
      "2022-03-27 00:24:58.972686 Epoch 50, Training Loss 0.5809748684201399\n",
      "2022-03-27 00:24:58.991934 Epoch 50, Training Loss 0.5821579196264067\n",
      "2022-03-27 00:24:59.011366 Epoch 50, Training Loss 0.5832707221855593\n",
      "2022-03-27 00:24:59.030378 Epoch 50, Training Loss 0.5845487232098494\n",
      "2022-03-27 00:24:59.049256 Epoch 50, Training Loss 0.5858562481982629\n",
      "2022-03-27 00:24:59.068275 Epoch 50, Training Loss 0.587050435259519\n",
      "2022-03-27 00:24:59.087723 Epoch 50, Training Loss 0.5885678632637424\n",
      "2022-03-27 00:24:59.106772 Epoch 50, Training Loss 0.5898826918791017\n",
      "2022-03-27 00:24:59.124718 Epoch 50, Training Loss 0.5911035963031642\n",
      "2022-03-27 00:24:59.143969 Epoch 50, Training Loss 0.5926503906469516\n",
      "2022-03-27 00:24:59.163389 Epoch 50, Training Loss 0.5937812932007148\n",
      "2022-03-27 00:24:59.182446 Epoch 50, Training Loss 0.5949526132677522\n",
      "2022-03-27 00:24:59.201431 Epoch 50, Training Loss 0.5958643410059498\n",
      "2022-03-27 00:24:59.221254 Epoch 50, Training Loss 0.5970938824631674\n",
      "2022-03-27 00:24:59.239828 Epoch 50, Training Loss 0.5984415479023438\n",
      "2022-03-27 00:24:59.258871 Epoch 50, Training Loss 0.5995960997041229\n",
      "2022-03-27 00:24:59.277366 Epoch 50, Training Loss 0.6011484744756118\n",
      "2022-03-27 00:24:59.296447 Epoch 50, Training Loss 0.6022365785315823\n",
      "2022-03-27 00:24:59.315523 Epoch 50, Training Loss 0.6032903666996285\n",
      "2022-03-27 00:24:59.334718 Epoch 50, Training Loss 0.6043477756592929\n",
      "2022-03-27 00:24:59.353751 Epoch 50, Training Loss 0.6056935950313382\n",
      "2022-03-27 00:24:59.372782 Epoch 50, Training Loss 0.6069894655586203\n",
      "2022-03-27 00:24:59.391677 Epoch 50, Training Loss 0.6082634599617375\n",
      "2022-03-27 00:24:59.410537 Epoch 50, Training Loss 0.609629945986716\n",
      "2022-03-27 00:24:59.430371 Epoch 50, Training Loss 0.6110359335799351\n",
      "2022-03-27 00:24:59.449467 Epoch 50, Training Loss 0.6121447666374313\n",
      "2022-03-27 00:24:59.468476 Epoch 50, Training Loss 0.6133284782966995\n",
      "2022-03-27 00:24:59.487434 Epoch 50, Training Loss 0.6145743769605446\n",
      "2022-03-27 00:24:59.506557 Epoch 50, Training Loss 0.6161474407938741\n",
      "2022-03-27 00:24:59.524600 Epoch 50, Training Loss 0.6175381700553553\n",
      "2022-03-27 00:24:59.543467 Epoch 50, Training Loss 0.6188108127592774\n",
      "2022-03-27 00:24:59.562524 Epoch 50, Training Loss 0.6202415004563149\n",
      "2022-03-27 00:24:59.581537 Epoch 50, Training Loss 0.6215074928977605\n",
      "2022-03-27 00:24:59.599566 Epoch 50, Training Loss 0.6225297173575672\n",
      "2022-03-27 00:24:59.618711 Epoch 50, Training Loss 0.62395908689255\n",
      "2022-03-27 00:24:59.637538 Epoch 50, Training Loss 0.6250427816530018\n",
      "2022-03-27 00:24:59.657499 Epoch 50, Training Loss 0.626453788841472\n",
      "2022-03-27 00:24:59.677395 Epoch 50, Training Loss 0.627713524548294\n",
      "2022-03-27 00:24:59.696455 Epoch 50, Training Loss 0.6291270413819481\n",
      "2022-03-27 00:24:59.715503 Epoch 50, Training Loss 0.6304572336661541\n",
      "2022-03-27 00:24:59.734453 Epoch 50, Training Loss 0.6316461414479844\n",
      "2022-03-27 00:24:59.761637 Epoch 50, Training Loss 0.633103874104712\n",
      "2022-03-27 00:24:59.787281 Epoch 50, Training Loss 0.6341980769658637\n",
      "2022-03-27 00:24:59.814313 Epoch 50, Training Loss 0.6356717789417032\n",
      "2022-03-27 00:24:59.841714 Epoch 50, Training Loss 0.6369607999654072\n",
      "2022-03-27 00:24:59.868600 Epoch 50, Training Loss 0.6382659848236367\n",
      "2022-03-27 00:24:59.895671 Epoch 50, Training Loss 0.6393836236670803\n",
      "2022-03-27 00:24:59.921789 Epoch 50, Training Loss 0.6408901796926318\n",
      "2022-03-27 00:24:59.946274 Epoch 50, Training Loss 0.641934655054146\n",
      "2022-03-27 00:24:59.964709 Epoch 50, Training Loss 0.6432582242104709\n",
      "2022-03-27 00:24:59.984664 Epoch 50, Training Loss 0.6442817662225659\n",
      "2022-03-27 00:25:00.003703 Epoch 50, Training Loss 0.6458666512880789\n",
      "2022-03-27 00:25:00.024232 Epoch 50, Training Loss 0.6472391201864423\n",
      "2022-03-27 00:25:00.043228 Epoch 50, Training Loss 0.6485138380771402\n",
      "2022-03-27 00:25:00.062301 Epoch 50, Training Loss 0.649572494999527\n",
      "2022-03-27 00:25:00.081428 Epoch 50, Training Loss 0.6510324703762903\n",
      "2022-03-27 00:25:00.099819 Epoch 50, Training Loss 0.6523395363632065\n",
      "2022-03-27 00:25:00.118769 Epoch 50, Training Loss 0.6538865261370569\n",
      "2022-03-27 00:25:00.137424 Epoch 50, Training Loss 0.655098536968841\n",
      "2022-03-27 00:25:00.156403 Epoch 50, Training Loss 0.6563850606951263\n",
      "2022-03-27 00:25:00.175333 Epoch 50, Training Loss 0.6577039192552152\n",
      "2022-03-27 00:25:00.193178 Epoch 50, Training Loss 0.6587984243317333\n",
      "2022-03-27 00:25:00.212834 Epoch 50, Training Loss 0.6599732196849325\n",
      "2022-03-27 00:25:00.231505 Epoch 50, Training Loss 0.661471762468138\n",
      "2022-03-27 00:25:00.249919 Epoch 50, Training Loss 0.662628548148343\n",
      "2022-03-27 00:25:00.268375 Epoch 50, Training Loss 0.6641715925825221\n",
      "2022-03-27 00:25:00.288275 Epoch 50, Training Loss 0.6655688711901759\n",
      "2022-03-27 00:25:00.307193 Epoch 50, Training Loss 0.6670389327094378\n",
      "2022-03-27 00:25:00.326325 Epoch 50, Training Loss 0.668499620445549\n",
      "2022-03-27 00:25:00.344416 Epoch 50, Training Loss 0.6698416074371094\n",
      "2022-03-27 00:25:00.363524 Epoch 50, Training Loss 0.671117853706755\n",
      "2022-03-27 00:25:00.381496 Epoch 50, Training Loss 0.6723545414712423\n",
      "2022-03-27 00:25:00.400543 Epoch 50, Training Loss 0.6735624011672671\n",
      "2022-03-27 00:25:00.419230 Epoch 50, Training Loss 0.6748296014793084\n",
      "2022-03-27 00:25:00.437988 Epoch 50, Training Loss 0.6760580918520612\n",
      "2022-03-27 00:25:00.456977 Epoch 50, Training Loss 0.6776017083993653\n",
      "2022-03-27 00:25:00.475722 Epoch 50, Training Loss 0.6790126305254524\n",
      "2022-03-27 00:25:00.494741 Epoch 50, Training Loss 0.6803307171215487\n",
      "2022-03-27 00:25:00.513789 Epoch 50, Training Loss 0.6814574135081543\n",
      "2022-03-27 00:25:00.531725 Epoch 50, Training Loss 0.6831072241906315\n",
      "2022-03-27 00:25:00.550153 Epoch 50, Training Loss 0.6845982386480511\n",
      "2022-03-27 00:25:00.569143 Epoch 50, Training Loss 0.6859624717394104\n",
      "2022-03-27 00:25:00.588539 Epoch 50, Training Loss 0.6869782392326218\n",
      "2022-03-27 00:25:00.607489 Epoch 50, Training Loss 0.6882532190941179\n",
      "2022-03-27 00:25:00.626176 Epoch 50, Training Loss 0.6893444104725138\n",
      "2022-03-27 00:25:00.644209 Epoch 50, Training Loss 0.6907710427670832\n",
      "2022-03-27 00:25:00.663143 Epoch 50, Training Loss 0.691958732236072\n",
      "2022-03-27 00:25:00.681046 Epoch 50, Training Loss 0.6929455910192426\n",
      "2022-03-27 00:25:00.700957 Epoch 50, Training Loss 0.6940742255476735\n",
      "2022-03-27 00:25:00.718998 Epoch 50, Training Loss 0.695188022635477\n",
      "2022-03-27 00:25:00.738500 Epoch 50, Training Loss 0.6963428566827798\n",
      "2022-03-27 00:25:00.756439 Epoch 50, Training Loss 0.6977508132110166\n",
      "2022-03-27 00:25:00.775445 Epoch 50, Training Loss 0.6989239346798118\n",
      "2022-03-27 00:25:00.793462 Epoch 50, Training Loss 0.7002958714047356\n",
      "2022-03-27 00:25:00.812378 Epoch 50, Training Loss 0.7015261706488821\n",
      "2022-03-27 00:25:00.830404 Epoch 50, Training Loss 0.7029235029159604\n",
      "2022-03-27 00:25:00.849416 Epoch 50, Training Loss 0.7044006650100278\n",
      "2022-03-27 00:25:00.868277 Epoch 50, Training Loss 0.7054984638148256\n",
      "2022-03-27 00:25:00.886990 Epoch 50, Training Loss 0.7065729049160657\n",
      "2022-03-27 00:25:00.906941 Epoch 50, Training Loss 0.7077512880572883\n",
      "2022-03-27 00:25:00.925456 Epoch 50, Training Loss 0.7089620090811454\n",
      "2022-03-27 00:25:00.943428 Epoch 50, Training Loss 0.7101593837713647\n",
      "2022-03-27 00:25:00.962293 Epoch 50, Training Loss 0.7116442452306333\n",
      "2022-03-27 00:25:00.980998 Epoch 50, Training Loss 0.7125027699543692\n",
      "2022-03-27 00:25:00.999982 Epoch 50, Training Loss 0.7138155026509024\n",
      "2022-03-27 00:25:01.017924 Epoch 50, Training Loss 0.7151799313247661\n",
      "2022-03-27 00:25:01.037376 Epoch 50, Training Loss 0.7163165299331441\n",
      "2022-03-27 00:25:01.055400 Epoch 50, Training Loss 0.7175862284572533\n",
      "2022-03-27 00:25:01.074248 Epoch 50, Training Loss 0.7189401795187265\n",
      "2022-03-27 00:25:01.093484 Epoch 50, Training Loss 0.7203821234995752\n",
      "2022-03-27 00:25:01.112516 Epoch 50, Training Loss 0.7216494742714231\n",
      "2022-03-27 00:25:01.130505 Epoch 50, Training Loss 0.7229822797086233\n",
      "2022-03-27 00:25:01.148454 Epoch 50, Training Loss 0.7242164225377086\n",
      "2022-03-27 00:25:01.168382 Epoch 50, Training Loss 0.7252166858871879\n",
      "2022-03-27 00:25:01.187393 Epoch 50, Training Loss 0.7263817082890465\n",
      "2022-03-27 00:25:01.206438 Epoch 50, Training Loss 0.7277903965367075\n",
      "2022-03-27 00:25:01.225530 Epoch 50, Training Loss 0.729170414188024\n",
      "2022-03-27 00:25:01.244500 Epoch 50, Training Loss 0.7305049754469596\n",
      "2022-03-27 00:25:01.263644 Epoch 50, Training Loss 0.7320027717239107\n",
      "2022-03-27 00:25:01.281703 Epoch 50, Training Loss 0.7331345672040339\n",
      "2022-03-27 00:25:01.299971 Epoch 50, Training Loss 0.7346593643850683\n",
      "2022-03-27 00:25:01.318979 Epoch 50, Training Loss 0.7359984552921237\n",
      "2022-03-27 00:25:01.338594 Epoch 50, Training Loss 0.7375475684242785\n",
      "2022-03-27 00:25:01.356513 Epoch 50, Training Loss 0.739176702240239\n",
      "2022-03-27 00:25:01.376469 Epoch 50, Training Loss 0.740396293837701\n",
      "2022-03-27 00:25:01.395482 Epoch 50, Training Loss 0.7414588414494644\n",
      "2022-03-27 00:25:01.413534 Epoch 50, Training Loss 0.7428263348081837\n",
      "2022-03-27 00:25:01.431558 Epoch 50, Training Loss 0.7440901157801109\n",
      "2022-03-27 00:25:01.450008 Epoch 50, Training Loss 0.74542064938094\n",
      "2022-03-27 00:25:01.468948 Epoch 50, Training Loss 0.7468490332288815\n",
      "2022-03-27 00:25:01.487715 Epoch 50, Training Loss 0.7482154224534778\n",
      "2022-03-27 00:25:01.506696 Epoch 50, Training Loss 0.7493643367381961\n",
      "2022-03-27 00:25:01.525318 Epoch 50, Training Loss 0.750810067671949\n",
      "2022-03-27 00:25:01.544286 Epoch 50, Training Loss 0.752038290281125\n",
      "2022-03-27 00:25:01.562057 Epoch 50, Training Loss 0.753264865606947\n",
      "2022-03-27 00:25:01.580736 Epoch 50, Training Loss 0.754488978392023\n",
      "2022-03-27 00:25:01.599176 Epoch 50, Training Loss 0.7557419267914179\n",
      "2022-03-27 00:25:01.617103 Epoch 50, Training Loss 0.7571329781618874\n",
      "2022-03-27 00:25:01.635635 Epoch 50, Training Loss 0.7584247413041342\n",
      "2022-03-27 00:25:01.654665 Epoch 50, Training Loss 0.7597706548850555\n",
      "2022-03-27 00:25:01.673613 Epoch 50, Training Loss 0.7610338299018343\n",
      "2022-03-27 00:25:01.692397 Epoch 50, Training Loss 0.7622327564469994\n",
      "2022-03-27 00:25:01.711530 Epoch 50, Training Loss 0.763522180831036\n",
      "2022-03-27 00:25:01.729451 Epoch 50, Training Loss 0.7646780216023136\n",
      "2022-03-27 00:25:01.748364 Epoch 50, Training Loss 0.7661646265355523\n",
      "2022-03-27 00:25:01.767410 Epoch 50, Training Loss 0.7672416176789861\n",
      "2022-03-27 00:25:01.786288 Epoch 50, Training Loss 0.7682604697506751\n",
      "2022-03-27 00:25:01.805519 Epoch 50, Training Loss 0.769439384836675\n",
      "2022-03-27 00:25:01.823701 Epoch 50, Training Loss 0.7706502972508941\n",
      "2022-03-27 00:25:01.842788 Epoch 50, Training Loss 0.771868512910955\n",
      "2022-03-27 00:25:01.860815 Epoch 50, Training Loss 0.7729279203030764\n",
      "2022-03-27 00:25:01.880391 Epoch 50, Training Loss 0.7742966548408694\n",
      "2022-03-27 00:25:01.898457 Epoch 50, Training Loss 0.7754760359406776\n",
      "2022-03-27 00:25:01.917658 Epoch 50, Training Loss 0.7766305512327063\n",
      "2022-03-27 00:25:01.938190 Epoch 50, Training Loss 0.7779027174805742\n",
      "2022-03-27 00:25:01.957248 Epoch 50, Training Loss 0.7790792390818486\n",
      "2022-03-27 00:25:01.976277 Epoch 50, Training Loss 0.7803310725999915\n",
      "2022-03-27 00:25:01.995193 Epoch 50, Training Loss 0.7816178265130124\n",
      "2022-03-27 00:25:02.012964 Epoch 50, Training Loss 0.78276093475654\n",
      "2022-03-27 00:25:02.032525 Epoch 50, Training Loss 0.7838328620966744\n",
      "2022-03-27 00:25:02.050563 Epoch 50, Training Loss 0.785212174705837\n",
      "2022-03-27 00:25:02.069563 Epoch 50, Training Loss 0.786723213427512\n",
      "2022-03-27 00:25:02.088542 Epoch 50, Training Loss 0.7879387548817393\n",
      "2022-03-27 00:25:02.107561 Epoch 50, Training Loss 0.7890998090014738\n",
      "2022-03-27 00:25:02.127462 Epoch 50, Training Loss 0.7902167193267656\n",
      "2022-03-27 00:25:02.146675 Epoch 50, Training Loss 0.7915911365805379\n",
      "2022-03-27 00:25:02.164711 Epoch 50, Training Loss 0.7930094771220556\n",
      "2022-03-27 00:25:02.183840 Epoch 50, Training Loss 0.7943137636422501\n",
      "2022-03-27 00:25:02.203589 Epoch 50, Training Loss 0.7956834475859962\n",
      "2022-03-27 00:25:02.222652 Epoch 50, Training Loss 0.7968044783300756\n",
      "2022-03-27 00:25:02.241443 Epoch 50, Training Loss 0.7982583218980628\n",
      "2022-03-27 00:25:02.259474 Epoch 50, Training Loss 0.799340828986424\n",
      "2022-03-27 00:25:02.278720 Epoch 50, Training Loss 0.8003590714443675\n",
      "2022-03-27 00:25:02.297663 Epoch 50, Training Loss 0.8015888231184781\n",
      "2022-03-27 00:25:02.316789 Epoch 50, Training Loss 0.8030379191993753\n",
      "2022-03-27 00:25:02.335446 Epoch 50, Training Loss 0.8042968103038076\n",
      "2022-03-27 00:25:02.354561 Epoch 50, Training Loss 0.8055229188536134\n",
      "2022-03-27 00:25:02.373676 Epoch 50, Training Loss 0.8068009075301382\n",
      "2022-03-27 00:25:02.393427 Epoch 50, Training Loss 0.8078710364411249\n",
      "2022-03-27 00:25:02.411499 Epoch 50, Training Loss 0.8091236614357785\n",
      "2022-03-27 00:25:02.429438 Epoch 50, Training Loss 0.810310155305716\n",
      "2022-03-27 00:25:02.448383 Epoch 50, Training Loss 0.8115219857991504\n",
      "2022-03-27 00:25:02.467782 Epoch 50, Training Loss 0.812780324912742\n",
      "2022-03-27 00:25:02.487400 Epoch 50, Training Loss 0.8140995209784154\n",
      "2022-03-27 00:25:02.505553 Epoch 50, Training Loss 0.8153078671154159\n",
      "2022-03-27 00:25:02.525501 Epoch 50, Training Loss 0.8167310354807188\n",
      "2022-03-27 00:25:02.545446 Epoch 50, Training Loss 0.8182759259820289\n",
      "2022-03-27 00:25:02.565450 Epoch 50, Training Loss 0.8196486189694661\n",
      "2022-03-27 00:25:02.591950 Epoch 50, Training Loss 0.8209443855316133\n",
      "2022-03-27 00:25:02.617993 Epoch 50, Training Loss 0.8219606617984869\n",
      "2022-03-27 00:25:02.644361 Epoch 50, Training Loss 0.8233840377891765\n",
      "2022-03-27 00:25:02.670210 Epoch 50, Training Loss 0.8246256608487396\n",
      "2022-03-27 00:25:02.696990 Epoch 50, Training Loss 0.8259977911744276\n",
      "2022-03-27 00:25:02.722844 Epoch 50, Training Loss 0.8274419063802265\n",
      "2022-03-27 00:25:02.749459 Epoch 50, Training Loss 0.8290738624989834\n",
      "2022-03-27 00:25:02.771495 Epoch 50, Training Loss 0.83012159736565\n",
      "2022-03-27 00:25:02.790598 Epoch 50, Training Loss 0.8312651527202343\n",
      "2022-03-27 00:25:02.809947 Epoch 50, Training Loss 0.8325669363026729\n",
      "2022-03-27 00:25:02.829534 Epoch 50, Training Loss 0.8340576798714641\n",
      "2022-03-27 00:25:02.847722 Epoch 50, Training Loss 0.8353253954359333\n",
      "2022-03-27 00:25:02.866819 Epoch 50, Training Loss 0.8366133292465259\n",
      "2022-03-27 00:25:02.884782 Epoch 50, Training Loss 0.837538193863676\n",
      "2022-03-27 00:25:02.904991 Epoch 50, Training Loss 0.8388993015984441\n",
      "2022-03-27 00:25:02.922988 Epoch 50, Training Loss 0.8398544387439327\n",
      "2022-03-27 00:25:02.942494 Epoch 50, Training Loss 0.8414308111685926\n",
      "2022-03-27 00:25:02.960601 Epoch 50, Training Loss 0.842674851569983\n",
      "2022-03-27 00:25:02.979586 Epoch 50, Training Loss 0.8439437187541171\n",
      "2022-03-27 00:25:02.998558 Epoch 50, Training Loss 0.8451587337514629\n",
      "2022-03-27 00:25:03.017670 Epoch 50, Training Loss 0.8463178347901005\n",
      "2022-03-27 00:25:03.036505 Epoch 50, Training Loss 0.8477570171398885\n",
      "2022-03-27 00:25:03.055903 Epoch 50, Training Loss 0.8489487973015631\n",
      "2022-03-27 00:25:03.074934 Epoch 50, Training Loss 0.8501548613123882\n",
      "2022-03-27 00:25:03.093971 Epoch 50, Training Loss 0.8513166715421945\n",
      "2022-03-27 00:25:03.111841 Epoch 50, Training Loss 0.8525018663052708\n",
      "2022-03-27 00:25:03.130386 Epoch 50, Training Loss 0.8536766983968828\n",
      "2022-03-27 00:25:03.148443 Epoch 50, Training Loss 0.8549249829233759\n",
      "2022-03-27 00:25:03.168294 Epoch 50, Training Loss 0.8563237896050944\n",
      "2022-03-27 00:25:03.187275 Epoch 50, Training Loss 0.8577403648735007\n",
      "2022-03-27 00:25:03.206338 Epoch 50, Training Loss 0.8589390785340458\n",
      "2022-03-27 00:25:03.224327 Epoch 50, Training Loss 0.8603421214139065\n",
      "2022-03-27 00:25:03.242953 Epoch 50, Training Loss 0.861598663241662\n",
      "2022-03-27 00:25:03.261951 Epoch 50, Training Loss 0.8631244314753491\n",
      "2022-03-27 00:25:03.280427 Epoch 50, Training Loss 0.8643383231309368\n",
      "2022-03-27 00:25:03.300540 Epoch 50, Training Loss 0.8656880068961922\n",
      "2022-03-27 00:25:03.319682 Epoch 50, Training Loss 0.8669825854813656\n",
      "2022-03-27 00:25:03.338666 Epoch 50, Training Loss 0.8685965554793472\n",
      "2022-03-27 00:25:03.357600 Epoch 50, Training Loss 0.8698817886354978\n",
      "2022-03-27 00:25:03.376718 Epoch 50, Training Loss 0.8712674007391381\n",
      "2022-03-27 00:25:03.395836 Epoch 50, Training Loss 0.8722320397186767\n",
      "2022-03-27 00:25:03.413871 Epoch 50, Training Loss 0.8734871764926959\n",
      "2022-03-27 00:25:03.433608 Epoch 50, Training Loss 0.8748889684372241\n",
      "2022-03-27 00:25:03.451616 Epoch 50, Training Loss 0.8760802213798093\n",
      "2022-03-27 00:25:03.470140 Epoch 50, Training Loss 0.8774704507854588\n",
      "2022-03-27 00:25:03.489091 Epoch 50, Training Loss 0.8789771138249761\n",
      "2022-03-27 00:25:03.509151 Epoch 50, Training Loss 0.8800487923805062\n",
      "2022-03-27 00:25:03.526816 Epoch 50, Training Loss 0.8812132981579627\n",
      "2022-03-27 00:25:03.545881 Epoch 50, Training Loss 0.8824895449611537\n",
      "2022-03-27 00:25:03.564846 Epoch 50, Training Loss 0.8836831696655439\n",
      "2022-03-27 00:25:03.583517 Epoch 50, Training Loss 0.8852693334107509\n",
      "2022-03-27 00:25:03.601522 Epoch 50, Training Loss 0.88661676454727\n",
      "2022-03-27 00:25:03.620813 Epoch 50, Training Loss 0.8878595684952748\n",
      "2022-03-27 00:25:03.639554 Epoch 50, Training Loss 0.8890616388424583\n",
      "2022-03-27 00:25:03.658637 Epoch 50, Training Loss 0.8904232048165158\n",
      "2022-03-27 00:25:03.677690 Epoch 50, Training Loss 0.8918239528413319\n",
      "2022-03-27 00:25:03.695826 Epoch 50, Training Loss 0.8932901600285259\n",
      "2022-03-27 00:25:03.714949 Epoch 50, Training Loss 0.8943436382829076\n",
      "2022-03-27 00:25:03.733316 Epoch 50, Training Loss 0.8954805347620679\n",
      "2022-03-27 00:25:03.752158 Epoch 50, Training Loss 0.8969181189146798\n",
      "2022-03-27 00:25:03.771211 Epoch 50, Training Loss 0.8981748267512797\n",
      "2022-03-27 00:25:03.790581 Epoch 50, Training Loss 0.8993246989786777\n",
      "2022-03-27 00:25:03.810701 Epoch 50, Training Loss 0.9004308739129234\n",
      "2022-03-27 00:25:03.829220 Epoch 50, Training Loss 0.901645494467767\n",
      "2022-03-27 00:25:03.847843 Epoch 50, Training Loss 0.9030233797667276\n",
      "2022-03-27 00:25:03.867866 Epoch 50, Training Loss 0.9042498172853913\n",
      "2022-03-27 00:25:03.886872 Epoch 50, Training Loss 0.9055324395751709\n",
      "2022-03-27 00:25:03.906879 Epoch 50, Training Loss 0.9068846491444141\n",
      "2022-03-27 00:25:03.925939 Epoch 50, Training Loss 0.9082837719899004\n",
      "2022-03-27 00:25:03.943839 Epoch 50, Training Loss 0.9097242360682134\n",
      "2022-03-27 00:25:03.961871 Epoch 50, Training Loss 0.9109113095209117\n",
      "2022-03-27 00:25:03.980780 Epoch 50, Training Loss 0.9120728536639982\n",
      "2022-03-27 00:25:03.999619 Epoch 50, Training Loss 0.9132427913910898\n",
      "2022-03-27 00:25:04.018642 Epoch 50, Training Loss 0.9142764645921605\n",
      "2022-03-27 00:25:04.037558 Epoch 50, Training Loss 0.9157812068681888\n",
      "2022-03-27 00:25:04.056588 Epoch 50, Training Loss 0.9172497854360839\n",
      "2022-03-27 00:25:04.076689 Epoch 50, Training Loss 0.918713169863157\n",
      "2022-03-27 00:25:04.095777 Epoch 50, Training Loss 0.9199418568855051\n",
      "2022-03-27 00:25:04.114736 Epoch 50, Training Loss 0.9213257338994604\n",
      "2022-03-27 00:25:04.133501 Epoch 50, Training Loss 0.922644620508794\n",
      "2022-03-27 00:25:04.151523 Epoch 50, Training Loss 0.9242761804319709\n",
      "2022-03-27 00:25:04.170251 Epoch 50, Training Loss 0.925927544951134\n",
      "2022-03-27 00:25:04.189273 Epoch 50, Training Loss 0.9274792194061572\n",
      "2022-03-27 00:25:04.209384 Epoch 50, Training Loss 0.928893622230081\n",
      "2022-03-27 00:25:04.228398 Epoch 50, Training Loss 0.9300757789855723\n",
      "2022-03-27 00:25:04.246486 Epoch 50, Training Loss 0.9313105306661952\n",
      "2022-03-27 00:25:04.264877 Epoch 50, Training Loss 0.9323187702147248\n",
      "2022-03-27 00:25:04.282972 Epoch 50, Training Loss 0.9334906414341744\n",
      "2022-03-27 00:25:04.301846 Epoch 50, Training Loss 0.9347741190734726\n",
      "2022-03-27 00:25:04.320886 Epoch 50, Training Loss 0.9362895693010687\n",
      "2022-03-27 00:25:04.339514 Epoch 50, Training Loss 0.9374935880798818\n",
      "2022-03-27 00:25:04.358537 Epoch 50, Training Loss 0.938774289758614\n",
      "2022-03-27 00:25:04.377255 Epoch 50, Training Loss 0.9400712174680227\n",
      "2022-03-27 00:25:04.396245 Epoch 50, Training Loss 0.9416542547895476\n",
      "2022-03-27 00:25:04.414251 Epoch 50, Training Loss 0.943122301184003\n",
      "2022-03-27 00:25:04.433717 Epoch 50, Training Loss 0.9445046193307013\n",
      "2022-03-27 00:25:04.452243 Epoch 50, Training Loss 0.9458242766844952\n",
      "2022-03-27 00:25:04.471545 Epoch 50, Training Loss 0.9466853391788804\n",
      "2022-03-27 00:25:04.490492 Epoch 50, Training Loss 0.9478646955831581\n",
      "2022-03-27 00:25:04.508607 Epoch 50, Training Loss 0.9491700848655018\n",
      "2022-03-27 00:25:04.528085 Epoch 50, Training Loss 0.9504207881057963\n",
      "2022-03-27 00:25:04.546130 Epoch 50, Training Loss 0.9519540441158177\n",
      "2022-03-27 00:25:04.565141 Epoch 50, Training Loss 0.9533052594612932\n",
      "2022-03-27 00:25:04.583030 Epoch 50, Training Loss 0.9545607497472592\n",
      "2022-03-27 00:25:04.600988 Epoch 50, Training Loss 0.9561033583510562\n",
      "2022-03-27 00:25:04.619979 Epoch 50, Training Loss 0.9573778898819633\n",
      "2022-03-27 00:25:04.639677 Epoch 50, Training Loss 0.9587868575549796\n",
      "2022-03-27 00:25:04.658773 Epoch 50, Training Loss 0.9601364989414849\n",
      "2022-03-27 00:25:04.679298 Epoch 50, Training Loss 0.9614957557309924\n",
      "2022-03-27 00:25:04.697324 Epoch 50, Training Loss 0.962611859945385\n",
      "2022-03-27 00:25:04.716369 Epoch 50, Training Loss 0.9639042782813997\n",
      "2022-03-27 00:25:04.735381 Epoch 50, Training Loss 0.9652578473243567\n",
      "2022-03-27 00:25:04.754226 Epoch 50, Training Loss 0.9664010146389836\n",
      "2022-03-27 00:25:04.773201 Epoch 50, Training Loss 0.9678428665451382\n",
      "2022-03-27 00:25:04.793244 Epoch 50, Training Loss 0.9691779273550224\n",
      "2022-03-27 00:25:04.812128 Epoch 50, Training Loss 0.9704857831415923\n",
      "2022-03-27 00:25:04.831069 Epoch 50, Training Loss 0.9717135523896083\n",
      "2022-03-27 00:25:04.850116 Epoch 50, Training Loss 0.9729361612626048\n",
      "2022-03-27 00:25:04.869059 Epoch 50, Training Loss 0.9742775529699252\n",
      "2022-03-27 00:25:04.887988 Epoch 50, Training Loss 0.9755379040070506\n",
      "2022-03-27 00:25:04.907839 Epoch 50, Training Loss 0.9768574516029309\n",
      "2022-03-27 00:25:04.926706 Epoch 50, Training Loss 0.9780650885056352\n",
      "2022-03-27 00:25:04.945788 Epoch 50, Training Loss 0.979320940459171\n",
      "2022-03-27 00:25:04.965914 Epoch 50, Training Loss 0.9804995074448988\n",
      "2022-03-27 00:25:04.983946 Epoch 50, Training Loss 0.9819346250932844\n",
      "2022-03-27 00:25:05.002712 Epoch 50, Training Loss 0.9831459250139154\n",
      "2022-03-27 00:25:05.021678 Epoch 50, Training Loss 0.984399660545237\n",
      "2022-03-27 00:25:05.039730 Epoch 50, Training Loss 0.9855259348211995\n",
      "2022-03-27 00:25:05.059543 Epoch 50, Training Loss 0.9868398061036454\n",
      "2022-03-27 00:25:05.067635 Epoch 50, Training Loss 0.9881982753801224\n",
      "2022-03-27 00:36:46.333075 Epoch 100, Training Loss 0.0011998526275615253\n",
      "2022-03-27 00:36:46.354080 Epoch 100, Training Loss 0.0022350017676877852\n",
      "2022-03-27 00:36:46.374084 Epoch 100, Training Loss 0.003443740898995753\n",
      "2022-03-27 00:36:46.394090 Epoch 100, Training Loss 0.004429950967164296\n",
      "2022-03-27 00:36:46.415095 Epoch 100, Training Loss 0.005697270488495107\n",
      "2022-03-27 00:36:46.435098 Epoch 100, Training Loss 0.007071848110774594\n",
      "2022-03-27 00:36:46.457103 Epoch 100, Training Loss 0.008007385160612024\n",
      "2022-03-27 00:36:46.476107 Epoch 100, Training Loss 0.009296300542324096\n",
      "2022-03-27 00:36:46.496111 Epoch 100, Training Loss 0.010500679083187562\n",
      "2022-03-27 00:36:46.515117 Epoch 100, Training Loss 0.011921754121170629\n",
      "2022-03-27 00:36:46.535122 Epoch 100, Training Loss 0.013109666535921414\n",
      "2022-03-27 00:36:46.556125 Epoch 100, Training Loss 0.014233458987282364\n",
      "2022-03-27 00:36:46.576149 Epoch 100, Training Loss 0.015620473248269552\n",
      "2022-03-27 00:36:46.596154 Epoch 100, Training Loss 0.016577765002579944\n",
      "2022-03-27 00:36:46.615158 Epoch 100, Training Loss 0.017670521269673885\n",
      "2022-03-27 00:36:46.635150 Epoch 100, Training Loss 0.018646101969892106\n",
      "2022-03-27 00:36:46.655155 Epoch 100, Training Loss 0.019789813653282497\n",
      "2022-03-27 00:36:46.675166 Epoch 100, Training Loss 0.021051264861050772\n",
      "2022-03-27 00:36:46.695171 Epoch 100, Training Loss 0.02233846237897263\n",
      "2022-03-27 00:36:46.715175 Epoch 100, Training Loss 0.0234604759137039\n",
      "2022-03-27 00:36:46.734185 Epoch 100, Training Loss 0.024666328640545115\n",
      "2022-03-27 00:36:46.754180 Epoch 100, Training Loss 0.0256662197277674\n",
      "2022-03-27 00:36:46.774182 Epoch 100, Training Loss 0.02688743284596202\n",
      "2022-03-27 00:36:46.794193 Epoch 100, Training Loss 0.028314599295711274\n",
      "2022-03-27 00:36:46.814191 Epoch 100, Training Loss 0.029446961019959902\n",
      "2022-03-27 00:36:46.834208 Epoch 100, Training Loss 0.03065229468333447\n",
      "2022-03-27 00:36:46.853206 Epoch 100, Training Loss 0.031849866480473665\n",
      "2022-03-27 00:36:46.873217 Epoch 100, Training Loss 0.03300219408386504\n",
      "2022-03-27 00:36:46.892215 Epoch 100, Training Loss 0.034383038883014104\n",
      "2022-03-27 00:36:46.912220 Epoch 100, Training Loss 0.035525739802728835\n",
      "2022-03-27 00:36:46.932224 Epoch 100, Training Loss 0.03676492349266091\n",
      "2022-03-27 00:36:46.952229 Epoch 100, Training Loss 0.037839101937115954\n",
      "2022-03-27 00:36:46.972239 Epoch 100, Training Loss 0.038979699011044124\n",
      "2022-03-27 00:36:46.992245 Epoch 100, Training Loss 0.040212633893313005\n",
      "2022-03-27 00:36:47.013243 Epoch 100, Training Loss 0.04141075768129295\n",
      "2022-03-27 00:36:47.032247 Epoch 100, Training Loss 0.042836239956833826\n",
      "2022-03-27 00:36:47.053245 Epoch 100, Training Loss 0.043938279837903466\n",
      "2022-03-27 00:36:47.073256 Epoch 100, Training Loss 0.04511852238489234\n",
      "2022-03-27 00:36:47.093267 Epoch 100, Training Loss 0.0463699366887817\n",
      "2022-03-27 00:36:47.113272 Epoch 100, Training Loss 0.04742014118472634\n",
      "2022-03-27 00:36:47.132276 Epoch 100, Training Loss 0.04840829983696608\n",
      "2022-03-27 00:36:47.153268 Epoch 100, Training Loss 0.04948731105955665\n",
      "2022-03-27 00:36:47.172285 Epoch 100, Training Loss 0.05055657006285685\n",
      "2022-03-27 00:36:47.192283 Epoch 100, Training Loss 0.051749767244929246\n",
      "2022-03-27 00:36:47.213288 Epoch 100, Training Loss 0.052820814616235015\n",
      "2022-03-27 00:36:47.233299 Epoch 100, Training Loss 0.053934710791043915\n",
      "2022-03-27 00:36:47.253298 Epoch 100, Training Loss 0.055272596800113885\n",
      "2022-03-27 00:36:47.272295 Epoch 100, Training Loss 0.05662766281905991\n",
      "2022-03-27 00:36:47.292306 Epoch 100, Training Loss 0.05785931978384247\n",
      "2022-03-27 00:36:47.311310 Epoch 100, Training Loss 0.05891944372745426\n",
      "2022-03-27 00:36:47.331321 Epoch 100, Training Loss 0.06001677232630113\n",
      "2022-03-27 00:36:47.351313 Epoch 100, Training Loss 0.06110901036835693\n",
      "2022-03-27 00:36:47.371330 Epoch 100, Training Loss 0.0620899206537115\n",
      "2022-03-27 00:36:47.391321 Epoch 100, Training Loss 0.06326779197244083\n",
      "2022-03-27 00:36:47.410339 Epoch 100, Training Loss 0.06441902268268264\n",
      "2022-03-27 00:36:47.431343 Epoch 100, Training Loss 0.06598500575860748\n",
      "2022-03-27 00:36:47.452348 Epoch 100, Training Loss 0.06710964532764367\n",
      "2022-03-27 00:36:47.472347 Epoch 100, Training Loss 0.06841430868334172\n",
      "2022-03-27 00:36:47.492345 Epoch 100, Training Loss 0.06995478249571817\n",
      "2022-03-27 00:36:47.512362 Epoch 100, Training Loss 0.07093943743144765\n",
      "2022-03-27 00:36:47.532354 Epoch 100, Training Loss 0.07196988374985697\n",
      "2022-03-27 00:36:47.552359 Epoch 100, Training Loss 0.07308407360330567\n",
      "2022-03-27 00:36:47.573376 Epoch 100, Training Loss 0.07421075825191215\n",
      "2022-03-27 00:36:47.592380 Epoch 100, Training Loss 0.07539179136076242\n",
      "2022-03-27 00:36:47.612381 Epoch 100, Training Loss 0.07639266912589597\n",
      "2022-03-27 00:36:47.633777 Epoch 100, Training Loss 0.07754501509849372\n",
      "2022-03-27 00:36:47.654388 Epoch 100, Training Loss 0.07894736879012164\n",
      "2022-03-27 00:36:47.674399 Epoch 100, Training Loss 0.0801886038859482\n",
      "2022-03-27 00:36:47.695391 Epoch 100, Training Loss 0.08128229530571061\n",
      "2022-03-27 00:36:47.714402 Epoch 100, Training Loss 0.08242066017806987\n",
      "2022-03-27 00:36:47.734412 Epoch 100, Training Loss 0.08340309396424257\n",
      "2022-03-27 00:36:47.754411 Epoch 100, Training Loss 0.08441601308715313\n",
      "2022-03-27 00:36:47.774415 Epoch 100, Training Loss 0.08554790971224266\n",
      "2022-03-27 00:36:47.794426 Epoch 100, Training Loss 0.08682718286124032\n",
      "2022-03-27 00:36:47.816431 Epoch 100, Training Loss 0.08801117547027901\n",
      "2022-03-27 00:36:47.836429 Epoch 100, Training Loss 0.08925357582928885\n",
      "2022-03-27 00:36:47.857434 Epoch 100, Training Loss 0.09031756591918828\n",
      "2022-03-27 00:36:47.877439 Epoch 100, Training Loss 0.09155856877031839\n",
      "2022-03-27 00:36:47.897449 Epoch 100, Training Loss 0.09280673782233997\n",
      "2022-03-27 00:36:47.916453 Epoch 100, Training Loss 0.09387082265466071\n",
      "2022-03-27 00:36:47.937448 Epoch 100, Training Loss 0.09515688539770863\n",
      "2022-03-27 00:36:47.957457 Epoch 100, Training Loss 0.0960887203283627\n",
      "2022-03-27 00:36:47.977461 Epoch 100, Training Loss 0.09721754434163613\n",
      "2022-03-27 00:36:47.997466 Epoch 100, Training Loss 0.09836050829923976\n",
      "2022-03-27 00:36:48.017456 Epoch 100, Training Loss 0.09957673314892118\n",
      "2022-03-27 00:36:48.037464 Epoch 100, Training Loss 0.10051051658742569\n",
      "2022-03-27 00:36:48.057466 Epoch 100, Training Loss 0.10172354473787196\n",
      "2022-03-27 00:36:48.077471 Epoch 100, Training Loss 0.10325674236278094\n",
      "2022-03-27 00:36:48.097478 Epoch 100, Training Loss 0.10437771357843638\n",
      "2022-03-27 00:36:48.118490 Epoch 100, Training Loss 0.10549730931401557\n",
      "2022-03-27 00:36:48.138496 Epoch 100, Training Loss 0.10656127173577429\n",
      "2022-03-27 00:36:48.157489 Epoch 100, Training Loss 0.10784742701084107\n",
      "2022-03-27 00:36:48.178513 Epoch 100, Training Loss 0.10912264384272154\n",
      "2022-03-27 00:36:48.198517 Epoch 100, Training Loss 0.11066863985012865\n",
      "2022-03-27 00:36:48.218522 Epoch 100, Training Loss 0.11191783094650035\n",
      "2022-03-27 00:36:48.237520 Epoch 100, Training Loss 0.11300275171809185\n",
      "2022-03-27 00:36:48.258531 Epoch 100, Training Loss 0.11417820310348745\n",
      "2022-03-27 00:36:48.278536 Epoch 100, Training Loss 0.11537581697449355\n",
      "2022-03-27 00:36:48.298540 Epoch 100, Training Loss 0.11653066306467862\n",
      "2022-03-27 00:36:48.317544 Epoch 100, Training Loss 0.11748165570561538\n",
      "2022-03-27 00:36:48.338549 Epoch 100, Training Loss 0.11880228426450354\n",
      "2022-03-27 00:36:48.357548 Epoch 100, Training Loss 0.12005307386293436\n",
      "2022-03-27 00:36:48.377552 Epoch 100, Training Loss 0.12088948694031562\n",
      "2022-03-27 00:36:48.397551 Epoch 100, Training Loss 0.12200115312395803\n",
      "2022-03-27 00:36:48.417550 Epoch 100, Training Loss 0.12308198068757802\n",
      "2022-03-27 00:36:48.436553 Epoch 100, Training Loss 0.12427183360699802\n",
      "2022-03-27 00:36:48.456566 Epoch 100, Training Loss 0.12536327239802425\n",
      "2022-03-27 00:36:48.476575 Epoch 100, Training Loss 0.12667730709780817\n",
      "2022-03-27 00:36:48.497565 Epoch 100, Training Loss 0.12789774588916614\n",
      "2022-03-27 00:36:48.517577 Epoch 100, Training Loss 0.1291205607869131\n",
      "2022-03-27 00:36:48.537574 Epoch 100, Training Loss 0.1303224340271767\n",
      "2022-03-27 00:36:48.557587 Epoch 100, Training Loss 0.13153947809773028\n",
      "2022-03-27 00:36:48.577598 Epoch 100, Training Loss 0.1328686714324805\n",
      "2022-03-27 00:36:48.597594 Epoch 100, Training Loss 0.13411084328161177\n",
      "2022-03-27 00:36:48.617602 Epoch 100, Training Loss 0.13541502690376223\n",
      "2022-03-27 00:36:48.637605 Epoch 100, Training Loss 0.13627308195509263\n",
      "2022-03-27 00:36:48.657621 Epoch 100, Training Loss 0.13752378877776358\n",
      "2022-03-27 00:36:48.677607 Epoch 100, Training Loss 0.13876996526632773\n",
      "2022-03-27 00:36:48.698620 Epoch 100, Training Loss 0.13988012845254005\n",
      "2022-03-27 00:36:48.718618 Epoch 100, Training Loss 0.1409947715147072\n",
      "2022-03-27 00:36:48.738640 Epoch 100, Training Loss 0.14217012785279842\n",
      "2022-03-27 00:36:48.759645 Epoch 100, Training Loss 0.1431233427103828\n",
      "2022-03-27 00:36:48.779649 Epoch 100, Training Loss 0.14435252653973182\n",
      "2022-03-27 00:36:48.799658 Epoch 100, Training Loss 0.14536460883477154\n",
      "2022-03-27 00:36:48.819646 Epoch 100, Training Loss 0.1463181752225627\n",
      "2022-03-27 00:36:48.839657 Epoch 100, Training Loss 0.14743667802847255\n",
      "2022-03-27 00:36:48.859667 Epoch 100, Training Loss 0.14853352079610996\n",
      "2022-03-27 00:36:48.879672 Epoch 100, Training Loss 0.1498988653387865\n",
      "2022-03-27 00:36:48.900677 Epoch 100, Training Loss 0.15102704155170704\n",
      "2022-03-27 00:36:48.920681 Epoch 100, Training Loss 0.15232537919298159\n",
      "2022-03-27 00:36:48.939679 Epoch 100, Training Loss 0.15358782127080367\n",
      "2022-03-27 00:36:48.959690 Epoch 100, Training Loss 0.15472247663056454\n",
      "2022-03-27 00:36:48.979689 Epoch 100, Training Loss 0.15598437708357107\n",
      "2022-03-27 00:36:48.999693 Epoch 100, Training Loss 0.15711380285985024\n",
      "2022-03-27 00:36:49.020704 Epoch 100, Training Loss 0.15849947045221352\n",
      "2022-03-27 00:36:49.046710 Epoch 100, Training Loss 0.1596264405476163\n",
      "2022-03-27 00:36:49.074710 Epoch 100, Training Loss 0.16086595602657483\n",
      "2022-03-27 00:36:49.100716 Epoch 100, Training Loss 0.1620776615179408\n",
      "2022-03-27 00:36:49.127728 Epoch 100, Training Loss 0.16356132204270424\n",
      "2022-03-27 00:36:49.154734 Epoch 100, Training Loss 0.1649720350189892\n",
      "2022-03-27 00:36:49.181740 Epoch 100, Training Loss 0.1660776836487948\n",
      "2022-03-27 00:36:49.208746 Epoch 100, Training Loss 0.16727875641849646\n",
      "2022-03-27 00:36:49.235747 Epoch 100, Training Loss 0.16850026451108402\n",
      "2022-03-27 00:36:49.262758 Epoch 100, Training Loss 0.17006638386975165\n",
      "2022-03-27 00:36:49.288765 Epoch 100, Training Loss 0.17131138244248412\n",
      "2022-03-27 00:36:49.315771 Epoch 100, Training Loss 0.1725270367034561\n",
      "2022-03-27 00:36:49.343766 Epoch 100, Training Loss 0.17372191020899722\n",
      "2022-03-27 00:36:49.369783 Epoch 100, Training Loss 0.17520742762424146\n",
      "2022-03-27 00:36:49.396789 Epoch 100, Training Loss 0.17650475229143792\n",
      "2022-03-27 00:36:49.422795 Epoch 100, Training Loss 0.17760167531954968\n",
      "2022-03-27 00:36:49.441793 Epoch 100, Training Loss 0.1787322828226992\n",
      "2022-03-27 00:36:49.459797 Epoch 100, Training Loss 0.17999137301579157\n",
      "2022-03-27 00:36:49.478802 Epoch 100, Training Loss 0.18111322420027554\n",
      "2022-03-27 00:36:49.497806 Epoch 100, Training Loss 0.18197272378770288\n",
      "2022-03-27 00:36:49.516802 Epoch 100, Training Loss 0.1829802476993912\n",
      "2022-03-27 00:36:49.535808 Epoch 100, Training Loss 0.1840518817419896\n",
      "2022-03-27 00:36:49.555825 Epoch 100, Training Loss 0.1852152720284279\n",
      "2022-03-27 00:36:49.574829 Epoch 100, Training Loss 0.18632865470388663\n",
      "2022-03-27 00:36:49.593834 Epoch 100, Training Loss 0.1872739833029335\n",
      "2022-03-27 00:36:49.612838 Epoch 100, Training Loss 0.1881996107375835\n",
      "2022-03-27 00:36:49.632836 Epoch 100, Training Loss 0.18935040706563788\n",
      "2022-03-27 00:36:49.651847 Epoch 100, Training Loss 0.19046590425779142\n",
      "2022-03-27 00:36:49.670845 Epoch 100, Training Loss 0.19181294453418468\n",
      "2022-03-27 00:36:49.689849 Epoch 100, Training Loss 0.1931837147763928\n",
      "2022-03-27 00:36:49.708860 Epoch 100, Training Loss 0.19437195821796233\n",
      "2022-03-27 00:36:49.727864 Epoch 100, Training Loss 0.19553643175403176\n",
      "2022-03-27 00:36:49.746862 Epoch 100, Training Loss 0.196848285625048\n",
      "2022-03-27 00:36:49.766867 Epoch 100, Training Loss 0.19809547927983276\n",
      "2022-03-27 00:36:49.786865 Epoch 100, Training Loss 0.19916640187773255\n",
      "2022-03-27 00:36:49.804875 Epoch 100, Training Loss 0.20037296803101248\n",
      "2022-03-27 00:36:49.823880 Epoch 100, Training Loss 0.2014074080892841\n",
      "2022-03-27 00:36:49.842890 Epoch 100, Training Loss 0.20244348567465079\n",
      "2022-03-27 00:36:49.861894 Epoch 100, Training Loss 0.20371013544404598\n",
      "2022-03-27 00:36:49.880893 Epoch 100, Training Loss 0.20465530862893594\n",
      "2022-03-27 00:36:49.899886 Epoch 100, Training Loss 0.2059501702980617\n",
      "2022-03-27 00:36:49.917896 Epoch 100, Training Loss 0.20710842376169952\n",
      "2022-03-27 00:36:49.937906 Epoch 100, Training Loss 0.20812290739220426\n",
      "2022-03-27 00:36:49.955908 Epoch 100, Training Loss 0.20907680351106103\n",
      "2022-03-27 00:36:49.975906 Epoch 100, Training Loss 0.21020715308311344\n",
      "2022-03-27 00:36:49.994918 Epoch 100, Training Loss 0.21092149455224157\n",
      "2022-03-27 00:36:50.013923 Epoch 100, Training Loss 0.21238163075483668\n",
      "2022-03-27 00:36:50.032921 Epoch 100, Training Loss 0.21344402706836496\n",
      "2022-03-27 00:36:50.051926 Epoch 100, Training Loss 0.21469617004284774\n",
      "2022-03-27 00:36:50.070930 Epoch 100, Training Loss 0.21579395954871117\n",
      "2022-03-27 00:36:50.089934 Epoch 100, Training Loss 0.21686764332034703\n",
      "2022-03-27 00:36:50.107944 Epoch 100, Training Loss 0.21801807119718294\n",
      "2022-03-27 00:36:50.126949 Epoch 100, Training Loss 0.21885709986662316\n",
      "2022-03-27 00:36:50.145953 Epoch 100, Training Loss 0.22007467428131786\n",
      "2022-03-27 00:36:50.164963 Epoch 100, Training Loss 0.2210938928987059\n",
      "2022-03-27 00:36:50.184956 Epoch 100, Training Loss 0.22237563910691635\n",
      "2022-03-27 00:36:50.203960 Epoch 100, Training Loss 0.22352484082017104\n",
      "2022-03-27 00:36:50.221964 Epoch 100, Training Loss 0.22477031188547764\n",
      "2022-03-27 00:36:50.240980 Epoch 100, Training Loss 0.22607207854690453\n",
      "2022-03-27 00:36:50.258978 Epoch 100, Training Loss 0.2275158859732206\n",
      "2022-03-27 00:36:50.278983 Epoch 100, Training Loss 0.22863332549934193\n",
      "2022-03-27 00:36:50.296978 Epoch 100, Training Loss 0.22964290035960008\n",
      "2022-03-27 00:36:50.317128 Epoch 100, Training Loss 0.2309179307554689\n",
      "2022-03-27 00:36:50.335982 Epoch 100, Training Loss 0.23188354825729604\n",
      "2022-03-27 00:36:50.354994 Epoch 100, Training Loss 0.2330405478892119\n",
      "2022-03-27 00:36:50.374006 Epoch 100, Training Loss 0.23420327978060984\n",
      "2022-03-27 00:36:50.394015 Epoch 100, Training Loss 0.23525829571287346\n",
      "2022-03-27 00:36:50.413020 Epoch 100, Training Loss 0.2366011560420551\n",
      "2022-03-27 00:36:50.433400 Epoch 100, Training Loss 0.2382708890053927\n",
      "2022-03-27 00:36:50.452016 Epoch 100, Training Loss 0.23951690207661874\n",
      "2022-03-27 00:36:50.471032 Epoch 100, Training Loss 0.2405208327123881\n",
      "2022-03-27 00:36:50.490031 Epoch 100, Training Loss 0.24181400974998085\n",
      "2022-03-27 00:36:50.509041 Epoch 100, Training Loss 0.24299490009732258\n",
      "2022-03-27 00:36:50.528045 Epoch 100, Training Loss 0.24412542566314072\n",
      "2022-03-27 00:36:50.547050 Epoch 100, Training Loss 0.24513413198768635\n",
      "2022-03-27 00:36:50.566054 Epoch 100, Training Loss 0.24646538983830404\n",
      "2022-03-27 00:36:50.584052 Epoch 100, Training Loss 0.24768798461045755\n",
      "2022-03-27 00:36:50.604063 Epoch 100, Training Loss 0.24881556271896949\n",
      "2022-03-27 00:36:50.623061 Epoch 100, Training Loss 0.24991654114954917\n",
      "2022-03-27 00:36:50.641066 Epoch 100, Training Loss 0.2508886752988371\n",
      "2022-03-27 00:36:50.660069 Epoch 100, Training Loss 0.2519718633435876\n",
      "2022-03-27 00:36:50.679064 Epoch 100, Training Loss 0.2534027068358858\n",
      "2022-03-27 00:36:50.699066 Epoch 100, Training Loss 0.2546907000986816\n",
      "2022-03-27 00:36:50.718070 Epoch 100, Training Loss 0.25581042457114705\n",
      "2022-03-27 00:36:50.738076 Epoch 100, Training Loss 0.25702875623922516\n",
      "2022-03-27 00:36:50.756080 Epoch 100, Training Loss 0.2583644156870635\n",
      "2022-03-27 00:36:50.775096 Epoch 100, Training Loss 0.2594168372166431\n",
      "2022-03-27 00:36:50.794106 Epoch 100, Training Loss 0.2606362008377719\n",
      "2022-03-27 00:36:50.814110 Epoch 100, Training Loss 0.26193944709685146\n",
      "2022-03-27 00:36:50.832115 Epoch 100, Training Loss 0.26317837986799764\n",
      "2022-03-27 00:36:50.852107 Epoch 100, Training Loss 0.2643129637326731\n",
      "2022-03-27 00:36:50.871111 Epoch 100, Training Loss 0.26537670549529285\n",
      "2022-03-27 00:36:50.890128 Epoch 100, Training Loss 0.26652240219628415\n",
      "2022-03-27 00:36:50.908132 Epoch 100, Training Loss 0.2678014480549356\n",
      "2022-03-27 00:36:50.928136 Epoch 100, Training Loss 0.26896352399035794\n",
      "2022-03-27 00:36:50.947135 Epoch 100, Training Loss 0.2701816500147895\n",
      "2022-03-27 00:36:50.966145 Epoch 100, Training Loss 0.2712611337299542\n",
      "2022-03-27 00:36:50.984136 Epoch 100, Training Loss 0.2726691626679257\n",
      "2022-03-27 00:36:51.003153 Epoch 100, Training Loss 0.273821299948046\n",
      "2022-03-27 00:36:51.023152 Epoch 100, Training Loss 0.2752927524964218\n",
      "2022-03-27 00:36:51.043150 Epoch 100, Training Loss 0.27673339508378597\n",
      "2022-03-27 00:36:51.062167 Epoch 100, Training Loss 0.2779788640149109\n",
      "2022-03-27 00:36:51.081171 Epoch 100, Training Loss 0.27909300828833716\n",
      "2022-03-27 00:36:51.100175 Epoch 100, Training Loss 0.28016515986998675\n",
      "2022-03-27 00:36:51.119174 Epoch 100, Training Loss 0.28110604891386787\n",
      "2022-03-27 00:36:51.138184 Epoch 100, Training Loss 0.28219816706064715\n",
      "2022-03-27 00:36:51.158189 Epoch 100, Training Loss 0.2835889447985403\n",
      "2022-03-27 00:36:51.177189 Epoch 100, Training Loss 0.28486020912599685\n",
      "2022-03-27 00:36:51.196191 Epoch 100, Training Loss 0.2858571465058095\n",
      "2022-03-27 00:36:51.215201 Epoch 100, Training Loss 0.2871380440719292\n",
      "2022-03-27 00:36:51.235200 Epoch 100, Training Loss 0.2882326656900099\n",
      "2022-03-27 00:36:51.261206 Epoch 100, Training Loss 0.289658967491306\n",
      "2022-03-27 00:36:51.288218 Epoch 100, Training Loss 0.29084202167018297\n",
      "2022-03-27 00:36:51.315218 Epoch 100, Training Loss 0.29216755213944806\n",
      "2022-03-27 00:36:51.341218 Epoch 100, Training Loss 0.2931186313671834\n",
      "2022-03-27 00:36:51.368225 Epoch 100, Training Loss 0.2943886981900696\n",
      "2022-03-27 00:36:51.395236 Epoch 100, Training Loss 0.2959094847864507\n",
      "2022-03-27 00:36:51.422242 Epoch 100, Training Loss 0.2970375224300053\n",
      "2022-03-27 00:36:51.449236 Epoch 100, Training Loss 0.2981715585722033\n",
      "2022-03-27 00:36:51.469239 Epoch 100, Training Loss 0.29955609763979607\n",
      "2022-03-27 00:36:51.488257 Epoch 100, Training Loss 0.300877116753927\n",
      "2022-03-27 00:36:51.507261 Epoch 100, Training Loss 0.3022180256788688\n",
      "2022-03-27 00:36:51.526261 Epoch 100, Training Loss 0.30325467354806185\n",
      "2022-03-27 00:36:51.545270 Epoch 100, Training Loss 0.3043600512892389\n",
      "2022-03-27 00:36:51.563268 Epoch 100, Training Loss 0.3055606831217666\n",
      "2022-03-27 00:36:51.581278 Epoch 100, Training Loss 0.30664842901632305\n",
      "2022-03-27 00:36:51.600282 Epoch 100, Training Loss 0.30771101153720065\n",
      "2022-03-27 00:36:51.619293 Epoch 100, Training Loss 0.3085466008966841\n",
      "2022-03-27 00:36:51.638291 Epoch 100, Training Loss 0.3097608413385308\n",
      "2022-03-27 00:36:51.658296 Epoch 100, Training Loss 0.3108355226876486\n",
      "2022-03-27 00:36:51.677300 Epoch 100, Training Loss 0.31181955192704947\n",
      "2022-03-27 00:36:51.696310 Epoch 100, Training Loss 0.3131598047435741\n",
      "2022-03-27 00:36:51.715308 Epoch 100, Training Loss 0.31456537800066914\n",
      "2022-03-27 00:36:51.733320 Epoch 100, Training Loss 0.3157315257260257\n",
      "2022-03-27 00:36:51.753328 Epoch 100, Training Loss 0.3168657426638981\n",
      "2022-03-27 00:36:51.771321 Epoch 100, Training Loss 0.31783482668649815\n",
      "2022-03-27 00:36:51.790333 Epoch 100, Training Loss 0.31910562469526327\n",
      "2022-03-27 00:36:51.808330 Epoch 100, Training Loss 0.3203196829694616\n",
      "2022-03-27 00:36:51.828334 Epoch 100, Training Loss 0.32149042566414077\n",
      "2022-03-27 00:36:51.847338 Epoch 100, Training Loss 0.32250626892080086\n",
      "2022-03-27 00:36:51.865343 Epoch 100, Training Loss 0.3236797229408303\n",
      "2022-03-27 00:36:51.884348 Epoch 100, Training Loss 0.32489845415820245\n",
      "2022-03-27 00:36:51.903351 Epoch 100, Training Loss 0.3261638914075349\n",
      "2022-03-27 00:36:51.922361 Epoch 100, Training Loss 0.32726387889184\n",
      "2022-03-27 00:36:51.941346 Epoch 100, Training Loss 0.3284057483953588\n",
      "2022-03-27 00:36:51.960364 Epoch 100, Training Loss 0.32960531870117576\n",
      "2022-03-27 00:36:51.979375 Epoch 100, Training Loss 0.33089622527437135\n",
      "2022-03-27 00:36:51.997373 Epoch 100, Training Loss 0.33213377410493544\n",
      "2022-03-27 00:36:52.016377 Epoch 100, Training Loss 0.3334077300927828\n",
      "2022-03-27 00:36:52.035381 Epoch 100, Training Loss 0.33451602022971033\n",
      "2022-03-27 00:36:52.054392 Epoch 100, Training Loss 0.3359189876510054\n",
      "2022-03-27 00:36:52.073397 Epoch 100, Training Loss 0.33698058753367277\n",
      "2022-03-27 00:36:52.092394 Epoch 100, Training Loss 0.3382308596692732\n",
      "2022-03-27 00:36:52.111538 Epoch 100, Training Loss 0.33930454016341577\n",
      "2022-03-27 00:36:52.130409 Epoch 100, Training Loss 0.3402616380120787\n",
      "2022-03-27 00:36:52.149407 Epoch 100, Training Loss 0.3415092053010945\n",
      "2022-03-27 00:36:52.168411 Epoch 100, Training Loss 0.34269581525527\n",
      "2022-03-27 00:36:52.187421 Epoch 100, Training Loss 0.34388260935883386\n",
      "2022-03-27 00:36:52.205420 Epoch 100, Training Loss 0.3448489993581991\n",
      "2022-03-27 00:36:52.224419 Epoch 100, Training Loss 0.34610066343756285\n",
      "2022-03-27 00:36:52.244429 Epoch 100, Training Loss 0.34724481026534837\n",
      "2022-03-27 00:36:52.262438 Epoch 100, Training Loss 0.348372830073242\n",
      "2022-03-27 00:36:52.282443 Epoch 100, Training Loss 0.34937863002347824\n",
      "2022-03-27 00:36:52.301441 Epoch 100, Training Loss 0.35074116080008505\n",
      "2022-03-27 00:36:52.319452 Epoch 100, Training Loss 0.351885060977448\n",
      "2022-03-27 00:36:52.338457 Epoch 100, Training Loss 0.3529515400567018\n",
      "2022-03-27 00:36:52.357454 Epoch 100, Training Loss 0.3538704977163573\n",
      "2022-03-27 00:36:52.375458 Epoch 100, Training Loss 0.35501476268634163\n",
      "2022-03-27 00:36:52.394463 Epoch 100, Training Loss 0.3562012563276169\n",
      "2022-03-27 00:36:52.413473 Epoch 100, Training Loss 0.35742338279933883\n",
      "2022-03-27 00:36:52.431457 Epoch 100, Training Loss 0.3587813644153078\n",
      "2022-03-27 00:36:52.451469 Epoch 100, Training Loss 0.35981992931317186\n",
      "2022-03-27 00:36:52.470465 Epoch 100, Training Loss 0.3609435331181187\n",
      "2022-03-27 00:36:52.490478 Epoch 100, Training Loss 0.36213012714215254\n",
      "2022-03-27 00:36:52.509495 Epoch 100, Training Loss 0.3631627372158763\n",
      "2022-03-27 00:36:52.528481 Epoch 100, Training Loss 0.3645585520797983\n",
      "2022-03-27 00:36:52.547497 Epoch 100, Training Loss 0.36541512821946304\n",
      "2022-03-27 00:36:52.567495 Epoch 100, Training Loss 0.36660469241459354\n",
      "2022-03-27 00:36:52.586512 Epoch 100, Training Loss 0.3678513535148347\n",
      "2022-03-27 00:36:52.605504 Epoch 100, Training Loss 0.3688927477278063\n",
      "2022-03-27 00:36:52.624515 Epoch 100, Training Loss 0.3698643160902936\n",
      "2022-03-27 00:36:52.643526 Epoch 100, Training Loss 0.37098287347027714\n",
      "2022-03-27 00:36:52.662529 Epoch 100, Training Loss 0.3720588735912157\n",
      "2022-03-27 00:36:52.681528 Epoch 100, Training Loss 0.3732978758757072\n",
      "2022-03-27 00:36:52.701532 Epoch 100, Training Loss 0.374445044476053\n",
      "2022-03-27 00:36:52.719536 Epoch 100, Training Loss 0.3755518014321242\n",
      "2022-03-27 00:36:52.738594 Epoch 100, Training Loss 0.3765872755014073\n",
      "2022-03-27 00:36:52.756545 Epoch 100, Training Loss 0.3775153147137683\n",
      "2022-03-27 00:36:52.775555 Epoch 100, Training Loss 0.37887514407372536\n",
      "2022-03-27 00:36:52.794559 Epoch 100, Training Loss 0.3802723623145267\n",
      "2022-03-27 00:36:52.813558 Epoch 100, Training Loss 0.3813907541429905\n",
      "2022-03-27 00:36:52.832568 Epoch 100, Training Loss 0.3826981035187421\n",
      "2022-03-27 00:36:52.851560 Epoch 100, Training Loss 0.38394090403681214\n",
      "2022-03-27 00:36:52.870577 Epoch 100, Training Loss 0.38505470668873215\n",
      "2022-03-27 00:36:52.889583 Epoch 100, Training Loss 0.386114131382969\n",
      "2022-03-27 00:36:52.909568 Epoch 100, Training Loss 0.3871515379537402\n",
      "2022-03-27 00:36:52.928590 Epoch 100, Training Loss 0.38857119391336464\n",
      "2022-03-27 00:36:52.947588 Epoch 100, Training Loss 0.38956708493440045\n",
      "2022-03-27 00:36:52.966592 Epoch 100, Training Loss 0.39081456167313755\n",
      "2022-03-27 00:36:52.985590 Epoch 100, Training Loss 0.3923702002181422\n",
      "2022-03-27 00:36:53.004601 Epoch 100, Training Loss 0.3935302521871484\n",
      "2022-03-27 00:36:53.023605 Epoch 100, Training Loss 0.3946647981700995\n",
      "2022-03-27 00:36:53.042610 Epoch 100, Training Loss 0.39581160967612206\n",
      "2022-03-27 00:36:53.060614 Epoch 100, Training Loss 0.3968112985496326\n",
      "2022-03-27 00:36:53.079618 Epoch 100, Training Loss 0.3981761270776734\n",
      "2022-03-27 00:36:53.098622 Epoch 100, Training Loss 0.3992675024530162\n",
      "2022-03-27 00:36:53.117633 Epoch 100, Training Loss 0.4003314263070636\n",
      "2022-03-27 00:36:53.136637 Epoch 100, Training Loss 0.40147395367207733\n",
      "2022-03-27 00:36:53.155635 Epoch 100, Training Loss 0.4025274502194446\n",
      "2022-03-27 00:36:53.174645 Epoch 100, Training Loss 0.4037026817841298\n",
      "2022-03-27 00:36:53.193637 Epoch 100, Training Loss 0.40499965217717165\n",
      "2022-03-27 00:36:53.212654 Epoch 100, Training Loss 0.4058597994887311\n",
      "2022-03-27 00:36:53.231659 Epoch 100, Training Loss 0.4072464981957165\n",
      "2022-03-27 00:36:53.250662 Epoch 100, Training Loss 0.4084460014272529\n",
      "2022-03-27 00:36:53.268661 Epoch 100, Training Loss 0.40953475686595264\n",
      "2022-03-27 00:36:53.287671 Epoch 100, Training Loss 0.41090122231132237\n",
      "2022-03-27 00:36:53.306669 Epoch 100, Training Loss 0.4123135122954083\n",
      "2022-03-27 00:36:53.325680 Epoch 100, Training Loss 0.4138277556432787\n",
      "2022-03-27 00:36:53.345684 Epoch 100, Training Loss 0.41516626026014536\n",
      "2022-03-27 00:36:53.364676 Epoch 100, Training Loss 0.41622308925594514\n",
      "2022-03-27 00:36:53.382931 Epoch 100, Training Loss 0.41714443437888493\n",
      "2022-03-27 00:36:53.402697 Epoch 100, Training Loss 0.4181629794332987\n",
      "2022-03-27 00:36:53.421695 Epoch 100, Training Loss 0.41932788887597106\n",
      "2022-03-27 00:36:53.440707 Epoch 100, Training Loss 0.4205046802225625\n",
      "2022-03-27 00:36:53.459784 Epoch 100, Training Loss 0.4218377888659992\n",
      "2022-03-27 00:36:53.478714 Epoch 100, Training Loss 0.422985552750585\n",
      "2022-03-27 00:36:53.498701 Epoch 100, Training Loss 0.4241652776060812\n",
      "2022-03-27 00:36:53.517711 Epoch 100, Training Loss 0.4253929361815343\n",
      "2022-03-27 00:36:53.537708 Epoch 100, Training Loss 0.4267945304093763\n",
      "2022-03-27 00:36:53.556713 Epoch 100, Training Loss 0.4280659918437528\n",
      "2022-03-27 00:36:53.574719 Epoch 100, Training Loss 0.4293743794988793\n",
      "2022-03-27 00:36:53.593720 Epoch 100, Training Loss 0.4307553756724843\n",
      "2022-03-27 00:36:53.613733 Epoch 100, Training Loss 0.4318346255423163\n",
      "2022-03-27 00:36:53.633751 Epoch 100, Training Loss 0.4330209445617998\n",
      "2022-03-27 00:36:53.652748 Epoch 100, Training Loss 0.4340651530743865\n",
      "2022-03-27 00:36:53.670752 Epoch 100, Training Loss 0.4352297480301479\n",
      "2022-03-27 00:36:53.690756 Epoch 100, Training Loss 0.43635366380672014\n",
      "2022-03-27 00:36:53.709761 Epoch 100, Training Loss 0.43794213323032155\n",
      "2022-03-27 00:36:53.728765 Epoch 100, Training Loss 0.43915795975024136\n",
      "2022-03-27 00:36:53.748770 Epoch 100, Training Loss 0.4403033706995532\n",
      "2022-03-27 00:36:53.768762 Epoch 100, Training Loss 0.44143464940283306\n",
      "2022-03-27 00:36:53.786784 Epoch 100, Training Loss 0.4426815432050954\n",
      "2022-03-27 00:36:53.805782 Epoch 100, Training Loss 0.44368946628497385\n",
      "2022-03-27 00:36:53.824793 Epoch 100, Training Loss 0.44465775265718055\n",
      "2022-03-27 00:36:53.844797 Epoch 100, Training Loss 0.44570925625998653\n",
      "2022-03-27 00:36:53.865802 Epoch 100, Training Loss 0.4470046229679566\n",
      "2022-03-27 00:36:53.891808 Epoch 100, Training Loss 0.4482045573041872\n",
      "2022-03-27 00:36:53.917808 Epoch 100, Training Loss 0.4494985009703185\n",
      "2022-03-27 00:36:53.944820 Epoch 100, Training Loss 0.4509094023643552\n",
      "2022-03-27 00:36:53.971880 Epoch 100, Training Loss 0.4520341284439692\n",
      "2022-03-27 00:36:53.998832 Epoch 100, Training Loss 0.45323513734066273\n",
      "2022-03-27 00:36:54.025838 Epoch 100, Training Loss 0.4544396944667982\n",
      "2022-03-27 00:36:54.052844 Epoch 100, Training Loss 0.4556789652008535\n",
      "2022-03-27 00:36:54.069837 Epoch 100, Training Loss 0.4568960639979223\n",
      "2022-03-27 00:36:54.085846 Epoch 100, Training Loss 0.4579099717804843\n",
      "2022-03-27 00:36:54.100849 Epoch 100, Training Loss 0.4590307727951528\n",
      "2022-03-27 00:36:54.114846 Epoch 100, Training Loss 0.46069069370589294\n",
      "2022-03-27 00:36:54.129850 Epoch 100, Training Loss 0.4616726453194533\n",
      "2022-03-27 00:36:54.144853 Epoch 100, Training Loss 0.4629538718544309\n",
      "2022-03-27 00:36:54.160856 Epoch 100, Training Loss 0.4640253317325621\n",
      "2022-03-27 00:36:54.175860 Epoch 100, Training Loss 0.46509353789831975\n",
      "2022-03-27 00:36:54.190864 Epoch 100, Training Loss 0.4660700309611952\n",
      "2022-03-27 00:36:54.205867 Epoch 100, Training Loss 0.4670592919944802\n",
      "2022-03-27 00:36:54.219872 Epoch 100, Training Loss 0.46821075068105517\n",
      "2022-03-27 00:36:54.235880 Epoch 100, Training Loss 0.46921675642737953\n",
      "2022-03-27 00:36:54.250877 Epoch 100, Training Loss 0.47048778355578935\n",
      "2022-03-27 00:36:54.265880 Epoch 100, Training Loss 0.4715416436000248\n",
      "2022-03-27 00:36:54.280885 Epoch 100, Training Loss 0.47270623459230604\n",
      "2022-03-27 00:36:54.295889 Epoch 100, Training Loss 0.4742064088049447\n",
      "2022-03-27 00:36:54.310891 Epoch 100, Training Loss 0.4753390373781209\n",
      "2022-03-27 00:36:54.325896 Epoch 100, Training Loss 0.47635292198956775\n",
      "2022-03-27 00:36:54.340898 Epoch 100, Training Loss 0.47744765816747076\n",
      "2022-03-27 00:36:54.355902 Epoch 100, Training Loss 0.47867273133429117\n",
      "2022-03-27 00:36:54.371905 Epoch 100, Training Loss 0.4798962117156104\n",
      "2022-03-27 00:36:54.386909 Epoch 100, Training Loss 0.48105035062946017\n",
      "2022-03-27 00:36:54.401913 Epoch 100, Training Loss 0.4820604123880186\n",
      "2022-03-27 00:36:54.416915 Epoch 100, Training Loss 0.48323380573631247\n",
      "2022-03-27 00:36:54.431919 Epoch 100, Training Loss 0.4846155398794452\n",
      "2022-03-27 00:36:54.447922 Epoch 100, Training Loss 0.4855580734627326\n",
      "2022-03-27 00:36:54.461927 Epoch 100, Training Loss 0.4868873875311878\n",
      "2022-03-27 00:36:54.476920 Epoch 100, Training Loss 0.4878304437603182\n",
      "2022-03-27 00:36:54.491933 Epoch 100, Training Loss 0.48876866126609275\n",
      "2022-03-27 00:36:54.507931 Epoch 100, Training Loss 0.48990555736414915\n",
      "2022-03-27 00:36:54.521940 Epoch 100, Training Loss 0.491243987589541\n",
      "2022-03-27 00:36:54.536943 Epoch 100, Training Loss 0.4923143516416135\n",
      "2022-03-27 00:36:54.551946 Epoch 100, Training Loss 0.4934584222486257\n",
      "2022-03-27 00:36:54.567950 Epoch 100, Training Loss 0.49457400770443477\n",
      "2022-03-27 00:36:54.582953 Epoch 100, Training Loss 0.4957090389850499\n",
      "2022-03-27 00:36:54.597957 Epoch 100, Training Loss 0.4968956750067299\n",
      "2022-03-27 00:36:54.612961 Epoch 100, Training Loss 0.4980983285952712\n",
      "2022-03-27 00:36:54.628963 Epoch 100, Training Loss 0.4992655587318303\n",
      "2022-03-27 00:36:54.644966 Epoch 100, Training Loss 0.5004189099802081\n",
      "2022-03-27 00:36:54.659976 Epoch 100, Training Loss 0.5013992300118937\n",
      "2022-03-27 00:36:54.674973 Epoch 100, Training Loss 0.5024841875218979\n",
      "2022-03-27 00:36:54.689983 Epoch 100, Training Loss 0.5038416532756728\n",
      "2022-03-27 00:36:54.704981 Epoch 100, Training Loss 0.5054920796695572\n",
      "2022-03-27 00:36:54.719984 Epoch 100, Training Loss 0.5068131998524337\n",
      "2022-03-27 00:36:54.734987 Epoch 100, Training Loss 0.5078674047194478\n",
      "2022-03-27 00:36:54.749990 Epoch 100, Training Loss 0.5091674481816304\n",
      "2022-03-27 00:36:54.764986 Epoch 100, Training Loss 0.5101920275584512\n",
      "2022-03-27 00:36:54.779998 Epoch 100, Training Loss 0.5113981142068458\n",
      "2022-03-27 00:36:54.796001 Epoch 100, Training Loss 0.5127305243631153\n",
      "2022-03-27 00:36:54.811004 Epoch 100, Training Loss 0.514220006173224\n",
      "2022-03-27 00:36:54.826008 Epoch 100, Training Loss 0.5155254514016154\n",
      "2022-03-27 00:36:54.841012 Epoch 100, Training Loss 0.5163372503522107\n",
      "2022-03-27 00:36:54.856014 Epoch 100, Training Loss 0.5173935181344561\n",
      "2022-03-27 00:36:54.871019 Epoch 100, Training Loss 0.5186282124970575\n",
      "2022-03-27 00:36:54.885022 Epoch 100, Training Loss 0.5197507344243472\n",
      "2022-03-27 00:36:54.900026 Epoch 100, Training Loss 0.521595266316553\n",
      "2022-03-27 00:36:54.915021 Epoch 100, Training Loss 0.5228697488375027\n",
      "2022-03-27 00:36:54.933027 Epoch 100, Training Loss 0.523963437284655\n",
      "2022-03-27 00:36:54.951033 Epoch 100, Training Loss 0.5250203534770195\n",
      "2022-03-27 00:36:54.972035 Epoch 100, Training Loss 0.5259888615754559\n",
      "2022-03-27 00:36:54.988038 Epoch 100, Training Loss 0.5272348174811019\n",
      "2022-03-27 00:36:55.004042 Epoch 100, Training Loss 0.5283793544068056\n",
      "2022-03-27 00:36:55.022051 Epoch 100, Training Loss 0.5295852015695304\n",
      "2022-03-27 00:36:55.039049 Epoch 100, Training Loss 0.5305077762859861\n",
      "2022-03-27 00:36:55.054168 Epoch 100, Training Loss 0.5317504613295846\n",
      "2022-03-27 00:36:55.070171 Epoch 100, Training Loss 0.532897478021929\n",
      "2022-03-27 00:36:55.085175 Epoch 100, Training Loss 0.534169285041292\n",
      "2022-03-27 00:36:55.101188 Epoch 100, Training Loss 0.5355716003176502\n",
      "2022-03-27 00:36:55.116184 Epoch 100, Training Loss 0.5367073185760957\n",
      "2022-03-27 00:36:55.131193 Epoch 100, Training Loss 0.537943442992847\n",
      "2022-03-27 00:36:55.146188 Epoch 100, Training Loss 0.5390518231465079\n",
      "2022-03-27 00:36:55.162193 Epoch 100, Training Loss 0.5403675574170964\n",
      "2022-03-27 00:36:55.177196 Epoch 100, Training Loss 0.5417068068633604\n",
      "2022-03-27 00:36:55.193198 Epoch 100, Training Loss 0.5432406502306614\n",
      "2022-03-27 00:36:55.207211 Epoch 100, Training Loss 0.5443553391015133\n",
      "2022-03-27 00:36:55.222214 Epoch 100, Training Loss 0.5454596837463281\n",
      "2022-03-27 00:36:55.237217 Epoch 100, Training Loss 0.546457727546887\n",
      "2022-03-27 00:36:55.252226 Epoch 100, Training Loss 0.5475220654321753\n",
      "2022-03-27 00:36:55.267226 Epoch 100, Training Loss 0.5485569030580009\n",
      "2022-03-27 00:36:55.283227 Epoch 100, Training Loss 0.5496171397321364\n",
      "2022-03-27 00:36:55.298237 Epoch 100, Training Loss 0.5508520860043938\n",
      "2022-03-27 00:36:55.313234 Epoch 100, Training Loss 0.5523345680035594\n",
      "2022-03-27 00:36:55.328243 Epoch 100, Training Loss 0.5536785201190988\n",
      "2022-03-27 00:36:55.343247 Epoch 100, Training Loss 0.5547480486390536\n",
      "2022-03-27 00:36:55.358250 Epoch 100, Training Loss 0.5560016306617376\n",
      "2022-03-27 00:36:55.373247 Epoch 100, Training Loss 0.5576931408146764\n",
      "2022-03-27 00:36:55.388250 Epoch 100, Training Loss 0.5589514993645651\n",
      "2022-03-27 00:36:55.403255 Epoch 100, Training Loss 0.5600681444415656\n",
      "2022-03-27 00:36:55.419258 Epoch 100, Training Loss 0.5612037539329675\n",
      "2022-03-27 00:36:55.434262 Epoch 100, Training Loss 0.5628840218267173\n",
      "2022-03-27 00:36:55.449266 Epoch 100, Training Loss 0.5642424997923624\n",
      "2022-03-27 00:36:55.464268 Epoch 100, Training Loss 0.5655469384492205\n",
      "2022-03-27 00:36:55.480271 Epoch 100, Training Loss 0.5667290846100244\n",
      "2022-03-27 00:36:55.495275 Epoch 100, Training Loss 0.5681371809271596\n",
      "2022-03-27 00:36:55.510278 Epoch 100, Training Loss 0.5695572500033756\n",
      "2022-03-27 00:36:55.525283 Epoch 100, Training Loss 0.5706469083533567\n",
      "2022-03-27 00:36:55.539285 Epoch 100, Training Loss 0.5716585526838327\n",
      "2022-03-27 00:36:55.554288 Epoch 100, Training Loss 0.5727628541876898\n",
      "2022-03-27 00:36:55.569291 Epoch 100, Training Loss 0.5740865693830163\n",
      "2022-03-27 00:36:55.584296 Epoch 100, Training Loss 0.5755332626802537\n",
      "2022-03-27 00:36:55.600294 Epoch 100, Training Loss 0.5764450390473046\n",
      "2022-03-27 00:36:55.615302 Epoch 100, Training Loss 0.5776365926046201\n",
      "2022-03-27 00:36:55.631306 Epoch 100, Training Loss 0.5792350121928603\n",
      "2022-03-27 00:36:55.647310 Epoch 100, Training Loss 0.580154750734339\n",
      "2022-03-27 00:36:55.662313 Epoch 100, Training Loss 0.5814750817273279\n",
      "2022-03-27 00:36:55.678316 Epoch 100, Training Loss 0.582561226528319\n",
      "2022-03-27 00:36:55.693320 Epoch 100, Training Loss 0.5836591616920803\n",
      "2022-03-27 00:36:55.709324 Epoch 100, Training Loss 0.5851992361076043\n",
      "2022-03-27 00:36:55.725328 Epoch 100, Training Loss 0.5862203389787308\n",
      "2022-03-27 00:36:55.741331 Epoch 100, Training Loss 0.5872934934733164\n",
      "2022-03-27 00:36:55.755335 Epoch 100, Training Loss 0.5886876198946668\n",
      "2022-03-27 00:36:55.770338 Epoch 100, Training Loss 0.5900210003413813\n",
      "2022-03-27 00:36:55.785342 Epoch 100, Training Loss 0.5912634364479338\n",
      "2022-03-27 00:36:55.801339 Epoch 100, Training Loss 0.5924108217439383\n",
      "2022-03-27 00:36:55.817348 Epoch 100, Training Loss 0.5936952074775306\n",
      "2022-03-27 00:36:55.832351 Epoch 100, Training Loss 0.5948059979607078\n",
      "2022-03-27 00:36:55.846357 Epoch 100, Training Loss 0.5961731903998139\n",
      "2022-03-27 00:36:55.862358 Epoch 100, Training Loss 0.5973919112511608\n",
      "2022-03-27 00:36:55.881375 Epoch 100, Training Loss 0.5989192392667542\n",
      "2022-03-27 00:36:55.901373 Epoch 100, Training Loss 0.600173134480596\n",
      "2022-03-27 00:36:55.920383 Epoch 100, Training Loss 0.60121815977499\n",
      "2022-03-27 00:36:55.941388 Epoch 100, Training Loss 0.6024335795046424\n",
      "2022-03-27 00:36:55.960386 Epoch 100, Training Loss 0.6037114161207243\n",
      "2022-03-27 00:36:55.980397 Epoch 100, Training Loss 0.6050975852457764\n",
      "2022-03-27 00:36:55.999396 Epoch 100, Training Loss 0.6064596583928599\n",
      "2022-03-27 00:36:56.019400 Epoch 100, Training Loss 0.6075078813773592\n",
      "2022-03-27 00:36:56.040405 Epoch 100, Training Loss 0.60883718416514\n",
      "2022-03-27 00:36:56.059415 Epoch 100, Training Loss 0.6099825349000408\n",
      "2022-03-27 00:36:56.078419 Epoch 100, Training Loss 0.6114437259981395\n",
      "2022-03-27 00:36:56.098418 Epoch 100, Training Loss 0.6125152191085279\n",
      "2022-03-27 00:36:56.118423 Epoch 100, Training Loss 0.6137218254301554\n",
      "2022-03-27 00:36:56.138420 Epoch 100, Training Loss 0.6148914315206621\n",
      "2022-03-27 00:36:56.157437 Epoch 100, Training Loss 0.6162329052415345\n",
      "2022-03-27 00:36:56.178442 Epoch 100, Training Loss 0.6177385989052561\n",
      "2022-03-27 00:36:56.198440 Epoch 100, Training Loss 0.6189968398464915\n",
      "2022-03-27 00:36:56.218446 Epoch 100, Training Loss 0.6200823134473522\n",
      "2022-03-27 00:36:56.238437 Epoch 100, Training Loss 0.621328522024862\n",
      "2022-03-27 00:36:56.258455 Epoch 100, Training Loss 0.6226090299503882\n",
      "2022-03-27 00:36:56.278446 Epoch 100, Training Loss 0.6240152263885264\n",
      "2022-03-27 00:36:56.297469 Epoch 100, Training Loss 0.6251681042296807\n",
      "2022-03-27 00:36:56.318468 Epoch 100, Training Loss 0.6261823918965771\n",
      "2022-03-27 00:36:56.338478 Epoch 100, Training Loss 0.6273984783293342\n",
      "2022-03-27 00:36:56.358477 Epoch 100, Training Loss 0.6286264792885012\n",
      "2022-03-27 00:36:56.379482 Epoch 100, Training Loss 0.6300108589022361\n",
      "2022-03-27 00:36:56.399487 Epoch 100, Training Loss 0.6314209136359222\n",
      "2022-03-27 00:36:56.419485 Epoch 100, Training Loss 0.6328310418464339\n",
      "2022-03-27 00:36:56.439501 Epoch 100, Training Loss 0.6341868702255552\n",
      "2022-03-27 00:36:56.459506 Epoch 100, Training Loss 0.6351316882216412\n",
      "2022-03-27 00:36:56.479505 Epoch 100, Training Loss 0.6363887331827217\n",
      "2022-03-27 00:36:56.499509 Epoch 100, Training Loss 0.6375316011783717\n",
      "2022-03-27 00:36:56.519513 Epoch 100, Training Loss 0.6386775994087424\n",
      "2022-03-27 00:36:56.539518 Epoch 100, Training Loss 0.6398971353650398\n",
      "2022-03-27 00:36:56.559522 Epoch 100, Training Loss 0.6410580509154084\n",
      "2022-03-27 00:36:56.579533 Epoch 100, Training Loss 0.6420920565914925\n",
      "2022-03-27 00:36:56.599538 Epoch 100, Training Loss 0.6432341476687995\n",
      "2022-03-27 00:36:56.619536 Epoch 100, Training Loss 0.6444458413459456\n",
      "2022-03-27 00:36:56.640541 Epoch 100, Training Loss 0.6457929014397399\n",
      "2022-03-27 00:36:56.661551 Epoch 100, Training Loss 0.6469036474099854\n",
      "2022-03-27 00:36:56.681550 Epoch 100, Training Loss 0.6481955418044039\n",
      "2022-03-27 00:36:56.701550 Epoch 100, Training Loss 0.6493371744899799\n",
      "2022-03-27 00:36:56.721559 Epoch 100, Training Loss 0.6504408560140663\n",
      "2022-03-27 00:36:56.741557 Epoch 100, Training Loss 0.651653576613692\n",
      "2022-03-27 00:36:56.760574 Epoch 100, Training Loss 0.6528485567520952\n",
      "2022-03-27 00:36:56.780572 Epoch 100, Training Loss 0.6542231459599321\n",
      "2022-03-27 00:36:56.801577 Epoch 100, Training Loss 0.6554407949185432\n",
      "2022-03-27 00:36:56.820587 Epoch 100, Training Loss 0.6564926438776734\n",
      "2022-03-27 00:36:56.841592 Epoch 100, Training Loss 0.6574693321419494\n",
      "2022-03-27 00:36:56.862597 Epoch 100, Training Loss 0.6584260511733687\n",
      "2022-03-27 00:36:56.882601 Epoch 100, Training Loss 0.6594494838086541\n",
      "2022-03-27 00:36:56.902606 Epoch 100, Training Loss 0.6606407641144969\n",
      "2022-03-27 00:36:56.922611 Epoch 100, Training Loss 0.6618108575606285\n",
      "2022-03-27 00:36:56.941615 Epoch 100, Training Loss 0.6631510090035246\n",
      "2022-03-27 00:36:56.961619 Epoch 100, Training Loss 0.6643680442324684\n",
      "2022-03-27 00:36:56.981618 Epoch 100, Training Loss 0.6656343530663444\n",
      "2022-03-27 00:36:57.001622 Epoch 100, Training Loss 0.666673887721108\n",
      "2022-03-27 00:36:57.021633 Epoch 100, Training Loss 0.6676293810462708\n",
      "2022-03-27 00:36:57.041621 Epoch 100, Training Loss 0.6689435649863289\n",
      "2022-03-27 00:36:57.062642 Epoch 100, Training Loss 0.6702268692995886\n",
      "2022-03-27 00:36:57.083636 Epoch 100, Training Loss 0.6715262720499502\n",
      "2022-03-27 00:36:57.102646 Epoch 100, Training Loss 0.6728138691171661\n",
      "2022-03-27 00:36:57.122650 Epoch 100, Training Loss 0.6739999148851771\n",
      "2022-03-27 00:36:57.142648 Epoch 100, Training Loss 0.6750769131933637\n",
      "2022-03-27 00:36:57.162665 Epoch 100, Training Loss 0.6762649271341846\n",
      "2022-03-27 00:36:57.182670 Epoch 100, Training Loss 0.6773036443211539\n",
      "2022-03-27 00:36:57.201662 Epoch 100, Training Loss 0.6785082878816463\n",
      "2022-03-27 00:36:57.221672 Epoch 100, Training Loss 0.6797637570544582\n",
      "2022-03-27 00:36:57.241677 Epoch 100, Training Loss 0.6806207200907686\n",
      "2022-03-27 00:36:57.262682 Epoch 100, Training Loss 0.6817929115899078\n",
      "2022-03-27 00:36:57.282676 Epoch 100, Training Loss 0.683130473660691\n",
      "2022-03-27 00:36:57.302677 Epoch 100, Training Loss 0.6845136398397138\n",
      "2022-03-27 00:36:57.321701 Epoch 100, Training Loss 0.6855464821581341\n",
      "2022-03-27 00:36:57.342706 Epoch 100, Training Loss 0.6866630270810383\n",
      "2022-03-27 00:36:57.362710 Epoch 100, Training Loss 0.6881876465152291\n",
      "2022-03-27 00:36:57.382709 Epoch 100, Training Loss 0.6895203434140481\n",
      "2022-03-27 00:36:57.401713 Epoch 100, Training Loss 0.69076193286025\n",
      "2022-03-27 00:36:57.421724 Epoch 100, Training Loss 0.6921123594731626\n",
      "2022-03-27 00:36:57.441906 Epoch 100, Training Loss 0.6933096974249691\n",
      "2022-03-27 00:36:57.462910 Epoch 100, Training Loss 0.6946812099050683\n",
      "2022-03-27 00:36:57.483903 Epoch 100, Training Loss 0.6962878995996606\n",
      "2022-03-27 00:36:57.502920 Epoch 100, Training Loss 0.6976485849189027\n",
      "2022-03-27 00:36:57.522924 Epoch 100, Training Loss 0.6985485782403775\n",
      "2022-03-27 00:36:57.542929 Epoch 100, Training Loss 0.6997979380133207\n",
      "2022-03-27 00:36:57.561933 Epoch 100, Training Loss 0.7011316071843248\n",
      "2022-03-27 00:36:57.581932 Epoch 100, Training Loss 0.7021521906871016\n",
      "2022-03-27 00:36:57.601936 Epoch 100, Training Loss 0.7033492806165115\n",
      "2022-03-27 00:36:57.621947 Epoch 100, Training Loss 0.7045447994833407\n",
      "2022-03-27 00:36:57.641951 Epoch 100, Training Loss 0.7057011697603308\n",
      "2022-03-27 00:36:57.661949 Epoch 100, Training Loss 0.7071571863825669\n",
      "2022-03-27 00:36:57.681954 Epoch 100, Training Loss 0.7083751983807215\n",
      "2022-03-27 00:36:57.701959 Epoch 100, Training Loss 0.7093539285232954\n",
      "2022-03-27 00:36:57.721969 Epoch 100, Training Loss 0.7105443067562854\n",
      "2022-03-27 00:36:57.741974 Epoch 100, Training Loss 0.7118211481577296\n",
      "2022-03-27 00:36:57.761972 Epoch 100, Training Loss 0.7128977598741536\n",
      "2022-03-27 00:36:57.781977 Epoch 100, Training Loss 0.7140500846573764\n",
      "2022-03-27 00:36:57.801983 Epoch 100, Training Loss 0.7151579270734811\n",
      "2022-03-27 00:36:57.821994 Epoch 100, Training Loss 0.7162402618266738\n",
      "2022-03-27 00:36:57.841990 Epoch 100, Training Loss 0.7173030932845972\n",
      "2022-03-27 00:36:57.861001 Epoch 100, Training Loss 0.7184166943325716\n",
      "2022-03-27 00:36:57.882006 Epoch 100, Training Loss 0.7197142007100917\n",
      "2022-03-27 00:36:57.902842 Epoch 100, Training Loss 0.7208376039781839\n",
      "2022-03-27 00:36:57.921858 Epoch 100, Training Loss 0.7221063550018594\n",
      "2022-03-27 00:36:57.941857 Epoch 100, Training Loss 0.7234190537039277\n",
      "2022-03-27 00:36:57.962861 Epoch 100, Training Loss 0.7247931977824482\n",
      "2022-03-27 00:36:57.982873 Epoch 100, Training Loss 0.72634396932619\n",
      "2022-03-27 00:36:58.002864 Epoch 100, Training Loss 0.7275061586019024\n",
      "2022-03-27 00:36:58.021866 Epoch 100, Training Loss 0.7288216280815242\n",
      "2022-03-27 00:36:58.041891 Epoch 100, Training Loss 0.7298858107813179\n",
      "2022-03-27 00:36:58.062889 Epoch 100, Training Loss 0.7311310234582028\n",
      "2022-03-27 00:36:58.082901 Epoch 100, Training Loss 0.7322119946979806\n",
      "2022-03-27 00:36:58.103893 Epoch 100, Training Loss 0.7332304879984892\n",
      "2022-03-27 00:36:58.124892 Epoch 100, Training Loss 0.7344689870734349\n",
      "2022-03-27 00:36:58.144914 Epoch 100, Training Loss 0.7356184930600169\n",
      "2022-03-27 00:36:58.165905 Epoch 100, Training Loss 0.7366721623236566\n",
      "2022-03-27 00:36:58.186911 Epoch 100, Training Loss 0.7380196373633412\n",
      "2022-03-27 00:36:58.206928 Epoch 100, Training Loss 0.7394612110636728\n",
      "2022-03-27 00:36:58.226933 Epoch 100, Training Loss 0.7405469361930856\n",
      "2022-03-27 00:36:58.246937 Epoch 100, Training Loss 0.741390691160241\n",
      "2022-03-27 00:36:58.267931 Epoch 100, Training Loss 0.7424748443886448\n",
      "2022-03-27 00:36:58.287934 Epoch 100, Training Loss 0.7439213992689576\n",
      "2022-03-27 00:36:58.307939 Epoch 100, Training Loss 0.745128473814796\n",
      "2022-03-27 00:36:58.328956 Epoch 100, Training Loss 0.7464728363029792\n",
      "2022-03-27 00:36:58.349962 Epoch 100, Training Loss 0.7475757746745253\n",
      "2022-03-27 00:36:58.368953 Epoch 100, Training Loss 0.7485669706483631\n",
      "2022-03-27 00:36:58.389964 Epoch 100, Training Loss 0.7499088369062185\n",
      "2022-03-27 00:36:58.409963 Epoch 100, Training Loss 0.7509869974287574\n",
      "2022-03-27 00:36:58.428979 Epoch 100, Training Loss 0.7522016829236999\n",
      "2022-03-27 00:36:58.449082 Epoch 100, Training Loss 0.7532952206823832\n",
      "2022-03-27 00:36:58.468972 Epoch 100, Training Loss 0.7544910845244327\n",
      "2022-03-27 00:36:58.488986 Epoch 100, Training Loss 0.7558113970719945\n",
      "2022-03-27 00:36:58.509991 Epoch 100, Training Loss 0.7569629722239112\n",
      "2022-03-27 00:36:58.530996 Epoch 100, Training Loss 0.7581705772663321\n",
      "2022-03-27 00:36:58.550992 Epoch 100, Training Loss 0.7593395019431248\n",
      "2022-03-27 00:36:58.571000 Epoch 100, Training Loss 0.7604283939694505\n",
      "2022-03-27 00:36:58.591015 Epoch 100, Training Loss 0.7615962252592492\n",
      "2022-03-27 00:36:58.610014 Epoch 100, Training Loss 0.7628397840215727\n",
      "2022-03-27 00:36:58.630012 Epoch 100, Training Loss 0.7639336667554762\n",
      "2022-03-27 00:36:58.651023 Epoch 100, Training Loss 0.7651882376664739\n",
      "2022-03-27 00:36:58.670033 Epoch 100, Training Loss 0.766654276710642\n",
      "2022-03-27 00:36:58.690027 Epoch 100, Training Loss 0.7678032409961876\n",
      "2022-03-27 00:36:58.710026 Epoch 100, Training Loss 0.7691583789675437\n",
      "2022-03-27 00:36:58.730032 Epoch 100, Training Loss 0.7701793159822674\n",
      "2022-03-27 00:36:58.750040 Epoch 100, Training Loss 0.7712802721563813\n",
      "2022-03-27 00:36:58.770050 Epoch 100, Training Loss 0.7723119818341092\n",
      "2022-03-27 00:36:58.790055 Epoch 100, Training Loss 0.7735178376097813\n",
      "2022-03-27 00:36:58.809053 Epoch 100, Training Loss 0.7745697403045566\n",
      "2022-03-27 00:36:58.829053 Epoch 100, Training Loss 0.7758008673825227\n",
      "2022-03-27 00:36:58.849055 Epoch 100, Training Loss 0.7770439195815865\n",
      "2022-03-27 00:36:58.868072 Epoch 100, Training Loss 0.7782353678017931\n",
      "2022-03-27 00:36:58.888077 Epoch 100, Training Loss 0.7795565032288242\n",
      "2022-03-27 00:36:58.908081 Epoch 100, Training Loss 0.7806980664010548\n",
      "2022-03-27 00:36:58.928086 Epoch 100, Training Loss 0.7820395671040811\n",
      "2022-03-27 00:36:58.949086 Epoch 100, Training Loss 0.7830964484635521\n",
      "2022-03-27 00:36:58.969086 Epoch 100, Training Loss 0.7844852491870256\n",
      "2022-03-27 00:36:58.989107 Epoch 100, Training Loss 0.7856936933439406\n",
      "2022-03-27 00:36:59.009110 Epoch 100, Training Loss 0.7867460516102783\n",
      "2022-03-27 00:36:59.029115 Epoch 100, Training Loss 0.7881719713930584\n",
      "2022-03-27 00:36:59.049113 Epoch 100, Training Loss 0.7892489420331043\n",
      "2022-03-27 00:36:59.070118 Epoch 100, Training Loss 0.7905492883966402\n",
      "2022-03-27 00:36:59.089122 Epoch 100, Training Loss 0.7918166747635893\n",
      "2022-03-27 00:36:59.109129 Epoch 100, Training Loss 0.7928982175067257\n",
      "2022-03-27 00:36:59.128131 Epoch 100, Training Loss 0.7944014800326599\n",
      "2022-03-27 00:36:59.150132 Epoch 100, Training Loss 0.7957612564954002\n",
      "2022-03-27 00:36:59.170134 Epoch 100, Training Loss 0.7969761508352616\n",
      "2022-03-27 00:36:59.191152 Epoch 100, Training Loss 0.7981437071967308\n",
      "2022-03-27 00:36:59.211156 Epoch 100, Training Loss 0.7991015388989997\n",
      "2022-03-27 00:36:59.232161 Epoch 100, Training Loss 0.8006278807702272\n",
      "2022-03-27 00:36:59.252148 Epoch 100, Training Loss 0.8020069583145248\n",
      "2022-03-27 00:36:59.272164 Epoch 100, Training Loss 0.8031442660047575\n",
      "2022-03-27 00:36:59.291170 Epoch 100, Training Loss 0.8042050609198372\n",
      "2022-03-27 00:36:59.312174 Epoch 100, Training Loss 0.8051472306251526\n",
      "2022-03-27 00:36:59.331726 Epoch 100, Training Loss 0.8063926706686044\n",
      "2022-03-27 00:36:59.351718 Epoch 100, Training Loss 0.8077035475417477\n",
      "2022-03-27 00:36:59.371714 Epoch 100, Training Loss 0.8095332887928809\n",
      "2022-03-27 00:36:59.392739 Epoch 100, Training Loss 0.8110248028012492\n",
      "2022-03-27 00:36:59.412738 Epoch 100, Training Loss 0.8120224834860438\n",
      "2022-03-27 00:36:59.432742 Epoch 100, Training Loss 0.8131430955494151\n",
      "2022-03-27 00:36:59.452741 Epoch 100, Training Loss 0.8144823504835749\n",
      "2022-03-27 00:36:59.472752 Epoch 100, Training Loss 0.8157520576206314\n",
      "2022-03-27 00:36:59.492762 Epoch 100, Training Loss 0.817046481477635\n",
      "2022-03-27 00:36:59.512767 Epoch 100, Training Loss 0.8183150008663802\n",
      "2022-03-27 00:36:59.532756 Epoch 100, Training Loss 0.8194496042435736\n",
      "2022-03-27 00:36:59.553776 Epoch 100, Training Loss 0.8205988786714461\n",
      "2022-03-27 00:36:59.573775 Epoch 100, Training Loss 0.8218689560890198\n",
      "2022-03-27 00:36:59.593785 Epoch 100, Training Loss 0.8229372071488129\n",
      "2022-03-27 00:36:59.614790 Epoch 100, Training Loss 0.8243818722112709\n",
      "2022-03-27 00:36:59.634788 Epoch 100, Training Loss 0.8258308045699468\n",
      "2022-03-27 00:36:59.654799 Epoch 100, Training Loss 0.827055560353467\n",
      "2022-03-27 00:36:59.674797 Epoch 100, Training Loss 0.8282275081747938\n",
      "2022-03-27 00:36:59.694802 Epoch 100, Training Loss 0.8295717282825724\n",
      "2022-03-27 00:36:59.714800 Epoch 100, Training Loss 0.8307826336082595\n",
      "2022-03-27 00:36:59.734811 Epoch 100, Training Loss 0.8320848957047133\n",
      "2022-03-27 00:36:59.754809 Epoch 100, Training Loss 0.8334165009696161\n",
      "2022-03-27 00:36:59.774826 Epoch 100, Training Loss 0.8346188928160216\n",
      "2022-03-27 00:36:59.795831 Epoch 100, Training Loss 0.8361666008944402\n",
      "2022-03-27 00:36:59.815829 Epoch 100, Training Loss 0.8371749027915623\n",
      "2022-03-27 00:36:59.836840 Epoch 100, Training Loss 0.838272426515589\n",
      "2022-03-27 00:36:59.855844 Epoch 100, Training Loss 0.839513988369871\n",
      "2022-03-27 00:36:59.876849 Epoch 100, Training Loss 0.8404482779905315\n",
      "2022-03-27 00:36:59.896847 Epoch 100, Training Loss 0.8418684456964283\n",
      "2022-03-27 00:36:59.920853 Epoch 100, Training Loss 0.8428205904905753\n",
      "2022-03-27 00:36:59.947853 Epoch 100, Training Loss 0.8439474441968572\n",
      "2022-03-27 00:36:59.974865 Epoch 100, Training Loss 0.8452590088862593\n",
      "2022-03-27 00:37:00.002860 Epoch 100, Training Loss 0.8463136238210341\n",
      "2022-03-27 00:37:00.028878 Epoch 100, Training Loss 0.8478032487737553\n",
      "2022-03-27 00:37:00.056878 Epoch 100, Training Loss 0.8491604748894187\n",
      "2022-03-27 00:37:00.083886 Epoch 100, Training Loss 0.8504593079657201\n",
      "2022-03-27 00:37:00.109902 Epoch 100, Training Loss 0.8517909164318953\n",
      "2022-03-27 00:37:00.136902 Epoch 100, Training Loss 0.8529366704509082\n",
      "2022-03-27 00:37:00.163914 Epoch 100, Training Loss 0.8541146147891384\n",
      "2022-03-27 00:37:00.189920 Epoch 100, Training Loss 0.8555023058905931\n",
      "2022-03-27 00:37:00.216909 Epoch 100, Training Loss 0.8568248329565044\n",
      "2022-03-27 00:37:00.243932 Epoch 100, Training Loss 0.8582691245371729\n",
      "2022-03-27 00:37:00.269926 Epoch 100, Training Loss 0.8594300549505921\n",
      "2022-03-27 00:37:00.296944 Epoch 100, Training Loss 0.8606883219593321\n",
      "2022-03-27 00:37:00.323932 Epoch 100, Training Loss 0.8617505400686922\n",
      "2022-03-27 00:37:00.342949 Epoch 100, Training Loss 0.8630922878794658\n",
      "2022-03-27 00:37:00.361959 Epoch 100, Training Loss 0.8643092430765976\n",
      "2022-03-27 00:37:00.381948 Epoch 100, Training Loss 0.8655249479481631\n",
      "2022-03-27 00:37:00.400957 Epoch 100, Training Loss 0.8666437667653993\n",
      "2022-03-27 00:37:00.419953 Epoch 100, Training Loss 0.8678839542829168\n",
      "2022-03-27 00:37:00.438956 Epoch 100, Training Loss 0.8690558626981038\n",
      "2022-03-27 00:37:00.457975 Epoch 100, Training Loss 0.8701716381723009\n",
      "2022-03-27 00:37:00.477985 Epoch 100, Training Loss 0.8713515900894809\n",
      "2022-03-27 00:37:00.497990 Epoch 100, Training Loss 0.87247905950717\n",
      "2022-03-27 00:37:00.516982 Epoch 100, Training Loss 0.8739109902125796\n",
      "2022-03-27 00:37:00.535979 Epoch 100, Training Loss 0.8750032582093993\n",
      "2022-03-27 00:37:00.555003 Epoch 100, Training Loss 0.8762515488335544\n",
      "2022-03-27 00:37:00.574007 Epoch 100, Training Loss 0.877570057387852\n",
      "2022-03-27 00:37:00.593011 Epoch 100, Training Loss 0.8787149034650125\n",
      "2022-03-27 00:37:00.614004 Epoch 100, Training Loss 0.879714553999474\n",
      "2022-03-27 00:37:00.635015 Epoch 100, Training Loss 0.8809329567815337\n",
      "2022-03-27 00:37:00.654019 Epoch 100, Training Loss 0.8822363477076411\n",
      "2022-03-27 00:37:00.674030 Epoch 100, Training Loss 0.883928608787639\n",
      "2022-03-27 00:37:00.693028 Epoch 100, Training Loss 0.8854212783029317\n",
      "2022-03-27 00:37:00.713026 Epoch 100, Training Loss 0.8865850094486686\n",
      "2022-03-27 00:37:00.732028 Epoch 100, Training Loss 0.8877110388272863\n",
      "2022-03-27 00:37:00.750030 Epoch 100, Training Loss 0.8888555938935341\n",
      "2022-03-27 00:37:00.769033 Epoch 100, Training Loss 0.8899528774459039\n",
      "2022-03-27 00:37:00.788037 Epoch 100, Training Loss 0.8911320246241586\n",
      "2022-03-27 00:37:00.807041 Epoch 100, Training Loss 0.8921317765322487\n",
      "2022-03-27 00:37:00.825046 Epoch 100, Training Loss 0.8934197348859304\n",
      "2022-03-27 00:37:00.845049 Epoch 100, Training Loss 0.8949240945336764\n",
      "2022-03-27 00:37:00.864054 Epoch 100, Training Loss 0.8961021270593414\n",
      "2022-03-27 00:37:00.883061 Epoch 100, Training Loss 0.8972993686680903\n",
      "2022-03-27 00:37:00.902062 Epoch 100, Training Loss 0.8986165582981256\n",
      "2022-03-27 00:37:00.921080 Epoch 100, Training Loss 0.899857310154249\n",
      "2022-03-27 00:37:00.939079 Epoch 100, Training Loss 0.900974847319181\n",
      "2022-03-27 00:37:00.958076 Epoch 100, Training Loss 0.9023249252982761\n",
      "2022-03-27 00:37:00.977090 Epoch 100, Training Loss 0.9035877988618963\n",
      "2022-03-27 00:37:00.996083 Epoch 100, Training Loss 0.9049674791600698\n",
      "2022-03-27 00:37:01.015092 Epoch 100, Training Loss 0.9063731271135228\n",
      "2022-03-27 00:37:01.035092 Epoch 100, Training Loss 0.9074398773100675\n",
      "2022-03-27 00:37:01.054106 Epoch 100, Training Loss 0.9086727286543688\n",
      "2022-03-27 00:37:01.073102 Epoch 100, Training Loss 0.9095391804909767\n",
      "2022-03-27 00:37:01.092105 Epoch 100, Training Loss 0.9105466858047964\n",
      "2022-03-27 00:37:01.111110 Epoch 100, Training Loss 0.9119670801912733\n",
      "2022-03-27 00:37:01.130123 Epoch 100, Training Loss 0.9130044474321253\n",
      "2022-03-27 00:37:01.149122 Epoch 100, Training Loss 0.9141575051543048\n",
      "2022-03-27 00:37:01.167124 Epoch 100, Training Loss 0.9155498909980745\n",
      "2022-03-27 00:37:01.186128 Epoch 100, Training Loss 0.9166367790278267\n",
      "2022-03-27 00:37:01.205131 Epoch 100, Training Loss 0.9179789901084607\n",
      "2022-03-27 00:37:01.224136 Epoch 100, Training Loss 0.9192723727897\n",
      "2022-03-27 00:37:01.243139 Epoch 100, Training Loss 0.9205203729364878\n",
      "2022-03-27 00:37:01.262144 Epoch 100, Training Loss 0.9218068327897649\n",
      "2022-03-27 00:37:01.281150 Epoch 100, Training Loss 0.9230019983733097\n",
      "2022-03-27 00:37:01.299156 Epoch 100, Training Loss 0.9240357441365566\n",
      "2022-03-27 00:37:01.318159 Epoch 100, Training Loss 0.9252098991590387\n",
      "2022-03-27 00:37:01.338180 Epoch 100, Training Loss 0.9262310271830205\n",
      "2022-03-27 00:37:01.356178 Epoch 100, Training Loss 0.9275648669361154\n",
      "2022-03-27 00:37:01.375184 Epoch 100, Training Loss 0.9285808120999495\n",
      "2022-03-27 00:37:01.395192 Epoch 100, Training Loss 0.9296482722167774\n",
      "2022-03-27 00:37:01.414192 Epoch 100, Training Loss 0.9307512548725928\n",
      "2022-03-27 00:37:01.433184 Epoch 100, Training Loss 0.9319243803048682\n",
      "2022-03-27 00:37:01.452186 Epoch 100, Training Loss 0.9334205103957135\n",
      "2022-03-27 00:37:01.462196 Epoch 100, Training Loss 0.9345754616705658\n",
      "2022-03-27 00:53:13.052147 Epoch 150, Training Loss 0.0013924422471419625\n",
      "2022-03-27 00:53:13.108156 Epoch 150, Training Loss 0.0027130500739797605\n",
      "2022-03-27 00:53:13.172162 Epoch 150, Training Loss 0.003617627084102777\n",
      "2022-03-27 00:53:13.241319 Epoch 150, Training Loss 0.0046138344975688575\n",
      "2022-03-27 00:53:13.303691 Epoch 150, Training Loss 0.006025930118682744\n",
      "2022-03-27 00:53:13.353129 Epoch 150, Training Loss 0.006992804501062769\n",
      "2022-03-27 00:53:13.405455 Epoch 150, Training Loss 0.008310974940009739\n",
      "2022-03-27 00:53:13.457442 Epoch 150, Training Loss 0.00963616637927492\n",
      "2022-03-27 00:53:13.501924 Epoch 150, Training Loss 0.011065877383322362\n",
      "2022-03-27 00:53:13.557912 Epoch 150, Training Loss 0.012253269057749483\n",
      "2022-03-27 00:53:13.613915 Epoch 150, Training Loss 0.013807908653298302\n",
      "2022-03-27 00:53:13.669927 Epoch 150, Training Loss 0.015142748118056666\n",
      "2022-03-27 00:53:13.733930 Epoch 150, Training Loss 0.016393010726060402\n",
      "2022-03-27 00:53:13.789935 Epoch 150, Training Loss 0.017308253842546507\n",
      "2022-03-27 00:53:13.837959 Epoch 150, Training Loss 0.018494325601841177\n",
      "2022-03-27 00:53:13.892428 Epoch 150, Training Loss 0.01945112359798168\n",
      "2022-03-27 00:53:13.940432 Epoch 150, Training Loss 0.020524433887828035\n",
      "2022-03-27 00:53:13.996459 Epoch 150, Training Loss 0.021347755513837576\n",
      "2022-03-27 00:53:14.044464 Epoch 150, Training Loss 0.022341333386843163\n",
      "2022-03-27 00:53:14.092468 Epoch 150, Training Loss 0.023461822887210895\n",
      "2022-03-27 00:53:14.148463 Epoch 150, Training Loss 0.024533406624098874\n",
      "2022-03-27 00:53:14.196480 Epoch 150, Training Loss 0.025712079937805606\n",
      "2022-03-27 00:53:14.252471 Epoch 150, Training Loss 0.02652660667743829\n",
      "2022-03-27 00:53:14.300477 Epoch 150, Training Loss 0.027647423469806875\n",
      "2022-03-27 00:53:14.348476 Epoch 150, Training Loss 0.02869148922088506\n",
      "2022-03-27 00:53:14.396502 Epoch 150, Training Loss 0.02967756422584319\n",
      "2022-03-27 00:53:14.450404 Epoch 150, Training Loss 0.030572325219888518\n",
      "2022-03-27 00:53:14.506395 Epoch 150, Training Loss 0.03171992515359084\n",
      "2022-03-27 00:53:14.562416 Epoch 150, Training Loss 0.032783012865754343\n",
      "2022-03-27 00:53:14.610421 Epoch 150, Training Loss 0.03409674329221096\n",
      "2022-03-27 00:53:14.658426 Epoch 150, Training Loss 0.0353763121778093\n",
      "2022-03-27 00:53:14.714414 Epoch 150, Training Loss 0.03658848955198322\n",
      "2022-03-27 00:53:14.762438 Epoch 150, Training Loss 0.03785567012284418\n",
      "2022-03-27 00:53:14.810463 Epoch 150, Training Loss 0.03892540306691319\n",
      "2022-03-27 00:53:14.866449 Epoch 150, Training Loss 0.03983733858293889\n",
      "2022-03-27 00:53:14.914457 Epoch 150, Training Loss 0.0408460151813829\n",
      "2022-03-27 00:53:14.968423 Epoch 150, Training Loss 0.04209422264867427\n",
      "2022-03-27 00:53:15.024457 Epoch 150, Training Loss 0.04323724415296179\n",
      "2022-03-27 00:53:15.072445 Epoch 150, Training Loss 0.044681692291098786\n",
      "2022-03-27 00:53:15.120452 Epoch 150, Training Loss 0.045685466293178864\n",
      "2022-03-27 00:53:15.176474 Epoch 150, Training Loss 0.04690127078529514\n",
      "2022-03-27 00:53:15.224479 Epoch 150, Training Loss 0.04811690385689211\n",
      "2022-03-27 00:53:15.272464 Epoch 150, Training Loss 0.04908972094430948\n",
      "2022-03-27 00:53:15.320490 Epoch 150, Training Loss 0.05052024522400878\n",
      "2022-03-27 00:53:15.376478 Epoch 150, Training Loss 0.0516339979513222\n",
      "2022-03-27 00:53:15.424484 Epoch 150, Training Loss 0.052960660451513424\n",
      "2022-03-27 00:53:15.472493 Epoch 150, Training Loss 0.054260716566344354\n",
      "2022-03-27 00:53:15.525833 Epoch 150, Training Loss 0.055509701180641\n",
      "2022-03-27 00:53:15.581821 Epoch 150, Training Loss 0.056787329866453205\n",
      "2022-03-27 00:53:15.637825 Epoch 150, Training Loss 0.05811845112944503\n",
      "2022-03-27 00:53:15.685832 Epoch 150, Training Loss 0.0595105046506428\n",
      "2022-03-27 00:53:15.733856 Epoch 150, Training Loss 0.06054640075434809\n",
      "2022-03-27 00:53:15.789846 Epoch 150, Training Loss 0.0616901455937749\n",
      "2022-03-27 00:53:15.837847 Epoch 150, Training Loss 0.06281906564522277\n",
      "2022-03-27 00:53:15.893858 Epoch 150, Training Loss 0.0639298665706459\n",
      "2022-03-27 00:53:15.949878 Epoch 150, Training Loss 0.06507879869102517\n",
      "2022-03-27 00:53:15.997867 Epoch 150, Training Loss 0.06617162835872387\n",
      "2022-03-27 00:53:16.067982 Epoch 150, Training Loss 0.06727664245059119\n",
      "2022-03-27 00:53:16.123970 Epoch 150, Training Loss 0.06828803167013867\n",
      "2022-03-27 00:53:16.179978 Epoch 150, Training Loss 0.06935438475645411\n",
      "2022-03-27 00:53:16.243985 Epoch 150, Training Loss 0.07074216564597986\n",
      "2022-03-27 00:53:16.315995 Epoch 150, Training Loss 0.07191706976622267\n",
      "2022-03-27 00:53:16.380481 Epoch 150, Training Loss 0.07314799531646397\n",
      "2022-03-27 00:53:16.452507 Epoch 150, Training Loss 0.07449519626624748\n",
      "2022-03-27 00:53:16.524508 Epoch 150, Training Loss 0.07564188665746117\n",
      "2022-03-27 00:53:16.578672 Epoch 150, Training Loss 0.0767568727131085\n",
      "2022-03-27 00:53:16.626694 Epoch 150, Training Loss 0.07786387731047238\n",
      "2022-03-27 00:53:16.674700 Epoch 150, Training Loss 0.07883043430955208\n",
      "2022-03-27 00:53:16.722687 Epoch 150, Training Loss 0.0799288564478345\n",
      "2022-03-27 00:53:16.778956 Epoch 150, Training Loss 0.08108252980520049\n",
      "2022-03-27 00:53:16.826977 Epoch 150, Training Loss 0.08238548444360114\n",
      "2022-03-27 00:53:16.890964 Epoch 150, Training Loss 0.08331594145511423\n",
      "2022-03-27 00:53:16.946974 Epoch 150, Training Loss 0.08438614574844575\n",
      "2022-03-27 00:53:17.002996 Epoch 150, Training Loss 0.08591460030706947\n",
      "2022-03-27 00:53:17.058987 Epoch 150, Training Loss 0.08708768709541281\n",
      "2022-03-27 00:53:17.120669 Epoch 150, Training Loss 0.08834217416355981\n",
      "2022-03-27 00:53:17.176657 Epoch 150, Training Loss 0.0893631672767727\n",
      "2022-03-27 00:53:17.232681 Epoch 150, Training Loss 0.0907973225616738\n",
      "2022-03-27 00:53:17.288892 Epoch 150, Training Loss 0.09193839952159111\n",
      "2022-03-27 00:53:17.352900 Epoch 150, Training Loss 0.09309448297981106\n",
      "2022-03-27 00:53:17.408906 Epoch 150, Training Loss 0.09451624720602694\n",
      "2022-03-27 00:53:17.464930 Epoch 150, Training Loss 0.09569046976011428\n",
      "2022-03-27 00:53:17.538247 Epoch 150, Training Loss 0.09692972471646945\n",
      "2022-03-27 00:53:17.596894 Epoch 150, Training Loss 0.09812782136985408\n",
      "2022-03-27 00:53:17.653370 Epoch 150, Training Loss 0.09937389061579009\n",
      "2022-03-27 00:53:17.705471 Epoch 150, Training Loss 0.10071411218179767\n",
      "2022-03-27 00:53:17.769481 Epoch 150, Training Loss 0.10208265372859243\n",
      "2022-03-27 00:53:17.817489 Epoch 150, Training Loss 0.10329111793157085\n",
      "2022-03-27 00:53:17.873491 Epoch 150, Training Loss 0.10458805585456321\n",
      "2022-03-27 00:53:17.929502 Epoch 150, Training Loss 0.10607239154293714\n",
      "2022-03-27 00:53:17.977501 Epoch 150, Training Loss 0.10729666126658545\n",
      "2022-03-27 00:53:18.033512 Epoch 150, Training Loss 0.10839795723290699\n",
      "2022-03-27 00:53:18.081518 Epoch 150, Training Loss 0.10960681564972528\n",
      "2022-03-27 00:53:18.129523 Epoch 150, Training Loss 0.11076130342605474\n",
      "2022-03-27 00:53:18.185400 Epoch 150, Training Loss 0.11188710338014471\n",
      "2022-03-27 00:53:18.237987 Epoch 150, Training Loss 0.11290450862911351\n",
      "2022-03-27 00:53:18.286006 Epoch 150, Training Loss 0.11407640797402853\n",
      "2022-03-27 00:53:18.333993 Epoch 150, Training Loss 0.11509305703670472\n",
      "2022-03-27 00:53:18.390002 Epoch 150, Training Loss 0.11646730599500944\n",
      "2022-03-27 00:53:18.438023 Epoch 150, Training Loss 0.11758829893358529\n",
      "2022-03-27 00:53:18.494014 Epoch 150, Training Loss 0.11903477042837216\n",
      "2022-03-27 00:53:18.550036 Epoch 150, Training Loss 0.12023982809632636\n",
      "2022-03-27 00:53:18.606025 Epoch 150, Training Loss 0.12149816934409959\n",
      "2022-03-27 00:53:18.670039 Epoch 150, Training Loss 0.12265632951351078\n",
      "2022-03-27 00:53:18.725531 Epoch 150, Training Loss 0.12377722565170445\n",
      "2022-03-27 00:53:18.769809 Epoch 150, Training Loss 0.1251087086584867\n",
      "2022-03-27 00:53:18.825801 Epoch 150, Training Loss 0.12642720166374655\n",
      "2022-03-27 00:53:18.873809 Epoch 150, Training Loss 0.12763044718281388\n",
      "2022-03-27 00:53:18.929827 Epoch 150, Training Loss 0.12890965630636192\n",
      "2022-03-27 00:53:18.977832 Epoch 150, Training Loss 0.12998359145410834\n",
      "2022-03-27 00:53:19.025836 Epoch 150, Training Loss 0.1312134481604447\n",
      "2022-03-27 00:53:19.081824 Epoch 150, Training Loss 0.1324937351981697\n",
      "2022-03-27 00:53:19.129830 Epoch 150, Training Loss 0.1335777409393769\n",
      "2022-03-27 00:53:19.177836 Epoch 150, Training Loss 0.13444625721563158\n",
      "2022-03-27 00:53:19.233861 Epoch 150, Training Loss 0.13541098567835816\n",
      "2022-03-27 00:53:19.279262 Epoch 150, Training Loss 0.13636133242446138\n",
      "2022-03-27 00:53:19.335268 Epoch 150, Training Loss 0.13766067526529513\n",
      "2022-03-27 00:53:19.383314 Epoch 150, Training Loss 0.13861086171911197\n",
      "2022-03-27 00:53:19.431279 Epoch 150, Training Loss 0.13971795557100145\n",
      "2022-03-27 00:53:19.487522 Epoch 150, Training Loss 0.14113800391516723\n",
      "2022-03-27 00:53:19.535527 Epoch 150, Training Loss 0.14209469459245883\n",
      "2022-03-27 00:53:19.591519 Epoch 150, Training Loss 0.14330072620945514\n",
      "2022-03-27 00:53:19.639539 Epoch 150, Training Loss 0.14445106048718134\n",
      "2022-03-27 00:53:19.687685 Epoch 150, Training Loss 0.1455865840015509\n",
      "2022-03-27 00:53:19.739587 Epoch 150, Training Loss 0.14656123038752913\n",
      "2022-03-27 00:53:19.793602 Epoch 150, Training Loss 0.14763803296076977\n",
      "2022-03-27 00:53:19.849878 Epoch 150, Training Loss 0.14901458774991047\n",
      "2022-03-27 00:53:19.897883 Epoch 150, Training Loss 0.15036365870014787\n",
      "2022-03-27 00:53:19.953889 Epoch 150, Training Loss 0.15147225844585682\n",
      "2022-03-27 00:53:20.001914 Epoch 150, Training Loss 0.1524980231319242\n",
      "2022-03-27 00:53:20.057921 Epoch 150, Training Loss 0.15382255282243498\n",
      "2022-03-27 00:53:20.105906 Epoch 150, Training Loss 0.1550507242874721\n",
      "2022-03-27 00:53:20.161931 Epoch 150, Training Loss 0.15624328334922985\n",
      "2022-03-27 00:53:20.217920 Epoch 150, Training Loss 0.15742412788788682\n",
      "2022-03-27 00:53:20.273925 Epoch 150, Training Loss 0.15865164926594785\n",
      "2022-03-27 00:53:20.330202 Epoch 150, Training Loss 0.15960776996429618\n",
      "2022-03-27 00:53:20.391229 Epoch 150, Training Loss 0.1606353288873687\n",
      "2022-03-27 00:53:20.455238 Epoch 150, Training Loss 0.16185047323136684\n",
      "2022-03-27 00:53:20.527247 Epoch 150, Training Loss 0.16291325499334602\n",
      "2022-03-27 00:53:20.583254 Epoch 150, Training Loss 0.16398572129056888\n",
      "2022-03-27 00:53:20.639277 Epoch 150, Training Loss 0.1649116203760552\n",
      "2022-03-27 00:53:20.695284 Epoch 150, Training Loss 0.1658785122892131\n",
      "2022-03-27 00:53:20.751288 Epoch 150, Training Loss 0.1669893993441101\n",
      "2022-03-27 00:53:20.799277 Epoch 150, Training Loss 0.16827428157982008\n",
      "2022-03-27 00:53:20.855280 Epoch 150, Training Loss 0.1692024501388335\n",
      "2022-03-27 00:53:20.918839 Epoch 150, Training Loss 0.17032151171923293\n",
      "2022-03-27 00:53:20.974847 Epoch 150, Training Loss 0.17150556431402025\n",
      "2022-03-27 00:53:21.030870 Epoch 150, Training Loss 0.17267613756992017\n",
      "2022-03-27 00:53:21.079202 Epoch 150, Training Loss 0.17388169410283608\n",
      "2022-03-27 00:53:21.127207 Epoch 150, Training Loss 0.17501314117780428\n",
      "2022-03-27 00:53:21.183212 Epoch 150, Training Loss 0.1761640080862948\n",
      "2022-03-27 00:53:21.239200 Epoch 150, Training Loss 0.17728651789448147\n",
      "2022-03-27 00:53:21.287224 Epoch 150, Training Loss 0.1784826071975786\n",
      "2022-03-27 00:53:21.343212 Epoch 150, Training Loss 0.1794562746039437\n",
      "2022-03-27 00:53:21.399219 Epoch 150, Training Loss 0.18049588159222127\n",
      "2022-03-27 00:53:21.487007 Epoch 150, Training Loss 0.1815859586229105\n",
      "2022-03-27 00:53:21.543006 Epoch 150, Training Loss 0.18253831538702825\n",
      "2022-03-27 00:53:21.591013 Epoch 150, Training Loss 0.18371786127614853\n",
      "2022-03-27 00:53:21.674214 Epoch 150, Training Loss 0.18502022383158165\n",
      "2022-03-27 00:53:21.738222 Epoch 150, Training Loss 0.18611916975901865\n",
      "2022-03-27 00:53:21.786247 Epoch 150, Training Loss 0.18752062709435172\n",
      "2022-03-27 00:53:21.834237 Epoch 150, Training Loss 0.18862186734328795\n",
      "2022-03-27 00:53:21.890257 Epoch 150, Training Loss 0.1899898590334236\n",
      "2022-03-27 00:53:21.938244 Epoch 150, Training Loss 0.19127841023228054\n",
      "2022-03-27 00:53:21.986269 Epoch 150, Training Loss 0.1922750856412951\n",
      "2022-03-27 00:53:22.046554 Epoch 150, Training Loss 0.19361026261163794\n",
      "2022-03-27 00:53:22.118350 Epoch 150, Training Loss 0.1947574808317072\n",
      "2022-03-27 00:53:22.166571 Epoch 150, Training Loss 0.1955578482669333\n",
      "2022-03-27 00:53:22.222372 Epoch 150, Training Loss 0.19660225975543946\n",
      "2022-03-27 00:53:22.286381 Epoch 150, Training Loss 0.19784267898410787\n",
      "2022-03-27 00:53:22.342401 Epoch 150, Training Loss 0.19904072251161345\n",
      "2022-03-27 00:53:22.390409 Epoch 150, Training Loss 0.2001866502377688\n",
      "2022-03-27 00:53:22.443781 Epoch 150, Training Loss 0.20132690256513902\n",
      "2022-03-27 00:53:22.491788 Epoch 150, Training Loss 0.20257741700657797\n",
      "2022-03-27 00:53:22.547791 Epoch 150, Training Loss 0.20386102315410018\n",
      "2022-03-27 00:53:22.603800 Epoch 150, Training Loss 0.20484065384511144\n",
      "2022-03-27 00:53:22.667824 Epoch 150, Training Loss 0.20581260094862155\n",
      "2022-03-27 00:53:22.731812 Epoch 150, Training Loss 0.20709368281657128\n",
      "2022-03-27 00:53:22.787820 Epoch 150, Training Loss 0.20833250788776467\n",
      "2022-03-27 00:53:22.843842 Epoch 150, Training Loss 0.20942857305107215\n",
      "2022-03-27 00:53:22.907831 Epoch 150, Training Loss 0.21036070943488488\n",
      "2022-03-27 00:53:22.963857 Epoch 150, Training Loss 0.21177814172966705\n",
      "2022-03-27 00:53:23.020277 Epoch 150, Training Loss 0.2129631851182874\n",
      "2022-03-27 00:53:23.068282 Epoch 150, Training Loss 0.21421344650675878\n",
      "2022-03-27 00:53:23.116270 Epoch 150, Training Loss 0.21524723114260016\n",
      "2022-03-27 00:53:23.172279 Epoch 150, Training Loss 0.21642533623044144\n",
      "2022-03-27 00:53:23.220626 Epoch 150, Training Loss 0.217706720847303\n",
      "2022-03-27 00:53:23.276613 Epoch 150, Training Loss 0.21882955962434755\n",
      "2022-03-27 00:53:23.324622 Epoch 150, Training Loss 0.2200996859756577\n",
      "2022-03-27 00:53:23.372627 Epoch 150, Training Loss 0.221078326177719\n",
      "2022-03-27 00:53:23.420648 Epoch 150, Training Loss 0.22237757663897542\n",
      "2022-03-27 00:53:23.478608 Epoch 150, Training Loss 0.22350710569440252\n",
      "2022-03-27 00:53:23.533201 Epoch 150, Training Loss 0.2246800020070332\n",
      "2022-03-27 00:53:23.596081 Epoch 150, Training Loss 0.22615796152283163\n",
      "2022-03-27 00:53:23.652107 Epoch 150, Training Loss 0.22761754497237827\n",
      "2022-03-27 00:53:23.716095 Epoch 150, Training Loss 0.2289000902791767\n",
      "2022-03-27 00:53:23.780103 Epoch 150, Training Loss 0.2298801281415593\n",
      "2022-03-27 00:53:23.838744 Epoch 150, Training Loss 0.23093814442834587\n",
      "2022-03-27 00:53:23.902768 Epoch 150, Training Loss 0.23206489950494694\n",
      "2022-03-27 00:53:23.966756 Epoch 150, Training Loss 0.23332724645924385\n",
      "2022-03-27 00:53:24.014969 Epoch 150, Training Loss 0.23442173187080248\n",
      "2022-03-27 00:53:24.086770 Epoch 150, Training Loss 0.23555025717486505\n",
      "2022-03-27 00:53:24.142777 Epoch 150, Training Loss 0.23658851886649265\n",
      "2022-03-27 00:53:24.206783 Epoch 150, Training Loss 0.23755665607464588\n",
      "2022-03-27 00:53:24.262788 Epoch 150, Training Loss 0.23908022289995648\n",
      "2022-03-27 00:53:24.326800 Epoch 150, Training Loss 0.2401466573138371\n",
      "2022-03-27 00:53:24.390802 Epoch 150, Training Loss 0.24147710326077687\n",
      "2022-03-27 00:53:24.446809 Epoch 150, Training Loss 0.24269095047965378\n",
      "2022-03-27 00:53:24.518818 Epoch 150, Training Loss 0.2437540793510349\n",
      "2022-03-27 00:53:24.582828 Epoch 150, Training Loss 0.24496749378836064\n",
      "2022-03-27 00:53:24.638834 Epoch 150, Training Loss 0.2462384676384499\n",
      "2022-03-27 00:53:24.694842 Epoch 150, Training Loss 0.24731331286223038\n",
      "2022-03-27 00:53:24.750865 Epoch 150, Training Loss 0.24834141981266344\n",
      "2022-03-27 00:53:24.798867 Epoch 150, Training Loss 0.24957211235600055\n",
      "2022-03-27 00:53:24.857817 Epoch 150, Training Loss 0.25061368736464656\n",
      "2022-03-27 00:53:24.905827 Epoch 150, Training Loss 0.251726964855438\n",
      "2022-03-27 00:53:24.961833 Epoch 150, Training Loss 0.2527478508784643\n",
      "2022-03-27 00:53:25.009852 Epoch 150, Training Loss 0.2538550512869949\n",
      "2022-03-27 00:53:25.065861 Epoch 150, Training Loss 0.2548633429705334\n",
      "2022-03-27 00:53:25.113863 Epoch 150, Training Loss 0.25595781290927505\n",
      "2022-03-27 00:53:25.170140 Epoch 150, Training Loss 0.25719901294354586\n",
      "2022-03-27 00:53:25.218145 Epoch 150, Training Loss 0.2582602038255433\n",
      "2022-03-27 00:53:25.266151 Epoch 150, Training Loss 0.2592710957807653\n",
      "2022-03-27 00:53:25.322138 Epoch 150, Training Loss 0.2605298017449391\n",
      "2022-03-27 00:53:25.370153 Epoch 150, Training Loss 0.26170598660283684\n",
      "2022-03-27 00:53:25.430577 Epoch 150, Training Loss 0.2629975344976196\n",
      "2022-03-27 00:53:25.482780 Epoch 150, Training Loss 0.26413474310084684\n",
      "2022-03-27 00:53:25.538782 Epoch 150, Training Loss 0.2651607716632316\n",
      "2022-03-27 00:53:25.594786 Epoch 150, Training Loss 0.26655237654895736\n",
      "2022-03-27 00:53:25.650797 Epoch 150, Training Loss 0.26750785668792626\n",
      "2022-03-27 00:53:25.706818 Epoch 150, Training Loss 0.2687018791885327\n",
      "2022-03-27 00:53:25.754805 Epoch 150, Training Loss 0.2697211945300822\n",
      "2022-03-27 00:53:25.810830 Epoch 150, Training Loss 0.2709776137948341\n",
      "2022-03-27 00:53:25.866820 Epoch 150, Training Loss 0.2718558656742506\n",
      "2022-03-27 00:53:25.922823 Epoch 150, Training Loss 0.2728930645434143\n",
      "2022-03-27 00:53:25.973067 Epoch 150, Training Loss 0.27389536817055526\n",
      "2022-03-27 00:53:26.030783 Epoch 150, Training Loss 0.27486505281284945\n",
      "2022-03-27 00:53:26.110793 Epoch 150, Training Loss 0.27605151238343906\n",
      "2022-03-27 00:53:26.166817 Epoch 150, Training Loss 0.2771403480826132\n",
      "2022-03-27 00:53:26.214806 Epoch 150, Training Loss 0.27839363696020275\n",
      "2022-03-27 00:53:26.270814 Epoch 150, Training Loss 0.27924080372161575\n",
      "2022-03-27 00:53:26.318813 Epoch 150, Training Loss 0.28028193634489307\n",
      "2022-03-27 00:53:26.374839 Epoch 150, Training Loss 0.2814848387180387\n",
      "2022-03-27 00:53:26.422843 Epoch 150, Training Loss 0.2825435655348746\n",
      "2022-03-27 00:53:26.478850 Epoch 150, Training Loss 0.2836883553611043\n",
      "2022-03-27 00:53:26.526855 Epoch 150, Training Loss 0.28478692026089525\n",
      "2022-03-27 00:53:26.582861 Epoch 150, Training Loss 0.2858467649315934\n",
      "2022-03-27 00:53:26.638850 Epoch 150, Training Loss 0.28685852267858014\n",
      "2022-03-27 00:53:26.686873 Epoch 150, Training Loss 0.2879728155825144\n",
      "2022-03-27 00:53:26.742863 Epoch 150, Training Loss 0.28945037265262946\n",
      "2022-03-27 00:53:26.798865 Epoch 150, Training Loss 0.29063674662729055\n",
      "2022-03-27 00:53:26.846875 Epoch 150, Training Loss 0.29177352793686223\n",
      "2022-03-27 00:53:26.911155 Epoch 150, Training Loss 0.2927174881443648\n",
      "2022-03-27 00:53:26.967146 Epoch 150, Training Loss 0.29390341683726784\n",
      "2022-03-27 00:53:27.015165 Epoch 150, Training Loss 0.2949936767215924\n",
      "2022-03-27 00:53:27.071458 Epoch 150, Training Loss 0.2961717036069202\n",
      "2022-03-27 00:53:27.127468 Epoch 150, Training Loss 0.29720840033362894\n",
      "2022-03-27 00:53:27.183488 Epoch 150, Training Loss 0.2985167416465252\n",
      "2022-03-27 00:53:27.239477 Epoch 150, Training Loss 0.29984896597654925\n",
      "2022-03-27 00:53:27.303503 Epoch 150, Training Loss 0.30090611060257155\n",
      "2022-03-27 00:53:27.359510 Epoch 150, Training Loss 0.3020311552087974\n",
      "2022-03-27 00:53:27.415498 Epoch 150, Training Loss 0.30339081139515733\n",
      "2022-03-27 00:53:27.468960 Epoch 150, Training Loss 0.30461920519619035\n",
      "2022-03-27 00:53:27.541225 Epoch 150, Training Loss 0.3056098340874743\n",
      "2022-03-27 00:53:27.600454 Epoch 150, Training Loss 0.3069110643070982\n",
      "2022-03-27 00:53:27.662897 Epoch 150, Training Loss 0.30804970479377397\n",
      "2022-03-27 00:53:27.719558 Epoch 150, Training Loss 0.3090427052182005\n",
      "2022-03-27 00:53:27.767566 Epoch 150, Training Loss 0.3102690840468687\n",
      "2022-03-27 00:53:27.819069 Epoch 150, Training Loss 0.311640803999913\n",
      "2022-03-27 00:53:27.875215 Epoch 150, Training Loss 0.31292563364328935\n",
      "2022-03-27 00:53:27.923222 Epoch 150, Training Loss 0.313923962052216\n",
      "2022-03-27 00:53:27.979226 Epoch 150, Training Loss 0.31496992562433035\n",
      "2022-03-27 00:53:28.027233 Epoch 150, Training Loss 0.3162207402231748\n",
      "2022-03-27 00:53:28.083221 Epoch 150, Training Loss 0.3175059694158452\n",
      "2022-03-27 00:53:28.139229 Epoch 150, Training Loss 0.318788615486506\n",
      "2022-03-27 00:53:28.187235 Epoch 150, Training Loss 0.3198229543235906\n",
      "2022-03-27 00:53:28.243236 Epoch 150, Training Loss 0.3211315372563384\n",
      "2022-03-27 00:53:28.299261 Epoch 150, Training Loss 0.32241479385539396\n",
      "2022-03-27 00:53:28.355249 Epoch 150, Training Loss 0.3235341946944556\n",
      "2022-03-27 00:53:28.411256 Epoch 150, Training Loss 0.3246458401460477\n",
      "2022-03-27 00:53:28.467262 Epoch 150, Training Loss 0.325922081461343\n",
      "2022-03-27 00:53:28.515272 Epoch 150, Training Loss 0.3272990737577229\n",
      "2022-03-27 00:53:28.571273 Epoch 150, Training Loss 0.3283586662901027\n",
      "2022-03-27 00:53:28.637920 Epoch 150, Training Loss 0.32949786577993034\n",
      "2022-03-27 00:53:28.711422 Epoch 150, Training Loss 0.3307763948617384\n",
      "2022-03-27 00:53:28.799433 Epoch 150, Training Loss 0.3317275551121558\n",
      "2022-03-27 00:53:28.863443 Epoch 150, Training Loss 0.3330293885429802\n",
      "2022-03-27 00:53:28.911447 Epoch 150, Training Loss 0.33410436044568603\n",
      "2022-03-27 00:53:28.967469 Epoch 150, Training Loss 0.33539395571669656\n",
      "2022-03-27 00:53:29.015474 Epoch 150, Training Loss 0.33633881273781857\n",
      "2022-03-27 00:53:29.063479 Epoch 150, Training Loss 0.33784067920406763\n",
      "2022-03-27 00:53:29.111676 Epoch 150, Training Loss 0.33902142755210857\n",
      "2022-03-27 00:53:29.159742 Epoch 150, Training Loss 0.3402108127808632\n",
      "2022-03-27 00:53:29.207496 Epoch 150, Training Loss 0.3414612266108813\n",
      "2022-03-27 00:53:29.263504 Epoch 150, Training Loss 0.34260407082565\n",
      "2022-03-27 00:53:29.311508 Epoch 150, Training Loss 0.3437585649283036\n",
      "2022-03-27 00:53:29.359493 Epoch 150, Training Loss 0.3448787766039524\n",
      "2022-03-27 00:53:29.423500 Epoch 150, Training Loss 0.34605582321391387\n",
      "2022-03-27 00:53:29.479508 Epoch 150, Training Loss 0.34737573468776617\n",
      "2022-03-27 00:53:29.551792 Epoch 150, Training Loss 0.3484958245626191\n",
      "2022-03-27 00:53:29.615523 Epoch 150, Training Loss 0.349949846487216\n",
      "2022-03-27 00:53:29.679899 Epoch 150, Training Loss 0.35113827735566727\n",
      "2022-03-27 00:53:29.743538 Epoch 150, Training Loss 0.35222937315321334\n",
      "2022-03-27 00:53:29.799827 Epoch 150, Training Loss 0.35340043597514065\n",
      "2022-03-27 00:53:29.847566 Epoch 150, Training Loss 0.3547875330118877\n",
      "2022-03-27 00:53:29.911828 Epoch 150, Training Loss 0.35613850856681006\n",
      "2022-03-27 00:53:29.967560 Epoch 150, Training Loss 0.3575025724480524\n",
      "2022-03-27 00:53:30.023566 Epoch 150, Training Loss 0.3585505310226889\n",
      "2022-03-27 00:53:30.079578 Epoch 150, Training Loss 0.359720498170999\n",
      "2022-03-27 00:53:30.127598 Epoch 150, Training Loss 0.3608300605850756\n",
      "2022-03-27 00:53:30.183604 Epoch 150, Training Loss 0.36182225306930443\n",
      "2022-03-27 00:53:30.231623 Epoch 150, Training Loss 0.36272733016392156\n",
      "2022-03-27 00:53:30.279613 Epoch 150, Training Loss 0.36384066649715\n",
      "2022-03-27 00:53:30.335621 Epoch 150, Training Loss 0.3649852636372647\n",
      "2022-03-27 00:53:30.383810 Epoch 150, Training Loss 0.36613875353122916\n",
      "2022-03-27 00:53:30.431630 Epoch 150, Training Loss 0.3674933289932778\n",
      "2022-03-27 00:53:30.479635 Epoch 150, Training Loss 0.3687198558426879\n",
      "2022-03-27 00:53:30.527641 Epoch 150, Training Loss 0.37004812400969095\n",
      "2022-03-27 00:53:30.583629 Epoch 150, Training Loss 0.37124571989259453\n",
      "2022-03-27 00:53:30.631653 Epoch 150, Training Loss 0.3723701018354167\n",
      "2022-03-27 00:53:30.679658 Epoch 150, Training Loss 0.3737385371304534\n",
      "2022-03-27 00:53:30.727978 Epoch 150, Training Loss 0.37481500067369405\n",
      "2022-03-27 00:53:30.775982 Epoch 150, Training Loss 0.37625408180229497\n",
      "2022-03-27 00:53:30.831996 Epoch 150, Training Loss 0.3772597338842309\n",
      "2022-03-27 00:53:30.885796 Epoch 150, Training Loss 0.3783412100104115\n",
      "2022-03-27 00:53:30.933801 Epoch 150, Training Loss 0.37960617027014415\n",
      "2022-03-27 00:53:30.990062 Epoch 150, Training Loss 0.38101110205321054\n",
      "2022-03-27 00:53:31.078004 Epoch 150, Training Loss 0.38204456358919364\n",
      "2022-03-27 00:53:31.135422 Epoch 150, Training Loss 0.383228673349561\n",
      "2022-03-27 00:53:31.193232 Epoch 150, Training Loss 0.3844587304403105\n",
      "2022-03-27 00:53:31.261343 Epoch 150, Training Loss 0.38572396783877516\n",
      "2022-03-27 00:53:31.317371 Epoch 150, Training Loss 0.38689291751598154\n",
      "2022-03-27 00:53:31.371260 Epoch 150, Training Loss 0.3882109628004186\n",
      "2022-03-27 00:53:31.419263 Epoch 150, Training Loss 0.3894699186925083\n",
      "2022-03-27 00:53:31.475272 Epoch 150, Training Loss 0.3908262287869173\n",
      "2022-03-27 00:53:31.529300 Epoch 150, Training Loss 0.39199651606247554\n",
      "2022-03-27 00:53:31.581577 Epoch 150, Training Loss 0.3934100271033509\n",
      "2022-03-27 00:53:31.632604 Epoch 150, Training Loss 0.3945460036739974\n",
      "2022-03-27 00:53:31.681016 Epoch 150, Training Loss 0.39571794234883145\n",
      "2022-03-27 00:53:31.729001 Epoch 150, Training Loss 0.39677570924124755\n",
      "2022-03-27 00:53:31.788816 Epoch 150, Training Loss 0.39792736404387236\n",
      "2022-03-27 00:53:31.845193 Epoch 150, Training Loss 0.39900686048790623\n",
      "2022-03-27 00:53:31.912171 Epoch 150, Training Loss 0.4005286704243906\n",
      "2022-03-27 00:53:31.968278 Epoch 150, Training Loss 0.4015645101247236\n",
      "2022-03-27 00:53:32.015794 Epoch 150, Training Loss 0.4025026493517639\n",
      "2022-03-27 00:53:32.071801 Epoch 150, Training Loss 0.4037362362265282\n",
      "2022-03-27 00:53:32.130173 Epoch 150, Training Loss 0.40510205333799965\n",
      "2022-03-27 00:53:32.182689 Epoch 150, Training Loss 0.40644884300049006\n",
      "2022-03-27 00:53:32.230625 Epoch 150, Training Loss 0.4075282495040113\n",
      "2022-03-27 00:53:32.282943 Epoch 150, Training Loss 0.40859744516784885\n",
      "2022-03-27 00:53:32.334522 Epoch 150, Training Loss 0.4097311038647771\n",
      "2022-03-27 00:53:32.385415 Epoch 150, Training Loss 0.4109870753324855\n",
      "2022-03-27 00:53:32.433803 Epoch 150, Training Loss 0.41216646649343586\n",
      "2022-03-27 00:53:32.489789 Epoch 150, Training Loss 0.41332083597512503\n",
      "2022-03-27 00:53:32.537794 Epoch 150, Training Loss 0.41450317375495305\n",
      "2022-03-27 00:53:32.587382 Epoch 150, Training Loss 0.4156977783536057\n",
      "2022-03-27 00:53:32.643405 Epoch 150, Training Loss 0.41660613713362027\n",
      "2022-03-27 00:53:32.701227 Epoch 150, Training Loss 0.4177514781122622\n",
      "2022-03-27 00:53:32.757419 Epoch 150, Training Loss 0.4187488112303302\n",
      "2022-03-27 00:53:32.809856 Epoch 150, Training Loss 0.4197093878713105\n",
      "2022-03-27 00:53:32.873859 Epoch 150, Training Loss 0.42080298081383377\n",
      "2022-03-27 00:53:32.936755 Epoch 150, Training Loss 0.42186158354325065\n",
      "2022-03-27 00:53:32.984780 Epoch 150, Training Loss 0.42302881688108224\n",
      "2022-03-27 00:53:33.037640 Epoch 150, Training Loss 0.42420006682500816\n",
      "2022-03-27 00:53:33.092748 Epoch 150, Training Loss 0.42542644976959815\n",
      "2022-03-27 00:53:33.140767 Epoch 150, Training Loss 0.42664740510913723\n",
      "2022-03-27 00:53:33.188756 Epoch 150, Training Loss 0.42787750031027344\n",
      "2022-03-27 00:53:33.236760 Epoch 150, Training Loss 0.42895897140588296\n",
      "2022-03-27 00:53:33.292767 Epoch 150, Training Loss 0.4302424508744798\n",
      "2022-03-27 00:53:33.340775 Epoch 150, Training Loss 0.4312529365729798\n",
      "2022-03-27 00:53:33.396775 Epoch 150, Training Loss 0.43247769037475975\n",
      "2022-03-27 00:53:33.452786 Epoch 150, Training Loss 0.433876976027818\n",
      "2022-03-27 00:53:33.508789 Epoch 150, Training Loss 0.4350694649664642\n",
      "2022-03-27 00:53:33.568862 Epoch 150, Training Loss 0.43599196399569207\n",
      "2022-03-27 00:53:33.624867 Epoch 150, Training Loss 0.43719063809765574\n",
      "2022-03-27 00:53:33.680876 Epoch 150, Training Loss 0.43841503838748885\n",
      "2022-03-27 00:53:33.736881 Epoch 150, Training Loss 0.439513222488296\n",
      "2022-03-27 00:53:33.784886 Epoch 150, Training Loss 0.4404494314242507\n",
      "2022-03-27 00:53:33.848895 Epoch 150, Training Loss 0.44151227766900414\n",
      "2022-03-27 00:53:33.904900 Epoch 150, Training Loss 0.4426506207422222\n",
      "2022-03-27 00:53:33.960904 Epoch 150, Training Loss 0.4439652400553379\n",
      "2022-03-27 00:53:34.022318 Epoch 150, Training Loss 0.4448940401796795\n",
      "2022-03-27 00:53:34.078326 Epoch 150, Training Loss 0.4462379294893016\n",
      "2022-03-27 00:53:34.135091 Epoch 150, Training Loss 0.4473053199403426\n",
      "2022-03-27 00:53:34.191097 Epoch 150, Training Loss 0.44851367621470595\n",
      "2022-03-27 00:53:34.247098 Epoch 150, Training Loss 0.4496990716670785\n",
      "2022-03-27 00:53:34.300809 Epoch 150, Training Loss 0.45102004946955027\n",
      "2022-03-27 00:53:34.351498 Epoch 150, Training Loss 0.45211973412872275\n",
      "2022-03-27 00:53:34.404707 Epoch 150, Training Loss 0.4534243640997221\n",
      "2022-03-27 00:53:34.472845 Epoch 150, Training Loss 0.4545545834866936\n",
      "2022-03-27 00:53:34.527634 Epoch 150, Training Loss 0.45565994141046956\n",
      "2022-03-27 00:53:34.583812 Epoch 150, Training Loss 0.4567586004429156\n",
      "2022-03-27 00:53:34.653673 Epoch 150, Training Loss 0.4580425061380772\n",
      "2022-03-27 00:53:34.703563 Epoch 150, Training Loss 0.4590128165529207\n",
      "2022-03-27 00:53:34.759308 Epoch 150, Training Loss 0.46025810118221566\n",
      "2022-03-27 00:53:34.809110 Epoch 150, Training Loss 0.46151667208317904\n",
      "2022-03-27 00:53:34.863921 Epoch 150, Training Loss 0.4626420876559089\n",
      "2022-03-27 00:53:34.914584 Epoch 150, Training Loss 0.4638988570026729\n",
      "2022-03-27 00:53:34.974945 Epoch 150, Training Loss 0.4646928520001414\n",
      "2022-03-27 00:53:35.025124 Epoch 150, Training Loss 0.4657414400059244\n",
      "2022-03-27 00:53:35.083229 Epoch 150, Training Loss 0.46684306074896126\n",
      "2022-03-27 00:53:35.134974 Epoch 150, Training Loss 0.4679689607809267\n",
      "2022-03-27 00:53:35.187150 Epoch 150, Training Loss 0.46896970828475854\n",
      "2022-03-27 00:53:35.240967 Epoch 150, Training Loss 0.4702801668582975\n",
      "2022-03-27 00:53:35.286738 Epoch 150, Training Loss 0.47144965777921555\n",
      "2022-03-27 00:53:35.346031 Epoch 150, Training Loss 0.47246696058746496\n",
      "2022-03-27 00:53:35.394676 Epoch 150, Training Loss 0.47371673782158386\n",
      "2022-03-27 00:53:35.445249 Epoch 150, Training Loss 0.47482069694172696\n",
      "2022-03-27 00:53:35.494680 Epoch 150, Training Loss 0.4762097440107399\n",
      "2022-03-27 00:53:35.548043 Epoch 150, Training Loss 0.4775275615475062\n",
      "2022-03-27 00:53:35.595503 Epoch 150, Training Loss 0.47857060975125987\n",
      "2022-03-27 00:53:35.645041 Epoch 150, Training Loss 0.47985184665226266\n",
      "2022-03-27 00:53:35.694933 Epoch 150, Training Loss 0.4807537815454976\n",
      "2022-03-27 00:53:35.751103 Epoch 150, Training Loss 0.4818566371412838\n",
      "2022-03-27 00:53:35.802600 Epoch 150, Training Loss 0.4829214979772982\n",
      "2022-03-27 00:53:35.849232 Epoch 150, Training Loss 0.48404829939612953\n",
      "2022-03-27 00:53:35.903379 Epoch 150, Training Loss 0.485316428229632\n",
      "2022-03-27 00:53:35.953915 Epoch 150, Training Loss 0.48669729071200046\n",
      "2022-03-27 00:53:36.004510 Epoch 150, Training Loss 0.48793165785882175\n",
      "2022-03-27 00:53:36.055009 Epoch 150, Training Loss 0.48911932430913685\n",
      "2022-03-27 00:53:36.112103 Epoch 150, Training Loss 0.49027109001298697\n",
      "2022-03-27 00:53:36.155004 Epoch 150, Training Loss 0.4913507269319061\n",
      "2022-03-27 00:53:36.214891 Epoch 150, Training Loss 0.4924151905814705\n",
      "2022-03-27 00:53:36.263965 Epoch 150, Training Loss 0.4935932332444984\n",
      "2022-03-27 00:53:36.310969 Epoch 150, Training Loss 0.49480168189843904\n",
      "2022-03-27 00:53:36.362341 Epoch 150, Training Loss 0.4958592363635597\n",
      "2022-03-27 00:53:36.412270 Epoch 150, Training Loss 0.49741614459420713\n",
      "2022-03-27 00:53:36.464003 Epoch 150, Training Loss 0.4985338390788154\n",
      "2022-03-27 00:53:36.511013 Epoch 150, Training Loss 0.49990600446605926\n",
      "2022-03-27 00:53:36.560377 Epoch 150, Training Loss 0.5011039785564403\n",
      "2022-03-27 00:53:36.612029 Epoch 150, Training Loss 0.5026009362524427\n",
      "2022-03-27 00:53:36.661017 Epoch 150, Training Loss 0.5038121094941483\n",
      "2022-03-27 00:53:36.704261 Epoch 150, Training Loss 0.5049757212019332\n",
      "2022-03-27 00:53:36.753970 Epoch 150, Training Loss 0.50621948233041\n",
      "2022-03-27 00:53:36.804145 Epoch 150, Training Loss 0.5076614314942713\n",
      "2022-03-27 00:53:36.853011 Epoch 150, Training Loss 0.5086893518562512\n",
      "2022-03-27 00:53:36.904366 Epoch 150, Training Loss 0.5097068481128234\n",
      "2022-03-27 00:53:36.960297 Epoch 150, Training Loss 0.5110358367185763\n",
      "2022-03-27 00:53:37.012240 Epoch 150, Training Loss 0.5122780902001559\n",
      "2022-03-27 00:53:37.063985 Epoch 150, Training Loss 0.5134289725052427\n",
      "2022-03-27 00:53:37.111928 Epoch 150, Training Loss 0.5143543272981863\n",
      "2022-03-27 00:53:37.162417 Epoch 150, Training Loss 0.5156574487076391\n",
      "2022-03-27 00:53:37.220411 Epoch 150, Training Loss 0.5169146256830991\n",
      "2022-03-27 00:53:37.272241 Epoch 150, Training Loss 0.518089581557247\n",
      "2022-03-27 00:53:37.322473 Epoch 150, Training Loss 0.5193666715146331\n",
      "2022-03-27 00:53:37.372153 Epoch 150, Training Loss 0.520478499972302\n",
      "2022-03-27 00:53:37.422280 Epoch 150, Training Loss 0.5219977118475053\n",
      "2022-03-27 00:53:37.474281 Epoch 150, Training Loss 0.5233514747961098\n",
      "2022-03-27 00:53:37.524251 Epoch 150, Training Loss 0.5246085972737169\n",
      "2022-03-27 00:53:37.575558 Epoch 150, Training Loss 0.5257330171744842\n",
      "2022-03-27 00:53:37.622240 Epoch 150, Training Loss 0.5267991800137493\n",
      "2022-03-27 00:53:37.674521 Epoch 150, Training Loss 0.5277748023305098\n",
      "2022-03-27 00:53:37.722405 Epoch 150, Training Loss 0.5286945596222987\n",
      "2022-03-27 00:53:37.770652 Epoch 150, Training Loss 0.5298854729251179\n",
      "2022-03-27 00:53:37.822124 Epoch 150, Training Loss 0.5309418859841574\n",
      "2022-03-27 00:53:37.874289 Epoch 150, Training Loss 0.5320233717141554\n",
      "2022-03-27 00:53:37.929788 Epoch 150, Training Loss 0.5332775089289526\n",
      "2022-03-27 00:53:37.982273 Epoch 150, Training Loss 0.5342755473940574\n",
      "2022-03-27 00:53:38.032254 Epoch 150, Training Loss 0.5353755892999946\n",
      "2022-03-27 00:53:38.084350 Epoch 150, Training Loss 0.5364976483385276\n",
      "2022-03-27 00:53:38.142241 Epoch 150, Training Loss 0.537916473644164\n",
      "2022-03-27 00:53:38.200793 Epoch 150, Training Loss 0.5389708330106857\n",
      "2022-03-27 00:53:38.252472 Epoch 150, Training Loss 0.5402526334880868\n",
      "2022-03-27 00:53:38.304906 Epoch 150, Training Loss 0.5415430325833733\n",
      "2022-03-27 00:53:38.360465 Epoch 150, Training Loss 0.5426208624601974\n",
      "2022-03-27 00:53:38.413003 Epoch 150, Training Loss 0.5435782360756184\n",
      "2022-03-27 00:53:38.464617 Epoch 150, Training Loss 0.5445107454076752\n",
      "2022-03-27 00:53:38.509395 Epoch 150, Training Loss 0.5456707038537926\n",
      "2022-03-27 00:53:38.562675 Epoch 150, Training Loss 0.54688458956416\n",
      "2022-03-27 00:53:38.614993 Epoch 150, Training Loss 0.5482635532346223\n",
      "2022-03-27 00:53:38.672221 Epoch 150, Training Loss 0.5493546472790906\n",
      "2022-03-27 00:53:38.741413 Epoch 150, Training Loss 0.550516378086851\n",
      "2022-03-27 00:53:38.806127 Epoch 150, Training Loss 0.5517262895698742\n",
      "2022-03-27 00:53:38.858589 Epoch 150, Training Loss 0.5528265200460049\n",
      "2022-03-27 00:53:38.906593 Epoch 150, Training Loss 0.554454452317694\n",
      "2022-03-27 00:53:38.954599 Epoch 150, Training Loss 0.5554604184292161\n",
      "2022-03-27 00:53:39.002588 Epoch 150, Training Loss 0.556601410784075\n",
      "2022-03-27 00:53:39.050608 Epoch 150, Training Loss 0.5576087997087738\n",
      "2022-03-27 00:53:39.098614 Epoch 150, Training Loss 0.5585260447638724\n",
      "2022-03-27 00:53:39.146619 Epoch 150, Training Loss 0.5598649088379062\n",
      "2022-03-27 00:53:39.210901 Epoch 150, Training Loss 0.5609991827126964\n",
      "2022-03-27 00:53:39.258616 Epoch 150, Training Loss 0.5621899903735237\n",
      "2022-03-27 00:53:39.306822 Epoch 150, Training Loss 0.5632633172794986\n",
      "2022-03-27 00:53:39.358089 Epoch 150, Training Loss 0.5644800564669588\n",
      "2022-03-27 00:53:39.414096 Epoch 150, Training Loss 0.5657031698452543\n",
      "2022-03-27 00:53:39.470100 Epoch 150, Training Loss 0.5667468624956468\n",
      "2022-03-27 00:53:39.518105 Epoch 150, Training Loss 0.5680391072007396\n",
      "2022-03-27 00:53:39.574313 Epoch 150, Training Loss 0.5691205024566797\n",
      "2022-03-27 00:53:39.630119 Epoch 150, Training Loss 0.570456408402499\n",
      "2022-03-27 00:53:39.678124 Epoch 150, Training Loss 0.5717013327361983\n",
      "2022-03-27 00:53:39.730724 Epoch 150, Training Loss 0.5728195084025488\n",
      "2022-03-27 00:53:39.778728 Epoch 150, Training Loss 0.5740988653943971\n",
      "2022-03-27 00:53:39.842660 Epoch 150, Training Loss 0.5753490817363914\n",
      "2022-03-27 00:53:39.898861 Epoch 150, Training Loss 0.5763757876727892\n",
      "2022-03-27 00:53:39.957204 Epoch 150, Training Loss 0.5775543596890881\n",
      "2022-03-27 00:53:40.005192 Epoch 150, Training Loss 0.5786205309126383\n",
      "2022-03-27 00:53:40.058052 Epoch 150, Training Loss 0.5798029818803149\n",
      "2022-03-27 00:53:40.110283 Epoch 150, Training Loss 0.5810747542954466\n",
      "2022-03-27 00:53:40.158303 Epoch 150, Training Loss 0.5821100913960001\n",
      "2022-03-27 00:53:40.214497 Epoch 150, Training Loss 0.5831993054550932\n",
      "2022-03-27 00:53:40.275323 Epoch 150, Training Loss 0.5845211527841475\n",
      "2022-03-27 00:53:40.323328 Epoch 150, Training Loss 0.5856061921552624\n",
      "2022-03-27 00:53:40.381609 Epoch 150, Training Loss 0.5870100408411392\n",
      "2022-03-27 00:53:40.434509 Epoch 150, Training Loss 0.588170539921202\n",
      "2022-03-27 00:53:40.486740 Epoch 150, Training Loss 0.5892327805156903\n",
      "2022-03-27 00:53:40.542703 Epoch 150, Training Loss 0.5903712781646367\n",
      "2022-03-27 00:53:40.598709 Epoch 150, Training Loss 0.5915319023992095\n",
      "2022-03-27 00:53:40.654709 Epoch 150, Training Loss 0.5925191545577915\n",
      "2022-03-27 00:53:40.709734 Epoch 150, Training Loss 0.5937399606570564\n",
      "2022-03-27 00:53:40.765736 Epoch 150, Training Loss 0.5949681115424846\n",
      "2022-03-27 00:53:40.817306 Epoch 150, Training Loss 0.5962151255449066\n",
      "2022-03-27 00:53:40.878887 Epoch 150, Training Loss 0.5973891766022539\n",
      "2022-03-27 00:53:40.930714 Epoch 150, Training Loss 0.5987090868565738\n",
      "2022-03-27 00:53:40.986739 Epoch 150, Training Loss 0.5999170851219645\n",
      "2022-03-27 00:53:41.034731 Epoch 150, Training Loss 0.6009686264540534\n",
      "2022-03-27 00:53:41.089196 Epoch 150, Training Loss 0.6023073004334784\n",
      "2022-03-27 00:53:41.133270 Epoch 150, Training Loss 0.6035415403678289\n",
      "2022-03-27 00:53:41.194150 Epoch 150, Training Loss 0.6049613015121206\n",
      "2022-03-27 00:53:41.245654 Epoch 150, Training Loss 0.6061520142780851\n",
      "2022-03-27 00:53:41.301658 Epoch 150, Training Loss 0.6072981028300722\n",
      "2022-03-27 00:53:41.351690 Epoch 150, Training Loss 0.6082480781523468\n",
      "2022-03-27 00:53:41.403385 Epoch 150, Training Loss 0.6093351876796664\n",
      "2022-03-27 00:53:41.468010 Epoch 150, Training Loss 0.6106202197654168\n",
      "2022-03-27 00:53:41.532016 Epoch 150, Training Loss 0.6113976516839489\n",
      "2022-03-27 00:53:41.604024 Epoch 150, Training Loss 0.612408569478013\n",
      "2022-03-27 00:53:41.676033 Epoch 150, Training Loss 0.6136650067308674\n",
      "2022-03-27 00:53:41.740039 Epoch 150, Training Loss 0.614790172528123\n",
      "2022-03-27 00:53:41.796047 Epoch 150, Training Loss 0.6162751406964744\n",
      "2022-03-27 00:53:41.866944 Epoch 150, Training Loss 0.6173830543027814\n",
      "2022-03-27 00:53:41.935066 Epoch 150, Training Loss 0.6185184698123152\n",
      "2022-03-27 00:53:42.001752 Epoch 150, Training Loss 0.6197097665056244\n",
      "2022-03-27 00:53:42.066923 Epoch 150, Training Loss 0.6207272319690041\n",
      "2022-03-27 00:53:42.125088 Epoch 150, Training Loss 0.6220044933469094\n",
      "2022-03-27 00:53:42.181922 Epoch 150, Training Loss 0.6229885818098512\n",
      "2022-03-27 00:53:42.236709 Epoch 150, Training Loss 0.6242585203531757\n",
      "2022-03-27 00:53:42.284731 Epoch 150, Training Loss 0.6252186818196036\n",
      "2022-03-27 00:53:42.348628 Epoch 150, Training Loss 0.6264739319339128\n",
      "2022-03-27 00:53:42.396370 Epoch 150, Training Loss 0.6273477104161401\n",
      "2022-03-27 00:53:42.444376 Epoch 150, Training Loss 0.6284628764290334\n",
      "2022-03-27 00:53:42.502189 Epoch 150, Training Loss 0.6295995990486096\n",
      "2022-03-27 00:53:42.550575 Epoch 150, Training Loss 0.6308137998556542\n",
      "2022-03-27 00:53:42.598319 Epoch 150, Training Loss 0.6319265609507061\n",
      "2022-03-27 00:53:42.654600 Epoch 150, Training Loss 0.6331483368068704\n",
      "2022-03-27 00:53:42.702313 Epoch 150, Training Loss 0.6344455183314546\n",
      "2022-03-27 00:53:42.758335 Epoch 150, Training Loss 0.63549900878116\n",
      "2022-03-27 00:53:42.806340 Epoch 150, Training Loss 0.6368440348473962\n",
      "2022-03-27 00:53:42.860253 Epoch 150, Training Loss 0.6379089977430261\n",
      "2022-03-27 00:53:42.908261 Epoch 150, Training Loss 0.6389016948087746\n",
      "2022-03-27 00:53:42.964447 Epoch 150, Training Loss 0.6401814701002272\n",
      "2022-03-27 00:53:43.010044 Epoch 150, Training Loss 0.6415147375877556\n",
      "2022-03-27 00:53:43.064092 Epoch 150, Training Loss 0.6429075074317815\n",
      "2022-03-27 00:53:43.120225 Epoch 150, Training Loss 0.6441077126566407\n",
      "2022-03-27 00:53:43.168223 Epoch 150, Training Loss 0.6453980536716978\n",
      "2022-03-27 00:53:43.224232 Epoch 150, Training Loss 0.6465692300625774\n",
      "2022-03-27 00:53:43.272253 Epoch 150, Training Loss 0.6478662201205788\n",
      "2022-03-27 00:53:43.320246 Epoch 150, Training Loss 0.6493559068121264\n",
      "2022-03-27 00:53:43.376893 Epoch 150, Training Loss 0.6503740818909062\n",
      "2022-03-27 00:53:43.421127 Epoch 150, Training Loss 0.6516785440237626\n",
      "2022-03-27 00:53:43.469114 Epoch 150, Training Loss 0.6527051136011968\n",
      "2022-03-27 00:53:43.525140 Epoch 150, Training Loss 0.6538404391702178\n",
      "2022-03-27 00:53:43.573147 Epoch 150, Training Loss 0.6551291601889578\n",
      "2022-03-27 00:53:43.621133 Epoch 150, Training Loss 0.6563338002432948\n",
      "2022-03-27 00:53:43.669146 Epoch 150, Training Loss 0.6576490250542341\n",
      "2022-03-27 00:53:43.717530 Epoch 150, Training Loss 0.6587348355509132\n",
      "2022-03-27 00:53:43.765553 Epoch 150, Training Loss 0.6598759548895804\n",
      "2022-03-27 00:53:43.813558 Epoch 150, Training Loss 0.6611485988892558\n",
      "2022-03-27 00:53:43.861544 Epoch 150, Training Loss 0.6622596617854769\n",
      "2022-03-27 00:53:43.918004 Epoch 150, Training Loss 0.6633612926658767\n",
      "2022-03-27 00:53:43.970306 Epoch 150, Training Loss 0.6646908684764676\n",
      "2022-03-27 00:53:44.026313 Epoch 150, Training Loss 0.6659023774706799\n",
      "2022-03-27 00:53:44.082321 Epoch 150, Training Loss 0.6669547226270447\n",
      "2022-03-27 00:53:44.132758 Epoch 150, Training Loss 0.6681808488600699\n",
      "2022-03-27 00:53:44.188763 Epoch 150, Training Loss 0.669358268570717\n",
      "2022-03-27 00:53:44.236770 Epoch 150, Training Loss 0.6707581777097015\n",
      "2022-03-27 00:53:44.284772 Epoch 150, Training Loss 0.6721536995809706\n",
      "2022-03-27 00:53:44.332796 Epoch 150, Training Loss 0.6735328845965588\n",
      "2022-03-27 00:53:44.390331 Epoch 150, Training Loss 0.6749285558605438\n",
      "2022-03-27 00:53:44.443572 Epoch 150, Training Loss 0.6761741350831278\n",
      "2022-03-27 00:53:44.511084 Epoch 150, Training Loss 0.677359611710624\n",
      "2022-03-27 00:53:44.567108 Epoch 150, Training Loss 0.6784933907601535\n",
      "2022-03-27 00:53:44.623096 Epoch 150, Training Loss 0.6796687242320126\n",
      "2022-03-27 00:53:44.671120 Epoch 150, Training Loss 0.6807189051757383\n",
      "2022-03-27 00:53:44.730522 Epoch 150, Training Loss 0.6818021114372537\n",
      "2022-03-27 00:53:44.779763 Epoch 150, Training Loss 0.6828506838939988\n",
      "2022-03-27 00:53:44.827770 Epoch 150, Training Loss 0.6841213318240612\n",
      "2022-03-27 00:53:44.888652 Epoch 150, Training Loss 0.6856293758315504\n",
      "2022-03-27 00:53:44.936641 Epoch 150, Training Loss 0.6867593717392143\n",
      "2022-03-27 00:53:45.028829 Epoch 150, Training Loss 0.687996238317636\n",
      "2022-03-27 00:53:45.084852 Epoch 150, Training Loss 0.6889053336952043\n",
      "2022-03-27 00:53:45.140841 Epoch 150, Training Loss 0.6899861941099776\n",
      "2022-03-27 00:53:45.188866 Epoch 150, Training Loss 0.6910077254942921\n",
      "2022-03-27 00:53:45.244854 Epoch 150, Training Loss 0.6920688182039334\n",
      "2022-03-27 00:53:45.300867 Epoch 150, Training Loss 0.6933307894660384\n",
      "2022-03-27 00:53:45.372406 Epoch 150, Training Loss 0.6946058305328154\n",
      "2022-03-27 00:53:45.436406 Epoch 150, Training Loss 0.6957082618837771\n",
      "2022-03-27 00:53:45.484429 Epoch 150, Training Loss 0.6968080063000359\n",
      "2022-03-27 00:53:45.537414 Epoch 150, Training Loss 0.6979705478681628\n",
      "2022-03-27 00:53:45.585420 Epoch 150, Training Loss 0.6989924824603683\n",
      "2022-03-27 00:53:45.641445 Epoch 150, Training Loss 0.7000933887098756\n",
      "2022-03-27 00:53:45.689450 Epoch 150, Training Loss 0.7014472673616141\n",
      "2022-03-27 00:53:45.745456 Epoch 150, Training Loss 0.7023571034526581\n",
      "2022-03-27 00:53:45.801441 Epoch 150, Training Loss 0.7036328045913326\n",
      "2022-03-27 00:53:45.849451 Epoch 150, Training Loss 0.7045960224345517\n",
      "2022-03-27 00:53:45.897473 Epoch 150, Training Loss 0.7057084530363302\n",
      "2022-03-27 00:53:45.953460 Epoch 150, Training Loss 0.7073546761594465\n",
      "2022-03-27 00:53:46.001654 Epoch 150, Training Loss 0.7083077524476649\n",
      "2022-03-27 00:53:46.060991 Epoch 150, Training Loss 0.7094673666045489\n",
      "2022-03-27 00:53:46.121124 Epoch 150, Training Loss 0.7107828150472373\n",
      "2022-03-27 00:53:46.169146 Epoch 150, Training Loss 0.7121513445511498\n",
      "2022-03-27 00:53:46.236792 Epoch 150, Training Loss 0.7135358045015798\n",
      "2022-03-27 00:53:46.297656 Epoch 150, Training Loss 0.7146071577468491\n",
      "2022-03-27 00:53:46.354970 Epoch 150, Training Loss 0.7158585574163501\n",
      "2022-03-27 00:53:46.411274 Epoch 150, Training Loss 0.7169095607822203\n",
      "2022-03-27 00:53:46.463804 Epoch 150, Training Loss 0.7182803301097792\n",
      "2022-03-27 00:53:46.516110 Epoch 150, Training Loss 0.7192223455442492\n",
      "2022-03-27 00:53:46.564118 Epoch 150, Training Loss 0.7203307705157248\n",
      "2022-03-27 00:53:46.632356 Epoch 150, Training Loss 0.7215847555938584\n",
      "2022-03-27 00:53:46.688346 Epoch 150, Training Loss 0.7228267511443409\n",
      "2022-03-27 00:53:46.752352 Epoch 150, Training Loss 0.7240807290577218\n",
      "2022-03-27 00:53:46.800360 Epoch 150, Training Loss 0.725093032164342\n",
      "2022-03-27 00:53:46.856372 Epoch 150, Training Loss 0.7263737112817252\n",
      "2022-03-27 00:53:46.907317 Epoch 150, Training Loss 0.7274349877596511\n",
      "2022-03-27 00:53:46.955341 Epoch 150, Training Loss 0.7282618413800779\n",
      "2022-03-27 00:53:47.011329 Epoch 150, Training Loss 0.7293615652929486\n",
      "2022-03-27 00:53:47.075358 Epoch 150, Training Loss 0.7305965221598935\n",
      "2022-03-27 00:53:47.123366 Epoch 150, Training Loss 0.7316194279571934\n",
      "2022-03-27 00:53:47.175485 Epoch 150, Training Loss 0.732735951233398\n",
      "2022-03-27 00:53:47.223473 Epoch 150, Training Loss 0.7339592027237348\n",
      "2022-03-27 00:53:47.271480 Epoch 150, Training Loss 0.7351937663677098\n",
      "2022-03-27 00:53:47.327490 Epoch 150, Training Loss 0.736336543508198\n",
      "2022-03-27 00:53:47.375494 Epoch 150, Training Loss 0.737563366322871\n",
      "2022-03-27 00:53:47.431499 Epoch 150, Training Loss 0.7386221060210176\n",
      "2022-03-27 00:53:47.503510 Epoch 150, Training Loss 0.7396559361606607\n",
      "2022-03-27 00:53:47.559508 Epoch 150, Training Loss 0.7407703474354561\n",
      "2022-03-27 00:53:47.615516 Epoch 150, Training Loss 0.7419335065442888\n",
      "2022-03-27 00:53:47.667494 Epoch 150, Training Loss 0.7429161394953423\n",
      "2022-03-27 00:53:47.723499 Epoch 150, Training Loss 0.7440500031499302\n",
      "2022-03-27 00:53:47.771524 Epoch 150, Training Loss 0.7451440117243305\n",
      "2022-03-27 00:53:47.819511 Epoch 150, Training Loss 0.7465232299721759\n",
      "2022-03-27 00:53:47.867534 Epoch 150, Training Loss 0.7475903159212274\n",
      "2022-03-27 00:53:47.915552 Epoch 150, Training Loss 0.7488322544585714\n",
      "2022-03-27 00:53:47.971529 Epoch 150, Training Loss 0.7499196509571027\n",
      "2022-03-27 00:53:48.019532 Epoch 150, Training Loss 0.750938691903868\n",
      "2022-03-27 00:53:48.067556 Epoch 150, Training Loss 0.7522022493202668\n",
      "2022-03-27 00:53:48.123562 Epoch 150, Training Loss 0.7532627205257221\n",
      "2022-03-27 00:53:48.171567 Epoch 150, Training Loss 0.7546397361456586\n",
      "2022-03-27 00:53:48.223754 Epoch 150, Training Loss 0.7560746088204786\n",
      "2022-03-27 00:53:48.279761 Epoch 150, Training Loss 0.7574307894920145\n",
      "2022-03-27 00:53:48.327749 Epoch 150, Training Loss 0.7584427727762696\n",
      "2022-03-27 00:53:48.375771 Epoch 150, Training Loss 0.7595560093365057\n",
      "2022-03-27 00:53:48.431777 Epoch 150, Training Loss 0.7606111313680859\n",
      "2022-03-27 00:53:48.479783 Epoch 150, Training Loss 0.7621152815611466\n",
      "2022-03-27 00:53:48.527787 Epoch 150, Training Loss 0.763200641593055\n",
      "2022-03-27 00:53:48.576205 Epoch 150, Training Loss 0.7643458631337451\n",
      "2022-03-27 00:53:48.632199 Epoch 150, Training Loss 0.7654602039805458\n",
      "2022-03-27 00:53:48.688200 Epoch 150, Training Loss 0.7666460941057376\n",
      "2022-03-27 00:53:48.742904 Epoch 150, Training Loss 0.7678813089800003\n",
      "2022-03-27 00:53:48.798929 Epoch 150, Training Loss 0.7687938820065745\n",
      "2022-03-27 00:53:48.854915 Epoch 150, Training Loss 0.7700062375849165\n",
      "2022-03-27 00:53:48.910928 Epoch 150, Training Loss 0.771287863364305\n",
      "2022-03-27 00:53:48.959095 Epoch 150, Training Loss 0.7722251088265568\n",
      "2022-03-27 00:53:49.015102 Epoch 150, Training Loss 0.7732477948031462\n",
      "2022-03-27 00:53:49.063094 Epoch 150, Training Loss 0.7742803423758358\n",
      "2022-03-27 00:53:49.111110 Epoch 150, Training Loss 0.7751742329286493\n",
      "2022-03-27 00:53:49.159099 Epoch 150, Training Loss 0.7763544699877424\n",
      "2022-03-27 00:53:49.215103 Epoch 150, Training Loss 0.7773046680271168\n",
      "2022-03-27 00:53:49.271366 Epoch 150, Training Loss 0.7783252714235155\n",
      "2022-03-27 00:53:49.327371 Epoch 150, Training Loss 0.7795433831946624\n",
      "2022-03-27 00:53:49.391672 Epoch 150, Training Loss 0.7808155702508014\n",
      "2022-03-27 00:53:49.439682 Epoch 150, Training Loss 0.7820356427251226\n",
      "2022-03-27 00:53:49.503684 Epoch 150, Training Loss 0.783256480913333\n",
      "2022-03-27 00:53:49.559696 Epoch 150, Training Loss 0.7843860838266895\n",
      "2022-03-27 00:53:49.608101 Epoch 150, Training Loss 0.7855787022644297\n",
      "2022-03-27 00:53:49.664119 Epoch 150, Training Loss 0.7866250466355278\n",
      "2022-03-27 00:53:49.720108 Epoch 150, Training Loss 0.7878246643506658\n",
      "2022-03-27 00:53:49.784113 Epoch 150, Training Loss 0.7890379808442977\n",
      "2022-03-27 00:53:49.849117 Epoch 150, Training Loss 0.7902329809525434\n",
      "2022-03-27 00:53:49.913124 Epoch 150, Training Loss 0.7911707360268859\n",
      "2022-03-27 00:53:49.969130 Epoch 150, Training Loss 0.792563806485642\n",
      "2022-03-27 00:53:50.025357 Epoch 150, Training Loss 0.793669763733359\n",
      "2022-03-27 00:53:50.081346 Epoch 150, Training Loss 0.7947690680508723\n",
      "2022-03-27 00:53:50.129369 Epoch 150, Training Loss 0.7956761022662873\n",
      "2022-03-27 00:53:50.193354 Epoch 150, Training Loss 0.7966607033901507\n",
      "2022-03-27 00:53:50.241379 Epoch 150, Training Loss 0.7979043373061568\n",
      "2022-03-27 00:53:50.297369 Epoch 150, Training Loss 0.7989087720661212\n",
      "2022-03-27 00:53:50.350975 Epoch 150, Training Loss 0.8000344275818456\n",
      "2022-03-27 00:53:50.406995 Epoch 150, Training Loss 0.8012803797526737\n",
      "2022-03-27 00:53:50.454987 Epoch 150, Training Loss 0.8023367173531476\n",
      "2022-03-27 00:53:50.510987 Epoch 150, Training Loss 0.8035980901297402\n",
      "2022-03-27 00:53:50.567211 Epoch 150, Training Loss 0.804883856221538\n",
      "2022-03-27 00:53:50.615209 Epoch 150, Training Loss 0.8060387696908868\n",
      "2022-03-27 00:53:50.671218 Epoch 150, Training Loss 0.8075482312523191\n",
      "2022-03-27 00:53:50.727227 Epoch 150, Training Loss 0.8088060263020304\n",
      "2022-03-27 00:53:50.775247 Epoch 150, Training Loss 0.8100223948278695\n",
      "2022-03-27 00:53:50.823237 Epoch 150, Training Loss 0.8110493490915469\n",
      "2022-03-27 00:53:50.879862 Epoch 150, Training Loss 0.8124888835813079\n",
      "2022-03-27 00:53:50.924022 Epoch 150, Training Loss 0.8134952066347118\n",
      "2022-03-27 00:53:50.972046 Epoch 150, Training Loss 0.8145399784188136\n",
      "2022-03-27 00:53:51.028052 Epoch 150, Training Loss 0.8158235999629321\n",
      "2022-03-27 00:53:51.076041 Epoch 150, Training Loss 0.8173173194955987\n",
      "2022-03-27 00:53:51.124062 Epoch 150, Training Loss 0.8184488277758479\n",
      "2022-03-27 00:53:51.180084 Epoch 150, Training Loss 0.8195195832978124\n",
      "2022-03-27 00:53:51.228056 Epoch 150, Training Loss 0.8207548406270458\n",
      "2022-03-27 00:53:51.284066 Epoch 150, Training Loss 0.8217802368618948\n",
      "2022-03-27 00:53:51.332087 Epoch 150, Training Loss 0.8227661853403692\n",
      "2022-03-27 00:53:51.388072 Epoch 150, Training Loss 0.824015972025864\n",
      "2022-03-27 00:53:51.441174 Epoch 150, Training Loss 0.825214956422596\n",
      "2022-03-27 00:53:51.529182 Epoch 150, Training Loss 0.8264973201715123\n",
      "2022-03-27 00:53:51.577189 Epoch 150, Training Loss 0.827783651364124\n",
      "2022-03-27 00:53:51.633212 Epoch 150, Training Loss 0.8288570799486107\n",
      "2022-03-27 00:53:51.681202 Epoch 150, Training Loss 0.8300666586517373\n",
      "2022-03-27 00:53:51.737225 Epoch 150, Training Loss 0.8316249534906939\n",
      "2022-03-27 00:53:51.785213 Epoch 150, Training Loss 0.8328628396744009\n",
      "2022-03-27 00:53:51.833236 Epoch 150, Training Loss 0.8338124138467452\n",
      "2022-03-27 00:53:51.889242 Epoch 150, Training Loss 0.8348508491692945\n",
      "2022-03-27 00:53:51.937247 Epoch 150, Training Loss 0.8362507000756081\n",
      "2022-03-27 00:53:51.998189 Epoch 150, Training Loss 0.8374737639104008\n",
      "2022-03-27 00:53:52.054176 Epoch 150, Training Loss 0.8386842062711106\n",
      "2022-03-27 00:53:52.102184 Epoch 150, Training Loss 0.8398970449367142\n",
      "2022-03-27 00:53:52.158206 Epoch 150, Training Loss 0.8408101595118832\n",
      "2022-03-27 00:53:52.206212 Epoch 150, Training Loss 0.8421052978624164\n",
      "2022-03-27 00:53:52.262218 Epoch 150, Training Loss 0.8436312776849703\n",
      "2022-03-27 00:53:52.318223 Epoch 150, Training Loss 0.8449615315555612\n",
      "2022-03-27 00:53:52.366229 Epoch 150, Training Loss 0.8461528700940749\n",
      "2022-03-27 00:53:52.422236 Epoch 150, Training Loss 0.847246437883743\n",
      "2022-03-27 00:53:52.475219 Epoch 150, Training Loss 0.8483433072524302\n",
      "2022-03-27 00:53:52.529477 Epoch 150, Training Loss 0.8496380053517764\n",
      "2022-03-27 00:53:52.585487 Epoch 150, Training Loss 0.8508428252871384\n",
      "2022-03-27 00:53:52.633835 Epoch 150, Training Loss 0.8523244455342402\n",
      "2022-03-27 00:53:52.681860 Epoch 150, Training Loss 0.853442305341706\n",
      "2022-03-27 00:53:52.737865 Epoch 150, Training Loss 0.8548766286171916\n",
      "2022-03-27 00:53:52.793852 Epoch 150, Training Loss 0.8561038766675593\n",
      "2022-03-27 00:53:52.841876 Epoch 150, Training Loss 0.8573301383448989\n",
      "2022-03-27 00:53:52.897863 Epoch 150, Training Loss 0.8583983257603462\n",
      "2022-03-27 00:53:52.953871 Epoch 150, Training Loss 0.8594759967930786\n",
      "2022-03-27 00:53:53.001876 Epoch 150, Training Loss 0.8605140408743983\n",
      "2022-03-27 00:53:53.062654 Epoch 150, Training Loss 0.8615022482317122\n",
      "2022-03-27 00:53:53.110640 Epoch 150, Training Loss 0.86258574054979\n",
      "2022-03-27 00:53:53.166652 Epoch 150, Training Loss 0.8639967334087547\n",
      "2022-03-27 00:53:53.230653 Epoch 150, Training Loss 0.8652978532606989\n",
      "2022-03-27 00:53:53.278678 Epoch 150, Training Loss 0.8662922620925757\n",
      "2022-03-27 00:53:53.326684 Epoch 150, Training Loss 0.8675150958168537\n",
      "2022-03-27 00:53:53.383056 Epoch 150, Training Loss 0.8688060578787723\n",
      "2022-03-27 00:53:53.431042 Epoch 150, Training Loss 0.8703858234998211\n",
      "2022-03-27 00:53:53.487052 Epoch 150, Training Loss 0.8714679560393018\n",
      "2022-03-27 00:53:53.535059 Epoch 150, Training Loss 0.8725721594470236\n",
      "2022-03-27 00:53:53.596204 Epoch 150, Training Loss 0.8738669354440002\n",
      "2022-03-27 00:53:53.644191 Epoch 150, Training Loss 0.8751886405450914\n",
      "2022-03-27 00:53:53.700217 Epoch 150, Training Loss 0.8764200379019198\n",
      "2022-03-27 00:53:53.756223 Epoch 150, Training Loss 0.877535285318599\n",
      "2022-03-27 00:53:53.812228 Epoch 150, Training Loss 0.8786352969648893\n",
      "2022-03-27 00:53:53.860235 Epoch 150, Training Loss 0.8797701325105585\n",
      "2022-03-27 00:53:53.908221 Epoch 150, Training Loss 0.8806560243791937\n",
      "2022-03-27 00:53:53.964245 Epoch 150, Training Loss 0.8816037483684852\n",
      "2022-03-27 00:53:54.012232 Epoch 150, Training Loss 0.8827699749823421\n",
      "2022-03-27 00:53:54.076240 Epoch 150, Training Loss 0.8839667170401424\n",
      "2022-03-27 00:53:54.128724 Epoch 150, Training Loss 0.8852573736092014\n",
      "2022-03-27 00:53:54.184731 Epoch 150, Training Loss 0.8864706760019903\n",
      "2022-03-27 00:53:54.232984 Epoch 150, Training Loss 0.8877842590174712\n",
      "2022-03-27 00:53:54.288990 Epoch 150, Training Loss 0.8890107708513889\n",
      "2022-03-27 00:53:54.345014 Epoch 150, Training Loss 0.8899825349488222\n",
      "2022-03-27 00:53:54.393002 Epoch 150, Training Loss 0.8910912501690028\n",
      "2022-03-27 00:53:54.449009 Epoch 150, Training Loss 0.8923534126690281\n",
      "2022-03-27 00:53:54.497014 Epoch 150, Training Loss 0.8935272292712765\n",
      "2022-03-27 00:53:54.553020 Epoch 150, Training Loss 0.8947052620256039\n",
      "2022-03-27 00:53:54.604054 Epoch 150, Training Loss 0.8958572036469988\n",
      "2022-03-27 00:53:54.665093 Epoch 150, Training Loss 0.8968876936399114\n",
      "2022-03-27 00:53:54.713103 Epoch 150, Training Loss 0.8980657580258596\n",
      "2022-03-27 00:53:54.770145 Epoch 150, Training Loss 0.8996398258392159\n",
      "2022-03-27 00:53:54.826148 Epoch 150, Training Loss 0.9007629374104082\n",
      "2022-03-27 00:53:54.874172 Epoch 150, Training Loss 0.9018479068108531\n",
      "2022-03-27 00:53:54.930179 Epoch 150, Training Loss 0.9027759348187605\n",
      "2022-03-27 00:53:54.978184 Epoch 150, Training Loss 0.9042266345847293\n",
      "2022-03-27 00:53:55.034173 Epoch 150, Training Loss 0.9054129094723851\n",
      "2022-03-27 00:53:55.082196 Epoch 150, Training Loss 0.9065779964332386\n",
      "2022-03-27 00:53:55.130201 Epoch 150, Training Loss 0.9076664443211178\n",
      "2022-03-27 00:53:55.191499 Epoch 150, Training Loss 0.908965332276376\n",
      "2022-03-27 00:53:55.239762 Epoch 150, Training Loss 0.910002715371149\n",
      "2022-03-27 00:53:55.295766 Epoch 150, Training Loss 0.9112966654398252\n",
      "2022-03-27 00:53:55.343789 Epoch 150, Training Loss 0.9124099369853964\n",
      "2022-03-27 00:53:55.399795 Epoch 150, Training Loss 0.9134528755836779\n",
      "2022-03-27 00:53:55.423798 Epoch 150, Training Loss 0.9146899722726144\n",
      "2022-03-27 01:24:07.151829 Epoch 200, Training Loss 0.0012519888560790235\n",
      "2022-03-27 01:24:07.169833 Epoch 200, Training Loss 0.0026340263578897853\n",
      "2022-03-27 01:24:07.189837 Epoch 200, Training Loss 0.0037901812349743855\n",
      "2022-03-27 01:24:07.207842 Epoch 200, Training Loss 0.005108485212716301\n",
      "2022-03-27 01:24:07.226846 Epoch 200, Training Loss 0.006106705159482443\n",
      "2022-03-27 01:24:07.245850 Epoch 200, Training Loss 0.007125713331315219\n",
      "2022-03-27 01:24:07.265854 Epoch 200, Training Loss 0.008306179052728522\n",
      "2022-03-27 01:24:07.284859 Epoch 200, Training Loss 0.009518110996012187\n",
      "2022-03-27 01:24:07.303863 Epoch 200, Training Loss 0.010869662627539672\n",
      "2022-03-27 01:24:07.321867 Epoch 200, Training Loss 0.011896639833669833\n",
      "2022-03-27 01:24:07.341873 Epoch 200, Training Loss 0.01290278552133409\n",
      "2022-03-27 01:24:07.373881 Epoch 200, Training Loss 0.014035128952597108\n",
      "2022-03-27 01:24:07.396888 Epoch 200, Training Loss 0.015294391709520384\n",
      "2022-03-27 01:24:07.422891 Epoch 200, Training Loss 0.016587898020854083\n",
      "2022-03-27 01:24:07.442895 Epoch 200, Training Loss 0.017770518884634424\n",
      "2022-03-27 01:24:07.462902 Epoch 200, Training Loss 0.01896696520583404\n",
      "2022-03-27 01:24:07.482918 Epoch 200, Training Loss 0.020330152700624198\n",
      "2022-03-27 01:24:07.501928 Epoch 200, Training Loss 0.02127856702146018\n",
      "2022-03-27 01:24:07.520933 Epoch 200, Training Loss 0.02245675648569756\n",
      "2022-03-27 01:24:07.539937 Epoch 200, Training Loss 0.02362714429645587\n",
      "2022-03-27 01:24:07.558922 Epoch 200, Training Loss 0.02479074983035817\n",
      "2022-03-27 01:24:07.578940 Epoch 200, Training Loss 0.025938366532630628\n",
      "2022-03-27 01:24:07.597950 Epoch 200, Training Loss 0.027198482504891007\n",
      "2022-03-27 01:24:07.616954 Epoch 200, Training Loss 0.0283323621658413\n",
      "2022-03-27 01:24:07.635959 Epoch 200, Training Loss 0.029522133452813033\n",
      "2022-03-27 01:24:07.654963 Epoch 200, Training Loss 0.03034445651047065\n",
      "2022-03-27 01:24:07.674967 Epoch 200, Training Loss 0.03177987370649567\n",
      "2022-03-27 01:24:07.692965 Epoch 200, Training Loss 0.03287689231545724\n",
      "2022-03-27 01:24:07.712976 Epoch 200, Training Loss 0.034143899788941875\n",
      "2022-03-27 01:24:07.731980 Epoch 200, Training Loss 0.035343631530356835\n",
      "2022-03-27 01:24:07.750444 Epoch 200, Training Loss 0.03646227397272349\n",
      "2022-03-27 01:24:07.769448 Epoch 200, Training Loss 0.03748377700291022\n",
      "2022-03-27 01:24:07.790441 Epoch 200, Training Loss 0.0385217902910374\n",
      "2022-03-27 01:24:07.809451 Epoch 200, Training Loss 0.03996751878572547\n",
      "2022-03-27 01:24:07.828462 Epoch 200, Training Loss 0.04133846464059542\n",
      "2022-03-27 01:24:07.847466 Epoch 200, Training Loss 0.04262796341610686\n",
      "2022-03-27 01:24:07.866470 Epoch 200, Training Loss 0.04375013518516365\n",
      "2022-03-27 01:24:07.885475 Epoch 200, Training Loss 0.04494108415930472\n",
      "2022-03-27 01:24:07.904479 Epoch 200, Training Loss 0.045879918672239686\n",
      "2022-03-27 01:24:07.924483 Epoch 200, Training Loss 0.047093550491211056\n",
      "2022-03-27 01:24:07.943481 Epoch 200, Training Loss 0.04840480298032541\n",
      "2022-03-27 01:24:07.968493 Epoch 200, Training Loss 0.049376132924233554\n",
      "2022-03-27 01:24:07.995799 Epoch 200, Training Loss 0.05055032963947872\n",
      "2022-03-27 01:24:08.022806 Epoch 200, Training Loss 0.051540187786302295\n",
      "2022-03-27 01:24:08.049806 Epoch 200, Training Loss 0.0526334356011637\n",
      "2022-03-27 01:24:08.075811 Epoch 200, Training Loss 0.05386947411710344\n",
      "2022-03-27 01:24:08.101823 Epoch 200, Training Loss 0.05497046939247405\n",
      "2022-03-27 01:24:08.127830 Epoch 200, Training Loss 0.056214172440721556\n",
      "2022-03-27 01:24:08.154836 Epoch 200, Training Loss 0.057343721008666644\n",
      "2022-03-27 01:24:08.172840 Epoch 200, Training Loss 0.058470825557513616\n",
      "2022-03-27 01:24:08.191838 Epoch 200, Training Loss 0.05962191038119519\n",
      "2022-03-27 01:24:08.210848 Epoch 200, Training Loss 0.06095015659661549\n",
      "2022-03-27 01:24:08.228853 Epoch 200, Training Loss 0.06202474343197425\n",
      "2022-03-27 01:24:08.247857 Epoch 200, Training Loss 0.0632170342728305\n",
      "2022-03-27 01:24:08.266861 Epoch 200, Training Loss 0.06435152156578611\n",
      "2022-03-27 01:24:08.285865 Epoch 200, Training Loss 0.0653424744715776\n",
      "2022-03-27 01:24:08.304869 Epoch 200, Training Loss 0.06641571357122163\n",
      "2022-03-27 01:24:08.322868 Epoch 200, Training Loss 0.06742456242861346\n",
      "2022-03-27 01:24:08.341872 Epoch 200, Training Loss 0.06872317629397068\n",
      "2022-03-27 01:24:08.360876 Epoch 200, Training Loss 0.06982601138636889\n",
      "2022-03-27 01:24:08.379887 Epoch 200, Training Loss 0.07114877046831429\n",
      "2022-03-27 01:24:08.398593 Epoch 200, Training Loss 0.07236123946316712\n",
      "2022-03-27 01:24:08.416597 Epoch 200, Training Loss 0.07359206089583199\n",
      "2022-03-27 01:24:08.436602 Epoch 200, Training Loss 0.07495106646167043\n",
      "2022-03-27 01:24:08.455593 Epoch 200, Training Loss 0.07615170858400253\n",
      "2022-03-27 01:24:08.474610 Epoch 200, Training Loss 0.07721657521279572\n",
      "2022-03-27 01:24:08.492608 Epoch 200, Training Loss 0.07851435186917824\n",
      "2022-03-27 01:24:08.511619 Epoch 200, Training Loss 0.07960079133967914\n",
      "2022-03-27 01:24:08.530623 Epoch 200, Training Loss 0.08076899481551422\n",
      "2022-03-27 01:24:08.550628 Epoch 200, Training Loss 0.08200066870130847\n",
      "2022-03-27 01:24:08.569632 Epoch 200, Training Loss 0.08304688212511789\n",
      "2022-03-27 01:24:08.587636 Epoch 200, Training Loss 0.0841404576893048\n",
      "2022-03-27 01:24:08.606641 Epoch 200, Training Loss 0.08520045548753666\n",
      "2022-03-27 01:24:08.625639 Epoch 200, Training Loss 0.08639734015440392\n",
      "2022-03-27 01:24:08.644649 Epoch 200, Training Loss 0.0873680437159965\n",
      "2022-03-27 01:24:08.663647 Epoch 200, Training Loss 0.08836637166759852\n",
      "2022-03-27 01:24:08.682657 Epoch 200, Training Loss 0.08927273628351938\n",
      "2022-03-27 01:24:08.701662 Epoch 200, Training Loss 0.09029303601635691\n",
      "2022-03-27 01:24:08.720666 Epoch 200, Training Loss 0.0912372600239561\n",
      "2022-03-27 01:24:08.738670 Epoch 200, Training Loss 0.09216036066374815\n",
      "2022-03-27 01:24:08.757675 Epoch 200, Training Loss 0.09329392659999526\n",
      "2022-03-27 01:24:08.775666 Epoch 200, Training Loss 0.09445204103694242\n",
      "2022-03-27 01:24:08.794758 Epoch 200, Training Loss 0.09551700179839073\n",
      "2022-03-27 01:24:08.813746 Epoch 200, Training Loss 0.09653192011596602\n",
      "2022-03-27 01:24:08.832771 Epoch 200, Training Loss 0.09763523890539204\n",
      "2022-03-27 01:24:08.852778 Epoch 200, Training Loss 0.09867090672788108\n",
      "2022-03-27 01:24:08.871767 Epoch 200, Training Loss 0.09973915763523268\n",
      "2022-03-27 01:24:08.890778 Epoch 200, Training Loss 0.1008338436598668\n",
      "2022-03-27 01:24:08.909782 Epoch 200, Training Loss 0.10198851627157167\n",
      "2022-03-27 01:24:08.927792 Epoch 200, Training Loss 0.10281653713692179\n",
      "2022-03-27 01:24:08.946797 Epoch 200, Training Loss 0.10390549692351495\n",
      "2022-03-27 01:24:08.964801 Epoch 200, Training Loss 0.1052780115543424\n",
      "2022-03-27 01:24:08.983793 Epoch 200, Training Loss 0.1063523733097574\n",
      "2022-03-27 01:24:09.002809 Epoch 200, Training Loss 0.10755280353833953\n",
      "2022-03-27 01:24:09.021814 Epoch 200, Training Loss 0.10853415079738783\n",
      "2022-03-27 01:24:09.039812 Epoch 200, Training Loss 0.1095356089837106\n",
      "2022-03-27 01:24:09.060816 Epoch 200, Training Loss 0.11066294295708541\n",
      "2022-03-27 01:24:09.079827 Epoch 200, Training Loss 0.1118914471257983\n",
      "2022-03-27 01:24:09.097831 Epoch 200, Training Loss 0.11297916192227922\n",
      "2022-03-27 01:24:09.116835 Epoch 200, Training Loss 0.11412430404092344\n",
      "2022-03-27 01:24:09.135840 Epoch 200, Training Loss 0.11570104461191866\n",
      "2022-03-27 01:24:09.154844 Epoch 200, Training Loss 0.11697039290157425\n",
      "2022-03-27 01:24:09.172842 Epoch 200, Training Loss 0.11846612572974866\n",
      "2022-03-27 01:24:09.191846 Epoch 200, Training Loss 0.11954378761598826\n",
      "2022-03-27 01:24:09.210856 Epoch 200, Training Loss 0.12076837373206682\n",
      "2022-03-27 01:24:09.228861 Epoch 200, Training Loss 0.12176193529382691\n",
      "2022-03-27 01:24:09.247859 Epoch 200, Training Loss 0.12300840690922554\n",
      "2022-03-27 01:24:09.266863 Epoch 200, Training Loss 0.12407056701457714\n",
      "2022-03-27 01:24:09.285873 Epoch 200, Training Loss 0.12524028850333466\n",
      "2022-03-27 01:24:09.303877 Epoch 200, Training Loss 0.12636339443418987\n",
      "2022-03-27 01:24:09.322882 Epoch 200, Training Loss 0.1275290571667654\n",
      "2022-03-27 01:24:09.341883 Epoch 200, Training Loss 0.1285993617666347\n",
      "2022-03-27 01:24:09.360890 Epoch 200, Training Loss 0.12963073157593416\n",
      "2022-03-27 01:24:09.378895 Epoch 200, Training Loss 0.1309695201914024\n",
      "2022-03-27 01:24:09.397257 Epoch 200, Training Loss 0.13190603919346314\n",
      "2022-03-27 01:24:09.416261 Epoch 200, Training Loss 0.13291327979253686\n",
      "2022-03-27 01:24:09.435265 Epoch 200, Training Loss 0.13407938795931199\n",
      "2022-03-27 01:24:09.453653 Epoch 200, Training Loss 0.13507062112888718\n",
      "2022-03-27 01:24:09.473648 Epoch 200, Training Loss 0.1360882186828672\n",
      "2022-03-27 01:24:09.491656 Epoch 200, Training Loss 0.13720363530966326\n",
      "2022-03-27 01:24:09.510673 Epoch 200, Training Loss 0.1384595717158159\n",
      "2022-03-27 01:24:09.528677 Epoch 200, Training Loss 0.13944721115214745\n",
      "2022-03-27 01:24:09.547681 Epoch 200, Training Loss 0.14079876019216864\n",
      "2022-03-27 01:24:09.566674 Epoch 200, Training Loss 0.1419013522165206\n",
      "2022-03-27 01:24:09.585683 Epoch 200, Training Loss 0.1428991048537252\n",
      "2022-03-27 01:24:09.603694 Epoch 200, Training Loss 0.14407631152731074\n",
      "2022-03-27 01:24:09.622698 Epoch 200, Training Loss 0.14529752289242756\n",
      "2022-03-27 01:24:09.641696 Epoch 200, Training Loss 0.14634528314061176\n",
      "2022-03-27 01:24:09.660700 Epoch 200, Training Loss 0.14766920367470177\n",
      "2022-03-27 01:24:09.679711 Epoch 200, Training Loss 0.14892653903692885\n",
      "2022-03-27 01:24:09.698715 Epoch 200, Training Loss 0.1502079331051663\n",
      "2022-03-27 01:24:09.717719 Epoch 200, Training Loss 0.15120662584938965\n",
      "2022-03-27 01:24:09.735724 Epoch 200, Training Loss 0.15232582096858402\n",
      "2022-03-27 01:24:09.754728 Epoch 200, Training Loss 0.1533350220421696\n",
      "2022-03-27 01:24:09.773728 Epoch 200, Training Loss 0.1544382890776905\n",
      "2022-03-27 01:24:09.792730 Epoch 200, Training Loss 0.15546555790449956\n",
      "2022-03-27 01:24:09.811741 Epoch 200, Training Loss 0.15673683305530597\n",
      "2022-03-27 01:24:09.830745 Epoch 200, Training Loss 0.15775384774903203\n",
      "2022-03-27 01:24:09.849302 Epoch 200, Training Loss 0.15890148975660123\n",
      "2022-03-27 01:24:09.867803 Epoch 200, Training Loss 0.16021416117163265\n",
      "2022-03-27 01:24:09.887822 Epoch 200, Training Loss 0.16118844284121034\n",
      "2022-03-27 01:24:09.907828 Epoch 200, Training Loss 0.16219092116636388\n",
      "2022-03-27 01:24:09.926831 Epoch 200, Training Loss 0.16318964218849416\n",
      "2022-03-27 01:24:09.945576 Epoch 200, Training Loss 0.16430337296422484\n",
      "2022-03-27 01:24:09.964581 Epoch 200, Training Loss 0.1656233844397318\n",
      "2022-03-27 01:24:09.982585 Epoch 200, Training Loss 0.16706861627986058\n",
      "2022-03-27 01:24:10.002589 Epoch 200, Training Loss 0.16826994362694528\n",
      "2022-03-27 01:24:10.021594 Epoch 200, Training Loss 0.16914348003199642\n",
      "2022-03-27 01:24:10.040598 Epoch 200, Training Loss 0.17047132974695367\n",
      "2022-03-27 01:24:10.059602 Epoch 200, Training Loss 0.17162153147675496\n",
      "2022-03-27 01:24:10.078607 Epoch 200, Training Loss 0.1728128377738816\n",
      "2022-03-27 01:24:10.099611 Epoch 200, Training Loss 0.17389759779586206\n",
      "2022-03-27 01:24:10.119616 Epoch 200, Training Loss 0.17513915157074209\n",
      "2022-03-27 01:24:10.138620 Epoch 200, Training Loss 0.17603703487254774\n",
      "2022-03-27 01:24:10.157626 Epoch 200, Training Loss 0.17692807819837195\n",
      "2022-03-27 01:24:10.175629 Epoch 200, Training Loss 0.17778555084677303\n",
      "2022-03-27 01:24:10.195027 Epoch 200, Training Loss 0.1786773441087864\n",
      "2022-03-27 01:24:10.214032 Epoch 200, Training Loss 0.1797738580600075\n",
      "2022-03-27 01:24:10.233036 Epoch 200, Training Loss 0.18074954272536062\n",
      "2022-03-27 01:24:10.252197 Epoch 200, Training Loss 0.18183660613911232\n",
      "2022-03-27 01:24:10.271201 Epoch 200, Training Loss 0.1829891271908265\n",
      "2022-03-27 01:24:10.290336 Epoch 200, Training Loss 0.1843028742334117\n",
      "2022-03-27 01:24:10.310353 Epoch 200, Training Loss 0.1855276520447353\n",
      "2022-03-27 01:24:10.329357 Epoch 200, Training Loss 0.1867652488943866\n",
      "2022-03-27 01:24:10.347361 Epoch 200, Training Loss 0.18792326592118538\n",
      "2022-03-27 01:24:10.366365 Epoch 200, Training Loss 0.18907067568405814\n",
      "2022-03-27 01:24:10.385370 Epoch 200, Training Loss 0.19025667442385194\n",
      "2022-03-27 01:24:10.404374 Epoch 200, Training Loss 0.19133014736882867\n",
      "2022-03-27 01:24:10.423364 Epoch 200, Training Loss 0.1924598657566568\n",
      "2022-03-27 01:24:10.442383 Epoch 200, Training Loss 0.19349728452275172\n",
      "2022-03-27 01:24:10.461387 Epoch 200, Training Loss 0.19482639157558646\n",
      "2022-03-27 01:24:10.480391 Epoch 200, Training Loss 0.19571470985632114\n",
      "2022-03-27 01:24:10.499395 Epoch 200, Training Loss 0.19692739097358625\n",
      "2022-03-27 01:24:10.519400 Epoch 200, Training Loss 0.1979515606637501\n",
      "2022-03-27 01:24:10.539405 Epoch 200, Training Loss 0.19911610965838517\n",
      "2022-03-27 01:24:10.558402 Epoch 200, Training Loss 0.20020608226661488\n",
      "2022-03-27 01:24:10.577413 Epoch 200, Training Loss 0.20136086334048026\n",
      "2022-03-27 01:24:10.596417 Epoch 200, Training Loss 0.20253270452894517\n",
      "2022-03-27 01:24:10.615422 Epoch 200, Training Loss 0.20381592560912032\n",
      "2022-03-27 01:24:10.634426 Epoch 200, Training Loss 0.20490005315112336\n",
      "2022-03-27 01:24:10.654431 Epoch 200, Training Loss 0.20620550752600744\n",
      "2022-03-27 01:24:10.674436 Epoch 200, Training Loss 0.20739749782835432\n",
      "2022-03-27 01:24:10.693440 Epoch 200, Training Loss 0.20854943601981454\n",
      "2022-03-27 01:24:10.712444 Epoch 200, Training Loss 0.20966323851929297\n",
      "2022-03-27 01:24:10.731448 Epoch 200, Training Loss 0.21066296283546312\n",
      "2022-03-27 01:24:10.751453 Epoch 200, Training Loss 0.21206439257887624\n",
      "2022-03-27 01:24:10.770450 Epoch 200, Training Loss 0.21311467413402274\n",
      "2022-03-27 01:24:10.790029 Epoch 200, Training Loss 0.21418388694753426\n",
      "2022-03-27 01:24:10.809046 Epoch 200, Training Loss 0.21524794616967516\n",
      "2022-03-27 01:24:10.827050 Epoch 200, Training Loss 0.21639629260963186\n",
      "2022-03-27 01:24:10.846054 Epoch 200, Training Loss 0.21773641203980312\n",
      "2022-03-27 01:24:10.866059 Epoch 200, Training Loss 0.21905052372256814\n",
      "2022-03-27 01:24:10.885063 Epoch 200, Training Loss 0.2202002956434284\n",
      "2022-03-27 01:24:10.903230 Epoch 200, Training Loss 0.22135812958792958\n",
      "2022-03-27 01:24:10.923301 Epoch 200, Training Loss 0.22223186691093932\n",
      "2022-03-27 01:24:10.943254 Epoch 200, Training Loss 0.22331813778108953\n",
      "2022-03-27 01:24:10.961243 Epoch 200, Training Loss 0.22419472225486775\n",
      "2022-03-27 01:24:10.981248 Epoch 200, Training Loss 0.22537292437175352\n",
      "2022-03-27 01:24:11.000252 Epoch 200, Training Loss 0.22656779650532072\n",
      "2022-03-27 01:24:11.019256 Epoch 200, Training Loss 0.22748193731698232\n",
      "2022-03-27 01:24:11.039261 Epoch 200, Training Loss 0.22841269905914735\n",
      "2022-03-27 01:24:11.058259 Epoch 200, Training Loss 0.22948499683224027\n",
      "2022-03-27 01:24:11.077270 Epoch 200, Training Loss 0.2307256819189662\n",
      "2022-03-27 01:24:11.096274 Epoch 200, Training Loss 0.2319242259882905\n",
      "2022-03-27 01:24:11.115278 Epoch 200, Training Loss 0.23326182007179846\n",
      "2022-03-27 01:24:11.134283 Epoch 200, Training Loss 0.2342185372739192\n",
      "2022-03-27 01:24:11.153287 Epoch 200, Training Loss 0.2354889688894267\n",
      "2022-03-27 01:24:11.172291 Epoch 200, Training Loss 0.23668269519610782\n",
      "2022-03-27 01:24:11.199297 Epoch 200, Training Loss 0.23787860431329674\n",
      "2022-03-27 01:24:11.227304 Epoch 200, Training Loss 0.2388730655088449\n",
      "2022-03-27 01:24:11.253309 Epoch 200, Training Loss 0.24006259662415974\n",
      "2022-03-27 01:24:11.279315 Epoch 200, Training Loss 0.24144088330171298\n",
      "2022-03-27 01:24:11.306567 Epoch 200, Training Loss 0.24225392632777124\n",
      "2022-03-27 01:24:11.332327 Epoch 200, Training Loss 0.2433100492143265\n",
      "2022-03-27 01:24:11.359334 Epoch 200, Training Loss 0.24431555441883215\n",
      "2022-03-27 01:24:11.386340 Epoch 200, Training Loss 0.24531753479367327\n",
      "2022-03-27 01:24:11.404344 Epoch 200, Training Loss 0.24653967971082233\n",
      "2022-03-27 01:24:11.423334 Epoch 200, Training Loss 0.2474960610842156\n",
      "2022-03-27 01:24:11.442346 Epoch 200, Training Loss 0.24863079731421703\n",
      "2022-03-27 01:24:11.461357 Epoch 200, Training Loss 0.24955854543944453\n",
      "2022-03-27 01:24:11.480361 Epoch 200, Training Loss 0.25068008968287414\n",
      "2022-03-27 01:24:11.499365 Epoch 200, Training Loss 0.25192926766927287\n",
      "2022-03-27 01:24:11.518370 Epoch 200, Training Loss 0.2531084355033572\n",
      "2022-03-27 01:24:11.537374 Epoch 200, Training Loss 0.2543230257985537\n",
      "2022-03-27 01:24:11.556368 Epoch 200, Training Loss 0.25539237680032734\n",
      "2022-03-27 01:24:11.575376 Epoch 200, Training Loss 0.2565803456946712\n",
      "2022-03-27 01:24:11.594387 Epoch 200, Training Loss 0.25780238664668537\n",
      "2022-03-27 01:24:11.613391 Epoch 200, Training Loss 0.25898190738295046\n",
      "2022-03-27 01:24:11.633396 Epoch 200, Training Loss 0.2601477324657733\n",
      "2022-03-27 01:24:11.652400 Epoch 200, Training Loss 0.26124434954370074\n",
      "2022-03-27 01:24:11.671404 Epoch 200, Training Loss 0.26262725855383423\n",
      "2022-03-27 01:24:11.689408 Epoch 200, Training Loss 0.26378527695260695\n",
      "2022-03-27 01:24:11.708407 Epoch 200, Training Loss 0.26483842311307904\n",
      "2022-03-27 01:24:11.727417 Epoch 200, Training Loss 0.26616911723485687\n",
      "2022-03-27 01:24:11.745421 Epoch 200, Training Loss 0.26748332404114705\n",
      "2022-03-27 01:24:11.766426 Epoch 200, Training Loss 0.26854572569012947\n",
      "2022-03-27 01:24:11.785430 Epoch 200, Training Loss 0.26979456815268377\n",
      "2022-03-27 01:24:11.804434 Epoch 200, Training Loss 0.27076006857940305\n",
      "2022-03-27 01:24:11.823439 Epoch 200, Training Loss 0.2717768594889385\n",
      "2022-03-27 01:24:11.842428 Epoch 200, Training Loss 0.27279114433566626\n",
      "2022-03-27 01:24:11.861447 Epoch 200, Training Loss 0.274095398843136\n",
      "2022-03-27 01:24:11.879451 Epoch 200, Training Loss 0.2752189478453468\n",
      "2022-03-27 01:24:11.899456 Epoch 200, Training Loss 0.27638331429122964\n",
      "2022-03-27 01:24:11.917460 Epoch 200, Training Loss 0.27767611754214977\n",
      "2022-03-27 01:24:11.936464 Epoch 200, Training Loss 0.278621983116545\n",
      "2022-03-27 01:24:11.955469 Epoch 200, Training Loss 0.27988102361369316\n",
      "2022-03-27 01:24:11.975467 Epoch 200, Training Loss 0.28092787630112886\n",
      "2022-03-27 01:24:11.993477 Epoch 200, Training Loss 0.2820139418325156\n",
      "2022-03-27 01:24:12.012481 Epoch 200, Training Loss 0.28315222857858213\n",
      "2022-03-27 01:24:12.031486 Epoch 200, Training Loss 0.2843077034901475\n",
      "2022-03-27 01:24:12.050490 Epoch 200, Training Loss 0.2855265181692665\n",
      "2022-03-27 01:24:12.069495 Epoch 200, Training Loss 0.28675564033600986\n",
      "2022-03-27 01:24:12.088499 Epoch 200, Training Loss 0.2877509751740624\n",
      "2022-03-27 01:24:12.107504 Epoch 200, Training Loss 0.28891103743287305\n",
      "2022-03-27 01:24:12.126508 Epoch 200, Training Loss 0.29010059591144555\n",
      "2022-03-27 01:24:12.145512 Epoch 200, Training Loss 0.2913084758059753\n",
      "2022-03-27 01:24:12.164516 Epoch 200, Training Loss 0.29250915588625254\n",
      "2022-03-27 01:24:12.183520 Epoch 200, Training Loss 0.2939699596303808\n",
      "2022-03-27 01:24:12.203518 Epoch 200, Training Loss 0.29512732253050256\n",
      "2022-03-27 01:24:12.221529 Epoch 200, Training Loss 0.29623073209886963\n",
      "2022-03-27 01:24:12.240530 Epoch 200, Training Loss 0.29737426595919575\n",
      "2022-03-27 01:24:12.259538 Epoch 200, Training Loss 0.29839544749016045\n",
      "2022-03-27 01:24:12.278536 Epoch 200, Training Loss 0.2996229938686351\n",
      "2022-03-27 01:24:12.297546 Epoch 200, Training Loss 0.30081783559011377\n",
      "2022-03-27 01:24:12.316551 Epoch 200, Training Loss 0.3020680599352893\n",
      "2022-03-27 01:24:12.334555 Epoch 200, Training Loss 0.30323104541320023\n",
      "2022-03-27 01:24:12.354559 Epoch 200, Training Loss 0.30448643737436865\n",
      "2022-03-27 01:24:12.373564 Epoch 200, Training Loss 0.3057642332885576\n",
      "2022-03-27 01:24:12.392561 Epoch 200, Training Loss 0.30713486724802297\n",
      "2022-03-27 01:24:12.412572 Epoch 200, Training Loss 0.30808429309474233\n",
      "2022-03-27 01:24:12.432577 Epoch 200, Training Loss 0.3090467084856594\n",
      "2022-03-27 01:24:12.450581 Epoch 200, Training Loss 0.3101584574450617\n",
      "2022-03-27 01:24:12.469585 Epoch 200, Training Loss 0.3112705426142954\n",
      "2022-03-27 01:24:12.488589 Epoch 200, Training Loss 0.31267925342330544\n",
      "2022-03-27 01:24:12.507594 Epoch 200, Training Loss 0.3141307961910277\n",
      "2022-03-27 01:24:12.526598 Epoch 200, Training Loss 0.31527995849814255\n",
      "2022-03-27 01:24:12.545602 Epoch 200, Training Loss 0.31626657482303316\n",
      "2022-03-27 01:24:12.564607 Epoch 200, Training Loss 0.3174393125202345\n",
      "2022-03-27 01:24:12.583597 Epoch 200, Training Loss 0.31878764611071025\n",
      "2022-03-27 01:24:12.602615 Epoch 200, Training Loss 0.32016646938250803\n",
      "2022-03-27 01:24:12.621620 Epoch 200, Training Loss 0.3211927180704863\n",
      "2022-03-27 01:24:12.640625 Epoch 200, Training Loss 0.32214943443417854\n",
      "2022-03-27 01:24:12.659628 Epoch 200, Training Loss 0.3233161496231928\n",
      "2022-03-27 01:24:12.678633 Epoch 200, Training Loss 0.32447640342480694\n",
      "2022-03-27 01:24:12.697637 Epoch 200, Training Loss 0.32560023337678834\n",
      "2022-03-27 01:24:12.716641 Epoch 200, Training Loss 0.3265683971097707\n",
      "2022-03-27 01:24:12.735646 Epoch 200, Training Loss 0.32749917920288224\n",
      "2022-03-27 01:24:12.754643 Epoch 200, Training Loss 0.32891522488935526\n",
      "2022-03-27 01:24:12.772654 Epoch 200, Training Loss 0.3301819246595778\n",
      "2022-03-27 01:24:12.791658 Epoch 200, Training Loss 0.33137712972548306\n",
      "2022-03-27 01:24:12.811663 Epoch 200, Training Loss 0.3326465906694417\n",
      "2022-03-27 01:24:12.829667 Epoch 200, Training Loss 0.3336988635685133\n",
      "2022-03-27 01:24:12.848672 Epoch 200, Training Loss 0.3349677452345943\n",
      "2022-03-27 01:24:12.867675 Epoch 200, Training Loss 0.33635553527061285\n",
      "2022-03-27 01:24:12.886680 Epoch 200, Training Loss 0.33758160197521414\n",
      "2022-03-27 01:24:12.906320 Epoch 200, Training Loss 0.33877895593338303\n",
      "2022-03-27 01:24:12.925078 Epoch 200, Training Loss 0.3397144456501202\n",
      "2022-03-27 01:24:12.944083 Epoch 200, Training Loss 0.3408723637423552\n",
      "2022-03-27 01:24:12.963087 Epoch 200, Training Loss 0.3419196813765084\n",
      "2022-03-27 01:24:12.982091 Epoch 200, Training Loss 0.3430987910541427\n",
      "2022-03-27 01:24:13.001095 Epoch 200, Training Loss 0.34431317219953705\n",
      "2022-03-27 01:24:13.021100 Epoch 200, Training Loss 0.345543113129828\n",
      "2022-03-27 01:24:13.040105 Epoch 200, Training Loss 0.3465811391468243\n",
      "2022-03-27 01:24:13.059109 Epoch 200, Training Loss 0.34764713315707646\n",
      "2022-03-27 01:24:13.078113 Epoch 200, Training Loss 0.349064247153909\n",
      "2022-03-27 01:24:13.096117 Epoch 200, Training Loss 0.35019376950190806\n",
      "2022-03-27 01:24:13.115121 Epoch 200, Training Loss 0.35133018281758593\n",
      "2022-03-27 01:24:13.134126 Epoch 200, Training Loss 0.3527993583465781\n",
      "2022-03-27 01:24:13.153130 Epoch 200, Training Loss 0.3541119806754315\n",
      "2022-03-27 01:24:13.172134 Epoch 200, Training Loss 0.35522025175716565\n",
      "2022-03-27 01:24:13.191139 Epoch 200, Training Loss 0.35643563116602883\n",
      "2022-03-27 01:24:13.210143 Epoch 200, Training Loss 0.3575550573866081\n",
      "2022-03-27 01:24:13.229147 Epoch 200, Training Loss 0.3586649621843987\n",
      "2022-03-27 01:24:13.248152 Epoch 200, Training Loss 0.3597721572574752\n",
      "2022-03-27 01:24:13.267156 Epoch 200, Training Loss 0.36102713442519496\n",
      "2022-03-27 01:24:13.286160 Epoch 200, Training Loss 0.36210793912258293\n",
      "2022-03-27 01:24:13.305165 Epoch 200, Training Loss 0.363018210053139\n",
      "2022-03-27 01:24:13.324159 Epoch 200, Training Loss 0.36402750091479563\n",
      "2022-03-27 01:24:13.343173 Epoch 200, Training Loss 0.36522166282319657\n",
      "2022-03-27 01:24:13.362178 Epoch 200, Training Loss 0.36625491345629974\n",
      "2022-03-27 01:24:13.381182 Epoch 200, Training Loss 0.3674995433491514\n",
      "2022-03-27 01:24:13.399186 Epoch 200, Training Loss 0.36866997498685444\n",
      "2022-03-27 01:24:13.418190 Epoch 200, Training Loss 0.36976721387385103\n",
      "2022-03-27 01:24:13.438195 Epoch 200, Training Loss 0.37079246986247694\n",
      "2022-03-27 01:24:13.458193 Epoch 200, Training Loss 0.37201000532835643\n",
      "2022-03-27 01:24:13.477204 Epoch 200, Training Loss 0.3732239768633147\n",
      "2022-03-27 01:24:13.495208 Epoch 200, Training Loss 0.37430629187532705\n",
      "2022-03-27 01:24:13.514212 Epoch 200, Training Loss 0.3754266713891188\n",
      "2022-03-27 01:24:13.533216 Epoch 200, Training Loss 0.37659070802771527\n",
      "2022-03-27 01:24:13.553752 Epoch 200, Training Loss 0.37807639602505033\n",
      "2022-03-27 01:24:13.572740 Epoch 200, Training Loss 0.37906159494843933\n",
      "2022-03-27 01:24:13.591754 Epoch 200, Training Loss 0.380202058331131\n",
      "2022-03-27 01:24:13.610753 Epoch 200, Training Loss 0.38148094519324927\n",
      "2022-03-27 01:24:13.630769 Epoch 200, Training Loss 0.38251351898588487\n",
      "2022-03-27 01:24:13.650774 Epoch 200, Training Loss 0.3837515503701652\n",
      "2022-03-27 01:24:13.670778 Epoch 200, Training Loss 0.38492558039057895\n",
      "2022-03-27 01:24:13.689783 Epoch 200, Training Loss 0.3861862719821198\n",
      "2022-03-27 01:24:13.708781 Epoch 200, Training Loss 0.38744905934004525\n",
      "2022-03-27 01:24:13.726785 Epoch 200, Training Loss 0.3888131502034414\n",
      "2022-03-27 01:24:13.745795 Epoch 200, Training Loss 0.39005624462881355\n",
      "2022-03-27 01:24:13.765787 Epoch 200, Training Loss 0.3910653982930781\n",
      "2022-03-27 01:24:13.783804 Epoch 200, Training Loss 0.3924620877141538\n",
      "2022-03-27 01:24:13.802802 Epoch 200, Training Loss 0.39373077776121057\n",
      "2022-03-27 01:24:13.821812 Epoch 200, Training Loss 0.39476949395730976\n",
      "2022-03-27 01:24:13.841811 Epoch 200, Training Loss 0.39575040363289815\n",
      "2022-03-27 01:24:13.861822 Epoch 200, Training Loss 0.3969725349065288\n",
      "2022-03-27 01:24:13.880826 Epoch 200, Training Loss 0.3979470091860008\n",
      "2022-03-27 01:24:13.899830 Epoch 200, Training Loss 0.39918407134692685\n",
      "2022-03-27 01:24:13.918835 Epoch 200, Training Loss 0.40045930189854656\n",
      "2022-03-27 01:24:13.937839 Epoch 200, Training Loss 0.4014611985067577\n",
      "2022-03-27 01:24:13.956843 Epoch 200, Training Loss 0.4028425449910371\n",
      "2022-03-27 01:24:13.975847 Epoch 200, Training Loss 0.40384962865153845\n",
      "2022-03-27 01:24:13.994846 Epoch 200, Training Loss 0.4048861038806798\n",
      "2022-03-27 01:24:14.020852 Epoch 200, Training Loss 0.406017999286237\n",
      "2022-03-27 01:24:14.047864 Epoch 200, Training Loss 0.40719298770665513\n",
      "2022-03-27 01:24:14.074857 Epoch 200, Training Loss 0.40824522523928786\n",
      "2022-03-27 01:24:14.099863 Epoch 200, Training Loss 0.4093066274814898\n",
      "2022-03-27 01:24:14.127290 Epoch 200, Training Loss 0.41028724767058095\n",
      "2022-03-27 01:24:14.153888 Epoch 200, Training Loss 0.4115874772638921\n",
      "2022-03-27 01:24:14.179894 Epoch 200, Training Loss 0.41298549490816455\n",
      "2022-03-27 01:24:14.206900 Epoch 200, Training Loss 0.4144532141630607\n",
      "2022-03-27 01:24:14.224898 Epoch 200, Training Loss 0.4158324499416839\n",
      "2022-03-27 01:24:14.242902 Epoch 200, Training Loss 0.4171102918169992\n",
      "2022-03-27 01:24:14.261912 Epoch 200, Training Loss 0.41794544504121744\n",
      "2022-03-27 01:24:14.280917 Epoch 200, Training Loss 0.41935059207174785\n",
      "2022-03-27 01:24:14.299921 Epoch 200, Training Loss 0.42051175915066846\n",
      "2022-03-27 01:24:14.318925 Epoch 200, Training Loss 0.42176795859471\n",
      "2022-03-27 01:24:14.336929 Epoch 200, Training Loss 0.42307853424335684\n",
      "2022-03-27 01:24:14.355934 Epoch 200, Training Loss 0.42422187808529493\n",
      "2022-03-27 01:24:14.374932 Epoch 200, Training Loss 0.42540733634358474\n",
      "2022-03-27 01:24:14.393942 Epoch 200, Training Loss 0.4267778018551409\n",
      "2022-03-27 01:24:14.412940 Epoch 200, Training Loss 0.4279801981223514\n",
      "2022-03-27 01:24:14.430944 Epoch 200, Training Loss 0.4291518934242561\n",
      "2022-03-27 01:24:14.449955 Epoch 200, Training Loss 0.4301966310614515\n",
      "2022-03-27 01:24:14.468959 Epoch 200, Training Loss 0.43134647059013775\n",
      "2022-03-27 01:24:14.488964 Epoch 200, Training Loss 0.4326495715724233\n",
      "2022-03-27 01:24:14.507962 Epoch 200, Training Loss 0.4338407995908157\n",
      "2022-03-27 01:24:14.526972 Epoch 200, Training Loss 0.43481433787919066\n",
      "2022-03-27 01:24:14.544977 Epoch 200, Training Loss 0.4358279872733309\n",
      "2022-03-27 01:24:14.563981 Epoch 200, Training Loss 0.43677263514465076\n",
      "2022-03-27 01:24:14.582985 Epoch 200, Training Loss 0.4381170297980004\n",
      "2022-03-27 01:24:14.601989 Epoch 200, Training Loss 0.43915406860354\n",
      "2022-03-27 01:24:14.620988 Epoch 200, Training Loss 0.44036253905662187\n",
      "2022-03-27 01:24:14.638998 Epoch 200, Training Loss 0.441585812650983\n",
      "2022-03-27 01:24:14.657996 Epoch 200, Training Loss 0.44281034358322163\n",
      "2022-03-27 01:24:14.676006 Epoch 200, Training Loss 0.44407992358402826\n",
      "2022-03-27 01:24:14.695011 Epoch 200, Training Loss 0.4456463778567741\n",
      "2022-03-27 01:24:14.714008 Epoch 200, Training Loss 0.4467847949403631\n",
      "2022-03-27 01:24:14.733019 Epoch 200, Training Loss 0.44817979500421784\n",
      "2022-03-27 01:24:14.751023 Epoch 200, Training Loss 0.44938152357745353\n",
      "2022-03-27 01:24:14.770027 Epoch 200, Training Loss 0.45043624819392136\n",
      "2022-03-27 01:24:14.789028 Epoch 200, Training Loss 0.45174206858095917\n",
      "2022-03-27 01:24:14.807030 Epoch 200, Training Loss 0.4529230202098027\n",
      "2022-03-27 01:24:14.826034 Epoch 200, Training Loss 0.4540827325390428\n",
      "2022-03-27 01:24:14.845044 Epoch 200, Training Loss 0.45543534318199547\n",
      "2022-03-27 01:24:14.863049 Epoch 200, Training Loss 0.4566793244360658\n",
      "2022-03-27 01:24:14.882053 Epoch 200, Training Loss 0.4578932669309094\n",
      "2022-03-27 01:24:14.902057 Epoch 200, Training Loss 0.4590683751703833\n",
      "2022-03-27 01:24:14.921062 Epoch 200, Training Loss 0.45991006943270984\n",
      "2022-03-27 01:24:14.939066 Epoch 200, Training Loss 0.46100538969039917\n",
      "2022-03-27 01:24:14.958064 Epoch 200, Training Loss 0.46209910504348445\n",
      "2022-03-27 01:24:14.976074 Epoch 200, Training Loss 0.4632346045483104\n",
      "2022-03-27 01:24:14.995079 Epoch 200, Training Loss 0.4646183618956515\n",
      "2022-03-27 01:24:15.013083 Epoch 200, Training Loss 0.4659019755128095\n",
      "2022-03-27 01:24:15.032081 Epoch 200, Training Loss 0.4669079248557615\n",
      "2022-03-27 01:24:15.051091 Epoch 200, Training Loss 0.4677652998653519\n",
      "2022-03-27 01:24:15.069095 Epoch 200, Training Loss 0.46904933818465916\n",
      "2022-03-27 01:24:15.088100 Epoch 200, Training Loss 0.47005060040737356\n",
      "2022-03-27 01:24:15.108098 Epoch 200, Training Loss 0.4713659390921483\n",
      "2022-03-27 01:24:15.127102 Epoch 200, Training Loss 0.47240673626780205\n",
      "2022-03-27 01:24:15.145113 Epoch 200, Training Loss 0.47332676254270023\n",
      "2022-03-27 01:24:15.164117 Epoch 200, Training Loss 0.4744736790809485\n",
      "2022-03-27 01:24:15.183121 Epoch 200, Training Loss 0.4757677907376643\n",
      "2022-03-27 01:24:15.202119 Epoch 200, Training Loss 0.476860331833515\n",
      "2022-03-27 01:24:15.221130 Epoch 200, Training Loss 0.4778685000394007\n",
      "2022-03-27 01:24:15.239415 Epoch 200, Training Loss 0.4792302175403556\n",
      "2022-03-27 01:24:15.258138 Epoch 200, Training Loss 0.4803033955871602\n",
      "2022-03-27 01:24:15.277143 Epoch 200, Training Loss 0.48137314194608527\n",
      "2022-03-27 01:24:15.295147 Epoch 200, Training Loss 0.4823005691056361\n",
      "2022-03-27 01:24:15.316152 Epoch 200, Training Loss 0.4834493835411413\n",
      "2022-03-27 01:24:15.335156 Epoch 200, Training Loss 0.4844583009210084\n",
      "2022-03-27 01:24:15.353160 Epoch 200, Training Loss 0.485708398434817\n",
      "2022-03-27 01:24:15.372164 Epoch 200, Training Loss 0.48703819162705364\n",
      "2022-03-27 01:24:15.391168 Epoch 200, Training Loss 0.4885937693478811\n",
      "2022-03-27 01:24:15.409167 Epoch 200, Training Loss 0.48969594810319983\n",
      "2022-03-27 01:24:15.427177 Epoch 200, Training Loss 0.4910315025188124\n",
      "2022-03-27 01:24:15.446181 Epoch 200, Training Loss 0.4922078978984862\n",
      "2022-03-27 01:24:15.465185 Epoch 200, Training Loss 0.4936339759155917\n",
      "2022-03-27 01:24:15.484189 Epoch 200, Training Loss 0.49479030091744247\n",
      "2022-03-27 01:24:15.502187 Epoch 200, Training Loss 0.49587548990993546\n",
      "2022-03-27 01:24:15.522198 Epoch 200, Training Loss 0.49704519066664266\n",
      "2022-03-27 01:24:15.541196 Epoch 200, Training Loss 0.49806930105704483\n",
      "2022-03-27 01:24:15.559206 Epoch 200, Training Loss 0.4991939666173647\n",
      "2022-03-27 01:24:15.577211 Epoch 200, Training Loss 0.5002980089705923\n",
      "2022-03-27 01:24:15.596215 Epoch 200, Training Loss 0.5016435722408392\n",
      "2022-03-27 01:24:15.615219 Epoch 200, Training Loss 0.5028311742846009\n",
      "2022-03-27 01:24:15.633217 Epoch 200, Training Loss 0.504062281164062\n",
      "2022-03-27 01:24:15.652227 Epoch 200, Training Loss 0.5053352290559607\n",
      "2022-03-27 01:24:15.671232 Epoch 200, Training Loss 0.5062790253888005\n",
      "2022-03-27 01:24:15.690236 Epoch 200, Training Loss 0.5076145958107756\n",
      "2022-03-27 01:24:15.709234 Epoch 200, Training Loss 0.50862002105969\n",
      "2022-03-27 01:24:15.728245 Epoch 200, Training Loss 0.5099614415784626\n",
      "2022-03-27 01:24:15.747249 Epoch 200, Training Loss 0.5111261974362766\n",
      "2022-03-27 01:24:15.765253 Epoch 200, Training Loss 0.5120522094047283\n",
      "2022-03-27 01:24:15.784257 Epoch 200, Training Loss 0.5130929933941882\n",
      "2022-03-27 01:24:15.803262 Epoch 200, Training Loss 0.514266931263687\n",
      "2022-03-27 01:24:15.821260 Epoch 200, Training Loss 0.5153599472911766\n",
      "2022-03-27 01:24:15.840270 Epoch 200, Training Loss 0.5168166191071806\n",
      "2022-03-27 01:24:15.859274 Epoch 200, Training Loss 0.5178910583791221\n",
      "2022-03-27 01:24:15.878279 Epoch 200, Training Loss 0.5190528786700704\n",
      "2022-03-27 01:24:15.897283 Epoch 200, Training Loss 0.5200218735905864\n",
      "2022-03-27 01:24:15.915281 Epoch 200, Training Loss 0.521330801346113\n",
      "2022-03-27 01:24:15.935292 Epoch 200, Training Loss 0.5224882608179546\n",
      "2022-03-27 01:24:15.954283 Epoch 200, Training Loss 0.5236872035219237\n",
      "2022-03-27 01:24:15.974294 Epoch 200, Training Loss 0.5248583555221558\n",
      "2022-03-27 01:24:15.992298 Epoch 200, Training Loss 0.5260279093252118\n",
      "2022-03-27 01:24:16.011303 Epoch 200, Training Loss 0.5268962434338181\n",
      "2022-03-27 01:24:16.029313 Epoch 200, Training Loss 0.5280418344165968\n",
      "2022-03-27 01:24:16.048317 Epoch 200, Training Loss 0.5292278670746348\n",
      "2022-03-27 01:24:16.067322 Epoch 200, Training Loss 0.5305274147969072\n",
      "2022-03-27 01:24:16.086326 Epoch 200, Training Loss 0.5318405491769161\n",
      "2022-03-27 01:24:16.104330 Epoch 200, Training Loss 0.533026828028052\n",
      "2022-03-27 01:24:16.123334 Epoch 200, Training Loss 0.5344161138205272\n",
      "2022-03-27 01:24:16.143333 Epoch 200, Training Loss 0.5357058086358678\n",
      "2022-03-27 01:24:16.162337 Epoch 200, Training Loss 0.5368467314773814\n",
      "2022-03-27 01:24:16.181347 Epoch 200, Training Loss 0.5378956562265411\n",
      "2022-03-27 01:24:16.199345 Epoch 200, Training Loss 0.5393755983970964\n",
      "2022-03-27 01:24:16.218350 Epoch 200, Training Loss 0.5405189811878497\n",
      "2022-03-27 01:24:16.237360 Epoch 200, Training Loss 0.5419350107155188\n",
      "2022-03-27 01:24:16.256365 Epoch 200, Training Loss 0.5431882399884637\n",
      "2022-03-27 01:24:16.275369 Epoch 200, Training Loss 0.5442869162468045\n",
      "2022-03-27 01:24:16.294360 Epoch 200, Training Loss 0.545568114884979\n",
      "2022-03-27 01:24:16.313377 Epoch 200, Training Loss 0.5464830352827106\n",
      "2022-03-27 01:24:16.332382 Epoch 200, Training Loss 0.5476373551446764\n",
      "2022-03-27 01:24:16.352380 Epoch 200, Training Loss 0.5487686743211868\n",
      "2022-03-27 01:24:16.372384 Epoch 200, Training Loss 0.5497567892989235\n",
      "2022-03-27 01:24:16.390395 Epoch 200, Training Loss 0.5504842884552753\n",
      "2022-03-27 01:24:16.409400 Epoch 200, Training Loss 0.5514399067062856\n",
      "2022-03-27 01:24:16.428403 Epoch 200, Training Loss 0.5525233481088867\n",
      "2022-03-27 01:24:16.447408 Epoch 200, Training Loss 0.5534690831170972\n",
      "2022-03-27 01:24:16.466412 Epoch 200, Training Loss 0.5548428758178525\n",
      "2022-03-27 01:24:16.485403 Epoch 200, Training Loss 0.5559089439146964\n",
      "2022-03-27 01:24:16.504414 Epoch 200, Training Loss 0.5571905972860048\n",
      "2022-03-27 01:24:16.523425 Epoch 200, Training Loss 0.5583194821996762\n",
      "2022-03-27 01:24:16.541423 Epoch 200, Training Loss 0.5595165232715704\n",
      "2022-03-27 01:24:16.561433 Epoch 200, Training Loss 0.5605642874832348\n",
      "2022-03-27 01:24:16.580438 Epoch 200, Training Loss 0.5618808057606982\n",
      "2022-03-27 01:24:16.599442 Epoch 200, Training Loss 0.5628055147350292\n",
      "2022-03-27 01:24:16.618441 Epoch 200, Training Loss 0.5638830307347086\n",
      "2022-03-27 01:24:16.637451 Epoch 200, Training Loss 0.5650958177988487\n",
      "2022-03-27 01:24:16.656455 Epoch 200, Training Loss 0.5664964441753104\n",
      "2022-03-27 01:24:16.675459 Epoch 200, Training Loss 0.5676766005928254\n",
      "2022-03-27 01:24:16.694464 Epoch 200, Training Loss 0.5688132219912146\n",
      "2022-03-27 01:24:16.713462 Epoch 200, Training Loss 0.5699501580289562\n",
      "2022-03-27 01:24:16.732472 Epoch 200, Training Loss 0.5710346756688774\n",
      "2022-03-27 01:24:16.752477 Epoch 200, Training Loss 0.5720457757830315\n",
      "2022-03-27 01:24:16.772481 Epoch 200, Training Loss 0.5728751571129655\n",
      "2022-03-27 01:24:16.791480 Epoch 200, Training Loss 0.573933104801056\n",
      "2022-03-27 01:24:16.811490 Epoch 200, Training Loss 0.5752796621426292\n",
      "2022-03-27 01:24:16.830495 Epoch 200, Training Loss 0.5766206587214604\n",
      "2022-03-27 01:24:16.849499 Epoch 200, Training Loss 0.5778850013642665\n",
      "2022-03-27 01:24:16.867491 Epoch 200, Training Loss 0.5792098909692691\n",
      "2022-03-27 01:24:16.887508 Epoch 200, Training Loss 0.5802771750161105\n",
      "2022-03-27 01:24:16.906499 Epoch 200, Training Loss 0.5813160955601031\n",
      "2022-03-27 01:24:16.925516 Epoch 200, Training Loss 0.5822951014694351\n",
      "2022-03-27 01:24:16.944520 Epoch 200, Training Loss 0.5832187306240696\n",
      "2022-03-27 01:24:16.963525 Epoch 200, Training Loss 0.584253924810673\n",
      "2022-03-27 01:24:16.983523 Epoch 200, Training Loss 0.5854548460534771\n",
      "2022-03-27 01:24:17.002533 Epoch 200, Training Loss 0.5865437546959313\n",
      "2022-03-27 01:24:17.021538 Epoch 200, Training Loss 0.5875862801014005\n",
      "2022-03-27 01:24:17.047544 Epoch 200, Training Loss 0.5887788833712068\n",
      "2022-03-27 01:24:17.073543 Epoch 200, Training Loss 0.5898186742802105\n",
      "2022-03-27 01:24:17.100556 Epoch 200, Training Loss 0.5910692080055051\n",
      "2022-03-27 01:24:17.127562 Epoch 200, Training Loss 0.5921806902684215\n",
      "2022-03-27 01:24:17.154568 Epoch 200, Training Loss 0.5935602871810689\n",
      "2022-03-27 01:24:17.181574 Epoch 200, Training Loss 0.5947957984779192\n",
      "2022-03-27 01:24:17.208568 Epoch 200, Training Loss 0.5960131546725398\n",
      "2022-03-27 01:24:17.229579 Epoch 200, Training Loss 0.5974579813230373\n",
      "2022-03-27 01:24:17.247583 Epoch 200, Training Loss 0.5985606154212562\n",
      "2022-03-27 01:24:17.266594 Epoch 200, Training Loss 0.5996808680274602\n",
      "2022-03-27 01:24:17.284591 Epoch 200, Training Loss 0.6009720560077512\n",
      "2022-03-27 01:24:17.303596 Epoch 200, Training Loss 0.6021490622207027\n",
      "2022-03-27 01:24:17.322600 Epoch 200, Training Loss 0.6032815628192004\n",
      "2022-03-27 01:24:17.340610 Epoch 200, Training Loss 0.6045349704487549\n",
      "2022-03-27 01:24:17.359614 Epoch 200, Training Loss 0.6055053988533556\n",
      "2022-03-27 01:24:17.377619 Epoch 200, Training Loss 0.6066449696907912\n",
      "2022-03-27 01:24:17.395623 Epoch 200, Training Loss 0.608094014094004\n",
      "2022-03-27 01:24:17.415627 Epoch 200, Training Loss 0.609557013758613\n",
      "2022-03-27 01:24:17.434631 Epoch 200, Training Loss 0.6108341861106551\n",
      "2022-03-27 01:24:17.452636 Epoch 200, Training Loss 0.6121747717070762\n",
      "2022-03-27 01:24:17.471640 Epoch 200, Training Loss 0.6133574995848224\n",
      "2022-03-27 01:24:17.490638 Epoch 200, Training Loss 0.6144464365814043\n",
      "2022-03-27 01:24:17.509648 Epoch 200, Training Loss 0.6156999434504058\n",
      "2022-03-27 01:24:17.527653 Epoch 200, Training Loss 0.616597077883113\n",
      "2022-03-27 01:24:17.546657 Epoch 200, Training Loss 0.6178727601190357\n",
      "2022-03-27 01:24:17.565661 Epoch 200, Training Loss 0.6190002110150769\n",
      "2022-03-27 01:24:17.584666 Epoch 200, Training Loss 0.6201537624192055\n",
      "2022-03-27 01:24:17.602674 Epoch 200, Training Loss 0.6212880974992767\n",
      "2022-03-27 01:24:17.622657 Epoch 200, Training Loss 0.6226192513847595\n",
      "2022-03-27 01:24:17.641672 Epoch 200, Training Loss 0.6238083165624867\n",
      "2022-03-27 01:24:17.660683 Epoch 200, Training Loss 0.6250178336792285\n",
      "2022-03-27 01:24:17.678687 Epoch 200, Training Loss 0.6261203864498821\n",
      "2022-03-27 01:24:17.697691 Epoch 200, Training Loss 0.6273786831847237\n",
      "2022-03-27 01:24:17.716690 Epoch 200, Training Loss 0.6285793423042882\n",
      "2022-03-27 01:24:17.734693 Epoch 200, Training Loss 0.6296986830813806\n",
      "2022-03-27 01:24:17.753704 Epoch 200, Training Loss 0.6310477782698238\n",
      "2022-03-27 01:24:17.772708 Epoch 200, Training Loss 0.6323592839643474\n",
      "2022-03-27 01:24:17.790712 Epoch 200, Training Loss 0.6338646163416031\n",
      "2022-03-27 01:24:17.809710 Epoch 200, Training Loss 0.6348862689169471\n",
      "2022-03-27 01:24:17.828715 Epoch 200, Training Loss 0.6361434320964472\n",
      "2022-03-27 01:24:17.846725 Epoch 200, Training Loss 0.6375034642036613\n",
      "2022-03-27 01:24:17.865729 Epoch 200, Training Loss 0.6386008492058806\n",
      "2022-03-27 01:24:17.884734 Epoch 200, Training Loss 0.6396923863217044\n",
      "2022-03-27 01:24:17.902731 Epoch 200, Training Loss 0.640749914947983\n",
      "2022-03-27 01:24:17.921736 Epoch 200, Training Loss 0.6419923227004078\n",
      "2022-03-27 01:24:17.940740 Epoch 200, Training Loss 0.6433094204081904\n",
      "2022-03-27 01:24:17.958744 Epoch 200, Training Loss 0.6443147943605243\n",
      "2022-03-27 01:24:17.977755 Epoch 200, Training Loss 0.6456277141028353\n",
      "2022-03-27 01:24:17.996759 Epoch 200, Training Loss 0.6468528290386395\n",
      "2022-03-27 01:24:18.015763 Epoch 200, Training Loss 0.6480495457149222\n",
      "2022-03-27 01:24:18.034768 Epoch 200, Training Loss 0.6492059678982591\n",
      "2022-03-27 01:24:18.053766 Epoch 200, Training Loss 0.6506483216419854\n",
      "2022-03-27 01:24:18.072776 Epoch 200, Training Loss 0.6517353761379067\n",
      "2022-03-27 01:24:18.090774 Epoch 200, Training Loss 0.6528140371260436\n",
      "2022-03-27 01:24:18.109784 Epoch 200, Training Loss 0.654136530502373\n",
      "2022-03-27 01:24:18.127777 Epoch 200, Training Loss 0.6553676509491319\n",
      "2022-03-27 01:24:18.146793 Epoch 200, Training Loss 0.6564587036057201\n",
      "2022-03-27 01:24:18.164797 Epoch 200, Training Loss 0.6575090164113837\n",
      "2022-03-27 01:24:18.183801 Epoch 200, Training Loss 0.6589561589538594\n",
      "2022-03-27 01:24:18.202805 Epoch 200, Training Loss 0.660027929126759\n",
      "2022-03-27 01:24:18.221810 Epoch 200, Training Loss 0.6612623824792749\n",
      "2022-03-27 01:24:18.239814 Epoch 200, Training Loss 0.6624070828985376\n",
      "2022-03-27 01:24:18.259812 Epoch 200, Training Loss 0.6636339868121135\n",
      "2022-03-27 01:24:18.277817 Epoch 200, Training Loss 0.664793189148159\n",
      "2022-03-27 01:24:18.296821 Epoch 200, Training Loss 0.6658928439287883\n",
      "2022-03-27 01:24:18.315831 Epoch 200, Training Loss 0.6671253763653738\n",
      "2022-03-27 01:24:18.334836 Epoch 200, Training Loss 0.668473754926106\n",
      "2022-03-27 01:24:18.352834 Epoch 200, Training Loss 0.6697413451074029\n",
      "2022-03-27 01:24:18.371838 Epoch 200, Training Loss 0.6709167201958044\n",
      "2022-03-27 01:24:18.390842 Epoch 200, Training Loss 0.6719619416824693\n",
      "2022-03-27 01:24:18.409853 Epoch 200, Training Loss 0.6729747508950246\n",
      "2022-03-27 01:24:18.428857 Epoch 200, Training Loss 0.674196456475636\n",
      "2022-03-27 01:24:18.446855 Epoch 200, Training Loss 0.6752630977526956\n",
      "2022-03-27 01:24:18.466865 Epoch 200, Training Loss 0.6761414701371546\n",
      "2022-03-27 01:24:18.485870 Epoch 200, Training Loss 0.6772037609611326\n",
      "2022-03-27 01:24:18.504874 Epoch 200, Training Loss 0.6782490536379997\n",
      "2022-03-27 01:24:18.523879 Epoch 200, Training Loss 0.6794734250401597\n",
      "2022-03-27 01:24:18.542877 Epoch 200, Training Loss 0.6805427679625313\n",
      "2022-03-27 01:24:18.561887 Epoch 200, Training Loss 0.6817072536177038\n",
      "2022-03-27 01:24:18.579891 Epoch 200, Training Loss 0.6830915175283047\n",
      "2022-03-27 01:24:18.598896 Epoch 200, Training Loss 0.6841974782059564\n",
      "2022-03-27 01:24:18.617900 Epoch 200, Training Loss 0.6853919038382332\n",
      "2022-03-27 01:24:18.636904 Epoch 200, Training Loss 0.6866662096794304\n",
      "2022-03-27 01:24:18.655908 Epoch 200, Training Loss 0.6878209843507508\n",
      "2022-03-27 01:24:18.674906 Epoch 200, Training Loss 0.688799716093961\n",
      "2022-03-27 01:24:18.693917 Epoch 200, Training Loss 0.6901097574349865\n",
      "2022-03-27 01:24:18.712915 Epoch 200, Training Loss 0.6912808540227163\n",
      "2022-03-27 01:24:18.730926 Epoch 200, Training Loss 0.692290593824728\n",
      "2022-03-27 01:24:18.749930 Epoch 200, Training Loss 0.6933405572343665\n",
      "2022-03-27 01:24:18.768934 Epoch 200, Training Loss 0.6946606197777916\n",
      "2022-03-27 01:24:18.786942 Epoch 200, Training Loss 0.6956965312018724\n",
      "2022-03-27 01:24:18.805942 Epoch 200, Training Loss 0.6969181903640328\n",
      "2022-03-27 01:24:18.824941 Epoch 200, Training Loss 0.6980407930853422\n",
      "2022-03-27 01:24:18.843951 Epoch 200, Training Loss 0.6990918488911045\n",
      "2022-03-27 01:24:18.861955 Epoch 200, Training Loss 0.7001766679079636\n",
      "2022-03-27 01:24:18.881959 Epoch 200, Training Loss 0.701475623089944\n",
      "2022-03-27 01:24:18.901964 Epoch 200, Training Loss 0.7024850429933699\n",
      "2022-03-27 01:24:18.919962 Epoch 200, Training Loss 0.7036025280995137\n",
      "2022-03-27 01:24:18.938974 Epoch 200, Training Loss 0.7047706666352499\n",
      "2022-03-27 01:24:18.956971 Epoch 200, Training Loss 0.7059548108474069\n",
      "2022-03-27 01:24:18.975969 Epoch 200, Training Loss 0.7074329243291675\n",
      "2022-03-27 01:24:18.993985 Epoch 200, Training Loss 0.708702269569992\n",
      "2022-03-27 01:24:19.012989 Epoch 200, Training Loss 0.7097421333460552\n",
      "2022-03-27 01:24:19.031994 Epoch 200, Training Loss 0.7108280049717944\n",
      "2022-03-27 01:24:19.050998 Epoch 200, Training Loss 0.7118801262677478\n",
      "2022-03-27 01:24:19.069002 Epoch 200, Training Loss 0.712929009171703\n",
      "2022-03-27 01:24:19.089062 Epoch 200, Training Loss 0.7140698304871465\n",
      "2022-03-27 01:24:19.108005 Epoch 200, Training Loss 0.7151800052589162\n",
      "2022-03-27 01:24:19.127015 Epoch 200, Training Loss 0.7165114968024251\n",
      "2022-03-27 01:24:19.146020 Epoch 200, Training Loss 0.7176473704750276\n",
      "2022-03-27 01:24:19.165011 Epoch 200, Training Loss 0.7190576438098917\n",
      "2022-03-27 01:24:19.184028 Epoch 200, Training Loss 0.7202662697533513\n",
      "2022-03-27 01:24:19.203032 Epoch 200, Training Loss 0.7212953613237347\n",
      "2022-03-27 01:24:19.222021 Epoch 200, Training Loss 0.7221697380628123\n",
      "2022-03-27 01:24:19.242041 Epoch 200, Training Loss 0.7233012711910336\n",
      "2022-03-27 01:24:19.260045 Epoch 200, Training Loss 0.7245481724629317\n",
      "2022-03-27 01:24:19.280050 Epoch 200, Training Loss 0.7256006484141435\n",
      "2022-03-27 01:24:19.299054 Epoch 200, Training Loss 0.726816464263155\n",
      "2022-03-27 01:24:19.320059 Epoch 200, Training Loss 0.7277590060782859\n",
      "2022-03-27 01:24:19.339051 Epoch 200, Training Loss 0.7290417943769099\n",
      "2022-03-27 01:24:19.358048 Epoch 200, Training Loss 0.7298659451324921\n",
      "2022-03-27 01:24:19.376076 Epoch 200, Training Loss 0.7310596615304727\n",
      "2022-03-27 01:24:19.395076 Epoch 200, Training Loss 0.7320869771568367\n",
      "2022-03-27 01:24:19.414074 Epoch 200, Training Loss 0.7331353897786201\n",
      "2022-03-27 01:24:19.433085 Epoch 200, Training Loss 0.7345002837040845\n",
      "2022-03-27 01:24:19.452083 Epoch 200, Training Loss 0.7356757807457234\n",
      "2022-03-27 01:24:19.470087 Epoch 200, Training Loss 0.7368760235474238\n",
      "2022-03-27 01:24:19.489097 Epoch 200, Training Loss 0.737983148421168\n",
      "2022-03-27 01:24:19.509096 Epoch 200, Training Loss 0.7391094440389472\n",
      "2022-03-27 01:24:19.527100 Epoch 200, Training Loss 0.7401993696951805\n",
      "2022-03-27 01:24:19.546110 Epoch 200, Training Loss 0.7412045264183102\n",
      "2022-03-27 01:24:19.565098 Epoch 200, Training Loss 0.7421428110745861\n",
      "2022-03-27 01:24:19.584100 Epoch 200, Training Loss 0.7432027860065861\n",
      "2022-03-27 01:24:19.603118 Epoch 200, Training Loss 0.7449012665492495\n",
      "2022-03-27 01:24:19.622111 Epoch 200, Training Loss 0.7458339185665941\n",
      "2022-03-27 01:24:19.640121 Epoch 200, Training Loss 0.7468686084765608\n",
      "2022-03-27 01:24:19.659132 Epoch 200, Training Loss 0.748046832926133\n",
      "2022-03-27 01:24:19.678136 Epoch 200, Training Loss 0.7490814700913246\n",
      "2022-03-27 01:24:19.696141 Epoch 200, Training Loss 0.7503022788583166\n",
      "2022-03-27 01:24:19.715144 Epoch 200, Training Loss 0.7514045099010858\n",
      "2022-03-27 01:24:19.735147 Epoch 200, Training Loss 0.7526906134984682\n",
      "2022-03-27 01:24:19.754148 Epoch 200, Training Loss 0.7537521168094157\n",
      "2022-03-27 01:24:19.774157 Epoch 200, Training Loss 0.7549452221454562\n",
      "2022-03-27 01:24:19.792150 Epoch 200, Training Loss 0.7560425575279519\n",
      "2022-03-27 01:24:19.810167 Epoch 200, Training Loss 0.7570871733643515\n",
      "2022-03-27 01:24:19.829171 Epoch 200, Training Loss 0.7583436715938247\n",
      "2022-03-27 01:24:19.848175 Epoch 200, Training Loss 0.7595439297159005\n",
      "2022-03-27 01:24:19.866179 Epoch 200, Training Loss 0.7608161814072553\n",
      "2022-03-27 01:24:19.885184 Epoch 200, Training Loss 0.7619697716077576\n",
      "2022-03-27 01:24:19.903185 Epoch 200, Training Loss 0.763017253650119\n",
      "2022-03-27 01:24:19.923179 Epoch 200, Training Loss 0.7641529049105047\n",
      "2022-03-27 01:24:19.942196 Epoch 200, Training Loss 0.7651304098803674\n",
      "2022-03-27 01:24:19.960198 Epoch 200, Training Loss 0.7663864931639504\n",
      "2022-03-27 01:24:19.979205 Epoch 200, Training Loss 0.767839088845436\n",
      "2022-03-27 01:24:19.998209 Epoch 200, Training Loss 0.7688739622188041\n",
      "2022-03-27 01:24:20.017213 Epoch 200, Training Loss 0.7699075051585732\n",
      "2022-03-27 01:24:20.036217 Epoch 200, Training Loss 0.7710156977329108\n",
      "2022-03-27 01:24:20.062223 Epoch 200, Training Loss 0.7720821028017937\n",
      "2022-03-27 01:24:20.088230 Epoch 200, Training Loss 0.7732458630638659\n",
      "2022-03-27 01:24:20.115236 Epoch 200, Training Loss 0.7744066447705564\n",
      "2022-03-27 01:24:20.142233 Epoch 200, Training Loss 0.7753186299063056\n",
      "2022-03-27 01:24:20.171250 Epoch 200, Training Loss 0.7765306151279098\n",
      "2022-03-27 01:24:20.198258 Epoch 200, Training Loss 0.7775401175022125\n",
      "2022-03-27 01:24:20.225261 Epoch 200, Training Loss 0.7789280349793641\n",
      "2022-03-27 01:24:20.252270 Epoch 200, Training Loss 0.7801360950598022\n",
      "2022-03-27 01:24:20.271274 Epoch 200, Training Loss 0.7811655995943357\n",
      "2022-03-27 01:24:20.290273 Epoch 200, Training Loss 0.7822743362325537\n",
      "2022-03-27 01:24:20.308277 Epoch 200, Training Loss 0.7833114274779854\n",
      "2022-03-27 01:24:20.327287 Epoch 200, Training Loss 0.7844240204300112\n",
      "2022-03-27 01:24:20.346292 Epoch 200, Training Loss 0.7856255812413248\n",
      "2022-03-27 01:24:20.365283 Epoch 200, Training Loss 0.7865801820974521\n",
      "2022-03-27 01:24:20.384300 Epoch 200, Training Loss 0.7877447705744477\n",
      "2022-03-27 01:24:20.403298 Epoch 200, Training Loss 0.7887466427157906\n",
      "2022-03-27 01:24:20.422309 Epoch 200, Training Loss 0.7900166801174583\n",
      "2022-03-27 01:24:20.441313 Epoch 200, Training Loss 0.791231779567421\n",
      "2022-03-27 01:24:20.459317 Epoch 200, Training Loss 0.7923779683497251\n",
      "2022-03-27 01:24:20.478322 Epoch 200, Training Loss 0.7938190245110056\n",
      "2022-03-27 01:24:20.497326 Epoch 200, Training Loss 0.7948863484213115\n",
      "2022-03-27 01:24:20.516330 Epoch 200, Training Loss 0.7960854403655547\n",
      "2022-03-27 01:24:20.535334 Epoch 200, Training Loss 0.7973118749116083\n",
      "2022-03-27 01:24:20.554339 Epoch 200, Training Loss 0.7985748482482208\n",
      "2022-03-27 01:24:20.573337 Epoch 200, Training Loss 0.7996022948980941\n",
      "2022-03-27 01:24:20.592347 Epoch 200, Training Loss 0.8008306045513933\n",
      "2022-03-27 01:24:20.610345 Epoch 200, Training Loss 0.8022098712756506\n",
      "2022-03-27 01:24:20.629356 Epoch 200, Training Loss 0.8035632444312201\n",
      "2022-03-27 01:24:20.648360 Epoch 200, Training Loss 0.8047151233991394\n",
      "2022-03-27 01:24:20.667364 Epoch 200, Training Loss 0.8059599205203678\n",
      "2022-03-27 01:24:20.686369 Epoch 200, Training Loss 0.806960581818505\n",
      "2022-03-27 01:24:20.705373 Epoch 200, Training Loss 0.8080534205564758\n",
      "2022-03-27 01:24:20.723377 Epoch 200, Training Loss 0.8091402568323228\n",
      "2022-03-27 01:24:20.742375 Epoch 200, Training Loss 0.8101772696465788\n",
      "2022-03-27 01:24:20.762386 Epoch 200, Training Loss 0.8114965538234662\n",
      "2022-03-27 01:24:20.781390 Epoch 200, Training Loss 0.812731028746461\n",
      "2022-03-27 01:24:20.799388 Epoch 200, Training Loss 0.8137422713934613\n",
      "2022-03-27 01:24:20.818399 Epoch 200, Training Loss 0.8149141894886865\n",
      "2022-03-27 01:24:20.836403 Epoch 200, Training Loss 0.8163252039943509\n",
      "2022-03-27 01:24:20.855407 Epoch 200, Training Loss 0.8176859737662099\n",
      "2022-03-27 01:24:20.874405 Epoch 200, Training Loss 0.8186533204887224\n",
      "2022-03-27 01:24:20.892409 Epoch 200, Training Loss 0.8197955199519692\n",
      "2022-03-27 01:24:20.910413 Epoch 200, Training Loss 0.8209147056960084\n",
      "2022-03-27 01:24:20.929424 Epoch 200, Training Loss 0.821939279051388\n",
      "2022-03-27 01:24:20.948428 Epoch 200, Training Loss 0.8231012972876849\n",
      "2022-03-27 01:24:20.968433 Epoch 200, Training Loss 0.8245028230692725\n",
      "2022-03-27 01:24:20.987437 Epoch 200, Training Loss 0.8257040749577915\n",
      "2022-03-27 01:24:21.007435 Epoch 200, Training Loss 0.8269774306308278\n",
      "2022-03-27 01:24:21.025446 Epoch 200, Training Loss 0.827964803142011\n",
      "2022-03-27 01:24:21.044450 Epoch 200, Training Loss 0.8290433876807123\n",
      "2022-03-27 01:24:21.062454 Epoch 200, Training Loss 0.8300433748823297\n",
      "2022-03-27 01:24:21.082458 Epoch 200, Training Loss 0.8310723551703841\n",
      "2022-03-27 01:24:21.100462 Epoch 200, Training Loss 0.8321875031951749\n",
      "2022-03-27 01:24:21.119467 Epoch 200, Training Loss 0.8335121661195974\n",
      "2022-03-27 01:24:21.139459 Epoch 200, Training Loss 0.834824316184539\n",
      "2022-03-27 01:24:21.158470 Epoch 200, Training Loss 0.8358453205022056\n",
      "2022-03-27 01:24:21.177480 Epoch 200, Training Loss 0.8368960955868596\n",
      "2022-03-27 01:24:21.196484 Epoch 200, Training Loss 0.8379050493240356\n",
      "2022-03-27 01:24:21.214482 Epoch 200, Training Loss 0.8389006252483944\n",
      "2022-03-27 01:24:21.233493 Epoch 200, Training Loss 0.8402619985363368\n",
      "2022-03-27 01:24:21.252497 Epoch 200, Training Loss 0.8413839700734219\n",
      "2022-03-27 01:24:21.271501 Epoch 200, Training Loss 0.8428030635237389\n",
      "2022-03-27 01:24:21.290493 Epoch 200, Training Loss 0.8439086001852284\n",
      "2022-03-27 01:24:21.309510 Epoch 200, Training Loss 0.8449671378983256\n",
      "2022-03-27 01:24:21.327514 Epoch 200, Training Loss 0.8462601448111522\n",
      "2022-03-27 01:24:21.346518 Epoch 200, Training Loss 0.8474927932557548\n",
      "2022-03-27 01:24:21.364516 Epoch 200, Training Loss 0.8487563191929741\n",
      "2022-03-27 01:24:21.384521 Epoch 200, Training Loss 0.8499674765807589\n",
      "2022-03-27 01:24:21.404531 Epoch 200, Training Loss 0.851143527442537\n",
      "2022-03-27 01:24:21.423536 Epoch 200, Training Loss 0.8521625699137177\n",
      "2022-03-27 01:24:21.441534 Epoch 200, Training Loss 0.8536261098311685\n",
      "2022-03-27 01:24:21.460544 Epoch 200, Training Loss 0.8546155850448267\n",
      "2022-03-27 01:24:21.479543 Epoch 200, Training Loss 0.8557904613444872\n",
      "2022-03-27 01:24:21.497553 Epoch 200, Training Loss 0.8567738349327956\n",
      "2022-03-27 01:24:21.516557 Epoch 200, Training Loss 0.8577825023847467\n",
      "2022-03-27 01:24:21.535561 Epoch 200, Training Loss 0.858666663508281\n",
      "2022-03-27 01:24:21.554565 Epoch 200, Training Loss 0.8597683664935324\n",
      "2022-03-27 01:24:21.573564 Epoch 200, Training Loss 0.8608350876499625\n",
      "2022-03-27 01:24:21.593574 Epoch 200, Training Loss 0.8619469664133418\n",
      "2022-03-27 01:24:21.611572 Epoch 200, Training Loss 0.8628458739699\n",
      "2022-03-27 01:24:21.630583 Epoch 200, Training Loss 0.8641144635579775\n",
      "2022-03-27 01:24:21.649587 Epoch 200, Training Loss 0.8653034516002821\n",
      "2022-03-27 01:24:21.667591 Epoch 200, Training Loss 0.8664403169051461\n",
      "2022-03-27 01:24:21.686596 Epoch 200, Training Loss 0.8678049545763703\n",
      "2022-03-27 01:24:21.705597 Epoch 200, Training Loss 0.8692232442024114\n",
      "2022-03-27 01:24:21.725598 Epoch 200, Training Loss 0.8705305748278528\n",
      "2022-03-27 01:24:21.743613 Epoch 200, Training Loss 0.871603805771874\n",
      "2022-03-27 01:24:21.762612 Epoch 200, Training Loss 0.8729043990907157\n",
      "2022-03-27 01:24:21.781611 Epoch 200, Training Loss 0.8744799061047147\n",
      "2022-03-27 01:24:21.801621 Epoch 200, Training Loss 0.8756298720836639\n",
      "2022-03-27 01:24:21.820613 Epoch 200, Training Loss 0.8768088873237601\n",
      "2022-03-27 01:24:21.840624 Epoch 200, Training Loss 0.8778099228658944\n",
      "2022-03-27 01:24:21.858628 Epoch 200, Training Loss 0.8787534292549124\n",
      "2022-03-27 01:24:21.876632 Epoch 200, Training Loss 0.8800964871483385\n",
      "2022-03-27 01:24:21.895643 Epoch 200, Training Loss 0.8811514989646805\n",
      "2022-03-27 01:24:21.914647 Epoch 200, Training Loss 0.882790086443162\n",
      "2022-03-27 01:24:21.932645 Epoch 200, Training Loss 0.8840172235160837\n",
      "2022-03-27 01:24:21.951655 Epoch 200, Training Loss 0.8851690122386073\n",
      "2022-03-27 01:24:21.970660 Epoch 200, Training Loss 0.8864032663500218\n",
      "2022-03-27 01:24:21.989664 Epoch 200, Training Loss 0.8877180178299584\n",
      "2022-03-27 01:24:22.008662 Epoch 200, Training Loss 0.8891648082324611\n",
      "2022-03-27 01:24:22.027673 Epoch 200, Training Loss 0.8903274037648955\n",
      "2022-03-27 01:24:22.045677 Epoch 200, Training Loss 0.8913665256841713\n",
      "2022-03-27 01:24:22.064681 Epoch 200, Training Loss 0.892659446467524\n",
      "2022-03-27 01:24:22.083685 Epoch 200, Training Loss 0.8937883136217551\n",
      "2022-03-27 01:24:22.102690 Epoch 200, Training Loss 0.8949065819725661\n",
      "2022-03-27 01:24:22.120694 Epoch 200, Training Loss 0.8957484041333503\n",
      "2022-03-27 01:24:22.140692 Epoch 200, Training Loss 0.8968331265022688\n",
      "2022-03-27 01:24:22.158702 Epoch 200, Training Loss 0.8981942779877606\n",
      "2022-03-27 01:24:22.177707 Epoch 200, Training Loss 0.899500504783962\n",
      "2022-03-27 01:24:22.196711 Epoch 200, Training Loss 0.9006511222218614\n",
      "2022-03-27 01:24:22.216715 Epoch 200, Training Loss 0.9021432499598969\n",
      "2022-03-27 01:24:22.235714 Epoch 200, Training Loss 0.9032034472278927\n",
      "2022-03-27 01:24:22.254724 Epoch 200, Training Loss 0.9043092550828938\n",
      "2022-03-27 01:24:22.262715 Epoch 200, Training Loss 0.905579031657075\n",
      "2022-03-27 01:36:02.450593 Epoch 250, Training Loss 0.0011806760145270307\n",
      "2022-03-27 01:36:02.472612 Epoch 250, Training Loss 0.002087835232010278\n",
      "2022-03-27 01:36:02.491617 Epoch 250, Training Loss 0.003202621513010596\n",
      "2022-03-27 01:36:02.509621 Epoch 250, Training Loss 0.0046164976513904075\n",
      "2022-03-27 01:36:02.529611 Epoch 250, Training Loss 0.005619714098513279\n",
      "2022-03-27 01:36:02.548615 Epoch 250, Training Loss 0.006583644529742658\n",
      "2022-03-27 01:36:02.568620 Epoch 250, Training Loss 0.007905950860294235\n",
      "2022-03-27 01:36:02.586624 Epoch 250, Training Loss 0.00891638427134365\n",
      "2022-03-27 01:36:02.605628 Epoch 250, Training Loss 0.009984512691912443\n",
      "2022-03-27 01:36:02.623632 Epoch 250, Training Loss 0.011193872488977965\n",
      "2022-03-27 01:36:02.642637 Epoch 250, Training Loss 0.012189478215659062\n",
      "2022-03-27 01:36:02.663643 Epoch 250, Training Loss 0.013434800421795273\n",
      "2022-03-27 01:36:02.682646 Epoch 250, Training Loss 0.014658572957338885\n",
      "2022-03-27 01:36:02.701664 Epoch 250, Training Loss 0.015806431050800606\n",
      "2022-03-27 01:36:02.721675 Epoch 250, Training Loss 0.01698179714515081\n",
      "2022-03-27 01:36:02.740400 Epoch 250, Training Loss 0.017959610046937947\n",
      "2022-03-27 01:36:02.759416 Epoch 250, Training Loss 0.018978229416605763\n",
      "2022-03-27 01:36:02.778421 Epoch 250, Training Loss 0.019961534939763492\n",
      "2022-03-27 01:36:02.797425 Epoch 250, Training Loss 0.021039092403543576\n",
      "2022-03-27 01:36:02.816423 Epoch 250, Training Loss 0.022167433329555383\n",
      "2022-03-27 01:36:02.835428 Epoch 250, Training Loss 0.02331552465858362\n",
      "2022-03-27 01:36:02.853438 Epoch 250, Training Loss 0.02442578418785349\n",
      "2022-03-27 01:36:02.873442 Epoch 250, Training Loss 0.025320347541433467\n",
      "2022-03-27 01:36:02.893441 Epoch 250, Training Loss 0.026286021827736778\n",
      "2022-03-27 01:36:02.912451 Epoch 250, Training Loss 0.027350328996053438\n",
      "2022-03-27 01:36:02.931456 Epoch 250, Training Loss 0.028742911641859947\n",
      "2022-03-27 01:36:02.951460 Epoch 250, Training Loss 0.02991980725846937\n",
      "2022-03-27 01:36:02.969464 Epoch 250, Training Loss 0.030966902099302054\n",
      "2022-03-27 01:36:02.989468 Epoch 250, Training Loss 0.03209767134293266\n",
      "2022-03-27 01:36:03.007473 Epoch 250, Training Loss 0.03328079686445348\n",
      "2022-03-27 01:36:03.026482 Epoch 250, Training Loss 0.03497658513695993\n",
      "2022-03-27 01:36:03.046482 Epoch 250, Training Loss 0.03608153024902734\n",
      "2022-03-27 01:36:03.066486 Epoch 250, Training Loss 0.03726091615074431\n",
      "2022-03-27 01:36:03.086295 Epoch 250, Training Loss 0.03826447109432172\n",
      "2022-03-27 01:36:03.105287 Epoch 250, Training Loss 0.039048068892315525\n",
      "2022-03-27 01:36:03.124303 Epoch 250, Training Loss 0.04012363851832612\n",
      "2022-03-27 01:36:03.144308 Epoch 250, Training Loss 0.04128994554509897\n",
      "2022-03-27 01:36:03.163312 Epoch 250, Training Loss 0.04223005873772799\n",
      "2022-03-27 01:36:03.182311 Epoch 250, Training Loss 0.043110263622020514\n",
      "2022-03-27 01:36:03.201321 Epoch 250, Training Loss 0.0441957228171551\n",
      "2022-03-27 01:36:03.220325 Epoch 250, Training Loss 0.0451802760743729\n",
      "2022-03-27 01:36:03.239329 Epoch 250, Training Loss 0.0464424381932944\n",
      "2022-03-27 01:36:03.258334 Epoch 250, Training Loss 0.04752875045132454\n",
      "2022-03-27 01:36:03.277332 Epoch 250, Training Loss 0.04880977896473292\n",
      "2022-03-27 01:36:03.296342 Epoch 250, Training Loss 0.04980804273844375\n",
      "2022-03-27 01:36:03.315341 Epoch 250, Training Loss 0.051075050364369934\n",
      "2022-03-27 01:36:03.334345 Epoch 250, Training Loss 0.05223498693512529\n",
      "2022-03-27 01:36:03.353355 Epoch 250, Training Loss 0.05341679658121465\n",
      "2022-03-27 01:36:03.372354 Epoch 250, Training Loss 0.05455255066342366\n",
      "2022-03-27 01:36:03.391364 Epoch 250, Training Loss 0.05565292077601108\n",
      "2022-03-27 01:36:03.410368 Epoch 250, Training Loss 0.05703889340390939\n",
      "2022-03-27 01:36:03.429372 Epoch 250, Training Loss 0.05812680812747887\n",
      "2022-03-27 01:36:03.448377 Epoch 250, Training Loss 0.05916288601772864\n",
      "2022-03-27 01:36:03.466375 Epoch 250, Training Loss 0.06018036893566551\n",
      "2022-03-27 01:36:03.485385 Epoch 250, Training Loss 0.061268971932818515\n",
      "2022-03-27 01:36:03.504389 Epoch 250, Training Loss 0.06223881496187976\n",
      "2022-03-27 01:36:03.523394 Epoch 250, Training Loss 0.06318406848346486\n",
      "2022-03-27 01:36:03.542398 Epoch 250, Training Loss 0.06446681821437748\n",
      "2022-03-27 01:36:03.561396 Epoch 250, Training Loss 0.06573289457489462\n",
      "2022-03-27 01:36:03.579407 Epoch 250, Training Loss 0.06691365130722066\n",
      "2022-03-27 01:36:03.598965 Epoch 250, Training Loss 0.06793802138179769\n",
      "2022-03-27 01:36:03.616983 Epoch 250, Training Loss 0.06937956245963836\n",
      "2022-03-27 01:36:03.634981 Epoch 250, Training Loss 0.07033923481736341\n",
      "2022-03-27 01:36:03.653985 Epoch 250, Training Loss 0.07147623808182718\n",
      "2022-03-27 01:36:03.671990 Epoch 250, Training Loss 0.07253769649873915\n",
      "2022-03-27 01:36:03.691000 Epoch 250, Training Loss 0.07373075221505616\n",
      "2022-03-27 01:36:03.708989 Epoch 250, Training Loss 0.07493529348727078\n",
      "2022-03-27 01:36:03.729008 Epoch 250, Training Loss 0.07597224692554425\n",
      "2022-03-27 01:36:03.747012 Epoch 250, Training Loss 0.07737608890399299\n",
      "2022-03-27 01:36:03.766016 Epoch 250, Training Loss 0.0784871566021229\n",
      "2022-03-27 01:36:03.785016 Epoch 250, Training Loss 0.0796431371622988\n",
      "2022-03-27 01:36:03.804025 Epoch 250, Training Loss 0.08093204172066105\n",
      "2022-03-27 01:36:03.823032 Epoch 250, Training Loss 0.08203525616384832\n",
      "2022-03-27 01:36:03.842034 Epoch 250, Training Loss 0.08308260611560948\n",
      "2022-03-27 01:36:03.860024 Epoch 250, Training Loss 0.08410101442995584\n",
      "2022-03-27 01:36:03.880042 Epoch 250, Training Loss 0.08508411499545397\n",
      "2022-03-27 01:36:03.899040 Epoch 250, Training Loss 0.08614084741953389\n",
      "2022-03-27 01:36:03.918051 Epoch 250, Training Loss 0.0871210918402123\n",
      "2022-03-27 01:36:03.938049 Epoch 250, Training Loss 0.08846743423920458\n",
      "2022-03-27 01:36:03.957060 Epoch 250, Training Loss 0.08959269470266064\n",
      "2022-03-27 01:36:03.975050 Epoch 250, Training Loss 0.09083811134633506\n",
      "2022-03-27 01:36:03.994074 Epoch 250, Training Loss 0.09177161718878295\n",
      "2022-03-27 01:36:04.013074 Epoch 250, Training Loss 0.09279329514564455\n",
      "2022-03-27 01:36:04.032071 Epoch 250, Training Loss 0.09378053343204586\n",
      "2022-03-27 01:36:04.051081 Epoch 250, Training Loss 0.0952532407267929\n",
      "2022-03-27 01:36:04.070086 Epoch 250, Training Loss 0.09622556786707905\n",
      "2022-03-27 01:36:04.089075 Epoch 250, Training Loss 0.09712263187179175\n",
      "2022-03-27 01:36:04.107095 Epoch 250, Training Loss 0.09806536667792083\n",
      "2022-03-27 01:36:04.126084 Epoch 250, Training Loss 0.09921862615648742\n",
      "2022-03-27 01:36:04.146102 Epoch 250, Training Loss 0.10040461445403526\n",
      "2022-03-27 01:36:04.165114 Epoch 250, Training Loss 0.1015400673117479\n",
      "2022-03-27 01:36:04.184111 Epoch 250, Training Loss 0.10257740795155011\n",
      "2022-03-27 01:36:04.204117 Epoch 250, Training Loss 0.1037421057291348\n",
      "2022-03-27 01:36:04.225120 Epoch 250, Training Loss 0.10469294691939487\n",
      "2022-03-27 01:36:04.244133 Epoch 250, Training Loss 0.10567542902953789\n",
      "2022-03-27 01:36:04.262133 Epoch 250, Training Loss 0.10699303558720347\n",
      "2022-03-27 01:36:04.281133 Epoch 250, Training Loss 0.10818303866154702\n",
      "2022-03-27 01:36:04.301144 Epoch 250, Training Loss 0.10950313626652788\n",
      "2022-03-27 01:36:04.320142 Epoch 250, Training Loss 0.11065331436788944\n",
      "2022-03-27 01:36:04.339152 Epoch 250, Training Loss 0.1114378608858494\n",
      "2022-03-27 01:36:04.360157 Epoch 250, Training Loss 0.11255226469100893\n",
      "2022-03-27 01:36:04.380161 Epoch 250, Training Loss 0.11354207542851148\n",
      "2022-03-27 01:36:04.399160 Epoch 250, Training Loss 0.11458108133977027\n",
      "2022-03-27 01:36:04.419170 Epoch 250, Training Loss 0.11583298193219373\n",
      "2022-03-27 01:36:04.438175 Epoch 250, Training Loss 0.11692802306941098\n",
      "2022-03-27 01:36:04.457179 Epoch 250, Training Loss 0.11811354253298181\n",
      "2022-03-27 01:36:04.476183 Epoch 250, Training Loss 0.11933328291339337\n",
      "2022-03-27 01:36:04.495181 Epoch 250, Training Loss 0.12048458176500657\n",
      "2022-03-27 01:36:04.514192 Epoch 250, Training Loss 0.12164829400799158\n",
      "2022-03-27 01:36:04.533190 Epoch 250, Training Loss 0.12285210127415865\n",
      "2022-03-27 01:36:04.552194 Epoch 250, Training Loss 0.12393718340512737\n",
      "2022-03-27 01:36:04.572205 Epoch 250, Training Loss 0.12497237195139346\n",
      "2022-03-27 01:36:04.591203 Epoch 250, Training Loss 0.1260531993625719\n",
      "2022-03-27 01:36:04.610213 Epoch 250, Training Loss 0.12692923725718427\n",
      "2022-03-27 01:36:04.629218 Epoch 250, Training Loss 0.1280204957098607\n",
      "2022-03-27 01:36:04.648222 Epoch 250, Training Loss 0.12909446729113683\n",
      "2022-03-27 01:36:04.667220 Epoch 250, Training Loss 0.13000745297697805\n",
      "2022-03-27 01:36:04.686218 Epoch 250, Training Loss 0.13125617988883992\n",
      "2022-03-27 01:36:04.705235 Epoch 250, Training Loss 0.1323975248409964\n",
      "2022-03-27 01:36:04.725239 Epoch 250, Training Loss 0.13360522393985172\n",
      "2022-03-27 01:36:04.744244 Epoch 250, Training Loss 0.13461670348101565\n",
      "2022-03-27 01:36:04.763248 Epoch 250, Training Loss 0.13585442921999472\n",
      "2022-03-27 01:36:04.783247 Epoch 250, Training Loss 0.13724780776311674\n",
      "2022-03-27 01:36:04.801251 Epoch 250, Training Loss 0.13831769802686197\n",
      "2022-03-27 01:36:04.820261 Epoch 250, Training Loss 0.13942055537572604\n",
      "2022-03-27 01:36:04.840266 Epoch 250, Training Loss 0.14039957271817397\n",
      "2022-03-27 01:36:04.859270 Epoch 250, Training Loss 0.14167635527718098\n",
      "2022-03-27 01:36:04.878274 Epoch 250, Training Loss 0.1427575387918126\n",
      "2022-03-27 01:36:04.905280 Epoch 250, Training Loss 0.1440344561853677\n",
      "2022-03-27 01:36:04.931286 Epoch 250, Training Loss 0.1450386591579603\n",
      "2022-03-27 01:36:04.957292 Epoch 250, Training Loss 0.14606505853440754\n",
      "2022-03-27 01:36:04.984298 Epoch 250, Training Loss 0.1470428324873795\n",
      "2022-03-27 01:36:05.011304 Epoch 250, Training Loss 0.14824498050353108\n",
      "2022-03-27 01:36:05.037448 Epoch 250, Training Loss 0.14940914786075388\n",
      "2022-03-27 01:36:05.064454 Epoch 250, Training Loss 0.1504892068140952\n",
      "2022-03-27 01:36:05.088459 Epoch 250, Training Loss 0.15166121988040407\n",
      "2022-03-27 01:36:05.107463 Epoch 250, Training Loss 0.15269588364664552\n",
      "2022-03-27 01:36:05.125468 Epoch 250, Training Loss 0.15387075308643644\n",
      "2022-03-27 01:36:05.145472 Epoch 250, Training Loss 0.15500901010640136\n",
      "2022-03-27 01:36:05.164476 Epoch 250, Training Loss 0.15614682268303678\n",
      "2022-03-27 01:36:05.183475 Epoch 250, Training Loss 0.15741559596317808\n",
      "2022-03-27 01:36:05.202485 Epoch 250, Training Loss 0.15849716736532538\n",
      "2022-03-27 01:36:05.221489 Epoch 250, Training Loss 0.15957011248144654\n",
      "2022-03-27 01:36:05.240093 Epoch 250, Training Loss 0.16062965386968744\n",
      "2022-03-27 01:36:05.259098 Epoch 250, Training Loss 0.1617185578626745\n",
      "2022-03-27 01:36:05.278102 Epoch 250, Training Loss 0.16309540945550668\n",
      "2022-03-27 01:36:05.297398 Epoch 250, Training Loss 0.16424589434548106\n",
      "2022-03-27 01:36:05.316104 Epoch 250, Training Loss 0.1653883176691392\n",
      "2022-03-27 01:36:05.334116 Epoch 250, Training Loss 0.16631520754845855\n",
      "2022-03-27 01:36:05.353113 Epoch 250, Training Loss 0.1672922765354976\n",
      "2022-03-27 01:36:05.372123 Epoch 250, Training Loss 0.16827467122041356\n",
      "2022-03-27 01:36:05.392127 Epoch 250, Training Loss 0.16938511284111102\n",
      "2022-03-27 01:36:05.411132 Epoch 250, Training Loss 0.17069124603820274\n",
      "2022-03-27 01:36:05.429136 Epoch 250, Training Loss 0.17172355511609247\n",
      "2022-03-27 01:36:05.448140 Epoch 250, Training Loss 0.1730363910155528\n",
      "2022-03-27 01:36:05.467139 Epoch 250, Training Loss 0.1741252951609814\n",
      "2022-03-27 01:36:05.486149 Epoch 250, Training Loss 0.17526100389183025\n",
      "2022-03-27 01:36:05.505153 Epoch 250, Training Loss 0.17641644275096982\n",
      "2022-03-27 01:36:05.524158 Epoch 250, Training Loss 0.1774367016294728\n",
      "2022-03-27 01:36:05.542162 Epoch 250, Training Loss 0.17863524668966718\n",
      "2022-03-27 01:36:05.561166 Epoch 250, Training Loss 0.17973461061182533\n",
      "2022-03-27 01:36:05.580170 Epoch 250, Training Loss 0.18101045153939815\n",
      "2022-03-27 01:36:05.600062 Epoch 250, Training Loss 0.18216005142997294\n",
      "2022-03-27 01:36:05.619060 Epoch 250, Training Loss 0.183091083222338\n",
      "2022-03-27 01:36:05.638077 Epoch 250, Training Loss 0.18416139834067402\n",
      "2022-03-27 01:36:05.657081 Epoch 250, Training Loss 0.1852357778555292\n",
      "2022-03-27 01:36:05.676085 Epoch 250, Training Loss 0.18616342719863443\n",
      "2022-03-27 01:36:05.695090 Epoch 250, Training Loss 0.1871899910595106\n",
      "2022-03-27 01:36:05.714094 Epoch 250, Training Loss 0.18867973857523535\n",
      "2022-03-27 01:36:05.732203 Epoch 250, Training Loss 0.18990546464920044\n",
      "2022-03-27 01:36:05.751220 Epoch 250, Training Loss 0.1911883309978963\n",
      "2022-03-27 01:36:05.769224 Epoch 250, Training Loss 0.19228601852036498\n",
      "2022-03-27 01:36:05.788228 Epoch 250, Training Loss 0.19321970096634478\n",
      "2022-03-27 01:36:05.808233 Epoch 250, Training Loss 0.1945375320704087\n",
      "2022-03-27 01:36:05.827237 Epoch 250, Training Loss 0.1958671982788369\n",
      "2022-03-27 01:36:05.845792 Epoch 250, Training Loss 0.19695948304422675\n",
      "2022-03-27 01:36:05.865803 Epoch 250, Training Loss 0.19826092180388663\n",
      "2022-03-27 01:36:05.884807 Epoch 250, Training Loss 0.19943246428314074\n",
      "2022-03-27 01:36:05.903812 Epoch 250, Training Loss 0.20060912155739183\n",
      "2022-03-27 01:36:05.921815 Epoch 250, Training Loss 0.20160084055817645\n",
      "2022-03-27 01:36:05.939807 Epoch 250, Training Loss 0.20263648650530355\n",
      "2022-03-27 01:36:05.958824 Epoch 250, Training Loss 0.2038086937059222\n",
      "2022-03-27 01:36:05.977828 Epoch 250, Training Loss 0.20480594526776266\n",
      "2022-03-27 01:36:05.996832 Epoch 250, Training Loss 0.20598204270043335\n",
      "2022-03-27 01:36:06.015831 Epoch 250, Training Loss 0.2072262207565405\n",
      "2022-03-27 01:36:06.034841 Epoch 250, Training Loss 0.2082749396333914\n",
      "2022-03-27 01:36:06.053845 Epoch 250, Training Loss 0.20923174662358315\n",
      "2022-03-27 01:36:06.072850 Epoch 250, Training Loss 0.21046785442420587\n",
      "2022-03-27 01:36:06.090854 Epoch 250, Training Loss 0.21174694022254262\n",
      "2022-03-27 01:36:06.109858 Epoch 250, Training Loss 0.212952626406994\n",
      "2022-03-27 01:36:06.128862 Epoch 250, Training Loss 0.2143007499330184\n",
      "2022-03-27 01:36:06.146860 Epoch 250, Training Loss 0.2153129371078423\n",
      "2022-03-27 01:36:06.165864 Epoch 250, Training Loss 0.2163925790573325\n",
      "2022-03-27 01:36:06.184875 Epoch 250, Training Loss 0.2175932463325198\n",
      "2022-03-27 01:36:06.203874 Epoch 250, Training Loss 0.21882608578638044\n",
      "2022-03-27 01:36:06.222878 Epoch 250, Training Loss 0.22008058101015018\n",
      "2022-03-27 01:36:06.241888 Epoch 250, Training Loss 0.2211417255499174\n",
      "2022-03-27 01:36:06.260892 Epoch 250, Training Loss 0.22248975379997507\n",
      "2022-03-27 01:36:06.280884 Epoch 250, Training Loss 0.223619747039912\n",
      "2022-03-27 01:36:06.298895 Epoch 250, Training Loss 0.22501162952169432\n",
      "2022-03-27 01:36:06.317905 Epoch 250, Training Loss 0.22597433897235508\n",
      "2022-03-27 01:36:06.335909 Epoch 250, Training Loss 0.2270951525634512\n",
      "2022-03-27 01:36:06.354914 Epoch 250, Training Loss 0.2283271729488812\n",
      "2022-03-27 01:36:06.373918 Epoch 250, Training Loss 0.22957825470153634\n",
      "2022-03-27 01:36:06.392916 Epoch 250, Training Loss 0.23070250462997904\n",
      "2022-03-27 01:36:06.411926 Epoch 250, Training Loss 0.23210638883473622\n",
      "2022-03-27 01:36:06.430925 Epoch 250, Training Loss 0.23324359164518468\n",
      "2022-03-27 01:36:06.449929 Epoch 250, Training Loss 0.23462214921136645\n",
      "2022-03-27 01:36:06.468939 Epoch 250, Training Loss 0.2356620419513234\n",
      "2022-03-27 01:36:06.487944 Epoch 250, Training Loss 0.2366818005928908\n",
      "2022-03-27 01:36:06.506948 Epoch 250, Training Loss 0.23767935810491558\n",
      "2022-03-27 01:36:06.525952 Epoch 250, Training Loss 0.23876165772033164\n",
      "2022-03-27 01:36:06.544957 Epoch 250, Training Loss 0.2397091209583575\n",
      "2022-03-27 01:36:06.562955 Epoch 250, Training Loss 0.24082347864994919\n",
      "2022-03-27 01:36:06.581965 Epoch 250, Training Loss 0.24202313821029176\n",
      "2022-03-27 01:36:06.600957 Epoch 250, Training Loss 0.24313061705330755\n",
      "2022-03-27 01:36:06.618967 Epoch 250, Training Loss 0.24426724272006003\n",
      "2022-03-27 01:36:06.638972 Epoch 250, Training Loss 0.2454863837765306\n",
      "2022-03-27 01:36:06.657976 Epoch 250, Training Loss 0.24672054535592608\n",
      "2022-03-27 01:36:06.676987 Epoch 250, Training Loss 0.24781413921309858\n",
      "2022-03-27 01:36:06.696130 Epoch 250, Training Loss 0.24901783595914426\n",
      "2022-03-27 01:36:06.715135 Epoch 250, Training Loss 0.2500340959909932\n",
      "2022-03-27 01:36:06.734133 Epoch 250, Training Loss 0.25103467077855257\n",
      "2022-03-27 01:36:06.752143 Epoch 250, Training Loss 0.25198880645930005\n",
      "2022-03-27 01:36:06.771141 Epoch 250, Training Loss 0.25307761136528173\n",
      "2022-03-27 01:36:06.790664 Epoch 250, Training Loss 0.2543803025846896\n",
      "2022-03-27 01:36:06.809670 Epoch 250, Training Loss 0.25559941414372084\n",
      "2022-03-27 01:36:06.827668 Epoch 250, Training Loss 0.2567039879081804\n",
      "2022-03-27 01:36:06.847678 Epoch 250, Training Loss 0.2578195790043267\n",
      "2022-03-27 01:36:06.866677 Epoch 250, Training Loss 0.25901589018609517\n",
      "2022-03-27 01:36:06.885676 Epoch 250, Training Loss 0.260156500827321\n",
      "2022-03-27 01:36:06.903691 Epoch 250, Training Loss 0.2610490791633001\n",
      "2022-03-27 01:36:06.923696 Epoch 250, Training Loss 0.262193066461007\n",
      "2022-03-27 01:36:06.942700 Epoch 250, Training Loss 0.263103242572921\n",
      "2022-03-27 01:36:06.962705 Epoch 250, Training Loss 0.26468311947629886\n",
      "2022-03-27 01:36:06.980708 Epoch 250, Training Loss 0.26610800372365184\n",
      "2022-03-27 01:36:06.999707 Epoch 250, Training Loss 0.26723685281355974\n",
      "2022-03-27 01:36:07.018717 Epoch 250, Training Loss 0.2686559151658012\n",
      "2022-03-27 01:36:07.037721 Epoch 250, Training Loss 0.26975185959540365\n",
      "2022-03-27 01:36:07.057726 Epoch 250, Training Loss 0.270903859297028\n",
      "2022-03-27 01:36:07.076730 Epoch 250, Training Loss 0.2720168729877228\n",
      "2022-03-27 01:36:07.094734 Epoch 250, Training Loss 0.27332716692439124\n",
      "2022-03-27 01:36:07.114726 Epoch 250, Training Loss 0.27450638613127687\n",
      "2022-03-27 01:36:07.132737 Epoch 250, Training Loss 0.2759001899863143\n",
      "2022-03-27 01:36:07.151747 Epoch 250, Training Loss 0.27711801638688577\n",
      "2022-03-27 01:36:07.170752 Epoch 250, Training Loss 0.2785105517758128\n",
      "2022-03-27 01:36:07.189756 Epoch 250, Training Loss 0.27979787551533536\n",
      "2022-03-27 01:36:07.208760 Epoch 250, Training Loss 0.28077641358156036\n",
      "2022-03-27 01:36:07.228765 Epoch 250, Training Loss 0.28169915918499\n",
      "2022-03-27 01:36:07.247769 Epoch 250, Training Loss 0.28281458030881174\n",
      "2022-03-27 01:36:07.267774 Epoch 250, Training Loss 0.28405719400976626\n",
      "2022-03-27 01:36:07.286778 Epoch 250, Training Loss 0.2851965024952998\n",
      "2022-03-27 01:36:07.305782 Epoch 250, Training Loss 0.28640808649075306\n",
      "2022-03-27 01:36:07.324787 Epoch 250, Training Loss 0.2874074061508374\n",
      "2022-03-27 01:36:07.343778 Epoch 250, Training Loss 0.28869223945281086\n",
      "2022-03-27 01:36:07.361795 Epoch 250, Training Loss 0.2899936570230957\n",
      "2022-03-27 01:36:07.381794 Epoch 250, Training Loss 0.29136842534975016\n",
      "2022-03-27 01:36:07.400798 Epoch 250, Training Loss 0.2925926065810806\n",
      "2022-03-27 01:36:07.419808 Epoch 250, Training Loss 0.29368082900791215\n",
      "2022-03-27 01:36:07.438806 Epoch 250, Training Loss 0.29476467506659915\n",
      "2022-03-27 01:36:07.457817 Epoch 250, Training Loss 0.2960259399907973\n",
      "2022-03-27 01:36:07.477821 Epoch 250, Training Loss 0.2974927523709319\n",
      "2022-03-27 01:36:07.543509 Epoch 250, Training Loss 0.2985753556499091\n",
      "2022-03-27 01:36:07.562513 Epoch 250, Training Loss 0.2998153284535079\n",
      "2022-03-27 01:36:07.580678 Epoch 250, Training Loss 0.3009539481319125\n",
      "2022-03-27 01:36:07.599693 Epoch 250, Training Loss 0.3021856220939275\n",
      "2022-03-27 01:36:07.619704 Epoch 250, Training Loss 0.3037152913067957\n",
      "2022-03-27 01:36:07.637708 Epoch 250, Training Loss 0.30486065797183826\n",
      "2022-03-27 01:36:07.657712 Epoch 250, Training Loss 0.30579431450275507\n",
      "2022-03-27 01:36:07.676717 Epoch 250, Training Loss 0.3070811089652274\n",
      "2022-03-27 01:36:07.695721 Epoch 250, Training Loss 0.308402055517182\n",
      "2022-03-27 01:36:07.722721 Epoch 250, Training Loss 0.30944359180567516\n",
      "2022-03-27 01:36:07.748733 Epoch 250, Training Loss 0.31086833260553265\n",
      "2022-03-27 01:36:07.775739 Epoch 250, Training Loss 0.31186412408223846\n",
      "2022-03-27 01:36:07.801733 Epoch 250, Training Loss 0.31298505482466327\n",
      "2022-03-27 01:36:07.828751 Epoch 250, Training Loss 0.31387859651499694\n",
      "2022-03-27 01:36:07.856757 Epoch 250, Training Loss 0.31537542128197066\n",
      "2022-03-27 01:36:07.882764 Epoch 250, Training Loss 0.3165628973327939\n",
      "2022-03-27 01:36:07.908769 Epoch 250, Training Loss 0.31760531496208955\n",
      "2022-03-27 01:36:07.927773 Epoch 250, Training Loss 0.31868802312085087\n",
      "2022-03-27 01:36:07.945772 Epoch 250, Training Loss 0.3197393694802014\n",
      "2022-03-27 01:36:07.965782 Epoch 250, Training Loss 0.32092390950683436\n",
      "2022-03-27 01:36:07.983780 Epoch 250, Training Loss 0.3218666331847305\n",
      "2022-03-27 01:36:08.001784 Epoch 250, Training Loss 0.3231303791713227\n",
      "2022-03-27 01:36:08.020795 Epoch 250, Training Loss 0.32425183408400593\n",
      "2022-03-27 01:36:08.039524 Epoch 250, Training Loss 0.32525313982878196\n",
      "2022-03-27 01:36:08.057528 Epoch 250, Training Loss 0.3264899989375678\n",
      "2022-03-27 01:36:08.076533 Epoch 250, Training Loss 0.3275919260881136\n",
      "2022-03-27 01:36:08.094706 Epoch 250, Training Loss 0.3286136558751011\n",
      "2022-03-27 01:36:08.114710 Epoch 250, Training Loss 0.3294622175528875\n",
      "2022-03-27 01:36:08.133715 Epoch 250, Training Loss 0.3305893935968199\n",
      "2022-03-27 01:36:08.151719 Epoch 250, Training Loss 0.3319655250557853\n",
      "2022-03-27 01:36:08.170729 Epoch 250, Training Loss 0.333226921308376\n",
      "2022-03-27 01:36:08.189734 Epoch 250, Training Loss 0.3344510598560733\n",
      "2022-03-27 01:36:08.208738 Epoch 250, Training Loss 0.3356571240193399\n",
      "2022-03-27 01:36:08.227742 Epoch 250, Training Loss 0.33688239795167735\n",
      "2022-03-27 01:36:08.246982 Epoch 250, Training Loss 0.3378669552485961\n",
      "2022-03-27 01:36:08.265745 Epoch 250, Training Loss 0.3391049485987105\n",
      "2022-03-27 01:36:08.284755 Epoch 250, Training Loss 0.3403602493236132\n",
      "2022-03-27 01:36:08.303759 Epoch 250, Training Loss 0.34176446905221475\n",
      "2022-03-27 01:36:08.323764 Epoch 250, Training Loss 0.3427053249400595\n",
      "2022-03-27 01:36:08.342762 Epoch 250, Training Loss 0.34356976393848426\n",
      "2022-03-27 01:36:08.361772 Epoch 250, Training Loss 0.3446859395717416\n",
      "2022-03-27 01:36:08.380764 Epoch 250, Training Loss 0.34570634090687\n",
      "2022-03-27 01:36:08.398772 Epoch 250, Training Loss 0.3469930858258396\n",
      "2022-03-27 01:36:08.417786 Epoch 250, Training Loss 0.34814073316886296\n",
      "2022-03-27 01:36:08.436794 Epoch 250, Training Loss 0.34930905821683156\n",
      "2022-03-27 01:36:08.456794 Epoch 250, Training Loss 0.35041612577255427\n",
      "2022-03-27 01:36:08.474798 Epoch 250, Training Loss 0.3513876008408149\n",
      "2022-03-27 01:36:08.493802 Epoch 250, Training Loss 0.3527634370205043\n",
      "2022-03-27 01:36:08.511800 Epoch 250, Training Loss 0.35386487475746425\n",
      "2022-03-27 01:36:08.531811 Epoch 250, Training Loss 0.3550707434906679\n",
      "2022-03-27 01:36:08.550809 Epoch 250, Training Loss 0.35618337927876836\n",
      "2022-03-27 01:36:08.569820 Epoch 250, Training Loss 0.3571022855656226\n",
      "2022-03-27 01:36:08.588582 Epoch 250, Training Loss 0.3581847453971043\n",
      "2022-03-27 01:36:08.607580 Epoch 250, Training Loss 0.35909977036973706\n",
      "2022-03-27 01:36:08.626591 Epoch 250, Training Loss 0.3603045912197484\n",
      "2022-03-27 01:36:08.645791 Epoch 250, Training Loss 0.36143263671404263\n",
      "2022-03-27 01:36:08.664783 Epoch 250, Training Loss 0.3622559767092585\n",
      "2022-03-27 01:36:08.682788 Epoch 250, Training Loss 0.3634987995600152\n",
      "2022-03-27 01:36:08.701804 Epoch 250, Training Loss 0.3648651656134964\n",
      "2022-03-27 01:36:08.720808 Epoch 250, Training Loss 0.36601911367052964\n",
      "2022-03-27 01:36:08.739813 Epoch 250, Training Loss 0.3671532668878355\n",
      "2022-03-27 01:36:08.758817 Epoch 250, Training Loss 0.36808831742047654\n",
      "2022-03-27 01:36:08.777815 Epoch 250, Training Loss 0.3690502086411352\n",
      "2022-03-27 01:36:08.797018 Epoch 250, Training Loss 0.37000796518972157\n",
      "2022-03-27 01:36:08.815824 Epoch 250, Training Loss 0.3712800055970926\n",
      "2022-03-27 01:36:08.835419 Epoch 250, Training Loss 0.37254578004712646\n",
      "2022-03-27 01:36:08.854423 Epoch 250, Training Loss 0.3737230749843675\n",
      "2022-03-27 01:36:08.873427 Epoch 250, Training Loss 0.37486734391783205\n",
      "2022-03-27 01:36:08.891620 Epoch 250, Training Loss 0.3760514367571877\n",
      "2022-03-27 01:36:08.910625 Epoch 250, Training Loss 0.3770798684081153\n",
      "2022-03-27 01:36:08.929629 Epoch 250, Training Loss 0.37828112471743924\n",
      "2022-03-27 01:36:08.948622 Epoch 250, Training Loss 0.3793729005567253\n",
      "2022-03-27 01:36:08.967625 Epoch 250, Training Loss 0.38054088543138237\n",
      "2022-03-27 01:36:08.985486 Epoch 250, Training Loss 0.3816669473562704\n",
      "2022-03-27 01:36:09.004490 Epoch 250, Training Loss 0.38301090709388713\n",
      "2022-03-27 01:36:09.022494 Epoch 250, Training Loss 0.38430988544698264\n",
      "2022-03-27 01:36:09.041499 Epoch 250, Training Loss 0.3854448658883419\n",
      "2022-03-27 01:36:09.060503 Epoch 250, Training Loss 0.386623247505149\n",
      "2022-03-27 01:36:09.079508 Epoch 250, Training Loss 0.3876847450233177\n",
      "2022-03-27 01:36:09.098511 Epoch 250, Training Loss 0.38870969597640853\n",
      "2022-03-27 01:36:09.117516 Epoch 250, Training Loss 0.38985667166197696\n",
      "2022-03-27 01:36:09.135520 Epoch 250, Training Loss 0.39076572069731513\n",
      "2022-03-27 01:36:09.154524 Epoch 250, Training Loss 0.39200465415444824\n",
      "2022-03-27 01:36:09.174523 Epoch 250, Training Loss 0.39324944380604093\n",
      "2022-03-27 01:36:09.192533 Epoch 250, Training Loss 0.3946079641504361\n",
      "2022-03-27 01:36:09.211537 Epoch 250, Training Loss 0.39600045487398994\n",
      "2022-03-27 01:36:09.230542 Epoch 250, Training Loss 0.39697265259140285\n",
      "2022-03-27 01:36:09.250534 Epoch 250, Training Loss 0.3980317779666627\n",
      "2022-03-27 01:36:09.268550 Epoch 250, Training Loss 0.39907920078548326\n",
      "2022-03-27 01:36:09.287555 Epoch 250, Training Loss 0.40032879226957746\n",
      "2022-03-27 01:36:09.305559 Epoch 250, Training Loss 0.4016205932173278\n",
      "2022-03-27 01:36:09.324563 Epoch 250, Training Loss 0.4027213780471431\n",
      "2022-03-27 01:36:09.343568 Epoch 250, Training Loss 0.4038821015211627\n",
      "2022-03-27 01:36:09.363559 Epoch 250, Training Loss 0.4050309206061351\n",
      "2022-03-27 01:36:09.382570 Epoch 250, Training Loss 0.4062426799855879\n",
      "2022-03-27 01:36:09.402574 Epoch 250, Training Loss 0.4072361314845512\n",
      "2022-03-27 01:36:09.421585 Epoch 250, Training Loss 0.40843924407458976\n",
      "2022-03-27 01:36:09.440589 Epoch 250, Training Loss 0.4097248795239822\n",
      "2022-03-27 01:36:09.459594 Epoch 250, Training Loss 0.4110314615089875\n",
      "2022-03-27 01:36:09.478598 Epoch 250, Training Loss 0.4119863209821989\n",
      "2022-03-27 01:36:09.497602 Epoch 250, Training Loss 0.4130505904212327\n",
      "2022-03-27 01:36:09.516586 Epoch 250, Training Loss 0.4144017932665013\n",
      "2022-03-27 01:36:09.534605 Epoch 250, Training Loss 0.41566945120806587\n",
      "2022-03-27 01:36:09.553609 Epoch 250, Training Loss 0.41697038066051806\n",
      "2022-03-27 01:36:09.572619 Epoch 250, Training Loss 0.41807607288860604\n",
      "2022-03-27 01:36:09.591624 Epoch 250, Training Loss 0.41939722249270095\n",
      "2022-03-27 01:36:09.610628 Epoch 250, Training Loss 0.420675771087027\n",
      "2022-03-27 01:36:09.628626 Epoch 250, Training Loss 0.42149986475324996\n",
      "2022-03-27 01:36:09.648167 Epoch 250, Training Loss 0.4225920030985342\n",
      "2022-03-27 01:36:09.667183 Epoch 250, Training Loss 0.42349133154620294\n",
      "2022-03-27 01:36:09.686182 Epoch 250, Training Loss 0.4248010879739776\n",
      "2022-03-27 01:36:09.705192 Epoch 250, Training Loss 0.4259286934457472\n",
      "2022-03-27 01:36:09.724196 Epoch 250, Training Loss 0.42691763145539463\n",
      "2022-03-27 01:36:09.743194 Epoch 250, Training Loss 0.42824206998586045\n",
      "2022-03-27 01:36:09.762205 Epoch 250, Training Loss 0.4294917875391138\n",
      "2022-03-27 01:36:09.782203 Epoch 250, Training Loss 0.4304285343650662\n",
      "2022-03-27 01:36:09.801208 Epoch 250, Training Loss 0.4315158798719001\n",
      "2022-03-27 01:36:09.820218 Epoch 250, Training Loss 0.4326696153491964\n",
      "2022-03-27 01:36:09.839222 Epoch 250, Training Loss 0.43380381010682384\n",
      "2022-03-27 01:36:09.858221 Epoch 250, Training Loss 0.43530541955662505\n",
      "2022-03-27 01:36:09.877231 Epoch 250, Training Loss 0.43646576177433627\n",
      "2022-03-27 01:36:09.896235 Epoch 250, Training Loss 0.4375372086187153\n",
      "2022-03-27 01:36:09.915234 Epoch 250, Training Loss 0.43852339261937934\n",
      "2022-03-27 01:36:09.935238 Epoch 250, Training Loss 0.43966638576953915\n",
      "2022-03-27 01:36:09.954248 Epoch 250, Training Loss 0.44086000940683856\n",
      "2022-03-27 01:36:09.973253 Epoch 250, Training Loss 0.44202270265430443\n",
      "2022-03-27 01:36:09.993257 Epoch 250, Training Loss 0.4433696141175907\n",
      "2022-03-27 01:36:10.012249 Epoch 250, Training Loss 0.4447659576487968\n",
      "2022-03-27 01:36:10.031253 Epoch 250, Training Loss 0.4458826530315077\n",
      "2022-03-27 01:36:10.049264 Epoch 250, Training Loss 0.4469437612901868\n",
      "2022-03-27 01:36:10.069275 Epoch 250, Training Loss 0.4479470933642229\n",
      "2022-03-27 01:36:10.088279 Epoch 250, Training Loss 0.44912359834936877\n",
      "2022-03-27 01:36:10.107283 Epoch 250, Training Loss 0.4504023660784182\n",
      "2022-03-27 01:36:10.126287 Epoch 250, Training Loss 0.4516826948851271\n",
      "2022-03-27 01:36:10.144526 Epoch 250, Training Loss 0.4525469064407641\n",
      "2022-03-27 01:36:10.164531 Epoch 250, Training Loss 0.4537781566152792\n",
      "2022-03-27 01:36:10.183529 Epoch 250, Training Loss 0.454801111193874\n",
      "2022-03-27 01:36:10.202539 Epoch 250, Training Loss 0.45587453611976353\n",
      "2022-03-27 01:36:10.221543 Epoch 250, Training Loss 0.45695642879247056\n",
      "2022-03-27 01:36:10.241548 Epoch 250, Training Loss 0.4587167041076114\n",
      "2022-03-27 01:36:10.260552 Epoch 250, Training Loss 0.46005985163666707\n",
      "2022-03-27 01:36:10.279557 Epoch 250, Training Loss 0.4611755958603471\n",
      "2022-03-27 01:36:10.298555 Epoch 250, Training Loss 0.46197175933881796\n",
      "2022-03-27 01:36:10.318565 Epoch 250, Training Loss 0.4631041135934308\n",
      "2022-03-27 01:36:10.337570 Epoch 250, Training Loss 0.4641173619900823\n",
      "2022-03-27 01:36:10.356574 Epoch 250, Training Loss 0.4650701062606119\n",
      "2022-03-27 01:36:10.375578 Epoch 250, Training Loss 0.46597762272485993\n",
      "2022-03-27 01:36:10.395583 Epoch 250, Training Loss 0.46693231001534424\n",
      "2022-03-27 01:36:10.414581 Epoch 250, Training Loss 0.46809939716173254\n",
      "2022-03-27 01:36:10.434592 Epoch 250, Training Loss 0.469357871186093\n",
      "2022-03-27 01:36:10.452596 Epoch 250, Training Loss 0.4705019299788853\n",
      "2022-03-27 01:36:10.472600 Epoch 250, Training Loss 0.47148269353925115\n",
      "2022-03-27 01:36:10.491605 Epoch 250, Training Loss 0.47248194825923656\n",
      "2022-03-27 01:36:10.510603 Epoch 250, Training Loss 0.47362958691309176\n",
      "2022-03-27 01:36:10.534608 Epoch 250, Training Loss 0.47480864697100256\n",
      "2022-03-27 01:36:10.560620 Epoch 250, Training Loss 0.4759785007028019\n",
      "2022-03-27 01:36:10.587626 Epoch 250, Training Loss 0.4773069691779973\n",
      "2022-03-27 01:36:10.613632 Epoch 250, Training Loss 0.47847081274937486\n",
      "2022-03-27 01:36:10.639638 Epoch 250, Training Loss 0.4798253273872463\n",
      "2022-03-27 01:36:10.666644 Epoch 250, Training Loss 0.48084384484974013\n",
      "2022-03-27 01:36:10.693651 Epoch 250, Training Loss 0.4820540607585322\n",
      "2022-03-27 01:36:10.720657 Epoch 250, Training Loss 0.48314645779712123\n",
      "2022-03-27 01:36:10.735647 Epoch 250, Training Loss 0.4842314664512644\n",
      "2022-03-27 01:36:10.750651 Epoch 250, Training Loss 0.4856666141306348\n",
      "2022-03-27 01:36:10.765654 Epoch 250, Training Loss 0.48680771037440773\n",
      "2022-03-27 01:36:10.780658 Epoch 250, Training Loss 0.4877365900732367\n",
      "2022-03-27 01:36:10.795662 Epoch 250, Training Loss 0.48873716547056234\n",
      "2022-03-27 01:36:10.810664 Epoch 250, Training Loss 0.49026426253721234\n",
      "2022-03-27 01:36:10.825669 Epoch 250, Training Loss 0.49145314226979797\n",
      "2022-03-27 01:36:10.841672 Epoch 250, Training Loss 0.4925276179752691\n",
      "2022-03-27 01:36:10.856677 Epoch 250, Training Loss 0.4934193222114192\n",
      "2022-03-27 01:36:10.871680 Epoch 250, Training Loss 0.4945015493408798\n",
      "2022-03-27 01:36:10.886682 Epoch 250, Training Loss 0.49582130669632835\n",
      "2022-03-27 01:36:10.901686 Epoch 250, Training Loss 0.49717296686623713\n",
      "2022-03-27 01:36:10.916690 Epoch 250, Training Loss 0.49847976348894024\n",
      "2022-03-27 01:36:10.931692 Epoch 250, Training Loss 0.49955119204033366\n",
      "2022-03-27 01:36:10.945697 Epoch 250, Training Loss 0.5008045168941283\n",
      "2022-03-27 01:36:10.960700 Epoch 250, Training Loss 0.5019028800375321\n",
      "2022-03-27 01:36:10.975704 Epoch 250, Training Loss 0.5030256964056693\n",
      "2022-03-27 01:36:10.991130 Epoch 250, Training Loss 0.5041472706800837\n",
      "2022-03-27 01:36:11.006133 Epoch 250, Training Loss 0.5052282424534068\n",
      "2022-03-27 01:36:11.021135 Epoch 250, Training Loss 0.5062335621365501\n",
      "2022-03-27 01:36:11.036140 Epoch 250, Training Loss 0.5071651235870693\n",
      "2022-03-27 01:36:11.051142 Epoch 250, Training Loss 0.5083812622310561\n",
      "2022-03-27 01:36:11.066146 Epoch 250, Training Loss 0.5096550109746206\n",
      "2022-03-27 01:36:11.081149 Epoch 250, Training Loss 0.510949041379992\n",
      "2022-03-27 01:36:11.096152 Epoch 250, Training Loss 0.5119552730446886\n",
      "2022-03-27 01:36:11.111156 Epoch 250, Training Loss 0.5130909079938288\n",
      "2022-03-27 01:36:11.126160 Epoch 250, Training Loss 0.5143537562521522\n",
      "2022-03-27 01:36:11.141551 Epoch 250, Training Loss 0.5154484112549316\n",
      "2022-03-27 01:36:11.156554 Epoch 250, Training Loss 0.5166386765287355\n",
      "2022-03-27 01:36:11.171558 Epoch 250, Training Loss 0.5176535196164075\n",
      "2022-03-27 01:36:11.185562 Epoch 250, Training Loss 0.518570574729339\n",
      "2022-03-27 01:36:11.200564 Epoch 250, Training Loss 0.5198408731109346\n",
      "2022-03-27 01:36:11.215568 Epoch 250, Training Loss 0.5212288701625736\n",
      "2022-03-27 01:36:11.230571 Epoch 250, Training Loss 0.522521289260796\n",
      "2022-03-27 01:36:11.245574 Epoch 250, Training Loss 0.5238748799504527\n",
      "2022-03-27 01:36:11.260579 Epoch 250, Training Loss 0.5251523097762671\n",
      "2022-03-27 01:36:11.275582 Epoch 250, Training Loss 0.5263284946341649\n",
      "2022-03-27 01:36:11.291100 Epoch 250, Training Loss 0.5275978977265565\n",
      "2022-03-27 01:36:11.306103 Epoch 250, Training Loss 0.5286149830007187\n",
      "2022-03-27 01:36:11.321106 Epoch 250, Training Loss 0.5299490562942631\n",
      "2022-03-27 01:36:11.336110 Epoch 250, Training Loss 0.5312744446880068\n",
      "2022-03-27 01:36:11.351113 Epoch 250, Training Loss 0.5325766012949102\n",
      "2022-03-27 01:36:11.366123 Epoch 250, Training Loss 0.5339954595279206\n",
      "2022-03-27 01:36:11.381121 Epoch 250, Training Loss 0.5349951838440907\n",
      "2022-03-27 01:36:11.396124 Epoch 250, Training Loss 0.536252444128856\n",
      "2022-03-27 01:36:11.411127 Epoch 250, Training Loss 0.5376118885739075\n",
      "2022-03-27 01:36:11.426130 Epoch 250, Training Loss 0.5387109861806836\n",
      "2022-03-27 01:36:11.442134 Epoch 250, Training Loss 0.5396188104244144\n",
      "2022-03-27 01:36:11.457138 Epoch 250, Training Loss 0.5407396660131567\n",
      "2022-03-27 01:36:11.472141 Epoch 250, Training Loss 0.541928779591075\n",
      "2022-03-27 01:36:11.487144 Epoch 250, Training Loss 0.5435632383427047\n",
      "2022-03-27 01:36:11.502149 Epoch 250, Training Loss 0.544617557159775\n",
      "2022-03-27 01:36:11.517152 Epoch 250, Training Loss 0.5460633870280917\n",
      "2022-03-27 01:36:11.532156 Epoch 250, Training Loss 0.5472998660238807\n",
      "2022-03-27 01:36:11.547157 Epoch 250, Training Loss 0.5482200650912722\n",
      "2022-03-27 01:36:11.562168 Epoch 250, Training Loss 0.5490974210717184\n",
      "2022-03-27 01:36:11.577164 Epoch 250, Training Loss 0.5501210671251692\n",
      "2022-03-27 01:36:11.592168 Epoch 250, Training Loss 0.5513967085829781\n",
      "2022-03-27 01:36:11.607171 Epoch 250, Training Loss 0.5527179359322618\n",
      "2022-03-27 01:36:11.622176 Epoch 250, Training Loss 0.5538122756859226\n",
      "2022-03-27 01:36:11.637506 Epoch 250, Training Loss 0.5548506448488406\n",
      "2022-03-27 01:36:11.653509 Epoch 250, Training Loss 0.5560084504392141\n",
      "2022-03-27 01:36:11.668514 Epoch 250, Training Loss 0.5572765317871747\n",
      "2022-03-27 01:36:11.683516 Epoch 250, Training Loss 0.5582963343319076\n",
      "2022-03-27 01:36:11.697520 Epoch 250, Training Loss 0.5597160988299134\n",
      "2022-03-27 01:36:11.713523 Epoch 250, Training Loss 0.5608861234486865\n",
      "2022-03-27 01:36:11.727527 Epoch 250, Training Loss 0.5619022010842247\n",
      "2022-03-27 01:36:11.742529 Epoch 250, Training Loss 0.5632277010651805\n",
      "2022-03-27 01:36:11.758533 Epoch 250, Training Loss 0.564317766586533\n",
      "2022-03-27 01:36:11.773536 Epoch 250, Training Loss 0.56554769234889\n",
      "2022-03-27 01:36:11.787540 Epoch 250, Training Loss 0.5665794136121755\n",
      "2022-03-27 01:36:11.802543 Epoch 250, Training Loss 0.5676422847049011\n",
      "2022-03-27 01:36:11.817548 Epoch 250, Training Loss 0.5686384812950174\n",
      "2022-03-27 01:36:11.832549 Epoch 250, Training Loss 0.5697489667426595\n",
      "2022-03-27 01:36:11.847553 Epoch 250, Training Loss 0.5708951703880144\n",
      "2022-03-27 01:36:11.862558 Epoch 250, Training Loss 0.5719648940331491\n",
      "2022-03-27 01:36:11.877560 Epoch 250, Training Loss 0.5732261172645842\n",
      "2022-03-27 01:36:11.893570 Epoch 250, Training Loss 0.5744898406898274\n",
      "2022-03-27 01:36:11.908567 Epoch 250, Training Loss 0.5756271688834481\n",
      "2022-03-27 01:36:11.923577 Epoch 250, Training Loss 0.5768248798597194\n",
      "2022-03-27 01:36:11.937575 Epoch 250, Training Loss 0.5780502408361801\n",
      "2022-03-27 01:36:11.953579 Epoch 250, Training Loss 0.57923683447911\n",
      "2022-03-27 01:36:11.968576 Epoch 250, Training Loss 0.5805343347589683\n",
      "2022-03-27 01:36:11.995580 Epoch 250, Training Loss 0.5816546500186481\n",
      "2022-03-27 01:36:12.016586 Epoch 250, Training Loss 0.5826900430652492\n",
      "2022-03-27 01:36:12.034591 Epoch 250, Training Loss 0.5836870487388748\n",
      "2022-03-27 01:36:12.049593 Epoch 250, Training Loss 0.5850367982064366\n",
      "2022-03-27 01:36:12.065597 Epoch 250, Training Loss 0.5860676987244345\n",
      "2022-03-27 01:36:12.081604 Epoch 250, Training Loss 0.5872617428717406\n",
      "2022-03-27 01:36:12.098605 Epoch 250, Training Loss 0.5883110692281552\n",
      "2022-03-27 01:36:12.113608 Epoch 250, Training Loss 0.5897169309046567\n",
      "2022-03-27 01:36:12.129610 Epoch 250, Training Loss 0.5909852751380648\n",
      "2022-03-27 01:36:12.143614 Epoch 250, Training Loss 0.5923134759259041\n",
      "2022-03-27 01:36:12.159616 Epoch 250, Training Loss 0.5936572647765469\n",
      "2022-03-27 01:36:12.173621 Epoch 250, Training Loss 0.5948518752441991\n",
      "2022-03-27 01:36:12.189631 Epoch 250, Training Loss 0.595951219425177\n",
      "2022-03-27 01:36:12.203634 Epoch 250, Training Loss 0.5972445027145279\n",
      "2022-03-27 01:36:12.218637 Epoch 250, Training Loss 0.5984732006364466\n",
      "2022-03-27 01:36:12.233641 Epoch 250, Training Loss 0.5995055776270454\n",
      "2022-03-27 01:36:12.248644 Epoch 250, Training Loss 0.6008853916164554\n",
      "2022-03-27 01:36:12.263647 Epoch 250, Training Loss 0.6019495628069124\n",
      "2022-03-27 01:36:12.278652 Epoch 250, Training Loss 0.6032256062530801\n",
      "2022-03-27 01:36:12.293654 Epoch 250, Training Loss 0.6043041413244994\n",
      "2022-03-27 01:36:12.308657 Epoch 250, Training Loss 0.6057202062186073\n",
      "2022-03-27 01:36:12.323661 Epoch 250, Training Loss 0.606695148417407\n",
      "2022-03-27 01:36:12.338664 Epoch 250, Training Loss 0.6078421070295221\n",
      "2022-03-27 01:36:12.353668 Epoch 250, Training Loss 0.6089425505427144\n",
      "2022-03-27 01:36:12.368671 Epoch 250, Training Loss 0.6102124909915583\n",
      "2022-03-27 01:36:12.382675 Epoch 250, Training Loss 0.6111805482441203\n",
      "2022-03-27 01:36:12.397679 Epoch 250, Training Loss 0.6125257287336432\n",
      "2022-03-27 01:36:12.412683 Epoch 250, Training Loss 0.613575038931254\n",
      "2022-03-27 01:36:12.427684 Epoch 250, Training Loss 0.6146480260450212\n",
      "2022-03-27 01:36:12.442689 Epoch 250, Training Loss 0.6159533755401211\n",
      "2022-03-27 01:36:12.458692 Epoch 250, Training Loss 0.6170129872801359\n",
      "2022-03-27 01:36:12.473697 Epoch 250, Training Loss 0.6182464556315975\n",
      "2022-03-27 01:36:12.489019 Epoch 250, Training Loss 0.6194368747189222\n",
      "2022-03-27 01:36:12.504023 Epoch 250, Training Loss 0.6206794589986582\n",
      "2022-03-27 01:36:12.519028 Epoch 250, Training Loss 0.6219463109055443\n",
      "2022-03-27 01:36:12.538043 Epoch 250, Training Loss 0.6231281154448419\n",
      "2022-03-27 01:36:12.558047 Epoch 250, Training Loss 0.6242687413302224\n",
      "2022-03-27 01:36:12.577051 Epoch 250, Training Loss 0.6252782331860584\n",
      "2022-03-27 01:36:12.597385 Epoch 250, Training Loss 0.6266908417729771\n",
      "2022-03-27 01:36:12.616390 Epoch 250, Training Loss 0.6277080348233128\n",
      "2022-03-27 01:36:12.636394 Epoch 250, Training Loss 0.6288077684162218\n",
      "2022-03-27 01:36:12.656399 Epoch 250, Training Loss 0.6299569313331028\n",
      "2022-03-27 01:36:12.677404 Epoch 250, Training Loss 0.6313446059708705\n",
      "2022-03-27 01:36:12.697396 Epoch 250, Training Loss 0.6325758474562174\n",
      "2022-03-27 01:36:12.716412 Epoch 250, Training Loss 0.6334230794626123\n",
      "2022-03-27 01:36:12.735417 Epoch 250, Training Loss 0.6347846382719171\n",
      "2022-03-27 01:36:12.755421 Epoch 250, Training Loss 0.6361104774353145\n",
      "2022-03-27 01:36:12.774426 Epoch 250, Training Loss 0.6372944329248365\n",
      "2022-03-27 01:36:12.794430 Epoch 250, Training Loss 0.638586702630343\n",
      "2022-03-27 01:36:12.813435 Epoch 250, Training Loss 0.6397617904426497\n",
      "2022-03-27 01:36:12.833439 Epoch 250, Training Loss 0.6407539189014289\n",
      "2022-03-27 01:36:12.852437 Epoch 250, Training Loss 0.641865625558302\n",
      "2022-03-27 01:36:12.872448 Epoch 250, Training Loss 0.6428279794390549\n",
      "2022-03-27 01:36:12.891452 Epoch 250, Training Loss 0.6437849565540128\n",
      "2022-03-27 01:36:12.911456 Epoch 250, Training Loss 0.644868709394694\n",
      "2022-03-27 01:36:12.931457 Epoch 250, Training Loss 0.6460160013964719\n",
      "2022-03-27 01:36:12.950467 Epoch 250, Training Loss 0.6468820180886846\n",
      "2022-03-27 01:36:12.970470 Epoch 250, Training Loss 0.6481470146295055\n",
      "2022-03-27 01:36:12.989474 Epoch 250, Training Loss 0.6494441629218324\n",
      "2022-03-27 01:36:13.009479 Epoch 250, Training Loss 0.6505808179335826\n",
      "2022-03-27 01:36:13.028477 Epoch 250, Training Loss 0.6515853101640101\n",
      "2022-03-27 01:36:13.048481 Epoch 250, Training Loss 0.6526757661643845\n",
      "2022-03-27 01:36:13.068492 Epoch 250, Training Loss 0.6540935167571162\n",
      "2022-03-27 01:36:13.088497 Epoch 250, Training Loss 0.6554121259228348\n",
      "2022-03-27 01:36:13.109501 Epoch 250, Training Loss 0.656775990410534\n",
      "2022-03-27 01:36:13.128506 Epoch 250, Training Loss 0.6577025736536821\n",
      "2022-03-27 01:36:13.148504 Epoch 250, Training Loss 0.6588567495346069\n",
      "2022-03-27 01:36:13.167515 Epoch 250, Training Loss 0.6601716924811263\n",
      "2022-03-27 01:36:13.188513 Epoch 250, Training Loss 0.6612504335010753\n",
      "2022-03-27 01:36:13.207524 Epoch 250, Training Loss 0.6624026053854267\n",
      "2022-03-27 01:36:13.227528 Epoch 250, Training Loss 0.663379682985413\n",
      "2022-03-27 01:36:13.246517 Epoch 250, Training Loss 0.6642566975730154\n",
      "2022-03-27 01:36:13.266531 Epoch 250, Training Loss 0.6653068428454192\n",
      "2022-03-27 01:36:13.285541 Epoch 250, Training Loss 0.6664920483556245\n",
      "2022-03-27 01:36:13.305546 Epoch 250, Training Loss 0.6676506862768432\n",
      "2022-03-27 01:36:13.325551 Epoch 250, Training Loss 0.6688784177955764\n",
      "2022-03-27 01:36:13.345549 Epoch 250, Training Loss 0.6700319124152289\n",
      "2022-03-27 01:36:13.364553 Epoch 250, Training Loss 0.6710041599810276\n",
      "2022-03-27 01:36:13.384564 Epoch 250, Training Loss 0.6724227438192538\n",
      "2022-03-27 01:36:13.404568 Epoch 250, Training Loss 0.6735581744205007\n",
      "2022-03-27 01:36:13.425573 Epoch 250, Training Loss 0.6746897264514737\n",
      "2022-03-27 01:36:13.445578 Epoch 250, Training Loss 0.6759712686928947\n",
      "2022-03-27 01:36:13.465576 Epoch 250, Training Loss 0.6770924134632511\n",
      "2022-03-27 01:36:13.484587 Epoch 250, Training Loss 0.6783647794095452\n",
      "2022-03-27 01:36:13.504591 Epoch 250, Training Loss 0.6794662824677079\n",
      "2022-03-27 01:36:13.524596 Epoch 250, Training Loss 0.6806360561676952\n",
      "2022-03-27 01:36:13.544601 Epoch 250, Training Loss 0.681696182474151\n",
      "2022-03-27 01:36:13.564605 Epoch 250, Training Loss 0.6827747523784637\n",
      "2022-03-27 01:36:13.583603 Epoch 250, Training Loss 0.6837159888366299\n",
      "2022-03-27 01:36:13.603613 Epoch 250, Training Loss 0.6847722235390598\n",
      "2022-03-27 01:36:13.623618 Epoch 250, Training Loss 0.6860565479911501\n",
      "2022-03-27 01:36:13.642622 Epoch 250, Training Loss 0.6873665022880525\n",
      "2022-03-27 01:36:13.662627 Epoch 250, Training Loss 0.6884623478593119\n",
      "2022-03-27 01:36:13.682625 Epoch 250, Training Loss 0.6898715440422067\n",
      "2022-03-27 01:36:13.701636 Epoch 250, Training Loss 0.6910761410318067\n",
      "2022-03-27 01:36:13.721640 Epoch 250, Training Loss 0.6924021519968272\n",
      "2022-03-27 01:36:13.741645 Epoch 250, Training Loss 0.6933930855425422\n",
      "2022-03-27 01:36:13.761649 Epoch 250, Training Loss 0.6945337638678148\n",
      "2022-03-27 01:36:13.782648 Epoch 250, Training Loss 0.6956698165067932\n",
      "2022-03-27 01:36:13.802652 Epoch 250, Training Loss 0.696959363911158\n",
      "2022-03-27 01:36:13.822663 Epoch 250, Training Loss 0.6981587032680316\n",
      "2022-03-27 01:36:13.841667 Epoch 250, Training Loss 0.6992148109104322\n",
      "2022-03-27 01:36:13.861672 Epoch 250, Training Loss 0.7003936390285297\n",
      "2022-03-27 01:36:13.881670 Epoch 250, Training Loss 0.7016770277944062\n",
      "2022-03-27 01:36:13.901681 Epoch 250, Training Loss 0.7026698161726412\n",
      "2022-03-27 01:36:13.920685 Epoch 250, Training Loss 0.7040221710186785\n",
      "2022-03-27 01:36:13.940690 Epoch 250, Training Loss 0.7051507983823566\n",
      "2022-03-27 01:36:13.960694 Epoch 250, Training Loss 0.7062102249058921\n",
      "2022-03-27 01:36:13.980699 Epoch 250, Training Loss 0.7071693615840219\n",
      "2022-03-27 01:36:14.000703 Epoch 250, Training Loss 0.7079619877326214\n",
      "2022-03-27 01:36:14.020708 Epoch 250, Training Loss 0.7091995516548986\n",
      "2022-03-27 01:36:14.039706 Epoch 250, Training Loss 0.7103639700833488\n",
      "2022-03-27 01:36:14.059717 Epoch 250, Training Loss 0.7115277826328716\n",
      "2022-03-27 01:36:14.080709 Epoch 250, Training Loss 0.7124530248477331\n",
      "2022-03-27 01:36:14.099726 Epoch 250, Training Loss 0.7137989878197155\n",
      "2022-03-27 01:36:14.119731 Epoch 250, Training Loss 0.7149463642741103\n",
      "2022-03-27 01:36:14.138729 Epoch 250, Training Loss 0.7158471532642384\n",
      "2022-03-27 01:36:14.158739 Epoch 250, Training Loss 0.7169166365852746\n",
      "2022-03-27 01:36:14.179745 Epoch 250, Training Loss 0.7179771953683984\n",
      "2022-03-27 01:36:14.198742 Epoch 250, Training Loss 0.7191258433377347\n",
      "2022-03-27 01:36:14.218753 Epoch 250, Training Loss 0.7203037795203421\n",
      "2022-03-27 01:36:14.237757 Epoch 250, Training Loss 0.721577955938666\n",
      "2022-03-27 01:36:14.257762 Epoch 250, Training Loss 0.7227281650618824\n",
      "2022-03-27 01:36:14.277760 Epoch 250, Training Loss 0.723778004993868\n",
      "2022-03-27 01:36:14.297771 Epoch 250, Training Loss 0.7250722465307816\n",
      "2022-03-27 01:36:14.316769 Epoch 250, Training Loss 0.7262154277175894\n",
      "2022-03-27 01:36:14.335780 Epoch 250, Training Loss 0.7272357586247232\n",
      "2022-03-27 01:36:14.355778 Epoch 250, Training Loss 0.7284239276748179\n",
      "2022-03-27 01:36:14.376789 Epoch 250, Training Loss 0.7294175505943006\n",
      "2022-03-27 01:36:14.396793 Epoch 250, Training Loss 0.7304809774126848\n",
      "2022-03-27 01:36:14.416798 Epoch 250, Training Loss 0.7318309056941811\n",
      "2022-03-27 01:36:14.436802 Epoch 250, Training Loss 0.732971211909638\n",
      "2022-03-27 01:36:14.456807 Epoch 250, Training Loss 0.7341288057587031\n",
      "2022-03-27 01:36:14.475811 Epoch 250, Training Loss 0.7353282795690209\n",
      "2022-03-27 01:36:14.496225 Epoch 250, Training Loss 0.7364763314919094\n",
      "2022-03-27 01:36:14.517072 Epoch 250, Training Loss 0.7374899622881809\n",
      "2022-03-27 01:36:14.536076 Epoch 250, Training Loss 0.7386084640269999\n",
      "2022-03-27 01:36:14.556081 Epoch 250, Training Loss 0.7397082037937915\n",
      "2022-03-27 01:36:14.576079 Epoch 250, Training Loss 0.7410549213514304\n",
      "2022-03-27 01:36:14.596269 Epoch 250, Training Loss 0.7422564651655115\n",
      "2022-03-27 01:36:14.616094 Epoch 250, Training Loss 0.743315731549202\n",
      "2022-03-27 01:36:14.636099 Epoch 250, Training Loss 0.7444787646650963\n",
      "2022-03-27 01:36:14.656103 Epoch 250, Training Loss 0.7457143851863149\n",
      "2022-03-27 01:36:14.676108 Epoch 250, Training Loss 0.7471501171741339\n",
      "2022-03-27 01:36:14.696201 Epoch 250, Training Loss 0.7485799940345842\n",
      "2022-03-27 01:36:14.716117 Epoch 250, Training Loss 0.7495664644729146\n",
      "2022-03-27 01:36:14.736121 Epoch 250, Training Loss 0.7505745256648344\n",
      "2022-03-27 01:36:14.755126 Epoch 250, Training Loss 0.751799013227453\n",
      "2022-03-27 01:36:14.775130 Epoch 250, Training Loss 0.7527463309600225\n",
      "2022-03-27 01:36:14.795135 Epoch 250, Training Loss 0.7539816378327586\n",
      "2022-03-27 01:36:14.815134 Epoch 250, Training Loss 0.7550261032855724\n",
      "2022-03-27 01:36:14.835144 Epoch 250, Training Loss 0.7560642011025372\n",
      "2022-03-27 01:36:14.856149 Epoch 250, Training Loss 0.7574217653335513\n",
      "2022-03-27 01:36:14.876153 Epoch 250, Training Loss 0.7585403192073793\n",
      "2022-03-27 01:36:14.896158 Epoch 250, Training Loss 0.7597211178611306\n",
      "2022-03-27 01:36:14.916163 Epoch 250, Training Loss 0.7608535221165709\n",
      "2022-03-27 01:36:14.935167 Epoch 250, Training Loss 0.7621783063844647\n",
      "2022-03-27 01:36:14.956171 Epoch 250, Training Loss 0.7632734564411671\n",
      "2022-03-27 01:36:14.976176 Epoch 250, Training Loss 0.764282546384865\n",
      "2022-03-27 01:36:14.995180 Epoch 250, Training Loss 0.7657075960312962\n",
      "2022-03-27 01:36:15.015179 Epoch 250, Training Loss 0.7668896004976824\n",
      "2022-03-27 01:36:15.035189 Epoch 250, Training Loss 0.76808796453354\n",
      "2022-03-27 01:36:15.055188 Epoch 250, Training Loss 0.7691566547774293\n",
      "2022-03-27 01:36:15.076199 Epoch 250, Training Loss 0.7704157521352744\n",
      "2022-03-27 01:36:15.096207 Epoch 250, Training Loss 0.7714523035851891\n",
      "2022-03-27 01:36:15.116208 Epoch 250, Training Loss 0.77251904158641\n",
      "2022-03-27 01:36:15.136212 Epoch 250, Training Loss 0.7736036536638694\n",
      "2022-03-27 01:36:15.155216 Epoch 250, Training Loss 0.7748066173947376\n",
      "2022-03-27 01:36:15.175221 Epoch 250, Training Loss 0.7759457414259996\n",
      "2022-03-27 01:36:15.195225 Epoch 250, Training Loss 0.7772210228168751\n",
      "2022-03-27 01:36:15.215225 Epoch 250, Training Loss 0.7782523052771683\n",
      "2022-03-27 01:36:15.235235 Epoch 250, Training Loss 0.779327516665544\n",
      "2022-03-27 01:36:15.255239 Epoch 250, Training Loss 0.7804185694745739\n",
      "2022-03-27 01:36:15.275244 Epoch 250, Training Loss 0.7815527164417765\n",
      "2022-03-27 01:36:15.295248 Epoch 250, Training Loss 0.7826800430217362\n",
      "2022-03-27 01:36:15.316247 Epoch 250, Training Loss 0.7837531786135701\n",
      "2022-03-27 01:36:15.336257 Epoch 250, Training Loss 0.7848916870858663\n",
      "2022-03-27 01:36:15.356262 Epoch 250, Training Loss 0.7861432488769522\n",
      "2022-03-27 01:36:15.376266 Epoch 250, Training Loss 0.7871956187288475\n",
      "2022-03-27 01:36:15.396271 Epoch 250, Training Loss 0.7882243020607688\n",
      "2022-03-27 01:36:15.416276 Epoch 250, Training Loss 0.7894580068490694\n",
      "2022-03-27 01:36:15.436280 Epoch 250, Training Loss 0.7906470230930601\n",
      "2022-03-27 01:36:15.456285 Epoch 250, Training Loss 0.791885658984294\n",
      "2022-03-27 01:36:15.475289 Epoch 250, Training Loss 0.7932106028584873\n",
      "2022-03-27 01:36:15.496294 Epoch 250, Training Loss 0.7944023419371651\n",
      "2022-03-27 01:36:15.516292 Epoch 250, Training Loss 0.7952360866777123\n",
      "2022-03-27 01:36:15.536303 Epoch 250, Training Loss 0.7962411469815637\n",
      "2022-03-27 01:36:15.556307 Epoch 250, Training Loss 0.7972043353273436\n",
      "2022-03-27 01:36:15.576312 Epoch 250, Training Loss 0.7984205668844531\n",
      "2022-03-27 01:36:15.596317 Epoch 250, Training Loss 0.7996134526284454\n",
      "2022-03-27 01:36:15.616321 Epoch 250, Training Loss 0.8008211777948052\n",
      "2022-03-27 01:36:15.636325 Epoch 250, Training Loss 0.8019222905263876\n",
      "2022-03-27 01:36:15.656330 Epoch 250, Training Loss 0.8031922780796695\n",
      "2022-03-27 01:36:15.676335 Epoch 250, Training Loss 0.804323172782693\n",
      "2022-03-27 01:36:15.697327 Epoch 250, Training Loss 0.8053894373766907\n",
      "2022-03-27 01:36:15.717344 Epoch 250, Training Loss 0.8063906084393602\n",
      "2022-03-27 01:36:15.737348 Epoch 250, Training Loss 0.8077413371914183\n",
      "2022-03-27 01:36:15.756353 Epoch 250, Training Loss 0.8086048462201872\n",
      "2022-03-27 01:36:15.776357 Epoch 250, Training Loss 0.8097852907522255\n",
      "2022-03-27 01:36:15.796347 Epoch 250, Training Loss 0.8109878710926036\n",
      "2022-03-27 01:36:15.816360 Epoch 250, Training Loss 0.812338189472018\n",
      "2022-03-27 01:36:15.836371 Epoch 250, Training Loss 0.8136158414813869\n",
      "2022-03-27 01:36:15.856375 Epoch 250, Training Loss 0.8149342841809363\n",
      "2022-03-27 01:36:15.876380 Epoch 250, Training Loss 0.816037996574436\n",
      "2022-03-27 01:36:15.896384 Epoch 250, Training Loss 0.817151353258611\n",
      "2022-03-27 01:36:15.916389 Epoch 250, Training Loss 0.8185524546429325\n",
      "2022-03-27 01:36:15.936394 Epoch 250, Training Loss 0.8196187870734183\n",
      "2022-03-27 01:36:15.956392 Epoch 250, Training Loss 0.8208821527183513\n",
      "2022-03-27 01:36:15.976403 Epoch 250, Training Loss 0.8221144796637319\n",
      "2022-03-27 01:36:15.996407 Epoch 250, Training Loss 0.8234184562702618\n",
      "2022-03-27 01:36:16.016412 Epoch 250, Training Loss 0.8247628285147041\n",
      "2022-03-27 01:36:16.036416 Epoch 250, Training Loss 0.8258955408545101\n",
      "2022-03-27 01:36:16.056421 Epoch 250, Training Loss 0.8270882715654495\n",
      "2022-03-27 01:36:16.076425 Epoch 250, Training Loss 0.8281693165869359\n",
      "2022-03-27 01:36:16.097417 Epoch 250, Training Loss 0.8291900232624825\n",
      "2022-03-27 01:36:16.116428 Epoch 250, Training Loss 0.8307035496777586\n",
      "2022-03-27 01:36:16.137439 Epoch 250, Training Loss 0.8318201619798266\n",
      "2022-03-27 01:36:16.157444 Epoch 250, Training Loss 0.8330753788618785\n",
      "2022-03-27 01:36:16.177448 Epoch 250, Training Loss 0.8342380341513992\n",
      "2022-03-27 01:36:16.197440 Epoch 250, Training Loss 0.8354757935799602\n",
      "2022-03-27 01:36:16.217457 Epoch 250, Training Loss 0.836499639026954\n",
      "2022-03-27 01:36:16.237462 Epoch 250, Training Loss 0.8374184481323222\n",
      "2022-03-27 01:36:16.258467 Epoch 250, Training Loss 0.8388272808945697\n",
      "2022-03-27 01:36:16.277471 Epoch 250, Training Loss 0.8403070329705162\n",
      "2022-03-27 01:36:16.297475 Epoch 250, Training Loss 0.841755874931355\n",
      "2022-03-27 01:36:16.317480 Epoch 250, Training Loss 0.842998183063229\n",
      "2022-03-27 01:36:16.337484 Epoch 250, Training Loss 0.8442958511812303\n",
      "2022-03-27 01:36:16.357489 Epoch 250, Training Loss 0.8454804795477396\n",
      "2022-03-27 01:36:16.377494 Epoch 250, Training Loss 0.8464070283390982\n",
      "2022-03-27 01:36:16.397498 Epoch 250, Training Loss 0.8476167834933152\n",
      "2022-03-27 01:36:16.417503 Epoch 250, Training Loss 0.8488287360162077\n",
      "2022-03-27 01:36:16.437507 Epoch 250, Training Loss 0.8498548598545591\n",
      "2022-03-27 01:36:16.457512 Epoch 250, Training Loss 0.8508855251552504\n",
      "2022-03-27 01:36:16.477516 Epoch 250, Training Loss 0.8520775371042969\n",
      "2022-03-27 01:36:16.497515 Epoch 250, Training Loss 0.8533656207649299\n",
      "2022-03-27 01:36:16.517525 Epoch 250, Training Loss 0.8543666612614146\n",
      "2022-03-27 01:36:16.537530 Epoch 250, Training Loss 0.8553884978337056\n",
      "2022-03-27 01:36:16.558534 Epoch 250, Training Loss 0.8565613600756506\n",
      "2022-03-27 01:36:16.582540 Epoch 250, Training Loss 0.8578795763995032\n",
      "2022-03-27 01:36:16.609546 Epoch 250, Training Loss 0.8591033793471353\n",
      "2022-03-27 01:36:16.636552 Epoch 250, Training Loss 0.860189614393522\n",
      "2022-03-27 01:36:16.663558 Epoch 250, Training Loss 0.861364129330496\n",
      "2022-03-27 01:36:16.690564 Epoch 250, Training Loss 0.862160933139684\n",
      "2022-03-27 01:36:16.718571 Epoch 250, Training Loss 0.8634885179112329\n",
      "2022-03-27 01:36:16.745577 Epoch 250, Training Loss 0.8645464611022978\n",
      "2022-03-27 01:36:16.772583 Epoch 250, Training Loss 0.8657717568337765\n",
      "2022-03-27 01:36:16.799583 Epoch 250, Training Loss 0.8672257145804823\n",
      "2022-03-27 01:36:16.826595 Epoch 250, Training Loss 0.8683713134140005\n",
      "2022-03-27 01:36:16.852601 Epoch 250, Training Loss 0.8694182536791047\n",
      "2022-03-27 01:36:16.878607 Epoch 250, Training Loss 0.870423446911985\n",
      "2022-03-27 01:36:16.905613 Epoch 250, Training Loss 0.8718344946499066\n",
      "2022-03-27 01:36:16.931613 Epoch 250, Training Loss 0.8732492544919329\n",
      "2022-03-27 01:36:16.958625 Epoch 250, Training Loss 0.8744588275547223\n",
      "2022-03-27 01:36:16.985631 Epoch 250, Training Loss 0.8755241311572092\n",
      "2022-03-27 01:36:17.004636 Epoch 250, Training Loss 0.8765957896666758\n",
      "2022-03-27 01:36:17.023640 Epoch 250, Training Loss 0.8778698782786689\n",
      "2022-03-27 01:36:17.042644 Epoch 250, Training Loss 0.8791715267978971\n",
      "2022-03-27 01:36:17.061649 Epoch 250, Training Loss 0.8802703844616785\n",
      "2022-03-27 01:36:17.081647 Epoch 250, Training Loss 0.8815653766207683\n",
      "2022-03-27 01:36:17.099657 Epoch 250, Training Loss 0.882725122029824\n",
      "2022-03-27 01:36:17.118662 Epoch 250, Training Loss 0.8838152725373387\n",
      "2022-03-27 01:36:17.137666 Epoch 250, Training Loss 0.8849978012502041\n",
      "2022-03-27 01:36:17.157670 Epoch 250, Training Loss 0.886028035827305\n",
      "2022-03-27 01:36:17.176675 Epoch 250, Training Loss 0.8872214938368639\n",
      "2022-03-27 01:36:17.196013 Epoch 250, Training Loss 0.8881115223593115\n",
      "2022-03-27 01:36:17.215678 Epoch 250, Training Loss 0.8891745594608814\n",
      "2022-03-27 01:36:17.234688 Epoch 250, Training Loss 0.8904208959368489\n",
      "2022-03-27 01:36:17.253697 Epoch 250, Training Loss 0.8912972370376977\n",
      "2022-03-27 01:36:17.272696 Epoch 250, Training Loss 0.8926148903949181\n",
      "2022-03-27 01:36:17.291695 Epoch 250, Training Loss 0.8939034164409199\n",
      "2022-03-27 01:36:17.310705 Epoch 250, Training Loss 0.8947272244316843\n",
      "2022-03-27 01:36:17.329709 Epoch 250, Training Loss 0.8959976761694759\n",
      "2022-03-27 01:36:17.348708 Epoch 250, Training Loss 0.8972902091415337\n",
      "2022-03-27 01:36:17.367718 Epoch 250, Training Loss 0.8983914050299798\n",
      "2022-03-27 01:36:17.375701 Epoch 250, Training Loss 0.8993598294380071\n",
      "2022-03-27 01:47:55.818405 Epoch 300, Training Loss 0.001372100294703413\n",
      "2022-03-27 01:47:55.833408 Epoch 300, Training Loss 0.002379431825159761\n",
      "2022-03-27 01:47:55.848412 Epoch 300, Training Loss 0.0036438852167495375\n",
      "2022-03-27 01:47:55.863417 Epoch 300, Training Loss 0.004957927386169238\n",
      "2022-03-27 01:47:55.878419 Epoch 300, Training Loss 0.005824038058595584\n",
      "2022-03-27 01:47:55.893422 Epoch 300, Training Loss 0.0073623037551675\n",
      "2022-03-27 01:47:55.908425 Epoch 300, Training Loss 0.008428154470365675\n",
      "2022-03-27 01:47:55.923429 Epoch 300, Training Loss 0.009717125340800761\n",
      "2022-03-27 01:47:55.938433 Epoch 300, Training Loss 0.010806046635903361\n",
      "2022-03-27 01:47:55.953435 Epoch 300, Training Loss 0.012117105524253357\n",
      "2022-03-27 01:47:55.968438 Epoch 300, Training Loss 0.013375543572408768\n",
      "2022-03-27 01:47:55.983442 Epoch 300, Training Loss 0.014397119393434061\n",
      "2022-03-27 01:47:55.998445 Epoch 300, Training Loss 0.015582166974196959\n",
      "2022-03-27 01:47:56.013450 Epoch 300, Training Loss 0.016604022220577424\n",
      "2022-03-27 01:47:56.029453 Epoch 300, Training Loss 0.017586410807831514\n",
      "2022-03-27 01:47:56.044456 Epoch 300, Training Loss 0.018523792781488364\n",
      "2022-03-27 01:47:56.058469 Epoch 300, Training Loss 0.0195977634481152\n",
      "2022-03-27 01:47:56.073473 Epoch 300, Training Loss 0.020546491341212825\n",
      "2022-03-27 01:47:56.088476 Epoch 300, Training Loss 0.02180925903417875\n",
      "2022-03-27 01:47:56.103479 Epoch 300, Training Loss 0.022808095683222233\n",
      "2022-03-27 01:47:56.118482 Epoch 300, Training Loss 0.02376091373546044\n",
      "2022-03-27 01:47:56.133485 Epoch 300, Training Loss 0.024908466336062498\n",
      "2022-03-27 01:47:56.148488 Epoch 300, Training Loss 0.02627296391350534\n",
      "2022-03-27 01:47:56.163491 Epoch 300, Training Loss 0.02730612795980995\n",
      "2022-03-27 01:47:56.178494 Epoch 300, Training Loss 0.028207986327388402\n",
      "2022-03-27 01:47:56.193498 Epoch 300, Training Loss 0.02938137487377352\n",
      "2022-03-27 01:47:56.208508 Epoch 300, Training Loss 0.030675470981451555\n",
      "2022-03-27 01:47:56.223506 Epoch 300, Training Loss 0.03180093282019086\n",
      "2022-03-27 01:47:56.238509 Epoch 300, Training Loss 0.03318105657082385\n",
      "2022-03-27 01:47:56.253513 Epoch 300, Training Loss 0.034153789739169736\n",
      "2022-03-27 01:47:56.268516 Epoch 300, Training Loss 0.03536280631409277\n",
      "2022-03-27 01:47:56.283912 Epoch 300, Training Loss 0.03622978308316692\n",
      "2022-03-27 01:47:56.298915 Epoch 300, Training Loss 0.03715341925011267\n",
      "2022-03-27 01:47:56.313920 Epoch 300, Training Loss 0.038192375922751855\n",
      "2022-03-27 01:47:56.328923 Epoch 300, Training Loss 0.03926556014343906\n",
      "2022-03-27 01:47:56.343926 Epoch 300, Training Loss 0.04034375603241689\n",
      "2022-03-27 01:47:56.358929 Epoch 300, Training Loss 0.04136370873207326\n",
      "2022-03-27 01:47:56.373932 Epoch 300, Training Loss 0.042216425585319926\n",
      "2022-03-27 01:47:56.388936 Epoch 300, Training Loss 0.04347696039072998\n",
      "2022-03-27 01:47:56.403939 Epoch 300, Training Loss 0.04478824580721843\n",
      "2022-03-27 01:47:56.418944 Epoch 300, Training Loss 0.045935515933634374\n",
      "2022-03-27 01:47:56.434946 Epoch 300, Training Loss 0.04709461674360973\n",
      "2022-03-27 01:47:56.449950 Epoch 300, Training Loss 0.04833296070928159\n",
      "2022-03-27 01:47:56.464953 Epoch 300, Training Loss 0.04941148190852016\n",
      "2022-03-27 01:47:56.479957 Epoch 300, Training Loss 0.050463065924242025\n",
      "2022-03-27 01:47:56.494961 Epoch 300, Training Loss 0.05150516197809478\n",
      "2022-03-27 01:47:56.509964 Epoch 300, Training Loss 0.05262666399521596\n",
      "2022-03-27 01:47:56.524967 Epoch 300, Training Loss 0.05375698513691993\n",
      "2022-03-27 01:47:56.539970 Epoch 300, Training Loss 0.054879919952138916\n",
      "2022-03-27 01:47:56.554973 Epoch 300, Training Loss 0.055944831017642985\n",
      "2022-03-27 01:47:56.568978 Epoch 300, Training Loss 0.05696235974426465\n",
      "2022-03-27 01:47:56.583981 Epoch 300, Training Loss 0.057932855680470574\n",
      "2022-03-27 01:47:56.598983 Epoch 300, Training Loss 0.058930414183365414\n",
      "2022-03-27 01:47:56.614989 Epoch 300, Training Loss 0.05998859495458091\n",
      "2022-03-27 01:47:56.629991 Epoch 300, Training Loss 0.060908491425502025\n",
      "2022-03-27 01:47:56.644995 Epoch 300, Training Loss 0.061944327238575576\n",
      "2022-03-27 01:47:56.659998 Epoch 300, Training Loss 0.06282250556494574\n",
      "2022-03-27 01:47:56.675002 Epoch 300, Training Loss 0.0641300543342405\n",
      "2022-03-27 01:47:56.690004 Epoch 300, Training Loss 0.06540580303467752\n",
      "2022-03-27 01:47:56.705001 Epoch 300, Training Loss 0.06632369832919381\n",
      "2022-03-27 01:47:56.720011 Epoch 300, Training Loss 0.06720702117666259\n",
      "2022-03-27 01:47:56.735014 Epoch 300, Training Loss 0.06848294526109915\n",
      "2022-03-27 01:47:56.750018 Epoch 300, Training Loss 0.06970142464503608\n",
      "2022-03-27 01:47:56.765021 Epoch 300, Training Loss 0.07089935833840724\n",
      "2022-03-27 01:47:56.780025 Epoch 300, Training Loss 0.07186955320255836\n",
      "2022-03-27 01:47:56.795028 Epoch 300, Training Loss 0.07325439364708902\n",
      "2022-03-27 01:47:56.810031 Epoch 300, Training Loss 0.07456883856707522\n",
      "2022-03-27 01:47:56.825034 Epoch 300, Training Loss 0.07582369744015471\n",
      "2022-03-27 01:47:56.840039 Epoch 300, Training Loss 0.07711013793335546\n",
      "2022-03-27 01:47:56.855037 Epoch 300, Training Loss 0.07842770470377734\n",
      "2022-03-27 01:47:56.870045 Epoch 300, Training Loss 0.07935917400338156\n",
      "2022-03-27 01:47:56.884049 Epoch 300, Training Loss 0.08038627918419021\n",
      "2022-03-27 01:47:56.899052 Epoch 300, Training Loss 0.08143930956530754\n",
      "2022-03-27 01:47:56.914056 Epoch 300, Training Loss 0.08267686662771513\n",
      "2022-03-27 01:47:56.930058 Epoch 300, Training Loss 0.08372892565129662\n",
      "2022-03-27 01:47:56.945062 Epoch 300, Training Loss 0.08465347448578271\n",
      "2022-03-27 01:47:56.960065 Epoch 300, Training Loss 0.08568545375638606\n",
      "2022-03-27 01:47:56.979082 Epoch 300, Training Loss 0.0868365057289143\n",
      "2022-03-27 01:47:56.999086 Epoch 300, Training Loss 0.08822811701718498\n",
      "2022-03-27 01:47:57.019091 Epoch 300, Training Loss 0.0895243235256361\n",
      "2022-03-27 01:47:57.038089 Epoch 300, Training Loss 0.09068079250852776\n",
      "2022-03-27 01:47:57.058094 Epoch 300, Training Loss 0.09196098457516917\n",
      "2022-03-27 01:47:57.078104 Epoch 300, Training Loss 0.09306367827803277\n",
      "2022-03-27 01:47:57.098109 Epoch 300, Training Loss 0.0941364571566472\n",
      "2022-03-27 01:47:57.117113 Epoch 300, Training Loss 0.09533686406167267\n",
      "2022-03-27 01:47:57.136117 Epoch 300, Training Loss 0.09654144001433916\n",
      "2022-03-27 01:47:57.156109 Epoch 300, Training Loss 0.09774209356978726\n",
      "2022-03-27 01:47:57.176126 Epoch 300, Training Loss 0.09908752299635612\n",
      "2022-03-27 01:47:57.195131 Epoch 300, Training Loss 0.10008944239457855\n",
      "2022-03-27 01:47:57.215135 Epoch 300, Training Loss 0.10129854319345616\n",
      "2022-03-27 01:47:57.234140 Epoch 300, Training Loss 0.10229521364812046\n",
      "2022-03-27 01:47:57.254144 Epoch 300, Training Loss 0.1037880712762818\n",
      "2022-03-27 01:47:57.275143 Epoch 300, Training Loss 0.10496129975904284\n",
      "2022-03-27 01:47:57.294147 Epoch 300, Training Loss 0.10619143825357832\n",
      "2022-03-27 01:47:57.313157 Epoch 300, Training Loss 0.10752106421743818\n",
      "2022-03-27 01:47:57.333162 Epoch 300, Training Loss 0.10874831851790934\n",
      "2022-03-27 01:47:57.353167 Epoch 300, Training Loss 0.10994071340012124\n",
      "2022-03-27 01:47:57.373159 Epoch 300, Training Loss 0.11095054291398324\n",
      "2022-03-27 01:47:57.393176 Epoch 300, Training Loss 0.11206147432937037\n",
      "2022-03-27 01:47:57.412180 Epoch 300, Training Loss 0.1132585617434948\n",
      "2022-03-27 01:47:57.432184 Epoch 300, Training Loss 0.11419487723609066\n",
      "2022-03-27 01:47:57.452183 Epoch 300, Training Loss 0.1154918721722215\n",
      "2022-03-27 01:47:57.471193 Epoch 300, Training Loss 0.11663205894972663\n",
      "2022-03-27 01:47:57.493192 Epoch 300, Training Loss 0.11760149404520878\n",
      "2022-03-27 01:47:57.512203 Epoch 300, Training Loss 0.1188631242955737\n",
      "2022-03-27 01:47:57.532207 Epoch 300, Training Loss 0.11999341380565673\n",
      "2022-03-27 01:47:57.551211 Epoch 300, Training Loss 0.12102227564662923\n",
      "2022-03-27 01:47:57.571216 Epoch 300, Training Loss 0.12254643623176438\n",
      "2022-03-27 01:47:57.590214 Epoch 300, Training Loss 0.1237221508837112\n",
      "2022-03-27 01:47:57.610219 Epoch 300, Training Loss 0.12498921704719133\n",
      "2022-03-27 01:47:57.630229 Epoch 300, Training Loss 0.12607819375479618\n",
      "2022-03-27 01:47:57.650234 Epoch 300, Training Loss 0.1270739659476463\n",
      "2022-03-27 01:47:57.669238 Epoch 300, Training Loss 0.1283567676611264\n",
      "2022-03-27 01:47:57.689230 Epoch 300, Training Loss 0.12932567180270124\n",
      "2022-03-27 01:47:57.709241 Epoch 300, Training Loss 0.13046528349447128\n",
      "2022-03-27 01:47:57.729252 Epoch 300, Training Loss 0.13161137029338066\n",
      "2022-03-27 01:47:57.749256 Epoch 300, Training Loss 0.13286296059103572\n",
      "2022-03-27 01:47:57.768248 Epoch 300, Training Loss 0.13402795829736364\n",
      "2022-03-27 01:47:57.788439 Epoch 300, Training Loss 0.13495016547725025\n",
      "2022-03-27 01:47:57.808270 Epoch 300, Training Loss 0.13639047711401645\n",
      "2022-03-27 01:47:57.827274 Epoch 300, Training Loss 0.13732929950784845\n",
      "2022-03-27 01:47:57.847273 Epoch 300, Training Loss 0.1382763025248447\n",
      "2022-03-27 01:47:57.866283 Epoch 300, Training Loss 0.1391688204939713\n",
      "2022-03-27 01:47:57.886287 Epoch 300, Training Loss 0.14031964212732242\n",
      "2022-03-27 01:47:57.906292 Epoch 300, Training Loss 0.141440140088196\n",
      "2022-03-27 01:47:57.927297 Epoch 300, Training Loss 0.14244057104715604\n",
      "2022-03-27 01:47:57.946289 Epoch 300, Training Loss 0.1436202518470452\n",
      "2022-03-27 01:47:57.966306 Epoch 300, Training Loss 0.14464088260670147\n",
      "2022-03-27 01:47:57.985368 Epoch 300, Training Loss 0.14578668517834695\n",
      "2022-03-27 01:47:58.005314 Epoch 300, Training Loss 0.1470969673770163\n",
      "2022-03-27 01:47:58.024313 Epoch 300, Training Loss 0.14807907821577224\n",
      "2022-03-27 01:47:58.044323 Epoch 300, Training Loss 0.14909931217008235\n",
      "2022-03-27 01:47:58.064328 Epoch 300, Training Loss 0.15015476949684456\n",
      "2022-03-27 01:47:58.084332 Epoch 300, Training Loss 0.15147995285670776\n",
      "2022-03-27 01:47:58.103336 Epoch 300, Training Loss 0.15255105022884086\n",
      "2022-03-27 01:47:58.124335 Epoch 300, Training Loss 0.15374762658267985\n",
      "2022-03-27 01:47:58.144346 Epoch 300, Training Loss 0.15463171605868717\n",
      "2022-03-27 01:47:58.165345 Epoch 300, Training Loss 0.15588764110794456\n",
      "2022-03-27 01:47:58.184355 Epoch 300, Training Loss 0.15685966770972132\n",
      "2022-03-27 01:47:58.204359 Epoch 300, Training Loss 0.15815677453794746\n",
      "2022-03-27 01:47:58.224358 Epoch 300, Training Loss 0.159372342517004\n",
      "2022-03-27 01:47:58.243368 Epoch 300, Training Loss 0.16023437423474343\n",
      "2022-03-27 01:47:58.262373 Epoch 300, Training Loss 0.16150954129445888\n",
      "2022-03-27 01:47:58.282377 Epoch 300, Training Loss 0.1626066954239555\n",
      "2022-03-27 01:47:58.302382 Epoch 300, Training Loss 0.16402595122451977\n",
      "2022-03-27 01:47:58.322374 Epoch 300, Training Loss 0.16504030253576196\n",
      "2022-03-27 01:47:58.342385 Epoch 300, Training Loss 0.16635843684606236\n",
      "2022-03-27 01:47:58.362395 Epoch 300, Training Loss 0.16755099781333943\n",
      "2022-03-27 01:47:58.382400 Epoch 300, Training Loss 0.1686626982201091\n",
      "2022-03-27 01:47:58.402404 Epoch 300, Training Loss 0.16997758826941176\n",
      "2022-03-27 01:47:58.421409 Epoch 300, Training Loss 0.17127763584751607\n",
      "2022-03-27 01:47:58.441413 Epoch 300, Training Loss 0.172396402301081\n",
      "2022-03-27 01:47:58.461418 Epoch 300, Training Loss 0.17374078994211944\n",
      "2022-03-27 01:47:58.481422 Epoch 300, Training Loss 0.1750610558425679\n",
      "2022-03-27 01:47:58.500426 Epoch 300, Training Loss 0.1762946412691375\n",
      "2022-03-27 01:47:58.520431 Epoch 300, Training Loss 0.1774572015113538\n",
      "2022-03-27 01:47:58.541417 Epoch 300, Training Loss 0.17859424143800956\n",
      "2022-03-27 01:47:58.560434 Epoch 300, Training Loss 0.1795911532838631\n",
      "2022-03-27 01:47:58.580445 Epoch 300, Training Loss 0.18058412375352573\n",
      "2022-03-27 01:47:58.599449 Epoch 300, Training Loss 0.18190663313621755\n",
      "2022-03-27 01:47:58.619441 Epoch 300, Training Loss 0.18302685289126833\n",
      "2022-03-27 01:47:58.640446 Epoch 300, Training Loss 0.18394714205161386\n",
      "2022-03-27 01:47:58.660463 Epoch 300, Training Loss 0.18517757613030847\n",
      "2022-03-27 01:47:58.681468 Epoch 300, Training Loss 0.18630013883571186\n",
      "2022-03-27 01:47:58.700472 Epoch 300, Training Loss 0.18749222258472686\n",
      "2022-03-27 01:47:58.720477 Epoch 300, Training Loss 0.18860567393510239\n",
      "2022-03-27 01:47:58.740475 Epoch 300, Training Loss 0.18986812828446897\n",
      "2022-03-27 01:47:58.760486 Epoch 300, Training Loss 0.19119863231163806\n",
      "2022-03-27 01:47:58.780490 Epoch 300, Training Loss 0.19221988327972725\n",
      "2022-03-27 01:47:58.800494 Epoch 300, Training Loss 0.19346166556448582\n",
      "2022-03-27 01:47:58.819499 Epoch 300, Training Loss 0.19473053747430788\n",
      "2022-03-27 01:47:58.839503 Epoch 300, Training Loss 0.19589520316294698\n",
      "2022-03-27 01:47:58.859508 Epoch 300, Training Loss 0.19702337465017958\n",
      "2022-03-27 01:47:58.879513 Epoch 300, Training Loss 0.19813244673602112\n",
      "2022-03-27 01:47:58.898517 Epoch 300, Training Loss 0.19921373413956683\n",
      "2022-03-27 01:47:58.918522 Epoch 300, Training Loss 0.20015404169516796\n",
      "2022-03-27 01:47:58.938787 Epoch 300, Training Loss 0.2012558932347066\n",
      "2022-03-27 01:47:58.958792 Epoch 300, Training Loss 0.20229182439996762\n",
      "2022-03-27 01:47:58.978790 Epoch 300, Training Loss 0.2034857103129482\n",
      "2022-03-27 01:47:58.999801 Epoch 300, Training Loss 0.2050267821535125\n",
      "2022-03-27 01:47:59.018793 Epoch 300, Training Loss 0.20645146624511465\n",
      "2022-03-27 01:47:59.038810 Epoch 300, Training Loss 0.2075650688937253\n",
      "2022-03-27 01:47:59.058814 Epoch 300, Training Loss 0.20851127166882197\n",
      "2022-03-27 01:47:59.078819 Epoch 300, Training Loss 0.20978637554151627\n",
      "2022-03-27 01:47:59.098823 Epoch 300, Training Loss 0.21088471917240212\n",
      "2022-03-27 01:47:59.118828 Epoch 300, Training Loss 0.21198499713407454\n",
      "2022-03-27 01:47:59.137832 Epoch 300, Training Loss 0.21309125644471638\n",
      "2022-03-27 01:47:59.157837 Epoch 300, Training Loss 0.21407108279445286\n",
      "2022-03-27 01:47:59.176835 Epoch 300, Training Loss 0.2153489361028842\n",
      "2022-03-27 01:47:59.197846 Epoch 300, Training Loss 0.2165675853067042\n",
      "2022-03-27 01:47:59.217844 Epoch 300, Training Loss 0.21755772226911677\n",
      "2022-03-27 01:47:59.237849 Epoch 300, Training Loss 0.21877653694823576\n",
      "2022-03-27 01:47:59.258854 Epoch 300, Training Loss 0.22010997349343947\n",
      "2022-03-27 01:47:59.277864 Epoch 300, Training Loss 0.2211580419022104\n",
      "2022-03-27 01:47:59.297869 Epoch 300, Training Loss 0.22241267996370945\n",
      "2022-03-27 01:47:59.318873 Epoch 300, Training Loss 0.22356686163741304\n",
      "2022-03-27 01:47:59.339878 Epoch 300, Training Loss 0.22458152766422848\n",
      "2022-03-27 01:47:59.359883 Epoch 300, Training Loss 0.22561143296758843\n",
      "2022-03-27 01:47:59.379887 Epoch 300, Training Loss 0.22671450777431887\n",
      "2022-03-27 01:47:59.401892 Epoch 300, Training Loss 0.22804534976439708\n",
      "2022-03-27 01:47:59.421883 Epoch 300, Training Loss 0.22937895872099015\n",
      "2022-03-27 01:47:59.442901 Epoch 300, Training Loss 0.23045651389814703\n",
      "2022-03-27 01:47:59.462906 Epoch 300, Training Loss 0.23153717712977964\n",
      "2022-03-27 01:47:59.481910 Epoch 300, Training Loss 0.2327183064292459\n",
      "2022-03-27 01:47:59.501915 Epoch 300, Training Loss 0.23408283175105024\n",
      "2022-03-27 01:47:59.521919 Epoch 300, Training Loss 0.2351704900679381\n",
      "2022-03-27 01:47:59.541924 Epoch 300, Training Loss 0.23628584373637537\n",
      "2022-03-27 01:47:59.561928 Epoch 300, Training Loss 0.23725864275946945\n",
      "2022-03-27 01:47:59.580933 Epoch 300, Training Loss 0.23838406145725105\n",
      "2022-03-27 01:47:59.600937 Epoch 300, Training Loss 0.23961973739097187\n",
      "2022-03-27 01:47:59.620942 Epoch 300, Training Loss 0.24061684977368017\n",
      "2022-03-27 01:47:59.641946 Epoch 300, Training Loss 0.24177959858608977\n",
      "2022-03-27 01:47:59.660951 Epoch 300, Training Loss 0.24281338085908719\n",
      "2022-03-27 01:47:59.680955 Epoch 300, Training Loss 0.24396963581404724\n",
      "2022-03-27 01:47:59.701948 Epoch 300, Training Loss 0.24501094915677823\n",
      "2022-03-27 01:47:59.721965 Epoch 300, Training Loss 0.24614404793590536\n",
      "2022-03-27 01:47:59.741969 Epoch 300, Training Loss 0.24703775365334338\n",
      "2022-03-27 01:47:59.760973 Epoch 300, Training Loss 0.24822245488691208\n",
      "2022-03-27 01:47:59.780979 Epoch 300, Training Loss 0.2492601921796189\n",
      "2022-03-27 01:47:59.800982 Epoch 300, Training Loss 0.25016588765337033\n",
      "2022-03-27 01:47:59.820987 Epoch 300, Training Loss 0.2514206702294557\n",
      "2022-03-27 01:47:59.840979 Epoch 300, Training Loss 0.25251677952459095\n",
      "2022-03-27 01:47:59.860990 Epoch 300, Training Loss 0.25353419064255933\n",
      "2022-03-27 01:47:59.881001 Epoch 300, Training Loss 0.2547747447819966\n",
      "2022-03-27 01:47:59.901005 Epoch 300, Training Loss 0.2559342180828914\n",
      "2022-03-27 01:47:59.921010 Epoch 300, Training Loss 0.2571897676686192\n",
      "2022-03-27 01:47:59.940002 Epoch 300, Training Loss 0.2581519849617463\n",
      "2022-03-27 01:47:59.961019 Epoch 300, Training Loss 0.25932224990461794\n",
      "2022-03-27 01:47:59.982024 Epoch 300, Training Loss 0.26036695072717986\n",
      "2022-03-27 01:48:00.002029 Epoch 300, Training Loss 0.26152024198980894\n",
      "2022-03-27 01:48:00.021026 Epoch 300, Training Loss 0.2627357396933124\n",
      "2022-03-27 01:48:00.042031 Epoch 300, Training Loss 0.26395019660215546\n",
      "2022-03-27 01:48:00.062036 Epoch 300, Training Loss 0.264921775635551\n",
      "2022-03-27 01:48:00.082046 Epoch 300, Training Loss 0.26604551595190296\n",
      "2022-03-27 01:48:00.102051 Epoch 300, Training Loss 0.26707314560785317\n",
      "2022-03-27 01:48:00.122055 Epoch 300, Training Loss 0.2680746509748347\n",
      "2022-03-27 01:48:00.142060 Epoch 300, Training Loss 0.2692826525177187\n",
      "2022-03-27 01:48:00.162064 Epoch 300, Training Loss 0.2703242231817806\n",
      "2022-03-27 01:48:00.182069 Epoch 300, Training Loss 0.2713559876622446\n",
      "2022-03-27 01:48:00.202073 Epoch 300, Training Loss 0.27259983575862384\n",
      "2022-03-27 01:48:00.222058 Epoch 300, Training Loss 0.2737722214682938\n",
      "2022-03-27 01:48:00.242082 Epoch 300, Training Loss 0.27515731953903844\n",
      "2022-03-27 01:48:00.262087 Epoch 300, Training Loss 0.2762875226147644\n",
      "2022-03-27 01:48:00.283086 Epoch 300, Training Loss 0.27733322688380774\n",
      "2022-03-27 01:48:00.303096 Epoch 300, Training Loss 0.2782823922079238\n",
      "2022-03-27 01:48:00.323371 Epoch 300, Training Loss 0.2793292889509664\n",
      "2022-03-27 01:48:00.343382 Epoch 300, Training Loss 0.28046870353581654\n",
      "2022-03-27 01:48:00.363386 Epoch 300, Training Loss 0.2816727669037821\n",
      "2022-03-27 01:48:00.383391 Epoch 300, Training Loss 0.28296420656506666\n",
      "2022-03-27 01:48:00.403389 Epoch 300, Training Loss 0.2839963554268908\n",
      "2022-03-27 01:48:00.423394 Epoch 300, Training Loss 0.285214477868946\n",
      "2022-03-27 01:48:00.443389 Epoch 300, Training Loss 0.28627053612028547\n",
      "2022-03-27 01:48:00.463393 Epoch 300, Training Loss 0.28734765135113843\n",
      "2022-03-27 01:48:00.484397 Epoch 300, Training Loss 0.28839536579063785\n",
      "2022-03-27 01:48:00.504418 Epoch 300, Training Loss 0.2896302432355369\n",
      "2022-03-27 01:48:00.525417 Epoch 300, Training Loss 0.2906317665144001\n",
      "2022-03-27 01:48:00.544421 Epoch 300, Training Loss 0.29192600164876875\n",
      "2022-03-27 01:48:00.565427 Epoch 300, Training Loss 0.2930712398055874\n",
      "2022-03-27 01:48:00.584436 Epoch 300, Training Loss 0.2940450437233576\n",
      "2022-03-27 01:48:00.604435 Epoch 300, Training Loss 0.2951587754137376\n",
      "2022-03-27 01:48:00.631447 Epoch 300, Training Loss 0.29611324791408256\n",
      "2022-03-27 01:48:00.659441 Epoch 300, Training Loss 0.29716955113898763\n",
      "2022-03-27 01:48:00.685453 Epoch 300, Training Loss 0.2981987263235595\n",
      "2022-03-27 01:48:00.713459 Epoch 300, Training Loss 0.29921775736162426\n",
      "2022-03-27 01:48:00.739466 Epoch 300, Training Loss 0.3004654201552691\n",
      "2022-03-27 01:48:00.766471 Epoch 300, Training Loss 0.30152623313467214\n",
      "2022-03-27 01:48:00.793478 Epoch 300, Training Loss 0.3025611313560125\n",
      "2022-03-27 01:48:00.820490 Epoch 300, Training Loss 0.3038322857731139\n",
      "2022-03-27 01:48:00.846496 Epoch 300, Training Loss 0.305114223447907\n",
      "2022-03-27 01:48:00.873496 Epoch 300, Training Loss 0.30620344634860985\n",
      "2022-03-27 01:48:00.901502 Epoch 300, Training Loss 0.3073664795407249\n",
      "2022-03-27 01:48:00.927514 Epoch 300, Training Loss 0.3087412740873254\n",
      "2022-03-27 01:48:00.954700 Epoch 300, Training Loss 0.309881354079527\n",
      "2022-03-27 01:48:00.980526 Epoch 300, Training Loss 0.31093699311661294\n",
      "2022-03-27 01:48:01.007526 Epoch 300, Training Loss 0.3121831317234527\n",
      "2022-03-27 01:48:01.025536 Epoch 300, Training Loss 0.31327272947791895\n",
      "2022-03-27 01:48:01.045535 Epoch 300, Training Loss 0.3145035931368923\n",
      "2022-03-27 01:48:01.064539 Epoch 300, Training Loss 0.3158253415313828\n",
      "2022-03-27 01:48:01.082549 Epoch 300, Training Loss 0.3169667888480379\n",
      "2022-03-27 01:48:01.102548 Epoch 300, Training Loss 0.3181324410621467\n",
      "2022-03-27 01:48:01.122539 Epoch 300, Training Loss 0.31924827888493645\n",
      "2022-03-27 01:48:01.142557 Epoch 300, Training Loss 0.3204365719462295\n",
      "2022-03-27 01:48:01.161567 Epoch 300, Training Loss 0.32147258001825085\n",
      "2022-03-27 01:48:01.180571 Epoch 300, Training Loss 0.322586714230535\n",
      "2022-03-27 01:48:01.199570 Epoch 300, Training Loss 0.3239275295563671\n",
      "2022-03-27 01:48:01.218574 Epoch 300, Training Loss 0.3251142973637642\n",
      "2022-03-27 01:48:01.237584 Epoch 300, Training Loss 0.3261943101273168\n",
      "2022-03-27 01:48:01.257576 Epoch 300, Training Loss 0.3275260829254794\n",
      "2022-03-27 01:48:01.276587 Epoch 300, Training Loss 0.3285595775412781\n",
      "2022-03-27 01:48:01.295591 Epoch 300, Training Loss 0.32943878995488063\n",
      "2022-03-27 01:48:01.313582 Epoch 300, Training Loss 0.3306133987958474\n",
      "2022-03-27 01:48:01.333600 Epoch 300, Training Loss 0.33176681376479167\n",
      "2022-03-27 01:48:01.352604 Epoch 300, Training Loss 0.33280534817434637\n",
      "2022-03-27 01:48:01.373605 Epoch 300, Training Loss 0.3339590074308693\n",
      "2022-03-27 01:48:01.391613 Epoch 300, Training Loss 0.3351256208651511\n",
      "2022-03-27 01:48:01.410617 Epoch 300, Training Loss 0.3363648949529204\n",
      "2022-03-27 01:48:01.429622 Epoch 300, Training Loss 0.33740759696192146\n",
      "2022-03-27 01:48:01.448620 Epoch 300, Training Loss 0.3384401356930013\n",
      "2022-03-27 01:48:01.467636 Epoch 300, Training Loss 0.33956540827556037\n",
      "2022-03-27 01:48:01.486635 Epoch 300, Training Loss 0.34081889304053753\n",
      "2022-03-27 01:48:01.505626 Epoch 300, Training Loss 0.34207498912921036\n",
      "2022-03-27 01:48:01.525633 Epoch 300, Training Loss 0.3431849554371651\n",
      "2022-03-27 01:48:01.543648 Epoch 300, Training Loss 0.3446990310993341\n",
      "2022-03-27 01:48:01.562658 Epoch 300, Training Loss 0.3458243295969561\n",
      "2022-03-27 01:48:01.581657 Epoch 300, Training Loss 0.3467610943347902\n",
      "2022-03-27 01:48:01.600667 Epoch 300, Training Loss 0.34801782633337525\n",
      "2022-03-27 01:48:01.619671 Epoch 300, Training Loss 0.3491314828700727\n",
      "2022-03-27 01:48:01.639663 Epoch 300, Training Loss 0.35064632988646816\n",
      "2022-03-27 01:48:01.657668 Epoch 300, Training Loss 0.35171241955379084\n",
      "2022-03-27 01:48:01.676684 Epoch 300, Training Loss 0.3529397674533717\n",
      "2022-03-27 01:48:01.696688 Epoch 300, Training Loss 0.35390312874408636\n",
      "2022-03-27 01:48:01.715686 Epoch 300, Training Loss 0.3549817607683294\n",
      "2022-03-27 01:48:01.735685 Epoch 300, Training Loss 0.3559984596031706\n",
      "2022-03-27 01:48:01.754703 Epoch 300, Training Loss 0.3574931597923074\n",
      "2022-03-27 01:48:01.773693 Epoch 300, Training Loss 0.35859703484093747\n",
      "2022-03-27 01:48:01.792704 Epoch 300, Training Loss 0.35947537963347664\n",
      "2022-03-27 01:48:01.810699 Epoch 300, Training Loss 0.36066650437272113\n",
      "2022-03-27 01:48:01.830763 Epoch 300, Training Loss 0.3616557042007251\n",
      "2022-03-27 01:48:01.849717 Epoch 300, Training Loss 0.36296140659800574\n",
      "2022-03-27 01:48:01.868721 Epoch 300, Training Loss 0.36386164512170854\n",
      "2022-03-27 01:48:01.887716 Epoch 300, Training Loss 0.36501201842447073\n",
      "2022-03-27 01:48:01.906723 Epoch 300, Training Loss 0.3660951849749631\n",
      "2022-03-27 01:48:01.926222 Epoch 300, Training Loss 0.3671417221846178\n",
      "2022-03-27 01:48:01.946213 Epoch 300, Training Loss 0.3681946287831992\n",
      "2022-03-27 01:48:01.965225 Epoch 300, Training Loss 0.36926592836904404\n",
      "2022-03-27 01:48:01.984554 Epoch 300, Training Loss 0.3702611303542886\n",
      "2022-03-27 01:48:02.004496 Epoch 300, Training Loss 0.3713169715288655\n",
      "2022-03-27 01:48:02.023351 Epoch 300, Training Loss 0.37278491121423823\n",
      "2022-03-27 01:48:02.042360 Epoch 300, Training Loss 0.373824520489139\n",
      "2022-03-27 01:48:02.060369 Epoch 300, Training Loss 0.3749654118514732\n",
      "2022-03-27 01:48:02.081374 Epoch 300, Training Loss 0.3762075949812789\n",
      "2022-03-27 01:48:02.100378 Epoch 300, Training Loss 0.37747098585528793\n",
      "2022-03-27 01:48:02.118376 Epoch 300, Training Loss 0.37868131906785013\n",
      "2022-03-27 01:48:02.138386 Epoch 300, Training Loss 0.3799934816329985\n",
      "2022-03-27 01:48:02.158385 Epoch 300, Training Loss 0.38112366237603795\n",
      "2022-03-27 01:48:02.177395 Epoch 300, Training Loss 0.3821082124319833\n",
      "2022-03-27 01:48:02.196400 Epoch 300, Training Loss 0.3836040496826172\n",
      "2022-03-27 01:48:02.216398 Epoch 300, Training Loss 0.3848400596157669\n",
      "2022-03-27 01:48:02.234396 Epoch 300, Training Loss 0.3859722815511172\n",
      "2022-03-27 01:48:02.253413 Epoch 300, Training Loss 0.38720912800725465\n",
      "2022-03-27 01:48:02.272399 Epoch 300, Training Loss 0.38857572226573134\n",
      "2022-03-27 01:48:02.292410 Epoch 300, Training Loss 0.38975271239609977\n",
      "2022-03-27 01:48:02.311414 Epoch 300, Training Loss 0.390994117769134\n",
      "2022-03-27 01:48:02.329424 Epoch 300, Training Loss 0.39208522050277045\n",
      "2022-03-27 01:48:02.349434 Epoch 300, Training Loss 0.3931414618364076\n",
      "2022-03-27 01:48:02.369427 Epoch 300, Training Loss 0.3941947872681386\n",
      "2022-03-27 01:48:02.388425 Epoch 300, Training Loss 0.3953207078797128\n",
      "2022-03-27 01:48:02.408429 Epoch 300, Training Loss 0.3967128510365401\n",
      "2022-03-27 01:48:02.426447 Epoch 300, Training Loss 0.39822752244027376\n",
      "2022-03-27 01:48:02.446457 Epoch 300, Training Loss 0.3994159776231517\n",
      "2022-03-27 01:48:02.464461 Epoch 300, Training Loss 0.4005610176059596\n",
      "2022-03-27 01:48:02.483459 Epoch 300, Training Loss 0.40175539369473373\n",
      "2022-03-27 01:48:02.502469 Epoch 300, Training Loss 0.4028473861534577\n",
      "2022-03-27 01:48:02.522462 Epoch 300, Training Loss 0.4038504774460707\n",
      "2022-03-27 01:48:02.541463 Epoch 300, Training Loss 0.4050748147013242\n",
      "2022-03-27 01:48:02.559476 Epoch 300, Training Loss 0.4060193326162255\n",
      "2022-03-27 01:48:02.578469 Epoch 300, Training Loss 0.4071941395549823\n",
      "2022-03-27 01:48:02.597485 Epoch 300, Training Loss 0.4083928587796438\n",
      "2022-03-27 01:48:02.617489 Epoch 300, Training Loss 0.4094850942301933\n",
      "2022-03-27 01:48:02.636500 Epoch 300, Training Loss 0.4104179034147726\n",
      "2022-03-27 01:48:02.656493 Epoch 300, Training Loss 0.41164565193073827\n",
      "2022-03-27 01:48:02.676504 Epoch 300, Training Loss 0.4125349870728105\n",
      "2022-03-27 01:48:02.695507 Epoch 300, Training Loss 0.4136983467947187\n",
      "2022-03-27 01:48:02.714511 Epoch 300, Training Loss 0.4148488089709026\n",
      "2022-03-27 01:48:02.733522 Epoch 300, Training Loss 0.4157106224685679\n",
      "2022-03-27 01:48:02.753526 Epoch 300, Training Loss 0.4167622051123158\n",
      "2022-03-27 01:48:02.774519 Epoch 300, Training Loss 0.4177541618456926\n",
      "2022-03-27 01:48:02.792529 Epoch 300, Training Loss 0.41865508986251126\n",
      "2022-03-27 01:48:02.811539 Epoch 300, Training Loss 0.4198572730164394\n",
      "2022-03-27 01:48:02.837546 Epoch 300, Training Loss 0.420886899275548\n",
      "2022-03-27 01:48:02.864551 Epoch 300, Training Loss 0.4222623513025396\n",
      "2022-03-27 01:48:02.891542 Epoch 300, Training Loss 0.4234015864636892\n",
      "2022-03-27 01:48:02.917564 Epoch 300, Training Loss 0.42449905561364215\n",
      "2022-03-27 01:48:02.944569 Epoch 300, Training Loss 0.42555365202676915\n",
      "2022-03-27 01:48:02.971557 Epoch 300, Training Loss 0.4266746617339151\n",
      "2022-03-27 01:48:02.998582 Epoch 300, Training Loss 0.4277858274519596\n",
      "2022-03-27 01:48:03.025582 Epoch 300, Training Loss 0.4287461585096081\n",
      "2022-03-27 01:48:03.044592 Epoch 300, Training Loss 0.42997924842493\n",
      "2022-03-27 01:48:03.062596 Epoch 300, Training Loss 0.43124648296009854\n",
      "2022-03-27 01:48:03.082600 Epoch 300, Training Loss 0.4322563583588661\n",
      "2022-03-27 01:48:03.101599 Epoch 300, Training Loss 0.43319496291372783\n",
      "2022-03-27 01:48:03.121596 Epoch 300, Training Loss 0.43435536641294087\n",
      "2022-03-27 01:48:03.140603 Epoch 300, Training Loss 0.43553424384587863\n",
      "2022-03-27 01:48:03.159612 Epoch 300, Training Loss 0.43658789199636416\n",
      "2022-03-27 01:48:03.177612 Epoch 300, Training Loss 0.4377404312648432\n",
      "2022-03-27 01:48:03.196617 Epoch 300, Training Loss 0.43885645178882665\n",
      "2022-03-27 01:48:03.214998 Epoch 300, Training Loss 0.44002732703143066\n",
      "2022-03-27 01:48:03.233629 Epoch 300, Training Loss 0.44103979370783053\n",
      "2022-03-27 01:48:03.253640 Epoch 300, Training Loss 0.44223660162037903\n",
      "2022-03-27 01:48:03.274638 Epoch 300, Training Loss 0.44326251409852596\n",
      "2022-03-27 01:48:03.293636 Epoch 300, Training Loss 0.44438876306919184\n",
      "2022-03-27 01:48:03.311646 Epoch 300, Training Loss 0.4455219219102884\n",
      "2022-03-27 01:48:03.330657 Epoch 300, Training Loss 0.4466086722091031\n",
      "2022-03-27 01:48:03.350661 Epoch 300, Training Loss 0.4476297262227139\n",
      "2022-03-27 01:48:03.369660 Epoch 300, Training Loss 0.44835197323423515\n",
      "2022-03-27 01:48:03.388652 Epoch 300, Training Loss 0.44973807757162987\n",
      "2022-03-27 01:48:03.407668 Epoch 300, Training Loss 0.450804420673024\n",
      "2022-03-27 01:48:03.426666 Epoch 300, Training Loss 0.4518611005047703\n",
      "2022-03-27 01:48:03.444683 Epoch 300, Training Loss 0.4531324029426136\n",
      "2022-03-27 01:48:03.464687 Epoch 300, Training Loss 0.4542518577459828\n",
      "2022-03-27 01:48:03.483691 Epoch 300, Training Loss 0.45532281052730883\n",
      "2022-03-27 01:48:03.502696 Epoch 300, Training Loss 0.4563512037629667\n",
      "2022-03-27 01:48:03.521686 Epoch 300, Training Loss 0.45746534994191224\n",
      "2022-03-27 01:48:03.541700 Epoch 300, Training Loss 0.45856214888260494\n",
      "2022-03-27 01:48:03.559703 Epoch 300, Training Loss 0.45975150667188114\n",
      "2022-03-27 01:48:03.578707 Epoch 300, Training Loss 0.4611582929063636\n",
      "2022-03-27 01:48:03.597717 Epoch 300, Training Loss 0.4623003605838932\n",
      "2022-03-27 01:48:03.616703 Epoch 300, Training Loss 0.46346987528569256\n",
      "2022-03-27 01:48:03.635714 Epoch 300, Training Loss 0.4646389454679416\n",
      "2022-03-27 01:48:03.655713 Epoch 300, Training Loss 0.46587803281481616\n",
      "2022-03-27 01:48:03.674716 Epoch 300, Training Loss 0.4669913921667182\n",
      "2022-03-27 01:48:03.693727 Epoch 300, Training Loss 0.46855652263707215\n",
      "2022-03-27 01:48:03.712724 Epoch 300, Training Loss 0.4697410259252924\n",
      "2022-03-27 01:48:03.731873 Epoch 300, Training Loss 0.47090473168951164\n",
      "2022-03-27 01:48:03.749880 Epoch 300, Training Loss 0.4725456163096611\n",
      "2022-03-27 01:48:03.769901 Epoch 300, Training Loss 0.4736358614071556\n",
      "2022-03-27 01:48:03.788894 Epoch 300, Training Loss 0.4749864707974827\n",
      "2022-03-27 01:48:03.807898 Epoch 300, Training Loss 0.4760820292450888\n",
      "2022-03-27 01:48:03.827895 Epoch 300, Training Loss 0.47734859296123083\n",
      "2022-03-27 01:48:03.846899 Epoch 300, Training Loss 0.4787674693347853\n",
      "2022-03-27 01:48:03.865906 Epoch 300, Training Loss 0.479746742702811\n",
      "2022-03-27 01:48:03.885908 Epoch 300, Training Loss 0.48080401156869385\n",
      "2022-03-27 01:48:03.904914 Epoch 300, Training Loss 0.4821387342632274\n",
      "2022-03-27 01:48:03.924917 Epoch 300, Training Loss 0.48349007079973244\n",
      "2022-03-27 01:48:03.943920 Epoch 300, Training Loss 0.4848392417516245\n",
      "2022-03-27 01:48:03.962927 Epoch 300, Training Loss 0.4858008180280476\n",
      "2022-03-27 01:48:03.981929 Epoch 300, Training Loss 0.48688295071996995\n",
      "2022-03-27 01:48:04.000934 Epoch 300, Training Loss 0.48833447397517427\n",
      "2022-03-27 01:48:04.019938 Epoch 300, Training Loss 0.4896186936999221\n",
      "2022-03-27 01:48:04.040944 Epoch 300, Training Loss 0.49069020342643915\n",
      "2022-03-27 01:48:04.059946 Epoch 300, Training Loss 0.4919834850389329\n",
      "2022-03-27 01:48:04.079952 Epoch 300, Training Loss 0.4930977469210125\n",
      "2022-03-27 01:48:04.098956 Epoch 300, Training Loss 0.49409064216077175\n",
      "2022-03-27 01:48:04.117962 Epoch 300, Training Loss 0.4953575348457717\n",
      "2022-03-27 01:48:04.136965 Epoch 300, Training Loss 0.4964792463175781\n",
      "2022-03-27 01:48:04.155975 Epoch 300, Training Loss 0.4976405920579915\n",
      "2022-03-27 01:48:04.175974 Epoch 300, Training Loss 0.49879112542437776\n",
      "2022-03-27 01:48:04.194978 Epoch 300, Training Loss 0.4998831298497632\n",
      "2022-03-27 01:48:04.213989 Epoch 300, Training Loss 0.5009902885655309\n",
      "2022-03-27 01:48:04.234000 Epoch 300, Training Loss 0.5020544296487823\n",
      "2022-03-27 01:48:04.254011 Epoch 300, Training Loss 0.5033188277040906\n",
      "2022-03-27 01:48:04.275004 Epoch 300, Training Loss 0.5046496500292093\n",
      "2022-03-27 01:48:04.293008 Epoch 300, Training Loss 0.5056302377482509\n",
      "2022-03-27 01:48:04.312018 Epoch 300, Training Loss 0.5068214492267354\n",
      "2022-03-27 01:48:04.331028 Epoch 300, Training Loss 0.50780261378459\n",
      "2022-03-27 01:48:04.351027 Epoch 300, Training Loss 0.5088728770728002\n",
      "2022-03-27 01:48:04.370025 Epoch 300, Training Loss 0.5101055411426613\n",
      "2022-03-27 01:48:04.390032 Epoch 300, Training Loss 0.5113287789132589\n",
      "2022-03-27 01:48:04.408034 Epoch 300, Training Loss 0.5123899877071381\n",
      "2022-03-27 01:48:04.427050 Epoch 300, Training Loss 0.5136896932064114\n",
      "2022-03-27 01:48:04.446039 Epoch 300, Training Loss 0.5150199524124565\n",
      "2022-03-27 01:48:04.466046 Epoch 300, Training Loss 0.515928478039744\n",
      "2022-03-27 01:48:04.486058 Epoch 300, Training Loss 0.5172495441058712\n",
      "2022-03-27 01:48:04.505052 Epoch 300, Training Loss 0.5184794850361621\n",
      "2022-03-27 01:48:04.524057 Epoch 300, Training Loss 0.5198952977919518\n",
      "2022-03-27 01:48:04.543065 Epoch 300, Training Loss 0.5208877871560929\n",
      "2022-03-27 01:48:04.561074 Epoch 300, Training Loss 0.5218618774353085\n",
      "2022-03-27 01:48:04.580072 Epoch 300, Training Loss 0.5232270870672162\n",
      "2022-03-27 01:48:04.599089 Epoch 300, Training Loss 0.5244973475670875\n",
      "2022-03-27 01:48:04.618087 Epoch 300, Training Loss 0.5255448164232551\n",
      "2022-03-27 01:48:04.637092 Epoch 300, Training Loss 0.5266375807697511\n",
      "2022-03-27 01:48:04.657094 Epoch 300, Training Loss 0.5275959421301741\n",
      "2022-03-27 01:48:04.675094 Epoch 300, Training Loss 0.5287898115032469\n",
      "2022-03-27 01:48:04.694105 Epoch 300, Training Loss 0.5299483309773838\n",
      "2022-03-27 01:48:04.712109 Epoch 300, Training Loss 0.5310521627326146\n",
      "2022-03-27 01:48:04.732107 Epoch 300, Training Loss 0.5322227286713203\n",
      "2022-03-27 01:48:04.751123 Epoch 300, Training Loss 0.5337919162209991\n",
      "2022-03-27 01:48:04.771129 Epoch 300, Training Loss 0.5348485641162414\n",
      "2022-03-27 01:48:04.789114 Epoch 300, Training Loss 0.5360040932969974\n",
      "2022-03-27 01:48:04.809124 Epoch 300, Training Loss 0.5370969532243431\n",
      "2022-03-27 01:48:04.828141 Epoch 300, Training Loss 0.5379598575937169\n",
      "2022-03-27 01:48:04.846145 Epoch 300, Training Loss 0.5391733609806851\n",
      "2022-03-27 01:48:04.866144 Epoch 300, Training Loss 0.5404308179150457\n",
      "2022-03-27 01:48:04.886142 Epoch 300, Training Loss 0.5415660536959958\n",
      "2022-03-27 01:48:04.906143 Epoch 300, Training Loss 0.542630555379726\n",
      "2022-03-27 01:48:04.926157 Epoch 300, Training Loss 0.5438054156730242\n",
      "2022-03-27 01:48:04.945191 Epoch 300, Training Loss 0.5449567511106086\n",
      "2022-03-27 01:48:04.964196 Epoch 300, Training Loss 0.5459961315707478\n",
      "2022-03-27 01:48:04.983827 Epoch 300, Training Loss 0.5471368034934754\n",
      "2022-03-27 01:48:05.003838 Epoch 300, Training Loss 0.5484270548729031\n",
      "2022-03-27 01:48:05.022950 Epoch 300, Training Loss 0.5498011231879749\n",
      "2022-03-27 01:48:05.041957 Epoch 300, Training Loss 0.551030630498286\n",
      "2022-03-27 01:48:05.059961 Epoch 300, Training Loss 0.55220187236281\n",
      "2022-03-27 01:48:05.079960 Epoch 300, Training Loss 0.5534475159919475\n",
      "2022-03-27 01:48:05.098982 Epoch 300, Training Loss 0.5547078297571149\n",
      "2022-03-27 01:48:05.118987 Epoch 300, Training Loss 0.5555774731861661\n",
      "2022-03-27 01:48:05.138981 Epoch 300, Training Loss 0.5567175318365511\n",
      "2022-03-27 01:48:05.157984 Epoch 300, Training Loss 0.557602291750481\n",
      "2022-03-27 01:48:05.176987 Epoch 300, Training Loss 0.5589138422628193\n",
      "2022-03-27 01:48:05.196004 Epoch 300, Training Loss 0.5600600240328123\n",
      "2022-03-27 01:48:05.215008 Epoch 300, Training Loss 0.5612191592945772\n",
      "2022-03-27 01:48:05.234007 Epoch 300, Training Loss 0.5623222737361098\n",
      "2022-03-27 01:48:05.254003 Epoch 300, Training Loss 0.5634728401823117\n",
      "2022-03-27 01:48:05.274016 Epoch 300, Training Loss 0.564653094009975\n",
      "2022-03-27 01:48:05.293014 Epoch 300, Training Loss 0.5657389511537674\n",
      "2022-03-27 01:48:05.312030 Epoch 300, Training Loss 0.566913485069714\n",
      "2022-03-27 01:48:05.331034 Epoch 300, Training Loss 0.56819581726323\n",
      "2022-03-27 01:48:05.350039 Epoch 300, Training Loss 0.5691150344546189\n",
      "2022-03-27 01:48:05.369043 Epoch 300, Training Loss 0.5703187903479847\n",
      "2022-03-27 01:48:05.388049 Epoch 300, Training Loss 0.5715276553960102\n",
      "2022-03-27 01:48:05.407034 Epoch 300, Training Loss 0.5727224202869493\n",
      "2022-03-27 01:48:05.426045 Epoch 300, Training Loss 0.5739223878554371\n",
      "2022-03-27 01:48:05.444060 Epoch 300, Training Loss 0.5750261223529611\n",
      "2022-03-27 01:48:05.464065 Epoch 300, Training Loss 0.5761629089979869\n",
      "2022-03-27 01:48:05.483063 Epoch 300, Training Loss 0.5774238781855844\n",
      "2022-03-27 01:48:05.503074 Epoch 300, Training Loss 0.5787781524231367\n",
      "2022-03-27 01:48:05.522062 Epoch 300, Training Loss 0.5799132060364384\n",
      "2022-03-27 01:48:05.542070 Epoch 300, Training Loss 0.580972036208643\n",
      "2022-03-27 01:48:05.560087 Epoch 300, Training Loss 0.5820613171895752\n",
      "2022-03-27 01:48:05.579085 Epoch 300, Training Loss 0.5833244287906705\n",
      "2022-03-27 01:48:05.599089 Epoch 300, Training Loss 0.5843320187857693\n",
      "2022-03-27 01:48:05.618094 Epoch 300, Training Loss 0.5856787023489433\n",
      "2022-03-27 01:48:05.637098 Epoch 300, Training Loss 0.5868234627539545\n",
      "2022-03-27 01:48:05.664104 Epoch 300, Training Loss 0.5879692016812541\n",
      "2022-03-27 01:48:05.692106 Epoch 300, Training Loss 0.5891840727737797\n",
      "2022-03-27 01:48:05.718116 Epoch 300, Training Loss 0.5904875214752334\n",
      "2022-03-27 01:48:05.744122 Epoch 300, Training Loss 0.5914285174568595\n",
      "2022-03-27 01:48:05.771135 Epoch 300, Training Loss 0.5925837895449471\n",
      "2022-03-27 01:48:05.797140 Epoch 300, Training Loss 0.5936787333482366\n",
      "2022-03-27 01:48:05.824142 Epoch 300, Training Loss 0.5947862843723248\n",
      "2022-03-27 01:48:05.849152 Epoch 300, Training Loss 0.5959758914797507\n",
      "2022-03-27 01:48:05.868150 Epoch 300, Training Loss 0.5970051607207569\n",
      "2022-03-27 01:48:05.887162 Epoch 300, Training Loss 0.5980992230308025\n",
      "2022-03-27 01:48:05.907149 Epoch 300, Training Loss 0.5992803551504374\n",
      "2022-03-27 01:48:05.925157 Epoch 300, Training Loss 0.600372616820933\n",
      "2022-03-27 01:48:05.944168 Epoch 300, Training Loss 0.6016419256282279\n",
      "2022-03-27 01:48:05.962165 Epoch 300, Training Loss 0.6027443282439581\n",
      "2022-03-27 01:48:05.981169 Epoch 300, Training Loss 0.6039549058203197\n",
      "2022-03-27 01:48:06.000186 Epoch 300, Training Loss 0.6048875989968819\n",
      "2022-03-27 01:48:06.020191 Epoch 300, Training Loss 0.6060076947407345\n",
      "2022-03-27 01:48:06.040183 Epoch 300, Training Loss 0.6070222214359762\n",
      "2022-03-27 01:48:06.058194 Epoch 300, Training Loss 0.6080565879412014\n",
      "2022-03-27 01:48:06.076204 Epoch 300, Training Loss 0.6091378195511411\n",
      "2022-03-27 01:48:06.095208 Epoch 300, Training Loss 0.6101567037117755\n",
      "2022-03-27 01:48:06.114206 Epoch 300, Training Loss 0.6113633922756175\n",
      "2022-03-27 01:48:06.133216 Epoch 300, Training Loss 0.6126292218332705\n",
      "2022-03-27 01:48:06.152221 Epoch 300, Training Loss 0.6137093749954877\n",
      "2022-03-27 01:48:06.171914 Epoch 300, Training Loss 0.6147112071209246\n",
      "2022-03-27 01:48:06.190930 Epoch 300, Training Loss 0.6158633626940305\n",
      "2022-03-27 01:48:06.208927 Epoch 300, Training Loss 0.6170658368588714\n",
      "2022-03-27 01:48:06.226938 Epoch 300, Training Loss 0.6181397311522833\n",
      "2022-03-27 01:48:06.246936 Epoch 300, Training Loss 0.6193701115715534\n",
      "2022-03-27 01:48:06.264947 Epoch 300, Training Loss 0.6204520795503845\n",
      "2022-03-27 01:48:06.283957 Epoch 300, Training Loss 0.6214457982793793\n",
      "2022-03-27 01:48:06.302955 Epoch 300, Training Loss 0.6225984785562891\n",
      "2022-03-27 01:48:06.321955 Epoch 300, Training Loss 0.6236103668694606\n",
      "2022-03-27 01:48:06.340957 Epoch 300, Training Loss 0.6247822184239507\n",
      "2022-03-27 01:48:06.360974 Epoch 300, Training Loss 0.6260063390597663\n",
      "2022-03-27 01:48:06.379972 Epoch 300, Training Loss 0.6270234200655652\n",
      "2022-03-27 01:48:06.398983 Epoch 300, Training Loss 0.6279587910303375\n",
      "2022-03-27 01:48:06.416981 Epoch 300, Training Loss 0.6289718215117979\n",
      "2022-03-27 01:48:06.436985 Epoch 300, Training Loss 0.6300970309835565\n",
      "2022-03-27 01:48:06.454990 Epoch 300, Training Loss 0.6310849763700724\n",
      "2022-03-27 01:48:06.473994 Epoch 300, Training Loss 0.632312396679388\n",
      "2022-03-27 01:48:06.494004 Epoch 300, Training Loss 0.6331363849322814\n",
      "2022-03-27 01:48:06.513003 Epoch 300, Training Loss 0.6340855316585287\n",
      "2022-03-27 01:48:06.532016 Epoch 300, Training Loss 0.635379149404633\n",
      "2022-03-27 01:48:06.550017 Epoch 300, Training Loss 0.6364005349023872\n",
      "2022-03-27 01:48:06.570016 Epoch 300, Training Loss 0.6375799132582477\n",
      "2022-03-27 01:48:06.589016 Epoch 300, Training Loss 0.6387509742508763\n",
      "2022-03-27 01:48:06.608025 Epoch 300, Training Loss 0.6399021230237868\n",
      "2022-03-27 01:48:06.626028 Epoch 300, Training Loss 0.6410968298344966\n",
      "2022-03-27 01:48:06.645026 Epoch 300, Training Loss 0.6422722670428284\n",
      "2022-03-27 01:48:06.663524 Epoch 300, Training Loss 0.6434912684628421\n",
      "2022-03-27 01:48:06.682697 Epoch 300, Training Loss 0.6447504652887964\n",
      "2022-03-27 01:48:06.701701 Epoch 300, Training Loss 0.6458911484921984\n",
      "2022-03-27 01:48:06.720850 Epoch 300, Training Loss 0.6470300394403355\n",
      "2022-03-27 01:48:06.740368 Epoch 300, Training Loss 0.6479886975282293\n",
      "2022-03-27 01:48:06.759384 Epoch 300, Training Loss 0.6491139236161166\n",
      "2022-03-27 01:48:06.777375 Epoch 300, Training Loss 0.650131301425607\n",
      "2022-03-27 01:48:06.795393 Epoch 300, Training Loss 0.6511507108998116\n",
      "2022-03-27 01:48:06.815397 Epoch 300, Training Loss 0.652315210350944\n",
      "2022-03-27 01:48:06.835408 Epoch 300, Training Loss 0.6532521997113971\n",
      "2022-03-27 01:48:06.854397 Epoch 300, Training Loss 0.6544707700267167\n",
      "2022-03-27 01:48:06.872404 Epoch 300, Training Loss 0.6556757473579758\n",
      "2022-03-27 01:48:06.891415 Epoch 300, Training Loss 0.6567868716881403\n",
      "2022-03-27 01:48:06.910418 Epoch 300, Training Loss 0.6578764465001538\n",
      "2022-03-27 01:48:06.929431 Epoch 300, Training Loss 0.6593092141096549\n",
      "2022-03-27 01:48:06.948421 Epoch 300, Training Loss 0.6603578204846443\n",
      "2022-03-27 01:48:06.967432 Epoch 300, Training Loss 0.6615325380926547\n",
      "2022-03-27 01:48:06.986431 Epoch 300, Training Loss 0.662712081618931\n",
      "2022-03-27 01:48:07.005437 Epoch 300, Training Loss 0.6640062983078725\n",
      "2022-03-27 01:48:07.024432 Epoch 300, Training Loss 0.6652854607843072\n",
      "2022-03-27 01:48:07.043443 Epoch 300, Training Loss 0.666441380520306\n",
      "2022-03-27 01:48:07.062459 Epoch 300, Training Loss 0.6675355609725503\n",
      "2022-03-27 01:48:07.080457 Epoch 300, Training Loss 0.668871495424939\n",
      "2022-03-27 01:48:07.100732 Epoch 300, Training Loss 0.6702413292187254\n",
      "2022-03-27 01:48:07.119460 Epoch 300, Training Loss 0.6713000392669912\n",
      "2022-03-27 01:48:07.138153 Epoch 300, Training Loss 0.6724412102071221\n",
      "2022-03-27 01:48:07.158171 Epoch 300, Training Loss 0.6734867610437486\n",
      "2022-03-27 01:48:07.178169 Epoch 300, Training Loss 0.6745806655767933\n",
      "2022-03-27 01:48:07.197179 Epoch 300, Training Loss 0.675439435883861\n",
      "2022-03-27 01:48:07.216185 Epoch 300, Training Loss 0.6766504455557869\n",
      "2022-03-27 01:48:07.234187 Epoch 300, Training Loss 0.6780274785540598\n",
      "2022-03-27 01:48:07.255180 Epoch 300, Training Loss 0.679065769971789\n",
      "2022-03-27 01:48:07.273190 Epoch 300, Training Loss 0.6802151638376134\n",
      "2022-03-27 01:48:07.292201 Epoch 300, Training Loss 0.6813421633542346\n",
      "2022-03-27 01:48:07.311205 Epoch 300, Training Loss 0.6826927902753396\n",
      "2022-03-27 01:48:07.330203 Epoch 300, Training Loss 0.6836025669141803\n",
      "2022-03-27 01:48:07.350208 Epoch 300, Training Loss 0.6847614770960015\n",
      "2022-03-27 01:48:07.369218 Epoch 300, Training Loss 0.6858489370102163\n",
      "2022-03-27 01:48:07.388210 Epoch 300, Training Loss 0.6870616406126095\n",
      "2022-03-27 01:48:07.407221 Epoch 300, Training Loss 0.6882943175637813\n",
      "2022-03-27 01:48:07.426231 Epoch 300, Training Loss 0.6894838918200539\n",
      "2022-03-27 01:48:07.445235 Epoch 300, Training Loss 0.6905444564721773\n",
      "2022-03-27 01:48:07.463233 Epoch 300, Training Loss 0.6915402486348701\n",
      "2022-03-27 01:48:07.482729 Epoch 300, Training Loss 0.6923854463088238\n",
      "2022-03-27 01:48:07.501739 Epoch 300, Training Loss 0.6935235496677096\n",
      "2022-03-27 01:48:07.520733 Epoch 300, Training Loss 0.6945792807794898\n",
      "2022-03-27 01:48:07.539729 Epoch 300, Training Loss 0.6955585288422187\n",
      "2022-03-27 01:48:07.557739 Epoch 300, Training Loss 0.69672033167861\n",
      "2022-03-27 01:48:07.577737 Epoch 300, Training Loss 0.6977632507643736\n",
      "2022-03-27 01:48:07.595754 Epoch 300, Training Loss 0.6989491998082231\n",
      "2022-03-27 01:48:07.615752 Epoch 300, Training Loss 0.7002083060076779\n",
      "2022-03-27 01:48:07.633763 Epoch 300, Training Loss 0.7012276153277863\n",
      "2022-03-27 01:48:07.652767 Epoch 300, Training Loss 0.7026543759781382\n",
      "2022-03-27 01:48:07.671765 Epoch 300, Training Loss 0.7039048058145186\n",
      "2022-03-27 01:48:07.690776 Epoch 300, Training Loss 0.7049634873562152\n",
      "2022-03-27 01:48:07.709786 Epoch 300, Training Loss 0.7059938527281632\n",
      "2022-03-27 01:48:07.728784 Epoch 300, Training Loss 0.7071694112037454\n",
      "2022-03-27 01:48:07.748789 Epoch 300, Training Loss 0.7084724227028429\n",
      "2022-03-27 01:48:07.767787 Epoch 300, Training Loss 0.7098086838374662\n",
      "2022-03-27 01:48:07.786804 Epoch 300, Training Loss 0.7109168599481168\n",
      "2022-03-27 01:48:07.806803 Epoch 300, Training Loss 0.7120656653895707\n",
      "2022-03-27 01:48:07.824812 Epoch 300, Training Loss 0.713121925854622\n",
      "2022-03-27 01:48:07.843816 Epoch 300, Training Loss 0.7144773909655373\n",
      "2022-03-27 01:48:07.862815 Epoch 300, Training Loss 0.7155611555442176\n",
      "2022-03-27 01:48:07.882825 Epoch 300, Training Loss 0.7167396492055614\n",
      "2022-03-27 01:48:07.901824 Epoch 300, Training Loss 0.7179526697339305\n",
      "2022-03-27 01:48:07.920834 Epoch 300, Training Loss 0.7189851368937041\n",
      "2022-03-27 01:48:07.938826 Epoch 300, Training Loss 0.7202163135913937\n",
      "2022-03-27 01:48:07.958836 Epoch 300, Training Loss 0.7212530202268029\n",
      "2022-03-27 01:48:07.977834 Epoch 300, Training Loss 0.7223502982912771\n",
      "2022-03-27 01:48:07.997845 Epoch 300, Training Loss 0.723713148890249\n",
      "2022-03-27 01:48:08.016849 Epoch 300, Training Loss 0.7246115734357663\n",
      "2022-03-27 01:48:08.035860 Epoch 300, Training Loss 0.7256314875677113\n",
      "2022-03-27 01:48:08.054852 Epoch 300, Training Loss 0.7268690981371019\n",
      "2022-03-27 01:48:08.073862 Epoch 300, Training Loss 0.7280417361375316\n",
      "2022-03-27 01:48:08.092875 Epoch 300, Training Loss 0.7292848053338278\n",
      "2022-03-27 01:48:08.111877 Epoch 300, Training Loss 0.730461804610689\n",
      "2022-03-27 01:48:08.130493 Epoch 300, Training Loss 0.7317079153969465\n",
      "2022-03-27 01:48:08.149498 Epoch 300, Training Loss 0.7326954177883275\n",
      "2022-03-27 01:48:08.168502 Epoch 300, Training Loss 0.7337606510390406\n",
      "2022-03-27 01:48:08.188506 Epoch 300, Training Loss 0.7349228791111265\n",
      "2022-03-27 01:48:08.207505 Epoch 300, Training Loss 0.7361516334364177\n",
      "2022-03-27 01:48:08.226503 Epoch 300, Training Loss 0.7373963402360296\n",
      "2022-03-27 01:48:08.245519 Epoch 300, Training Loss 0.7386981591849071\n",
      "2022-03-27 01:48:08.265524 Epoch 300, Training Loss 0.7397099913995894\n",
      "2022-03-27 01:48:08.284528 Epoch 300, Training Loss 0.7409218414055417\n",
      "2022-03-27 01:48:08.304534 Epoch 300, Training Loss 0.7420882816662264\n",
      "2022-03-27 01:48:08.323531 Epoch 300, Training Loss 0.7433046969153997\n",
      "2022-03-27 01:48:08.342541 Epoch 300, Training Loss 0.7445276980967168\n",
      "2022-03-27 01:48:08.361545 Epoch 300, Training Loss 0.7457907843925154\n",
      "2022-03-27 01:48:08.380830 Epoch 300, Training Loss 0.7467476979393484\n",
      "2022-03-27 01:48:08.398834 Epoch 300, Training Loss 0.7478165352893302\n",
      "2022-03-27 01:48:08.418839 Epoch 300, Training Loss 0.7489292428773993\n",
      "2022-03-27 01:48:08.439842 Epoch 300, Training Loss 0.750042926167588\n",
      "2022-03-27 01:48:08.457848 Epoch 300, Training Loss 0.7508763096216694\n",
      "2022-03-27 01:48:08.476852 Epoch 300, Training Loss 0.7517795553597648\n",
      "2022-03-27 01:48:08.496856 Epoch 300, Training Loss 0.7528494947096881\n",
      "2022-03-27 01:48:08.515860 Epoch 300, Training Loss 0.7539857248668476\n",
      "2022-03-27 01:48:08.534865 Epoch 300, Training Loss 0.755308374953087\n",
      "2022-03-27 01:48:08.553849 Epoch 300, Training Loss 0.7563614036573474\n",
      "2022-03-27 01:48:08.572867 Epoch 300, Training Loss 0.7575774650134699\n",
      "2022-03-27 01:48:08.591878 Epoch 300, Training Loss 0.7587831253590791\n",
      "2022-03-27 01:48:08.610882 Epoch 300, Training Loss 0.7601473643956587\n",
      "2022-03-27 01:48:08.630881 Epoch 300, Training Loss 0.761292142255227\n",
      "2022-03-27 01:48:08.650885 Epoch 300, Training Loss 0.7624661239516705\n",
      "2022-03-27 01:48:08.672906 Epoch 300, Training Loss 0.7637244873796888\n",
      "2022-03-27 01:48:08.699918 Epoch 300, Training Loss 0.7647457503144394\n",
      "2022-03-27 01:48:08.726924 Epoch 300, Training Loss 0.7661309192705033\n",
      "2022-03-27 01:48:08.753932 Epoch 300, Training Loss 0.7672297459124299\n",
      "2022-03-27 01:48:08.779725 Epoch 300, Training Loss 0.7684086210587445\n",
      "2022-03-27 01:48:08.806731 Epoch 300, Training Loss 0.7694727295957258\n",
      "2022-03-27 01:48:08.833737 Epoch 300, Training Loss 0.7708660230764648\n",
      "2022-03-27 01:48:08.860737 Epoch 300, Training Loss 0.7722558410423795\n",
      "2022-03-27 01:48:08.876736 Epoch 300, Training Loss 0.773847826377815\n",
      "2022-03-27 01:48:08.891738 Epoch 300, Training Loss 0.7752759222636747\n",
      "2022-03-27 01:48:08.906741 Epoch 300, Training Loss 0.776202209160456\n",
      "2022-03-27 01:48:08.921745 Epoch 300, Training Loss 0.777123381643344\n",
      "2022-03-27 01:48:08.935749 Epoch 300, Training Loss 0.7780873067391193\n",
      "2022-03-27 01:48:08.950753 Epoch 300, Training Loss 0.7794185876084105\n",
      "2022-03-27 01:48:08.965755 Epoch 300, Training Loss 0.7805748475939417\n",
      "2022-03-27 01:48:08.980759 Epoch 300, Training Loss 0.7816103179284069\n",
      "2022-03-27 01:48:08.995762 Epoch 300, Training Loss 0.7826758019454644\n",
      "2022-03-27 01:48:09.010766 Epoch 300, Training Loss 0.7838869620771969\n",
      "2022-03-27 01:48:09.025769 Epoch 300, Training Loss 0.7850014285358322\n",
      "2022-03-27 01:48:09.041778 Epoch 300, Training Loss 0.7860534534125072\n",
      "2022-03-27 01:48:09.056777 Epoch 300, Training Loss 0.7870925106965673\n",
      "2022-03-27 01:48:09.071781 Epoch 300, Training Loss 0.7879895022153245\n",
      "2022-03-27 01:48:09.086785 Epoch 300, Training Loss 0.7891700981980394\n",
      "2022-03-27 01:48:09.101785 Epoch 300, Training Loss 0.7903710019862865\n",
      "2022-03-27 01:48:09.116789 Epoch 300, Training Loss 0.7914984410681078\n",
      "2022-03-27 01:48:09.131321 Epoch 300, Training Loss 0.7927050672833572\n",
      "2022-03-27 01:48:09.146324 Epoch 300, Training Loss 0.7938675556493842\n",
      "2022-03-27 01:48:09.161327 Epoch 300, Training Loss 0.7951473938991956\n",
      "2022-03-27 01:48:09.176330 Epoch 300, Training Loss 0.7963457725694417\n",
      "2022-03-27 01:48:09.192334 Epoch 300, Training Loss 0.7975665113657636\n",
      "2022-03-27 01:48:09.207337 Epoch 300, Training Loss 0.7987324628226288\n",
      "2022-03-27 01:48:09.222341 Epoch 300, Training Loss 0.7999270813696829\n",
      "2022-03-27 01:48:09.237344 Epoch 300, Training Loss 0.8012479299017231\n",
      "2022-03-27 01:48:09.252348 Epoch 300, Training Loss 0.8023621548929483\n",
      "2022-03-27 01:48:09.267353 Epoch 300, Training Loss 0.8036724201706059\n",
      "2022-03-27 01:48:09.282840 Epoch 300, Training Loss 0.804962304165906\n",
      "2022-03-27 01:48:09.297843 Epoch 300, Training Loss 0.8060695567094457\n",
      "2022-03-27 01:48:09.312847 Epoch 300, Training Loss 0.8072277666510218\n",
      "2022-03-27 01:48:09.327849 Epoch 300, Training Loss 0.8082111781210546\n",
      "2022-03-27 01:48:09.342853 Epoch 300, Training Loss 0.8095413899177786\n",
      "2022-03-27 01:48:09.357864 Epoch 300, Training Loss 0.810577009187635\n",
      "2022-03-27 01:48:09.372862 Epoch 300, Training Loss 0.8117506655738177\n",
      "2022-03-27 01:48:09.387865 Epoch 300, Training Loss 0.8129473600698554\n",
      "2022-03-27 01:48:09.402873 Epoch 300, Training Loss 0.8139900369260012\n",
      "2022-03-27 01:48:09.417870 Epoch 300, Training Loss 0.8153404784781854\n",
      "2022-03-27 01:48:09.433024 Epoch 300, Training Loss 0.8164106059409774\n",
      "2022-03-27 01:48:09.448027 Epoch 300, Training Loss 0.8173890536855859\n",
      "2022-03-27 01:48:09.463031 Epoch 300, Training Loss 0.818339727037703\n",
      "2022-03-27 01:48:09.478034 Epoch 300, Training Loss 0.8197251722178496\n",
      "2022-03-27 01:48:09.493039 Epoch 300, Training Loss 0.8209490846185123\n",
      "2022-03-27 01:48:09.508041 Epoch 300, Training Loss 0.8219898553455577\n",
      "2022-03-27 01:48:09.523231 Epoch 300, Training Loss 0.8232383731076175\n",
      "2022-03-27 01:48:09.538228 Epoch 300, Training Loss 0.8243268184802112\n",
      "2022-03-27 01:48:09.553231 Epoch 300, Training Loss 0.8256453991393604\n",
      "2022-03-27 01:48:09.568234 Epoch 300, Training Loss 0.8269309667522645\n",
      "2022-03-27 01:48:09.583238 Epoch 300, Training Loss 0.8282617869432015\n",
      "2022-03-27 01:48:09.597242 Epoch 300, Training Loss 0.8294788858927119\n",
      "2022-03-27 01:48:09.612245 Epoch 300, Training Loss 0.830653030854052\n",
      "2022-03-27 01:48:09.627248 Epoch 300, Training Loss 0.8317602389608808\n",
      "2022-03-27 01:48:09.643252 Epoch 300, Training Loss 0.8330002767807992\n",
      "2022-03-27 01:48:09.657256 Epoch 300, Training Loss 0.834040628293591\n",
      "2022-03-27 01:48:09.673260 Epoch 300, Training Loss 0.8356304542945169\n",
      "2022-03-27 01:48:09.688255 Epoch 300, Training Loss 0.8366087909092379\n",
      "2022-03-27 01:48:09.703265 Epoch 300, Training Loss 0.837570367871648\n",
      "2022-03-27 01:48:09.718270 Epoch 300, Training Loss 0.8387753506145819\n",
      "2022-03-27 01:48:09.735273 Epoch 300, Training Loss 0.8400534075086988\n",
      "2022-03-27 01:48:09.750276 Epoch 300, Training Loss 0.8412997340759658\n",
      "2022-03-27 01:48:09.765279 Epoch 300, Training Loss 0.8425196565478049\n",
      "2022-03-27 01:48:09.780290 Epoch 300, Training Loss 0.843542574006883\n",
      "2022-03-27 01:48:09.795286 Epoch 300, Training Loss 0.8445923679015216\n",
      "2022-03-27 01:48:09.810282 Epoch 300, Training Loss 0.8457247755868965\n",
      "2022-03-27 01:48:09.825767 Epoch 300, Training Loss 0.8469289508469574\n",
      "2022-03-27 01:48:09.840771 Epoch 300, Training Loss 0.8479160092523336\n",
      "2022-03-27 01:48:09.854775 Epoch 300, Training Loss 0.8494245246090852\n",
      "2022-03-27 01:48:09.870770 Epoch 300, Training Loss 0.8506854125453384\n",
      "2022-03-27 01:48:09.885781 Epoch 300, Training Loss 0.8519206373283016\n",
      "2022-03-27 01:48:09.899785 Epoch 300, Training Loss 0.8529288552301314\n",
      "2022-03-27 01:48:09.915781 Epoch 300, Training Loss 0.8540897675030067\n",
      "2022-03-27 01:48:09.930794 Epoch 300, Training Loss 0.8552370508156164\n",
      "2022-03-27 01:48:09.945796 Epoch 300, Training Loss 0.8563123461230636\n",
      "2022-03-27 01:48:09.960800 Epoch 300, Training Loss 0.8572619345486926\n",
      "2022-03-27 01:48:09.975803 Epoch 300, Training Loss 0.8583529852997617\n",
      "2022-03-27 01:48:09.990806 Epoch 300, Training Loss 0.8594668664590782\n",
      "2022-03-27 01:48:10.004809 Epoch 300, Training Loss 0.8604809630404958\n",
      "2022-03-27 01:48:10.019813 Epoch 300, Training Loss 0.8615879722873269\n",
      "2022-03-27 01:48:10.035815 Epoch 300, Training Loss 0.8626684858975813\n",
      "2022-03-27 01:48:10.050819 Epoch 300, Training Loss 0.8638609903852653\n",
      "2022-03-27 01:48:10.065823 Epoch 300, Training Loss 0.8650327460540225\n",
      "2022-03-27 01:48:10.080826 Epoch 300, Training Loss 0.8660737689193863\n",
      "2022-03-27 01:48:10.096829 Epoch 300, Training Loss 0.8671823559362261\n",
      "2022-03-27 01:48:10.112833 Epoch 300, Training Loss 0.8681582972369231\n",
      "2022-03-27 01:48:10.127837 Epoch 300, Training Loss 0.8692726995938879\n",
      "2022-03-27 01:48:10.142840 Epoch 300, Training Loss 0.8702973907865832\n",
      "2022-03-27 01:48:10.157843 Epoch 300, Training Loss 0.8713860002625019\n",
      "2022-03-27 01:48:10.172847 Epoch 300, Training Loss 0.8726093903984256\n",
      "2022-03-27 01:48:10.187851 Epoch 300, Training Loss 0.8740265996712248\n",
      "2022-03-27 01:48:10.202853 Epoch 300, Training Loss 0.8749913281339514\n",
      "2022-03-27 01:48:10.217856 Epoch 300, Training Loss 0.8761455223840826\n",
      "2022-03-27 01:48:10.232861 Epoch 300, Training Loss 0.8773557430947833\n",
      "2022-03-27 01:48:10.248864 Epoch 300, Training Loss 0.8784618542322418\n",
      "2022-03-27 01:48:10.263867 Epoch 300, Training Loss 0.8795629514910072\n",
      "2022-03-27 01:48:10.279872 Epoch 300, Training Loss 0.8806255419388451\n",
      "2022-03-27 01:48:10.295874 Epoch 300, Training Loss 0.8816785820000007\n",
      "2022-03-27 01:48:10.310878 Epoch 300, Training Loss 0.8827606910634833\n",
      "2022-03-27 01:48:10.324882 Epoch 300, Training Loss 0.8839188942976315\n",
      "2022-03-27 01:48:10.339885 Epoch 300, Training Loss 0.884881269215318\n",
      "2022-03-27 01:48:10.353889 Epoch 300, Training Loss 0.8863175957251692\n",
      "2022-03-27 01:48:10.369884 Epoch 300, Training Loss 0.8875642289286074\n",
      "2022-03-27 01:48:10.384897 Epoch 300, Training Loss 0.888926501164351\n",
      "2022-03-27 01:48:10.399899 Epoch 300, Training Loss 0.8902772924174434\n",
      "2022-03-27 01:48:10.414901 Epoch 300, Training Loss 0.8917642973573007\n",
      "2022-03-27 01:48:10.429905 Epoch 300, Training Loss 0.893017006194805\n",
      "2022-03-27 01:48:10.444908 Epoch 300, Training Loss 0.8939971609798538\n",
      "2022-03-27 01:48:10.451902 Epoch 300, Training Loss 0.8950015129640584\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jPvGdr5kqIac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.76\n",
      "Accuracy val: 0.68\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKT6Q55W9CdBSnXTIQ7hwC",
   "name": "RealTime HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
